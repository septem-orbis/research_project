@article{Cabral2024,
  title = {Open Information Extraction with LLM for the Portuguese Language},
  author = {Cabral, Bruno and Souza, Marlo and Claro, Daniela Barreiro},
  year = {2024},
  journal = {LINGUAMATICA},
  volume = {16},
  pages = {167-182},
  doi = {10.21814/lm.16.2.454},
  abstract = {In this study, we investigate the application of Large Language Models (LLMs) for Open Information Extraction (OpenIE) in the Portuguese language. While most OpenIE methods have been developed with a focus on the English language, few works in the literature explore multilingual and cross-linguistic scenarios. Although there is a growing interest in OpenIE methods for Portuguese, the use of LLMs specifically focused on OpenIE in this language remains underexplored. We analyze the feasibility of incorporating both open and commercial LLMs using few- shot prompt engineering for OpenIE in Portuguese. We provide a detailed analysis of the performance of these LLMs in OpenIE tasks, demonstrating that they achieve performance metrics comparable to state-ofthe-art systems. Additionally, we refine and release an open LLM for OpenIE, named PortOIE-Llama, which outperforms commercial LLMs in our experiments. Our results highlight the potential of LLMs in OpenIE tasks in Portuguese and suggest that further refinement and fine-tuning of larger models can further enhance these outcomes.},
}

@book{Guo2024,
  title = {Emerging developments and technologies in digital government},
  author = {Guo, Yuanyuan},
  year = {2024},
  journal = {Emerging Developments and Technologies in Digital Government},
  pages = {1 – 423},
  doi = {10.4018/9798369323632},
  publisher = {IGI Global},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193059172&doi=10.4018%2f9798369323632&partnerID=40&md5=b63aeb0ecee6ac9610c4d8f175d67046},
  abstract = {As the digital government field continues to evolve rapidly, scholars and professionals must stay ahead of the curve by developing innovative solutions and gaining comprehensive insights. The global landscape of digital governance is undergoing transformative shifts, necessitating a deep understanding of historical developments, current practices, and emerging trends. This urgent demand for knowledge forms the crux of the problem that the book, Emerging Developments and Technologies in Digital Government, addresses with expert knowledge and insights. The book serves as an indispensable resource for academic scholars grappling with the complexities of digital government. It critically examines historical transitions from technology-centric paradigms to people-centric models, shedding light on the global impact of open data initiatives and the vital role of human-computer interaction in reshaping government websites. For professionals and researchers across disciplines such as library sciences, administrative management, sociology, and information technology, this book becomes a beacon, offering insights and tangible solutions to navigate the multifaceted dimensions of digital government. Structured as a comprehensive guide, the book integrates topics ranging from the theoretical innovation in digital government to the transformative influence of generative AI and the Metaverse. By tackling global challenges, strategies, and comparative analyses of digital government practices, it provides a roadmap for navigating the intricate landscape of digital governance. As scholars and executives seek a forward-thinking compass to navigate the complexities of the digital age, Emerging Developments and Technologies in Digital Government emerge as the definitive solution, offering a panoramic view of the field and equipping readers to shape the future of digital governance. © 2024 by IGI Global. All rights reserved.},
}

@article{Donner2024,
  title = {TRUExT: Trustworthiness Regressor Unified Explainable Tool},
  author = {Donner, Catherine and Danala, Gopichandh and Jentner, Wolfgang and Ebert, David},
  year = {2024},
  pages = {5325-5334},
  doi = {10.1109/BigData62323.2024.10825100},
  abstract = {One of the most pressing public policy issues that has involved transdisciplinary research in the field of data science is rapidly detecting widespread misinformation. While data science can pose a lot of potential for solving the big-data problem of misinformation on an automated scale, it likewise requires insights from the field of communications and journalism to define quantifiable features that can assist in more accurate misinformation predictions. Currently, the preeminent tools used for misinformation detection are large language models (LLMs) as they are renowned for their ability to capture the context and meaning of textual data. However, despite advancements in developing effective data science models and tools for identifying misinformation, there are not many available options for evaluating news article content for misinformation potential. This study proposes TRUExT, an explainable, regression-based data tool that integrates multiple communication-based natural language processing (NLP) dimensions with a base LLM to holistically evaluate trustworthiness in news articles. It was found that the Hugging Face LLM RoBERTa with the added NLP dimensions as features was the most effective foundational model after testing multiple LLMs. Furthermore, TRUExT introduced a potential big-data solution to the growing problem of misinformation through research intersecting data science and communications to capture not only the technicality of misinformation data predictions but also certain communication factors in the data. In the future, this tool could likewise be deployed to be used by U.S.-based stakeholders who have an important role in the ongoing information war.},
}

@article{Jamil2024,
  title = {Smart Science Needs Linked Open Data with a Dash of Large Language Models and Extended Relations},
  author = {Jamil, Hasan M.},
  year = {2024},
  doi = {10.1145/3663742.3663971},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3663742.3663971},
  abstract = {Quality scientific inquiries depend on access to data distributed over the entire globe. Linked open data (LOD) and FAIRness play major roles in ensuring access to data that scientists need to answer interesting questions. However, a data model and a query language to compute responses to complex scientific inquiries remain outstanding. As the recent emergence of large language models (LLM) reshape how we interact with machines, an intriguing prospect of posing scientific inquiries to smart machines suddenly appears realizable in which a natural language ChatBot is empowered with a LOD knowledgebase as its data source. In this paper, we introduce a model for an LLM interpreter, called ProAb, that aims to answer natural language scientific queries using a structured query language called Needle in which the LOD is viewed as a set of tables. We discuss the contours of ProAb, present its preliminary and experimental design, and highlight its salient features using an illustrative example. It should be apparent that a full automation of ProAb is feasible with further research.},
}

@article{Balan2023,
  title = {DECORAIT - DECentralized Opt-in/out Registry for AI Training},
  author = {Balan, Kar and Gilbert, Andrew and Black, Alexander and Jenni, Simon and Parsons, Andy and Collomosse, John},
  year = {2023},
  doi = {10.1145/3626495.3626506},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3626495.3626506},
  abstract = {We present DECORAIT; a decentralized registry through which content creators may assert their right to opt in or out of AI training and receive rewards for their contributions. Generative AI (GenAI) enables images to be synthesized using AI models trained on vast amounts of data scraped from public sources. Model and content creators who may wish to share their work openly without sanctioning its use for training are thus presented with a data governance challenge. Further, establishing the provenance of GenAI training data is important to creatives to ensure fair recognition and reward for their such use. We report a prototype of DECORAIT, which explores hierarchical clustering and a combination of on/off-chain storage to create a scalable decentralized registry to trace the provenance of GenAI training data to determine training consent and reward creatives who contribute that data. DECORAIT combines distributed ledger technology (DLT) with visual fingerprinting, leveraging the emerging C2PA (Coalition for Content Provenance and Authenticity) standard to create a secure, open registry through which creatives may express consent and data ownership for GenAI.},
}

@article{Kiesler2024,
  title = {Where's the Data? Finding and Reusing Datasets in Computing Education},
  author = {Kiesler, Natalie and Impagliazzo, John and Biernacka, Katarzyna and Kapoor, Amanpreet and Kazmi, Zain and Ramagoni, Sujeeth Goud and Sane, Aamod and Tran, Keith and Taneja, Shubbhi and Wu, Zihan},
  year = {2024},
  pages = {31–60},
  doi = {10.1145/3598579.3689378},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3598579.3689378},
  abstract = {Computing education research (CER) is a rapidly advancing discipline, offering vast potential for data-driven, secondary research or replication studies. Although gathering and analyzing data for research seem straightforward, making research data publicly available to the community remains a challenge. Likewise, finding and reusing high-quality, prominent, and well-documented research data proves to be a daunting task. In this working group paper, the authors present their search for available datasets in the CER context (e.g., in databases and repositories). The available datasets are further analyzed using a newly developed metadata scheme and presented to the community as a resource. The second component of this work is a summary of the community's perspective and concerns on publishing their research data, which has been gathered through a survey among 52 computing education researchers. Based on this status quo, this report presents recommendations for measures and future steps for the community to become more accessible and establish open data practices. We thus emphasize the potential of making research data available to enhance productivity, transparency, and reproducibility in the CER community.},
}

@article{Chen2024,
  title = {Optimization Study of Higher Education Data Governance in the Era of AI},
  author = {Chen, Jiayan and Chen, Yuehao and Cheng, Xi and Liu, Jiayi and Wang, Yijun and Li, Yiqun},
  year = {2024},
  pages = {708–712},
  doi = {10.1145/3660043.3660169},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3660043.3660169},
  abstract = {Artificial intelligence technology has opened up new horizons for higher education, but it has also increased the scale and complexity of higher education data, posing numerous challenges for data governance. Firstly, the issue of data dependency not only affects the data itself but also has a reciprocal effect on technological development. Secondly, information silos hinder effective data sharing and integration between different departments or institutions. Additionally, security issues of privacy require finding a balance in open data sharing. Based on three major issues, this article aims to provide a reasonable evaluation and improvement scheme for educational data governance in the era of AI. Based at the principles of safety, effectiveness, and openness, a Dynamic Evaluation Model for Higher Education Data Governance in the era of AI is established, relying on seven evaluation indicators and three stages of dynamic changes. This model will analyze the current situation of higher education in China and propose optimization recommendations.},
}

@article{Khanshan2024,
  title = {Evaluation of Code Generation for Simulating Participant Behavior in Experience Sampling Method by Iterative In-Context Learning of a Large Language Model},
  author = {Khanshan, Alireza and Van Gorp, Pieter and Markopoulos, Panos},
  year = {2024},
  journal = {Proc. ACM Hum.-Comput. Interact.},
  volume = {8},
  doi = {10.1145/3661143},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3661143},
  abstract = {The Experience Sampling Method (ESM) is commonly used to understand behaviors, thoughts, and feelings in the wild by collecting self-reports. Sustaining sufficient response rates, especially in long-running studies remains challenging. To avoid low response rates and dropouts, experimenters rely on their experience, proposed methodologies from earlier studies, trial and error, or the scarcely available participant behavior data from previous ESM protocols. This approach often fails in finding the acceptable study parameters, resulting in redesigning the protocol and repeating the experiment. Research has shown the potential of machine learning to personalize ESM protocols such that ESM prompts are delivered at opportune moments, leading to higher response rates. The corresponding training process is hindered due to the scarcity of open data in the ESM domain, causing a cold start, which could be mitigated by simulating participant behavior. Such simulations provide training data and insights for the experimenters to update their study design choices. Creating this simulation requires behavioral science, psychology, and programming expertise. Large language models (LLMs) have emerged as facilitators for information inquiry and programming, albeit random and occasionally unreliable. We aspire to assess the readiness of LLMs in an ESM use case. We conducted research using GPT-3.5 turbo-16k to tackle an ESM simulation problem. We explored several prompt design alternatives to generate ESM simulation programs, evaluated the output code in terms of semantics and syntax, and interviewed ESM practitioners. We found that engineering LLM-enabled ESM simulations have the potential to facilitate data generation, but they perpetuate trust and reliability challenges.},
}

@article{Thalhath2025,
  title = {Metadata application proﬁle as a mechanism for semantic interoperability in FAIR and open data publishing},
  author = {Nishad Thalhath and Mitsuharu Nagamori and Tetsuo Sakaguchi},
  year = {2025},
  journal = {Data and Information Management},
  volume = {9},
  pages = {100068},
  doi = {https://doi.org/10.1016/j.dim.2024.100068},
  url = {https://www.sciencedirect.com/science/article/pii/S2543925124000044},
  abstract = {Application profiles, also known as metadata application profiles, are customised collections of vocabularies adapted from various namespaces and tailored for specific local applications. These profiles act as constrainers and explainers for the (meta)data. Semantic interoperability is the ability of computer systems to exchange data in a mutually understandable manner, facilitating data sharing across diverse platforms and applications without compromising its meaning. As a critical component of semantic interoperability, application profiles enforce semantics to (meta)data, enhancing its openness, interoperability, and reusability. This study assesses the feasibility of representing a comprehensive application profile in a format aligned with the semantic web, ensuring interoperability between profiles and datasets. Dublin Core Description Set Profiles (DSP) is adapted as the modeling framework for metadata application profiles, steering the associated datasets toward RDF compliance. The research outcomes include “Yet Another Metadata Application Profiles” (YAMA) as a preprocessor grounded in the DSP framework for developing and managing metadata application profiles. YAMA facilitates the generation of various standard formats of application profiles, ensuring they are represented in human-readable documentation, machine-actionable forms, and even data validation languages. A data mapping extension to YAMA is proposed to ensure the semantic interoperability of open data, bridging non-RDF data structures to RDF, thus enabling the publication of 5-star open data. This ensures smooth dataset integration and the creation of linkable, semantically rich open datasets. The work emphasizes the pivotal role of application profiles in fortifying the semantic interoperability of (meta)data, thereby elevating dataset openness.},
}

@article{Sandoval-Almazan2024,
  title = {Examining public managers' competencies of artificial intelligence implementation in local government: A quantitative study},
  author = {Rodrigo Sandoval-Almazan and Adrian Osiel Millan-Vargas and Rigoberto Garcia-Contreras},
  year = {2024},
  journal = {Government Information Quarterly},
  volume = {41},
  pages = {101986},
  doi = {https://doi.org/10.1016/j.giq.2024.101986},
  url = {https://www.sciencedirect.com/science/article/pii/S0740624X24000789},
  abstract = {The implementation of artificial intelligence in the public sector is a fast-evolving tendency in recent years. Despite much research on AI in government- ethics, algorithms, chatbots, AI systems-implement- there is very little data and understanding of the public manager's perception, adaptation, challenges, and resistance on this topic. What are the skills and knowledge needed to implement AI in the government? This research aims to investigate public managers' competencies to face AI challenges in the public sector. A survey was conducted among 38 key public managers from the government of the State of Mexico in the central region to assess their perceptions of AI. Using the competences for civil servants' framework from Balbo di Vinadio et al. (2022), we analyze three competences: (1) Digital Management and Execution (2) Digital Planning and Design (3) Data use and governance and their levels of. The findings point out that there is a lack of skills, and the competence of digital management and execution is the one that explains better this perception of AI in the local government.},
}

@article{Fejzic2025,
  title = {Stakeholder engagement for co-designing European climate and energy research priorities},
  author = {Emir Fejzic and Will Usher},
  year = {2025},
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {215},
  pages = {115574},
  doi = {https://doi.org/10.1016/j.rser.2025.115574},
  url = {https://www.sciencedirect.com/science/article/pii/S1364032125002473},
  abstract = {Achieving the Sustainable Development Goals (SDGs) constitutes a global commitment that necessitates the development of innovative strategies to integrate research, policy, and practice effectively. In the European Union (EU), multi-stakeholder engagement has become a vital strategy for tackling complex climate and energy research challenges. This approach is crucial to establishing research priorities that effectively address SDGs 7 and 13. Despite its recognized importance, the existing literature offers no comprehensive overview and guidance on effective multi-stakeholder engagement in EU-funded climate and energy research. This study shows that a scoping review, combined with stakeholder co-design workshops, can reveal key gaps and inform guidelines for robust multi-stakeholder engagement. A systematic review of 23 published articles using criteria drawn from the broader stakeholder engagement literature found that engagement terminology is rarely defined and often used interchangeably, indicating a gap between the literature and its real-world application. This study also provides guidelines for conducting effective stakeholder engagement, drawing upon the broader stakeholder engagement literature, the outcomes of the scoping review, and lessons learned during the European Climate and Energy Modelling forum project. Three co-design workshops engaging 85 stakeholders conducted in 2021 and 2022 uncovered 83 research priorities centred on policy, regulation, and using energy and climate models to inform policymaking. These research priorities are provided as an open data set. The findings of the study underscore the need for standardized engagement practices to enhance the impact of EU-funded climate and energy research and guide future policy and research initiatives.},
}

@article{Gilmour2025,
  title = {Registers of beneficial owners based on blockchain technology: Implications for the accounting profession},
  author = {Paul Gilmour and Durgesh Pandey and Doron Goldbarsht},
  year = {2025},
  journal = {Technological Forecasting and Social Change},
  volume = {214},
  pages = {124051},
  doi = {https://doi.org/10.1016/j.techfore.2025.124051},
  url = {https://www.sciencedirect.com/science/article/pii/S0040162525000824},
  abstract = {Central registers of beneficial owners are fraught with legal loopholes, and trust, privacy, and verification issues that devalue accountants' anti-money laundering compliance efforts. Yet, central registers have become important policy tools for governments in enhancing corporate transparency. We propose a blockchain-based solution that provides a more open, verifiable, and secure framework for registering beneficial ownership information, while ensuring greater transparency and trust. This paper examines the role of blockchain technology within registers of beneficial owners and highlights important implications for the accounting sector. We offer a fresh perspective into how blockchain technology supports company disclosure and contribute new insights into research on ‘digital trust’. This paper bridges the gap between the conceptual and real-world implementation of registers of beneficial owners and promotes a more nuanced understanding on how new digital infrastructures impact accountants' compliance responsibilities.},
}

@article{Shimpo2024,
  title = {Community garden management for resilient cities: A case study in suburban Tokyo during the COVID-19 pandemic},
  author = {Naomi Shimpo},
  year = {2024},
  journal = {Landscape and Urban Planning},
  volume = {251},
  pages = {105148},
  doi = {https://doi.org/10.1016/j.landurbplan.2024.105148},
  url = {https://www.sciencedirect.com/science/article/pii/S0169204624001476},
  abstract = {Numerous studies underscore the role of community gardens in sustaining food security, physical and mental health, and social networks during the COVID-19 pandemic, contributing to community resilience in different contexts of each country. Despite the rich history of urban gardening, Japan remains a geographical gap. This study conducted a mixed-method case study in suburban Tokyo and addresses the unique response of community gardeners to the pandemic within the Japanese context. The survey revealed that gardeners proactively established rules to navigate the crisis quickly through discussion and sustained their gardening activities. The findings also showed that their continuing activities helped the gardeners maintaining physical and mental health, and notably keeping their ikigai, sense of purpose in life in the unusual days. Thus, this study provided new evidence that community gardens may contribute to urban resilience, which indicates the significance of incorporating them into urban green space planning as a preparatory measure for future social crises.},
}

@article{Kim2025,
  title = {The unseen carbon: Scope 3 emissions transform understanding of electricity generation in import-dependent nations},
  author = {Seokju Kim and Sanghyuk Koh and Boreum Lee},
  year = {2025},
  journal = {iScience},
  volume = {28},
  pages = {111725},
  doi = {https://doi.org/10.1016/j.isci.2024.111725},
  url = {https://www.sciencedirect.com/science/article/pii/S2589004224029523},
  abstract = {Summary Energy system decarbonization requires accurate emissions accounting. While many import-dependent nations rely on foreign fuels, infrastructure, and technologies for electricity generation, their emissions calculations often overlook critical supply chain and life cycle impacts. This study applies life cycle assessment to evaluate the comprehensive carbon footprint of electricity production in import-dependent economies, encompassing direct operations (Scope 1), energy use (Scope 2), and supply chain emissions (Scope 3). Analysis reveals that conventional accounting methods significantly underestimate environmental impacts by excluding upstream and downstream emissions. This expanded assessment framework provides policymakers and investors with more accurate data for evaluating decarbonization strategies. The findings demonstrate the importance of incorporating complete supply chain emissions into national inventories to effectively guide energy transition policies and investment decisions toward genuine carbon reduction goals.},
}

@article{Wang2024,
  title = {Dye4AI: Assuring Data Boundary on Generative AI Services},
  author = {Wang, Shu and Sun, Kun and Zhai, Yan},
  year = {2024},
  pages = {2281–2295},
  doi = {10.1145/3658644.3670299},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3658644.3670299},
  abstract = {Generative artificial intelligence (AI) is versatile for various applications, but security and privacy concerns with third-party AI vendors hinder its broader adoption in sensitive scenarios. Hence, it is essential for users to validate the AI trustworthiness and ensure the security of data boundaries. In this paper, we present a dye testing system named Dye4AI, which injects crafted trigger data into human-AI dialogue and observes AI responses towards specific prompts to diagnose data flow in AI model evolution. Our dye testing procedure contains 3 stages: trigger generation, trigger insertion, and trigger retrieval. First, to retain both uniqueness and stealthiness, we design a new trigger that transforms a pseudo-random number to a intelligible format. Second, with a custom-designed three-step conversation strategy, we insert each trigger item into dialogue and confirm the model memorizes the new trigger knowledge in the current session. Finally, we routinely try to recover triggers with specific prompts in new sessions, as triggers can present in new sessions only if AI vendors leverage user data for model fine-tuning. Extensive experiments on six LLMs demonstrate our dye testing scheme is effective in ensuring the data boundary, even for models with various architectures and parameter sizes. Also, larger and premier models tend to be more suitable for Dye4AI, e.g., trigger can be retrieved in OpenLLaMa-13B even with only 2 insertions per trigger item. Moreover, we analyze the prompt selection in dye testing, providing insights for future testing systems on generative AI services.},
}

@article{Liesenfeld2024,
  title = {Rethinking open source generative AI: open-washing and the EU AI Act},
  author = {Liesenfeld, Andreas and Dingemanse, Mark},
  year = {2024},
  pages = {1774–1787},
  doi = {10.1145/3630106.3659005},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3630106.3659005},
  abstract = {The past year has seen a steep rise in generative AI systems that claim to be open. But how open are they really? The question of what counts as open source in generative AI is poised to take on particular importance in light of the upcoming EU AI Act that regulates open source systems differently, creating an urgent need for practical openness assessment. Here we use an evidence-based framework that distinguishes 14 dimensions of openness, from training datasets to scientific and technical documentation and from licensing to access methods. Surveying over 45 generative AI systems (both text and text-to-image), we find that while the term open source is widely used, many models are ‘open weight’ at best and many providers seek to evade scientific, legal and regulatory scrutiny by withholding information on training and fine-tuning data. We argue that openness in generative AI is necessarily composite (consisting of multiple elements) and gradient (coming in degrees), and point out the risk of relying on single features like access or licensing to declare models open or not. Evidence-based openness assessment can help foster a generative AI landscape in which models can be effectively regulated, model providers can be held accountable, scientists can scrutinise generative AI, and end users can make informed decisions.},
}

@article{Sun2022,
  title = {Investigating Explainability of Generative AI for Code through Scenario-based Design},
  author = {Sun, Jiao and Liao, Q. Vera and Muller, Michael and Agarwal, Mayank and Houde, Stephanie and Talamadupula, Kartik and Weisz, Justin D.},
  year = {2022},
  pages = {212–228},
  doi = {10.1145/3490099.3511119},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3490099.3511119},
  abstract = {What does it mean for a generative AI model to be explainable? The emergent discipline of explainable AI (XAI) has made great strides in helping people understand discriminative models. Less attention has been paid to generative models that produce artifacts, rather than decisions, as output. Meanwhile, generative AI (GenAI) technologies are maturing and being applied to application domains such as software engineering. Using scenario-based design and question-driven XAI design approaches, we explore users’ explainability needs for GenAI in three software engineering use cases: natural language to code, code translation, and code auto-completion. We conducted 9 workshops with 43 software engineers in which real examples from state-of-the-art generative AI models were used to elicit users’ explainability needs. Drawing from prior work, we also propose 4 types of XAI features for GenAI for code and gathered additional design ideas from participants. Our work explores explainability needs for GenAI for code and demonstrates how human-centered approaches can drive the technical development of XAI in novel domains.},
}

@article{Plantinga2024,
  title = {Implementing e-Participation in Africa: What Roles Can Public Officials Play?},
  author = {Plantinga, Paul and Dlamini, Nonkululeko and Gordon, Tanja},
  year = {2024},
  journal = {ACM J. Comput. Sustain. Soc.},
  volume = {2},
  doi = {10.1145/3648438},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3648438},
  abstract = {A key question in e-participation is what roles public officials can play to harness the benefits of emerging technologies and practices, mitigate potential harms, and, ultimately, ensure more inclusive and effective public involvement in decision making. This article presents results from a desktop analysis of e-participation projects from the African continent to highlight the diversity of public official roles and associated skills and perspectives that would be relevant to e-participation implementation. The identified roles and activities range from legal specialists developing guidelines to comply with personal data protection legislation and stakeholder managers designing models of collaboration with commons-based platforms to communications officials learning how to moderate social media conversations and technology developers exploring new ways of verifying online identity.},
}

@article{Jahan2024,
  title = {Automated Derivation of UML Sequence Diagrams from User Stories: Unleashing the Power of Generative AI vs. a Rule-Based Approach},
  author = {Jahan, Munima and Hassan, Mohammad Mahdi and Golpayegani, Reza and Ranjbaran, Golshid and Roy, Chanchal and Roy, Banani and Schneider, Kevin},
  year = {2024},
  pages = {138–148},
  doi = {10.1145/3640310.3674081},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3640310.3674081},
  abstract = {User stories are informal, non-technical descriptions of features from a user's perspective that guide collaboration and iterative development in Agile projects. However, ambiguities in user stories can lead to miscommunication among stakeholders. Design models, such as UML sequence diagrams, are essential for enhancing communication, clarifying system behavior, and improving the development process. This paper presents an automated approach for generating behavioral models specifically sequence diagrams from natural language requirements expressed as user stories. We also investigate the effectiveness of a Large Language Model (LLM) in using generative AI for this task. By applying our approach and ChatGPT to two benchmark datasets with the same set of user stories, we generated corresponding sequence diagrams for comparison. Expert evaluations in Software Engineering reveal that our approach effectively produces relevant, simplified diagrams for straightforward user stories, whereas the LLM tends to create more complex diagrams that sometimes go beyond the simplicity of the original user stories.},
}

@article{Seo2024,
  title = {MAIDR Meets AI: Exploring Multimodal LLM-Based Data Visualization Interpretation by and with Blind and Low-Vision Users},
  author = {Seo, JooYoung and Kamath, Sanchita S. and Zeidieh, Aziz and Venkatesh, Saairam and McCurry, Sean},
  year = {2024},
  doi = {10.1145/3663548.3675660},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3663548.3675660},
  abstract = {This paper investigates how blind and low-vision (BLV) users interact with multimodal large language models (LLMs) to interpret data visualizations. Building upon our previous work on the multimodal access and interactive data representation (MAIDR) framework, our mixed-visual-ability team co-designed maidrAI, an LLM extension providing multiple AI responses to users’ visual queries. To explore generative AI-based data representation, we conducted user studies with 8 BLV participants, tasking them with interpreting box plots using our system. We examined how participants personalize LLMs through prompt engineering, their preferences for data visualization descriptions, and strategies for verifying LLM responses. Our findings highlight three dimensions affecting BLV users’ decision-making process: modal preference, LLM customization, and multimodal data representation. This research contributes to designing more accessible data visualization tools for BLV users and advances the understanding of inclusive generative AI applications.},
}

@article{Huang2024,
  title = {More than an IT system in the government: The work divide challenges in human-AI coworking context},
  author = {Huang, Hsini and Chen, Yen-Yu and Kuo, Nai-Ling and Hung, Mei-Jen},
  year = {2024},
  pages = {29–41},
  doi = {10.1145/3657054.3657058},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3657054.3657058},
  abstract = {The study examines the collaboration between AI and humans in Taiwanese government agencies and the reasons for different modes of collaboration. Through interviews, four distinct patterns were identified based on the division of labor and the level of influence on citizens' rights. In cases where citizens' rights are minimally affected and the division of labor is independent, AI serves as a low-risk tool for routine tasks. For applications significantly affecting citizens' rights, especially those with mature AI technologies, government agencies tend to adopt a conservative approach, using AI as a reference or support rather than a primary decision-maker. The analysis also reveals instances where AI assists in tasks beyond its current capabilities, emphasizing the importance of Human-AI collaboration in resolving complex issues. Overall, the role of AI in Human-AI collaboration transcends traditional categorizations, showcasing nuanced relationships and varying degrees of collaboration across different contexts. Additionally, the study draws insights from past literature on ICTs' impact on bureaucratic systems, highlighting the evolving roles of public servants and potential challenges arising from technological advancements.},
}

@article{Wehnert2024,
  title = {LIRAI'24: 2nd Workshop on Legal Information Retrieval meets Artificial Intelligence},
  author = {Wehnert, Sabine and Fiorelli, Manuel and Picca, Davide and De Luca, Ernesto William and Stellato, Armando},
  year = {2024},
  pages = {390–392},
  doi = {10.1145/3648188.3675120},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3648188.3675120},
  abstract = {LIRAI is a workshop series on Legal Information Retrieval and Legal Artificial Intelligence. It provides a forum for discussing current trends and challenges in legal artificial intelligence, specifically related to the hypertext nature of legal documents and retrieval tasks. The second edition of LIRAI focuses on three main directions: explainable / justifiable artificial intelligence, hybrid systems that combine formal approaches and machine learning-based methods, including deep learning-based methods, and finally generative artificial intelligence. We call for contributions on these topics in the form of short and long papers, and we aim to publish them as open-access proceedings on CEUR-WS.org once again.},
}

@article{Elliker2024,
  title = {Coherent Multi-Table Data Synthesis for Tabular and Time-Series Data with GANs},
  author = {Elliker, Cl\'{e}ment and Tonnelier, Emeric and Shabou, Aymen},
  year = {2024},
  pages = {245–252},
  doi = {10.1145/3626232.3653255},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3626232.3653255},
  abstract = {As the usage of user-private-data is increasingly monitored by regulatory institutions for security purposes, its transfer becomes more constrained. Synthetic data has recently emerged as a viable alternative to prevent the disclosure of user-protected information that complies with data sharing regulations. Both public and private sectors commonly use a combination of tabular and time-series tables that often contains user-related sensitive information. They are usually intrinsically interlinked as they describe the users and their behaviors over different perimeters. Moreover, it contains both numerical and categorical features, adding complexity to the anonymization task. State of the art generative methods, specialized either in tabular or time-series data, are able to generate high quality synthetic data. However, if each table is generated independently, it becomes impossible to link them. As a result, the usability of such synthetic data is impacted. To address this issue, we not only propose a coherent multi-table generative model that uses Generative Adversarial Networks (GANs) to sample both tabular and time-series tables , but also a conditional time-series generative model that handles both numerical and categorical features. Additionally, many experiments are conducted to analyse the inner modules of our model and evaluate it on an in-house private dataset in order to prove the viability of the synthetic data generated for machine learning tasks.},
}

@article{Andres2023,
  title = {The Human-Built Environment-Natural Environment Relation - An Immersive Multisensory Exploration with 'System of a Sound'},
  author = {Andres, Josh and Ocampo, Rodolfo and Bown, Oliver and Hill, Charlton and Pegram, Caroline and Schmidt, Adrian and Shave, Justin and Wright, Brendan},
  year = {2023},
  pages = {8–11},
  doi = {10.1145/3581754.3584119},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3581754.3584119},
  abstract = {The relationship between human activity, the built environment, such as homes, offices, and schools, and the surrounding natural environment hide interaction patterns that can be better understood. Intelligent user interfaces today use dashboards with tables and figures, missing a way where people can innately relate to this relationship. We explored emotion-oriented data sonification as an experimental way for people to engage with data. Our audio-visual and gestural intelligent user interface, "System of a Sound", takes real-time data streams centred on the location where it is installed and uses a large language model to choose music samples that elicit matching emotions. The samples are combined using an artificial intelligence music engine to compose a live soundscape that reveals interaction patterns between human activity, the built environment, and the surrounding natural environment. This exciting IUI Demo offers conference participants emotion-oriented data sonification for data exploration.},
}

@article{Dammu2025,
  title = {Towards Ethical and Personalized Web Navigation Agents: A Framework for User-Aligned Task Execution},
  author = {Dammu, Preetam Prabhu Srikar},
  year = {2025},
  pages = {1074–1076},
  doi = {10.1145/3701551.3707420},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3701551.3707420},
  abstract = {Generative AI has advanced the capabilities of autonomous agents, enabling autonomous execution of complex web navigation tasks that can reshape digital interactions across various domains. Yet, to reach their full potential, these agents must be ethically aligned and personalized to individual user needs-a challenge complicated by privacy concerns and the risk of reinforcing biases. This work introduces a novel framework that enables responsible, user-guided personalization of web navigation agents, ensuring alignment with ethical standards and user preferences. By developing agents capable of perceiving, reasoning, and adapting in alignment with user preferences, this work proposes an approach that transcends generic task execution. Employing a structured representation of user-specific tasks, the agent utilizes interactive and reasoning actions to personalize workflows, adapting responsively to individual contexts. Evaluation through task success metrics and user satisfaction scores further assesses the ethical alignment and utility of personalized interactions. This research lays the groundwork for responsible agents that offer personalized assistance while adhering to ethical and privacy standards, with implications for information retrieval, e-commerce, and other knowledge-intensive applications.},
}

@article{Giarelis2024,
  title = {A Review of Greek NLP Technologies for Chatbot Development},
  author = {Giarelis, Nikolaos and Mastrokostas, Charalampos and Siachos, Ilias and Karacapilidis, Nikos},
  year = {2024},
  pages = {15–20},
  doi = {10.1145/3635059.3635062},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3635059.3635062},
  abstract = {The advent of Generative AI has certainly boosted the interest in developing innovative chatbot applications. Despite a vast amount of machine learning (ML) and natural language processing (NLP) research and English language resources that greatly improve chatbot technology, the corresponding research and resources for the Greek language are limited. The contribution of this paper is twofold: (i) it reports on the state-of-the-art research in Greek NLP, as far as language resources, embeddings-based techniques, deep learning models, and existing chatbot applications are concerned; (ii) it offers a set of insights on current NLP models and chatbot implementation methodologies, and outlines a set of pending issues and future research directions.},
}

@article{Busch2025,
  title = {The Artificial Bureaucrat: Artificial Intelligence in Street-Level Work},
  author = {Busch, Peter Andr\'{e}},
  year = {2025},
  journal = {Digit. Gov.: Res. Pract.},
  doi = {10.1145/3721138},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3721138},
  abstract = {Public service provision in the frontline, coined street-level bureaucracy, has been gradually impacted by information and communications technology (ICT) for decades. This impact, however, has mostly considered ICT as a tool suitable for tasks with low complexity. With recent advances in artificial intelligence (AI), there are examples of AI use for more complex street-level work. Examples include cases where AI is used for assessing eligibility for social benefits, predictive policing models, automated grading, and diagnostics in healthcare. While these applications demonstrate potential benefits, they also introduce new challenges related to privacy, accountability, corporatization and alienation of street-level work, and client service experiences. This article is a critical reflection on the street-level potential of AI in providing public services. This study contributes to the ongoing debate on AI's impact in street-level work by emphasizing both the potential benefits and risks associated with AI integration in frontline service provision. While AI may mitigate some limitations of human decision-making (e.g., subjectivity, inconsistency, and bias), it can also introduce challenges that require careful consideration (e.g., lack of transparency, data-driven bias, and limited contextual adaptation). By critically reflecting on AI's street-level potential, this article calls for a balanced approach to AI adoption in street-level work.},
}

@article{Bhaskar2024,
  title = {Reproscreener: Leveraging LLMs for Assessing Computational Reproducibility of Machine Learning Pipelines},
  author = {Bhaskar, Adhithya and Stodden, Victoria},
  year = {2024},
  pages = {101–109},
  doi = {10.1145/3641525.3663629},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3641525.3663629},
  abstract = {The increasing reliance on machine learning models in scientific research and day-to-day applications – and the near-opacity of their associated computational methods – creates a widely recognized need to enable others to verify results coming from Machine Learning Pipelines. In this work we use an empirical approach to build on efforts to define and deploy structured publication standards that allow machine learning research to be automatically assessed and verified, enabling greater reliability and trust in results. To automate the assessment of a set of publication standards for Machine Learning Pipelines we developed Reproscreener; a novel, open-source software tool (see https://reproscreener.org/). We benchmark Reproscreener’s automatic reproducibility assessment against a novel manually labeled “gold standard” dataset of machine learning arXiv preprints. Our empirical evaluation has a dual goal: to assess Reproscreener’s performance; and to uncover gaps and opportunities in current reproducibility standards. We develop reproducibility assessment metrics we called the Repo Metrics to provide a novel overall assessment of the re-executability potential of the Machine Learning Pipeline, called the ReproScore. We used two approaches to the automatic identification of reproducibility metrics, keywords and LLM tools, and found the reproducibility metric evaluation performance of Large Language Model (LLM) tools superior to keyword associations.},
}

@article{Zhuang2024,
  title = {LiteMoE: Customizing On-device LLM Serving via Proxy Submodel Tuning},
  author = {Zhuang, Yan and Zheng, Zhenzhe and Wu, Fan and Chen, Guihai},
  year = {2024},
  pages = {521–534},
  doi = {10.1145/3666025.3699355},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3666025.3699355},
  abstract = {Considering limited on-device resources, current practices are attempting to deploy a system-level mixture-of-experts (MoE)-based foundation LLM shared by multiple mobile apps on a device to support mobile intelligence. However, mobile apps are hard to customize their services that require tuning adapters associated with the LLM using private in-app data. The difficulty arises due to both the limited on-device resources and the restricted control that apps have over the foundation LLM. To address this issue, in this work, we propose LiteMoE, a novel proxy submodel tuning framework that supports mobile apps to efficiently fine-tune customized adapters on devices using proxy submodels. The key technique behind LiteMoE is a post-training submodel extraction method, whereby without additional re-training, we can identify and reserve critical experts, match and merge moderate experts, to extract a lightweight and effective proxy submodel from the foundation LLM for a certain app. We implemented a prototype of LiteMoE and evaluated it over various MoE-based LLMs and mobile computing tasks. The results show that with LiteMoE, mobile apps are able to fine-tune customized adapters on resource-limited devices, achieving 12.7% accuracy improvement and 6.6\texttimes{} memory reduction compared with operating the original foundation LLM.},
}

@article{Chandrasekar2024,
  title = {Leveraging Large Language Models for Effective Organizational Navigation},
  author = {Chandrasekar, Haresh and Gupta, Srishti and Liu, Chun-Tzu and Tsai, Chun-Hua},
  year = {2024},
  pages = {1020–1022},
  doi = {10.1145/3657054.3657272},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3657054.3657272},
  abstract = {The advent of the internet has significantly enhanced accessibility to information, facilitating the engagement of diverse communities with online resources. Despite the abundance of information available, navigating the structures of large organizations and effectively digesting essential personalized information remains a challenge. Consequently, individuals may be deterred from extracting valuable insights from already available resources. This paper addresses this issue by integrating a university’s official website into an AI chatbot powered by large language models (LLMs). We demonstrate use cases to provide information tailored to general information-seeking and personalized information needs for college major selection. We present a novel approach for individuals to gain insights into large organizations via interactive conversation. Based on our system demonstration, we further delve into the role of generative AI in synthesizing vast organizational datasets into user-friendly formats accessible to the public and its implications for E-government and open government research.},
}

@article{Koziolek2024,
  title = {LLM-based and Retrieval-Augmented Control Code Generation},
  author = {Koziolek, Heiko and Gr\"{u}ner, Sten and Hark, Rhaban and Ashiwal, Virendra and Linsbauer, Sofia and Eskandani, Nafise},
  year = {2024},
  pages = {22–29},
  doi = {10.1145/3643795.3648384},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3643795.3648384},
  abstract = {Control code is designed and implemented for industrial automation applications that manage power plants, petrochemical processes, or steel production. Popular large language models (LLM) can synthesize low-level control code in the Structured Text programming notation according to the standard IEC 61131-3, but are not aware of proprietary control code function block libraries, which are often used in practice. To automate control logic implementation tasks, we proposed a retrieval-augmented control code generation method that can integrate such function blocks into the generated code. With this method control engineers can benefit from the code generation capabilities of LLMs, re-use proprietary and well-tested function blocks, and speed up typical programming tasks significantly. We have evaluated the method using a prototypical implementation based on GPT-4, LangChain, Open-PLC, and the open-source OSCAT function block library. In several spot sample tests, we successfully generated IEC 61131-3 ST code that integrated the desired function blocks, could be compiled, and validated through simulations.},
}

@article{Deng2025,
  title = {Leveraging NLP in Finance: A Synergistic Approach Using Large Language Models and Chain-of-Thought Reasoning},
  author = {Deng, Yong and Zhang, Xintong and Zhou, Danping and Zhang, Dequan and Huang, Boya},
  year = {2025},
  pages = {494–500},
  doi = {10.1145/3716895.3716983},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3716895.3716983},
  abstract = {In this paper, we explore the synergy between large language models (LLMs) and chain-of-thought(CoT) reasoning in the context of extracting valuable insights from public data sources for financial applications. We emphasize how this integration can significantly enhance the financial industry's ability to process and interpret vast amounts of unstructured data, leading to more informed decision-making and strategic advantages. By combining LLMs' extensive knowledge and natural language generation capabilities with the logical reasoning of CoT processes, we demonstrate the potential for extracting precise and actionable financial information from publicly available resources. This paper also delves into the challenges and limitations encountered in this integration and suggests future research directions to further streamline and optimize these algorithms for real-world financial use cases. Overall, our discussion underscores the transformative power of combining advanced NLP techniques with financial expertise to unlock the full potential of public data for financial gain.},
}

@article{Safaei2024,
  title = {The End of the Policy Analyst? Testing the Capability of Artificial Intelligence to Generate Plausible, Persuasive, and Useful Policy Analysis},
  author = {Safaei, Mehrdad and Longo, Justin},
  year = {2024},
  journal = {Digit. Gov.: Res. Pract.},
  volume = {5},
  doi = {10.1145/3604570},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3604570},
  abstract = {Policy advising in government centers on the analysis of public problems and the developing of recommendations for dealing with them. In carrying out this work, policy analysts consult a variety of sources and work to synthesize that body of evidence into useful decision support documents commonly called briefing notes. Advances in natural language processing (NLP) have led to the continuing development of tools that can undertake a similar task. Given a brief prompt, a large language model (LLM) can synthesize information in content databases. This article documents the findings from an experiment that tested whether contemporary NLP technology is capable of producing public policy relevant briefing notes that expert evaluators judge to be useful. The research involved two stages. First, briefing notes were created using three models: NLP generated; human generated; and NLP generated/human edited. Next, two panels of retired senior public servants (with only one panel informed of the use of NLP in the experiment) were asked to judge the briefing notes using a heuristic evaluation rubric. The findings indicate that contemporary NLP tools were not able to, on their own, generate useful policy briefings. However, the feedback from the expert evaluators indicates that automatically generated briefing notes might serve as a useful supplement to the work of human policy analysts. And the speed with which the capabilities of NLP tools are developing, supplemented with access to a larger corpus of previously prepared policy briefings and other policy-relevant material, suggests that the quality of automatically generated briefings may improve significantly in the coming years. The article concludes with reflections on what such improvements might mean for the future practice of policy analysis.},
}

@article{Lucke2024,
  title = {A few Thoughts on the Use of ChatGPT, GPT 3.5, GPT-4 and LLMs in Parliaments: Reflecting on the results of experimenting with LLMs in the parliamentarian context},
  author = {Lucke, J\"{o}rn Von and Frank, Sander},
  year = {2024},
  journal = {Digit. Gov.: Res. Pract.},
  doi = {10.1145/3665333},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3665333},
  abstract = {Starting in November 2022 with the free provision of ChatGPT, large language models (LLM) are now publicly available. This has significantly increased the number of publications which scopes potential changes caused by the application of generative artificial intelligence (AI) in various societal domains. The private use of AI and the economic integration of generative LLMs have increased significantly. However, for parliamentarians and parliamentary professionals, the technology often remains abstract, impacting everyday work only peripherally. Due to the special responsibility of parliaments, governments, and administrations as the organizational instances of society, and through the inherent legitimations by society itself, there is a necessity to examine the implications of the use of generative LLMs within these institutions and traditional structures as well as their influence on political system logic. The paper analyzes the responses that the generative LLMs GPT 3.5 and GPT 4 have provided via ChatGPT, based on the same input command (prompt) over different times. The responses help to assess how LLMs can be used in the parliamentary context, to reflect what dangers exist as well as to respond to the question on how a business model of an AI department in parliament might look like. Furthermore, it shall be explored whether there are fluctuations in the quality of the responses and how these should be evaluated against the backdrop of the need for accurate and precise workflows in parliamentary operations. Ultimately, the paper aims to provide an answer as to whether the application of ChatGPT together with the LLMs GPT-3.5 and GPT-4 could already deliver this necessary quality and consistency for the parliamentarian working environment today.},
}

@article{Mayeesha2025,
  title = {AI4Bangladesh: AI Ethics for Bangladesh - Challenges, Risks, Principles, and Suggestions},
  author = {Mayeesha, Tasmiah Tahsin and Islam, Farzana and Ahmed, Nova},
  year = {2025},
  pages = {260–272},
  doi = {10.1145/3700794.3700820},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3700794.3700820},
  abstract = {In recent times, the term AI ethics caught the attention among the academics, legislators, developers, and among AI users to promote ethical AI development. While countries in the North have led the way in discussions about the direction of ethical and responsible artificial intelligence development and deployment, perspectives from developing countries like Bangladesh are underrepresented. Based on 32 qualitative interviews with different stakeholders, including machine learning practitioners, academic researchers, and policymakers in the emerging AI ecosystem in Bangladesh, this work closely examines the ongoing challenges and opportunities to ensure AI ethics in Bangladesh with emerging AI usage. In Bangladesh, the government has not yet fully implemented measures to empower citizens with AI-related skills, policies, resources, and data ethics, and a significant portion of the population lacks knowledge in AI. In this paper, we are presenting the findings of AI4Bangladesh project that intend to create the roadmap for ethical AI in Bangladesh. We outline the core challenges, present situation, and risks of AI for Bangladesh; propose seven AI ethics principles, and offer suggestions to ensure a transparent, accountable, and fair AI ecosystem for Bangladesh.},
}

@article{Cremaschi2025,
  title = {stEELlm: An LLM for Generating Semantic Annotations of Tabular Data},
  author = {Cremaschi, Marco and D'Adda, Fabio and Maurino, Andrea},
  year = {2025},
  journal = {ACM Trans. Intell. Syst. Technol.},
  doi = {10.1145/3719206},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3719206},
  abstract = {The capabilities of LLMs represent a pivotal step in transforming how we manage and interact with information and data. We witness an increasingly pervasive use of such models in various computational tasks. In some preliminary works, attempts to integrate Knowledge Graphs and Large Language Models (LLMs) can be identified, in particular, to perform the classic tasks related to the construction of Knowledge Graphs through semantic annotation of texts. Nowadays, tables are widely used and play a crucial role in creating, organising, and sharing information that could be used to produce factual knowledge to be integrated into a Knowledge Graph. However, table-to-KG techniques through LLM have not been extensively investigated. This paper presents stEELlm, an innovative Semantic Table Interpretation approach obtained by fine-tuning the Mixtral 8x7B model. Conducted experiments demonstrate the capabilities of our model to successfully create semantic annotations of heterogeneous datasets, a scenario where classic approaches based on heuristics tend to fail.},
}

@article{Zhang2024,
  title = {“It's a Fair Game”, or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents},
  author = {Zhang, Zhiping and Jia, Michelle and Lee, Hao-Ping (Hank) and Yao, Bingsheng and Das, Sauvik and Lerner, Ada and Wang, Dakuo and Li, Tianshi},
  year = {2024},
  doi = {10.1145/3613904.3642385},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3613904.3642385},
  abstract = {The widespread use of Large Language Model (LLM)-based conversational agents (CAs), especially in high-stakes domains, raises many privacy concerns. Building ethical LLM-based CAs that respect user privacy requires an in-depth understanding of the privacy risks that concern users the most. However, existing research, primarily model-centered, does not provide insight into users’ perspectives. To bridge this gap, we analyzed sensitive disclosures in real-world ChatGPT conversations and conducted semi-structured interviews with 19 LLM-based CA users. We found that users are constantly faced with trade-offs between privacy, utility, and convenience when using LLM-based CAs. However, users’ erroneous mental models and the dark patterns in system design limited their awareness and comprehension of the privacy risks. Additionally, the human-like interactions encouraged more sensitive disclosures, which complicated users’ ability to navigate the trade-offs. We discuss practical design guidelines and the needs for paradigm shifts to protect the privacy of LLM-based CA users.},
}

@article{Sharma2024,
  title = {OpenROAD-Assistant: An Open-Source Large Language Model for Physical Design Tasks},
  author = {Sharma, Utsav and Wu, Bing-Yue and Kankipati, Sai Rahul Dhanvi and Chhabria, Vidya A. and Rovinski, Austin},
  year = {2024},
  doi = {10.1145/3670474.3685960},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3670474.3685960},
  abstract = {Large language models (LLMs) have shown significant potential in serving as domain-specific chatbots. Recently, these models have emerged as powerful tools for chip design, providing both natural language responses and script generation for domain-specific inquiries. Previous work has demonstrated the effectiveness of LLMs in assisting with physical design automation; however, these approaches often rely on proprietary tools, APIs, technologies, and designs. As a result, access to these models is extremely limited, particularly for new chip designers who could greatly benefit from a design assistant. This paper introduces OpenROAD-Assistant, an open-source chatbot for OpenROAD that relies only on public data and responds to queries in either prose or Python script using the OpenROAD APIs. OpenROAD-Assistant leverages the Llama3-8B foundation model and employs retrieval-aware fine-tuning (RAFT) to respond to physical design-specific questions for OpenROAD. Notably, OpenROAD-Assistant outperforms other foundational models such as ChatGPT3.5, ChatGPT4, Code Llama, Claude3, and other ablation study baselines on the measured metrics (pass@k for scripting and BERTScore/BARTScore for question-answering). OpenROAD-Assistant achieves a 77% pass@1 score, 80% pass@3 score for scripting, and it achieves a 98% BERTScore and 96% BARTScore on question-answering.},
}

@article{Lee2024,
  title = {Development of the AI Implementation Framework in Taipei City},
  author = {Lee, Dasheng and Chao, Shih-Lung and Chen, Hui-Min},
  year = {2024},
  pages = {90–103},
  doi = {10.1145/3657054.3657065},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3657054.3657065},
  abstract = {Taipei City has been experimenting with the use of Artificial Intelligence (AI) tools to enhance its smart capabilities, aiming to increase citizen satisfaction. This initiative is part of the city's Smart City Proof of Concept (PoC) projects, which have been progressively rolled out since 2015. Most of these projects incorporate AI tools or algorithms, such as the combination of the Internet of Things (IoT) with AI to form AIoT, or the application of Large Language Models (LLMs). The objective is to leverage the latest technological developments to achieve a smarter Taipei. This study analyzes the execution of 302 PoC projects, categorizing them into 22 technological segments that together form an AI framework for smart city construction applications. This framework corresponds to 15 major issues of concern to Taipei's residents, with the potential to address or mitigate 13 of them. According to the IMD Smart City Index Report 2023, Taipei's smart city rating improved from a B in 2021 to an A in 2023, indicating progress. The results demonstrate that the AI framework derived from dissecting multiple PoC projects can effectively enhance the city's smart construction ratings. This framework, aligned with major municipal concerns, proposes solutions driven by AI, guiding Taipei's digital transformation into a smarter city and enabling its citizens to enjoy an improved quality of life.},
}

@article{Zhang2024_01,
  title = {The Application of Artificial Intelligence in the Field of Modern Infrastructure Investment},
  author = {Zhang, Kai and Zhang, Ting},
  year = {2024},
  pages = {355–359},
  doi = {10.1145/3697355.3697413},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3697355.3697413},
  abstract = {The use of artificial intelligence can effectively assist investment decision-making and risk control in the infrastructure field and is an important direction in the field of modern infrastructure investment. This article explores the application research of artificial intelligence in the field of modern infrastructure investment. This article analyzes the advantages of artificial intelligence in data analysis and prediction, and intelligent generation of innovative content, combined with the overall trend of intensive and efficient, intelligent and green, dynamic risk control, and value-driven modern infrastructure investment, and studies how to give play to the decision-making and analysis advantages of artificial intelligence in investment decisions, project risk control, and resource allocation, as well as how to give play to the innovative value in expanding investment value, optimizing design plans, construction stages, and investment plans. Through cases, it is confirmed that the application prospects of artificial intelligence in the field of modern infrastructure investment are broad, which can bring more intelligent and efficient investment and management models to infrastructure investment projects.},
}

@article{Fan2025,
  title = {HeLoRA: LoRA-heterogeneous Federated Fine-tuning for Foundation Models},
  author = {Fan, Boyu and Su, Xiang and Tarkoma, Sasu and Hui, Pan},
  year = {2025},
  journal = {ACM Trans. Internet Technol.},
  doi = {10.1145/3723877},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3723877},
  abstract = {Foundation models (FMs) have achieved state-of-the-art performance across various domains, benefiting from their vast number of parameters and the extensive amount of publicly available training data. However, real-world deployments reveal challenges such as system heterogeneity, where not all devices can handle the complexity of FMs, and emerging privacy concerns that limit the availability of public data. To address these challenges, we propose HeLoRA, a novel approach combining low-rank adaptation (LoRA) with federated learning to enable heterogeneous federated fine-tuning. HeLoRA allows clients to fine-tune models with different complexities by adjusting the rank values of LoRA matrices, tailoring the process to each device’s capabilities. To tackle the challenge of aggregating models with different structures, HeLoRA introduces two variants, i.e., HeLoRA-Pad and HeLoRA-KD. HeLoRA-Pad employs context-based padding to standardize the LoRA matrices, aligning them with the global model through a rank-based adaptive aggregation strategy. In contrast, HeLoRA-KD leverages the idea of deep mutual learning for aggregation, allowing heterogeneous models to retain their original structures. Extensive experiments with various datasets and ablation studies demonstrate that HeLoRA outperforms existing baselines, promising to enhance the practical deployment of FMs in diverse real-world environments.},
}

@article{Mainaly2025,
  title = {A Glimpse of Lawrence's Legacy: From "Siri Discipline" to Disciplining Artificial Intelligence},
  author = {Mainaly, Shiva Hari},
  year = {2025},
  journal = {Commun. Des. Q. Rev},
  volume = {12},
  pages = {102–104},
  doi = {10.1145/3655727.3655739},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3655727.3655739},
  abstract = {Have you ever wondered how a researcher from the periphery can gain an enduring foothold in the pantheon of researchers from the center? This essay will attempt to answer that question. Halcyon Lawrence was a researcher, writer, and professor from the Global South who has made a mark on a community of technical communication scholars, writers, researchers, and professors with her widely discussed research articles dealing with the pros and cons, perils and promises, boon and bane of speech recognition tools and technology. Lawrence's research explores the thickets of speech recognition and proposes strategic and revisionary measures toward neutralizing the lopsided corpora of speech recognition software, vaporware, and artificial intelligence (AI)-powered technology. To crystalize her contributions to justice, data justice, and racial-linguistic justice, I chose a chapter, "Siri Discipline," she (2021) wrote for the book Your Computer is on Fire (Mullaney et al, 2021). My essay highlights how her ideas have gained more traction in relation to the current disruption of the AI revolution (Gopal, 2020). That disruption is often exemplified through ChatGPT, a platform that shows how Lawrence's core insight from "Siri Discipline" can have a direct bearing on normative frameworks being developed to address burgeoning challenges ushered in by the AI revolution.},
}

@article{Li2023,
  title = {The Dimensions of Data Labor: A Road Map for Researchers, Activists, and Policymakers to Empower Data Producers},
  author = {Li, Hanlin and Vincent, Nicholas and Chancellor, Stevie and Hecht, Brent},
  year = {2023},
  pages = {1151–1161},
  doi = {10.1145/3593013.3594070},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3593013.3594070},
  abstract = {Many recent technological advances (e.g. ChatGPT and search engines) are possible only because of massive amounts of user-generated data produced through user interactions with computing systems or scraped from the web (e.g. behavior logs, user-generated content, and artwork). However, data producers have little say in what data is captured, how it is used, or who it benefits. Organizations with the ability to access and process this data, e.g. OpenAI and Google, possess immense power in shaping the technology landscape. By synthesizing related literature that reconceptualizes the production of data for computing as “data labor”, we outline opportunities for researchers, policymakers, and activists to empower data producers in their relationship with tech companies, e.g advocating for transparency about data reuse, creating feedback channels between data producers and companies, and potentially developing mechanisms to share data’s revenue more broadly. In doing so, we characterize data labor with six important dimensions - legibility, end-use awareness, collaboration requirement, openness, replaceability, and livelihood overlap - based on the parallels between data labor and various other types of labor in the computing literature.},
}

@article{Zhang2025,
  title = {A Survey on Privacy-Preserving Caching at Network Edge: Classification, Solutions, and Challenges},
  author = {Zhang, Xianzhi and Zhou, Yipeng and Wu, Di and Sheng, Quan Z. and Riaz, Shazia and Hu, Miao and Xiao, Linchang},
  year = {2025},
  journal = {ACM Comput. Surv.},
  volume = {57},
  doi = {10.1145/3706630},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3706630},
  abstract = {Caching content at the edge network is a popular and effective technique widely deployed to alleviate the burden of network backhaul, shorten service delay, and improve service quality. However, there has been some controversy over privacy violations in caching content at the edge network. On the one hand, the multi-access open edge network provides an ideal entrance or interface for external attackers to obtain private data from edge caches by extracting sensitive information. On the other hand, privacy can be infringed on by curious edge caching providers through caching trace analysis targeting the achievement of better caching performance or higher profits. Therefore, an in-depth understanding of privacy issues in edge caching networks is vital and indispensable for creating a privacy-preserving caching service at the edge network. In this article, we are among the first to fill this gap by examining privacy-preserving techniques for caching content at the edge network. First, we provide an introduction to the background of privacy-preserving edge caching. Next, we summarize the key privacy issues and present a taxonomy for caching at the edge network from the perspective of private information. Additionally, we conduct a retrospective review of the state-of-the-art countermeasures against privacy leakage from content caching at the edge network. Finally, we conclude the survey and envision challenges for future research.},
}

@article{Baecker2024,
  title = {Digital Dreams Have Become Nightmares: What We Must Do},
  author = {Baecker, Ronald M. and Grudin, Jonathan},
  year = {2024},
  volume = {56},
  publisher = {Association for Computing Machinery},
  abstract = {This book offers a compelling discussion of the digital dreams that have come true, their often unintended side effects (nightmares), and what must be done to counteract the nightmares. It is intended as an impetus to further conversation not only in homes and workplaces, but in academic courses and even legislative debates. Equally importantly, the book is a presentation of what digital technology professionals need to know about these topics and the actions they should undertake individually and in support of other citizens, societal initiatives, and government. The author begins by introducing the amazing progress made in digital technologies over the past 80 years. Pioneering engineers dreamed of potential uses of technology through their writing and technical achievements, further inspiring thousands of researchers to bring the dreams to life, and to dream new dreams as well. The second part of the book describes the myriad adverse side effects and unanticipated challenges that arose as those dreams were pursued and achieved. Examples include rampant misinformation on social media, ransomware, autonomous weapons, and the premature use of AI before it is reliable and safe.The book closes with a positive call to action, outlining ways to address the challenges through ethical career choices, careful analysis, thoughtful design, research, citizen engagement, legislation/regulation, and careful consideration of how bad actors may use technology. Readers of Digital Dreams Have Become Nightmares should become more knowledgeable, wiser, and also cautiously optimistic, determined to affect positive changes through their design, creation, and use of technology.“Are you feeling happy about the role of information technology in the world today? You should read this book for a dose of reality. Are you in despair about it? This book is the prescription for that condition, too! Nobody else could cover the landscape as Ron Baecker does.” - Clayton Lewis, Emeritus Professor, University of Colorado Boulder“This book is a captivating review of important computing developments. Many things talked about as new today have been around for a long time. Much can be learned from the past. The book also teaches a careful and consistent method that enables the reader to do this kind of work as the need arises. The book suggests the need will arise.” - John Leslie King, Emeritus Professor, University of Michigan},
}

@article{Chaudhry2024,
  title = {Concerns and Challenges of AI Tools in the UI/UX Design Process: A Cross-Sectional Survey},
  author = {Chaudhry, Beenish Moalla},
  year = {2024},
  doi = {10.1145/3613905.3650878},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3613905.3650878},
  abstract = {The study aimed to gain a comprehensive understanding of AI integration in UI/UX design process from the perspective of professionals directly engaged with these tools. Two surveys were conducted in August 2023, to explore attitudes, ethical concerns, and future views on AI-assisted design technologies through both closed and open-ended questions. Using both thematic and descriptive analyses, the results showed that designers were cautiously optimistic about AI tools. This shows how important it is to think about ethics, understand how AI works, and find a good balance between integrating AI into the design workflow. These insights are crucial for AI tool developers, design educators, and the design community to develop more effective, ethical, and user-centric AI tools.},
}

@article{Gramacki2024,
  title = {Evaluation of Code LLMs on Geospatial Code Generation},
  author = {Gramacki, Piotr and Martins, Bruno and Szyma\'{n}ski, Piotr},
  year = {2024},
  pages = {54–62},
  doi = {10.1145/3687123.3698286},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3687123.3698286},
  abstract = {Software development support tools have been studied for a long time, with recent approaches using Large Language Models (LLMs) for code generation. These models can generate Python code for data science and machine learning applications. LLMs are helpful for software engineers because they increase productivity in daily work. An LLM can also serve as a "mentor" for inexperienced software developers, and be a viable learning support. High-quality code generation with LLMs can also be beneficial in geospatial data science. However, this domain poses different challenges, and code generation LLMs are typically not evaluated on geospatial tasks. Here, we show how we constructed an evaluation benchmark for code generation models, based on a selection of geospatial tasks. We categorised geospatial tasks based on their complexity and required tools. Then, we created a dataset with tasks that test model capabilities in spatial reasoning, spatial data processing, and geospatial tools usage. The dataset consists of specific coding problems that were manually created for high quality. For every problem, we proposed a set of test scenarios that make it possible to automatically check the generated code for correctness. In addition, we tested a selection of existing code generation LLMs for code generation in the geospatial domain. We share our dataset and reproducible evaluation code on a public GitHub repository1, arguing that this can serve as an evaluation benchmark for new LLMs in the future. Our dataset will hopefully contribute to the development new models capable of solving geospatial coding tasks with high accuracy. These models will enable the creation of coding assistants tailored for geospatial applications.},
}

@article{Liesenfeld2023,
  title = {Opening up ChatGPT: Tracking openness, transparency, and accountability in instruction-tuned text generators},
  author = {Liesenfeld, Andreas and Lopez, Alianda and Dingemanse, Mark},
  year = {2023},
  doi = {10.1145/3571884.3604316},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3571884.3604316},
  abstract = {Large language models that exhibit instruction-following behaviour represent one of the biggest recent upheavals in conversational interfaces, a trend in large part fuelled by the release of OpenAI’s ChatGPT, a proprietary large language model for text generation fine-tuned through reinforcement learning from human feedback (LLM+RLHF). We review the risks of relying on proprietary software and survey the first crop of open-source projects of comparable architecture and functionality. The main contribution of this paper is to show that openness is differentiated, and to offer scientific documentation of degrees of openness in this fast-moving field. We evaluate projects in terms of openness of code, training data, model weights, RLHF data, licensing, scientific documentation, and access methods. We find that while there is a fast-growing list of projects billing themselves as ‘open source’, many inherit undocumented data of dubious legality, few share the all-important instruction-tuning (a key site where human annotation labour is involved), and careful scientific documentation is exceedingly rare. Degrees of openness are relevant to fairness and accountability at all points, from data collection and curation to model architecture, and from training and fine-tuning to release and deployment.},
}

@article{Santana2024,
  title = {Computational Notebook as a Tool for Supporting Black Engineers Non-Profit Organizations in the Context of Workforce Participation},
  author = {Santana, Vagner Figueredo de and Quigley, Lauren Thomas and Coates, Jessica and Graves, Maxine},
  year = {2024},
  doi = {10.1145/3613905.3637130},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3613905.3637130},
  abstract = {Data is frequently referred to as the ‘new oil’ due to its high strategic value and the possibilities it brings. Additionally, its use by powerful organizations has the potential to help, but also to cause harm and function as an additional oppressive mechanism towards vulnerable communities. Hence, there is a need for providing easier methods for people and organizations to access, analyze, and interpret reliable, publicly available data (e.g., U.S. Census), and create data narratives that reflect their social position in the society and inform how to improve that position. In this case study, we document our journey on using computational notebooks to increase autonomy for organizations tackling racial and social justice issues by making meaningful use of publicly available data. The project considered a participatory action research approach run in partnership with a non-profit organization supporting Black engineers. The reported case study involved participants representing staff and volunteer leadership of the organization. We expect that the presented reflections help move towards a more equitable way of performing data analysis while using popular and freely available technologies.},
}

@article{Taylor2024,
  title = {Carefully Unmaking the “Marginalized User”: A Diffractive Analysis of a Gay Online Community},
  author = {Taylor, Jordan and Deng, Wesley Hanwen and Holstein, Kenneth and Fox, Sarah and Zhu, Haiyi},
  year = {2024},
  journal = {ACM Trans. Comput.-Hum. Interact.},
  volume = {31},
  doi = {10.1145/3673229},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3673229},
  abstract = {HCI scholars are increasingly engaging in research about “marginalized groups,” such as LGBTQ+ people. While normative habitual readings of marginalized people in HCI often highlight real problems, this work has been criticized for flattening heterogeneous experiences and overemphasizing harms. Some have advocated for expanding how we approach research on marginalized people (e.g., assets-based design, the everyday, and joy). Sensitized by unmaking literature, we explore this tension between conditions, experiences, and representations of marginality in HCI scholarship. To do so, we perform a diffractive analysis of posts in a gay online community by bringing two readings of the same data together: a normative habitual reading of marginalization and an expanded reading. By examining the relationship between empirical material and its representations by HCI researchers, we explore how to carefully unmake HCI research, thus maintaining and repairing our research community. We discuss the political and designerly implications of different readings of marginalized people and offer considerations for attending to the processes and afterlives of HCI research.},
}

@article{Maratsi2024,
  title = {Towards Cross-Domain Linking of Data: A Semantic Mapping of Cultural Heritage Ontologies},
  author = {Maratsi, Maria Ioanna and Ahmed, Umair and Alexopoulos, Charalampos and Charalabidis, Yannis and Polini, Andrea},
  year = {2024},
  pages = {165–176},
  doi = {10.1145/3657054.3657077},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3657054.3657077},
  abstract = {The Linked Open Vocabularies (LOV) registry, designed with the Linked Data principles at core, provides an environment suitable for research which targets domain-specific, but also potentially reusable, information representation. The main purpose of this study is to follow the recommendations pertaining to the utilisation of LOV as a basis for experimentation in order to examine how information within the Cultural Heritage (CH) domain can be improved in terms of reusability and interoperability. The present lack of cross-domain knowledge transfer forms the motivation behind this study, with the aim of facilitating the transition from conventional, domain-specific knowledge representation to reusable and semantically interoperable information. The methodology of this study involves the manual semantic mapping of elements from 12 vocabularies in the LOV registry, reinforced by a small-scale experiment using contemporary large language models (LLMs), particularly GPT, for a preliminary assessment of the mapping process. The findings revealed several key aspects to consider regarding the alignment of semantically adjacent vocabulary elements in the CH domain and beyond, emphasising the potential unveiled by linking domain-focused schemata to standardised, established ones while preserving the conceptual hierarchies inherent to each individual knowledge domain. The contribution of this research pertains to the vision of linking data across different domains by initiating the alignment among representation schemata in CH, with the ultimate aim to expand beyond the boundaries of the in-word knowledge domain, while employing combinatory methodological approaches of technological means and human expertise to facilitate this process.},
}

@article{Huang2024_01,
  title = {Disambiguate Entity Matching using Large Language Models through Relation Discovery},
  author = {Huang, Zezhou},
  year = {2024},
  pages = {36–39},
  doi = {10.1145/3665601.3669844},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3665601.3669844},
  abstract = {Entity matching is a critical problem in data integration, central to tasks like fuzzy joins for tuple enrichment. Traditional approaches have focused on overcoming fuzzy term representations through methods such as edit distance, Jaccard similarity, and more recently, embeddings and deep neural networks, including advancements from large language models (LLMs) like GPT. However, when integrating with external databases, the core challenge in entity matching extends beyond term fuzziness to the ambiguity in defining what constitutes a "match". This is because external databases contain tuples with varying levels of detail and granularity among entities, and an "exact match" in traditional entity matching rarely happens. As a result, understanding how entities are related and the potential nuances is critical, especially for high-stake tasks for responsible AI. In this work, we study a case problem of entity matching for ESG reporting. We propose a novel approach that shifts focus from purely identifying semantic similarities to understanding and defining the "relations" between entities for resolving ambiguities in matching, with a human-in-the-loop process to make the final decision. By pre-defining a set of relations relevant to the task at hand, our method allows analysts to navigate the spectrum of similarity more effectively, from exact matches to conceptually related entities, and responsibly perform downstream tasks.},
}

@article{Neo2024,
  title = {Towards Fair Graph Anomaly Detection: Problem, Benchmark Datasets, and Evaluation},
  author = {Neo, Neng Kai Nigel and Lee, Yeon-Chang and Jin, Yiqiao and Kim, Sang-Wook and Kumar, Srijan},
  year = {2024},
  pages = {1752–1762},
  doi = {10.1145/3627673.3679754},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3627673.3679754},
  abstract = {The Fair Graph Anomaly Detection (FairGAD) problem aims to accurately detect anomalous nodes in an input graph while avoiding biased predictions against individuals from sensitive subgroups. However, the current literature does not comprehensively discuss this problem, nor does it provide realistic datasets that encompass actual graph structures, anomaly labels, and sensitive attributes. To bridge this gap, we introduce a formal definition of the FairGAD problem and present two novel datasets constructed from the social media platforms Reddit and Twitter. These datasets comprise 1.2 million and 400,000 edges associated with 9,000 and 47,000 nodes, respectively, and leverage political leanings as sensitive attributes and misinformation spreaders as anomaly labels. We demonstrate that our FairGAD datasets significantly differ from the synthetic datasets used by the research community. Using our datasets, we investigate the performance-fairness trade-off in nine existing GAD and non- graph AD methods on five state-of-the-art fairness methods. Code and datasets are available at https://github.com/nigelnnk/FairGAD.},
}

@article{Wang2024_01,
  title = {Security and Privacy on Generative Data in AIGC: A Survey},
  author = {Wang, Tao and Zhang, Yushu and Qi, Shuren and Zhao, Ruoyu and Xia, Zhihua and Weng, Jian},
  year = {2024},
  journal = {ACM Comput. Surv.},
  volume = {57},
  doi = {10.1145/3703626},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3703626},
  abstract = {The advent of artificial intelligence-generated content (AIGC) represents a pivotal moment in the evolution of information technology. With AIGC, it can be effortless to generate high-quality data that is challenging for the public to distinguish. Nevertheless, the proliferation of generative data across cyberspace brings security and privacy issues, including privacy leakages of individuals and media forgery for fraudulent purposes. Consequently, both academia and industry begin to emphasize the trustworthiness of generative data, successively providing a series of countermeasures for security and privacy. In this survey, we systematically review the security and privacy on generative data in AIGC, particularly for the first time analyzing them from the perspective of information security properties. Specifically, we reveal the successful experiences of state-of-the-art countermeasures in terms of the foundational properties of privacy, controllability, authenticity, and compliance, respectively. Finally, we show some representative benchmarks, present a statistical analysis, and summarize the potential exploration directions from each of these properties.},
}

@article{Malik2024,
  title = {Building Natural Language Interface for Product Search},
  author = {Malik, Vijit and Puranik, Vinayak and Majumder, Anirban and Sembium, Vivek},
  year = {2024},
  pages = {4768–4776},
  doi = {10.1145/3627673.3680070},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3627673.3680070},
  abstract = {Automatic extraction of attribute preferences from search queries is a critical problem in providing accurate product recommendations to customer. The task becomes even more challenging in cold-start settings where we do not have any supervised/labelled data available to train ML models. In this work, we implement a novel dataset generation pipeline (LLM-API) that leverages Large Language Models (LLMs), search logs and proprietary product information data from an ecommerce website to create a high quality dataset. Our proposed pipeline of LLM-API is robust as it can generalize to any product category with minimal changes in the LLM prompts. For the problem of converting product search queries to API calls we propose a multi-task schema generator model which we train on our generated dataset. Experiments on an internal test set reveals that our proposed model achieves an improvement of ≈9.6% and ≈5% in Exact Match and Micro-F1 respectively, over competitive baselines. Benchmarking our approach on public test set of search queries further reveals a gain of ≈8.6% and ≈10.5% in Exact Match and Micro-F1. We further demonstrate that our approach outperforms a state-of-the-art LLM (Claude) applied on our task using few-shot prompting and CoT reasoning, while at the same time, achieves improvement in inference latency.},
}

@article{Dar2024,
  title = {Assessing the Feasibility and Ethics of Economic Status Prediction using Deep Learning on Household Images},
  author = {Dar, Aatif Nisar and Sengupta, Nandana and Arora, Chetan},
  year = {2024},
  journal = {ACM J. Comput. Sustain. Soc.},
  volume = {2},
  doi = {10.1145/3675160},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3675160},
  abstract = {Precise and comprehensive measurements of socioeconomic status are crucial for both academic investigations and policy making. However, in developing countries, such measures are available at the local household level only at extremely low frequencies (e.g., via a decadal census). A number of works have attempted predicting economic status at aggregated geographical levels such as district or neighborhood using deep learning on images with varying degrees of success. However the utility of such an approach at the household level remains open. In this study, we utilize deep learning models on household images collected from four northeastern states in India to assess the feasibility and ethics of household-level income status prediction. We categorize households into classes based on income and then train a Swin Transformer model with cross-entropy loss and triplet loss to predict the socioeconomic class of the household. We then compare the prediction accuracy of our model with predictions using a simple list of household assets and predictions from a set of expert human annotators. We find that the use of deep learning on images does not lead to any substantial gains in prediction accuracy. Further, we note that human accuracy on this prediction tasks is low, raising questions on the information contained within the images. Our study raises important questions regarding the ethical implications of utilizing household images for predicting socioeconomic status. We explore these ethical implications, emphasizing the importance of a cautious and considerate approach in incorporating image-based techniques.},
}

@article{Ji2024,
  title = {Research and Practice on the Localisation Adaptation of the Power Enterprise Information System Development Platform Xinchuang (localization)},
  author = {Ji, Ya and Liu, Jiali and Zhang, Wei and Du, Lanna and Li, Qi and Jia, Ning},
  year = {2024},
  pages = {389–394},
  doi = {10.1145/3674225.3674294},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3674225.3674294},
  abstract = {The market size of low-code is rising year by year, and under the wave of enterprise digital transformation, super huge new business scenarios are needed for applications. It can alleviate or even solve the huge market demand and traditional development productivity triggered by the contradictory relationship between supply and demand issues, is bound to benefit from the application development market, and will eventually become an inevitable product of the digital transformation process under the trend of cost reduction and efficiency. With the accelerated implementation of the national big data development strategy, the strategic value of data elements highlighted, data security has been related to all aspects of economic and social development, it is crucial to balance the relationship between the free flow of data, development and utilisation and data security. At present, the common low-code platform applied by electric power enterprises unilaterally pursues powerful functions and efficient performance, and does not support the transformation of Xinchuang (localisation). In order to respond positively to the national policy on data security, Saiyan low-code development platform supports data classification and grading, desensitisation, watermarking, and encrypted storage of important data, and has been adapted to localised environments such as Nanda's general Gbase database, Damon database, Kirin OS, Unicorn OS, and Kingdee's middleware, etc., and adopts cryptographic machines and cryptographic state of Beijing CA Company, which has been certified by the national authoritative body in 2023. The cryptographic machine and database of Beijing CA Company (GBase8c of Nanda General and GaussDB of Huawei) have been adopted, and through a large number of localisation projects of the company, a set of perfect and stable localisation substitution scheme has been summed up, which has pushed forward the process of localisation transformation of the platform.},
}

@article{Struppek2024,
  title = {Exploiting Cultural Biases via Homoglyphs in Text-to-Image Synthesis},
  author = {Struppek, Lukas and Hintersdorf, Dom and Friedrich, Felix and br, Manuel and Schramowski, Patrick and Kersting, Kristian},
  year = {2024},
  journal = {J. Artif. Int. Res.},
  volume = {78},
  doi = {10.1613/jair.1.15388},
  publisher = {AI Access Foundation},
  url = {https://doi.org/10.1613/jair.1.15388},
  abstract = {Models for text-to-image synthesis, such as DALL-E 2 and Stable Diffusion, have recently drawn a lot of interest from academia and the general public. These models are capable of producing high-quality images that depict a variety of concepts and styles when conditioned on textual descriptions. However, these models adopt cultural characteristics associated with specific Unicode scripts from their vast amount of training data, which may not be immediately apparent. We show that by simply inserting single non-Latin characters in the textual description, common models reflect cultural biases in their generated images. We analyze this behavior both qualitatively and quantitatively and identify a model’s text encoder as the root cause of the phenomenon. Such behavior can be interpreted as a model feature, offering users a simple way to customize the image generation and reflect their own cultural background. Yet, malicious users or service providers may also try to intentionally bias the image generation. One goal might be to create racist stereotypes by replacing Latin characters with similarly-looking characters from non-Latin scripts, so-called homoglyphs. To mitigate such unnoticed script attacks, we propose a novel homoglyph unlearning method to fine-tune a text encoder, making it robust against homoglyph manipulations.},
}

@article{Talia2023,
  title = {From Algorithms to Thinking Machines: The New Digital Power},
  author = {Talia, Domenico},
  year = {2023},
  volume = {54},
  publisher = {Association for Computing Machinery},
  abstract = {This book introduces and provides an analysis of the basic concepts of algorithms, data, and computation and discusses the role of algorithms in ruling and shaping our world. It provides a clear understanding of the power and impact on humanity of the pervasive use of algorithms.From Algorithms to Thinking Machines combines a layman’s approach with a well-founded scientific description to discuss both principles and applications of algorithms, Big Data, and machine intelligence. The book provides a clear and deep description of algorithms, software systems, data-driven applications, machine learning, and data science concepts, as well as the evolution and impact of artificial intelligence.After introducing computing concepts, the book examines the relationships between algorithms and human work, discussing how jobs are being affected and how computers and software programs are influencing human life and the labor sphere. Topics such as value alignment, collective intelligence, Big Data impact, automatic decision methods, social control, and political uses of algorithms are illustrated and discussed at length without excessive technical detail. Issues related to how corporations, governments, and autocratic regimes are exploiting algorithms and machine intelligence methods to influence people, laws, and markets are extensively addressed. Ethics principles in software programming and human value insertion into artificial intelligence algorithms are also discussed.},
}

@article{Cai2025,
  title = {Automated Program Refinement: Guide and Verify Code Large Language Model with Refinement Calculus},
  author = {Cai, Yufan and Hou, Zhe and Sanan, David and Luan, Xiaokun and Lin, Yun and Sun, Jun and Dong, Jin Song},
  year = {2025},
  journal = {Proc. ACM Program. Lang.},
  volume = {9},
  doi = {10.1145/3704905},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3704905},
  abstract = {Recently, the rise of code-centric Large Language Models (LLMs) has reshaped the software engineering world with low-barrier tools like Copilot that can easily generate code. However, there is no correctness guarantee for the code generated by LLMs, which suffer from the hallucination problem, and their output is fraught with risks. Besides, the end-to-end process from specification to code through LLMs is a non-transparent and uncontrolled black box. This opacity makes it difficult for users to understand and trust the generated code. Addressing these challenges is both necessary and critical. In contrast, program refinement transforms high-level specification statements into executable code while preserving correctness. Traditional tools for program refinement are primarily designed for formal methods experts and lack automation and extensibility. We apply program refinement to guide LLM and validate the LLM-generated code while transforming refinement into a more accessible and flexible framework.                To initiate this vision, we propose Refine4LLM, an approach that aims to:                (1) Formally refine the specifications,                (2) Automatically prompt and guide the LLM using refinement calculus,                (3) Interact with the LLM to generate the code,                (4) Verify that the generated code satisfies the constraints, thus guaranteeing its correctness,                (5) Learn and build more advanced refinement laws to extend the refinement calculus.                We evaluated Refine4LLM against the state-of-the-art baselines on program refinement and LLMs benchmarks.The experiment results show that Refine4LLM can efficiently generate more robust code and reduce the time for refinement and verification.},
}

@article{Menzies2024,
  title = {SEA4DQ 2024 Workshop Summary},
  author = {Menzies, Tim and Xu, Bowen and Jin Kang, Hong and Zhang, Jie M. and Gesi, Jiri and Sen, Sagar and Cassoli, Beatriz and Jourdan, Nicolas and Shi, Jieke and Nguyen, Phu and Golendukhina, Valentina},
  year = {2024},
  journal = {SIGSOFT Softw. Eng. Notes},
  volume = {49},
  pages = {29–30},
  doi = {10.1145/3696117.3696125},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3696117.3696125},
  abstract = {Welcome to the sixth edition of the workshop on Machine Learning Techniques for Software Quality Evaluation (SEA4DQ 2024), held in Brazil, July 16th, 2024, co-located with ESEC / FSE 2024 [1]. Three papers from all over the world were submitted, and all of them were accepted based on the overall positive reviews. The program also featured two keynotes by Denys Poshyvanyk on Towards an Interpretable Science of Deep Learning for Software Engineering: A Causal Inference View and Qinghua Lu on Responsible AI Engineering from A Data Perspective.},
}

@article{Feuer2024,
  title = {ArcheType: A Novel Framework for Open-Source Column Type Annotation Using Large Language Models},
  author = {Feuer, Benjamin and Liu, Yurong and Hegde, Chinmay and Freire, Juliana},
  year = {2024},
  journal = {Proc. VLDB Endow.},
  volume = {17},
  pages = {2279–2292},
  doi = {10.14778/3665844.3665857},
  publisher = {VLDB Endowment},
  url = {https://doi.org/10.14778/3665844.3665857},
  abstract = {Existing deep-learning approaches to semantic column type annotation (CTA) have important shortcomings: they rely on semantic types which are fixed at training time; require a large number of training samples per type; incur high run-time inference costs; and their performance can degrade when evaluated on novel datasets, even when types remain constant. Large language models have exhibited strong zero-shot classification performance on a wide range of tasks and in this paper we explore their use for CTA. We introduce ArcheType, a simple, practical method for context sampling, prompt serialization, model querying, and label remapping, which enables large language models to solve CTA problems in a fully zero-shot manner. We ablate each component of our method separately, and establish that improvements to context sampling and label remapping provide the most consistent gains. ArcheType establishes a new state-of-the-art performance on zero-shot CTA benchmarks (including three new domain-specific benchmarks which we release along with this paper), and when used in conjunction with classical CTA techniques, it outperforms a SOTA DoDuo model on the fine-tuned SOTAB benchmark.},
}

@article{Huang2024_02,
  title = {Optimizing Numerical Estimation and Operational Efficiency in the Legal Domain through Large Language Models},
  author = {Huang, Jia-Hong and Yang, Chao-Chun and Shen, Yixian and Pacces, Alessio M. and Kanoulas, Evangelos},
  year = {2024},
  pages = {4554–4562},
  doi = {10.1145/3627673.3680025},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3627673.3680025},
  abstract = {The legal landscape encompasses a wide array of lawsuit types, presenting lawyers with challenges in delivering timely and accurate information to clients, particularly concerning critical aspects like potential imprisonment duration or financial repercussions. Compounded by the scarcity of legal experts, there's an urgent need to enhance the efficiency of traditional legal workflows. Recent advances in deep learning, especially Large Language Models (LLMs), offer promising solutions to this challenge. Leveraging LLMs' mathematical reasoning capabilities, we propose a novel approach integrating LLM-based methodologies with specially designed prompts to address precision requirements in legal Artificial Intelligence (LegalAI) applications. The proposed work seeks to bridge the gap between traditional legal practices and modern technological advancements, paving the way for a more accessible, efficient, and equitable legal system. To validate this method, we introduce a curated dataset tailored to precision-oriented LegalAI tasks, serving as a benchmark for evaluating LLM-based approaches. Extensive experimentation confirms the efficacy of our methodology in generating accurate numerical estimates within the legal domain, emphasizing the role of LLMs in streamlining legal processes and meeting the evolving demands of LegalAI. Github: https://github.com/Jhhuangkay/Optimizing-Numerical-Estimation-and-Operational-Efficiency-in-the-Legal-Domain.},
}

@article{Peris2023,
  title = {Privacy in the Time of Language Models},
  author = {Peris, Charith and Dupuy, Christophe and Majmudar, Jimit and Parikh, Rahil and Smaili, Sami and Zemel, Richard and Gupta, Rahul},
  year = {2023},
  pages = {1291–1292},
  doi = {10.1145/3539597.3575792},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3539597.3575792},
  abstract = {Pretrained large language models (LLMs) have consistently shown state-of-the-art performance across multiple natural language processing (NLP) tasks. These models are of much interest for a variety of industrial applications that use NLP as a core component. However, LLMs have also been shown to memorize portions of their training data, which can contain private information. Therefore, when building and deploying LLMs, it is of value to apply privacy-preserving techniques that protect sensitive data.In this talk, we discuss privacy measurement and preservation techniques for LLMs that can be applied in the context of industrial applications and present case studies of preliminary solutions. We discuss select strategies and metrics relevant for measuring memorization in LLMs that can, in turn, be used to measure privacy-risk in these models. We then discuss privacy-preservation techniques that can be applied at different points of the LLM training life-cycle; including our work on an algorithm for fine-tuning LLMs with improved privacy. In addition, we discuss our work on privacy-preserving solutions that can be applied to LLMs during inference and are feasible for use at run time.},
}

@article{Siddiq2024,
  title = {Re(gEx|DoS)Eval: Evaluating Generated Regular Expressions and their Proneness to DoS Attacks},
  author = {Siddiq, Mohammed Latif and Zhang, Jiahao and Roney, Lindsay and Santos, Joanna C. S.},
  year = {2024},
  pages = {52–56},
  doi = {10.1145/3639476.3639757},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3639476.3639757},
  abstract = {With the recent advances of code generation techniques based on Large Language Models (LLMs), developers are using them for a vast range of tasks, including regex generation. Despite the efforts to generate regexes from natural language, there is no benchmark for LLMs with real-world data and robust test sets. Moreover, a regex can be prone to Denial of Service (DoS) attacks due to catastrophic backtracking. Hence, we need a systematic evaluation process to evaluate the correctness and security of the regexes generated by the language models. In this paper, we describe Re(gEx|DoS)Eval, a framework that includes a dataset of 762 regex descriptions (prompts) from real users, refined prompts with examples, and a robust set of tests. We introduce the pass@k and vulnerable@k metrics to evaluate the generated regexes based on the functional correctness and proneness of ReDoS attacks. Moreover, we demonstrate the Re(gEx|DoS)Eval with three LLMs (T5, Phi, and GPT-3), and describe the future plans to extend this framework.},
}

@article{Pedrassoli Chitayat2024,
  title = {From Passive Viewer to Active Fan: Towards the Design and Large-Scale Evaluation of Interactive Audience Experiences in Esports and Beyond},
  author = {Pedrassoli Chitayat, Alan and Coates, Alistair and Block, Florian and Drachen, Anders and Walker, James Alfred and Dean, James and Mcconachie, Mark and York, Peter},
  year = {2024},
  pages = {94–107},
  doi = {10.1145/3639701.3656318},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3639701.3656318},
  abstract = {Esports - competitive video games watched by online audiences - are the fastest growing form of mainstream entertainment. Esports coverage is predominantly delivered via online video streaming platforms which include interactive elements. However, there is limited understanding of how audiences engage with such interactive content. This paper presents a large-scale case study of an interactive data-driven streaming extension developed for Dota 2, reaching over 300,000 people during the DreamLeague Season 15 DPC Western Europe tournament. The extension provides interactive live statistics, analysis and highlights reels of ongoing matches. This paper presents an analysis of audience telemetry collected over the course of the four week tournament, introducing a novel approach to analysing usage data delivered seamlessly in conjunction to a linear broadcast feed. The work presented advances our general understanding of the evolving consumption patterns in esports, and leverages esports as a lens to understand future challenges and opportunities in interactive viewing across sports and entertainment.},
}

@article{Fan2024,
  title = {Giving without Notifying: Assessing Compliance of Data Transmission in Android Apps},
  author = {Fan, Ming and Shi, Jifei and Wang, Yin and Yu, Le and Zhang, Xicheng and Wang, Haijun and Jin, Wuxia and Liu, Ting},
  year = {2024},
  pages = {1595–1606},
  doi = {10.1145/3691620.3695528},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3691620.3695528},
  abstract = {Mobile apps often access personal information to meet business needs, raising concerns about privacy breaches. Compliance detection methods are proposed to check for inconsistencies between program code and privacy policies. However, existing methods face challenges with the low efficiency of static data flow analysis tools and often neglect physical data transmission destinations.To address these issues, we propose an automated compliance detection method called GNChecker. It uses an efficient static data flow analysis technique with a segmentation strategy, significantly reducing the search scope and improving efficiency. Additionally, a fine-grained consistency detection framework is proposed by integrating static data flow and dynamic traffic flow results into a unified tuple form, i.e., (information type, transmission address). Evaluation results on 50 popular apps show that GNChecker outperforms state-of-the-art data flow analysis tools. Among 1,134 real-world apps, GNChecker identified 1,410 true non-compliant transmission behaviors in 379 apps, significantly surpassing existing compliance detection tools.},
}

@article{Ding2023,
  title = {HPC-GPT: Integrating Large Language Model for High-Performance Computing},
  author = {Ding, Xianzhong and Chen, Le and Emani, Murali and Liao, Chunhua and Lin, Pei-Hung and Vanderbruggen, Tristan and Xie, Zhen and Cerpa, Alberto and Du, Wan},
  year = {2023},
  pages = {951–960},
  doi = {10.1145/3624062.3624172},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3624062.3624172},
  abstract = {Large Language Models (LLMs), including the LLaMA model, have exhibited their efficacy across various general-domain natural language processing (NLP) tasks. However, their performance in high-performance computing (HPC) domain tasks has been less than optimal due to the specialized expertise required to interpret the model’s responses. In response to this challenge, we propose HPC-GPT, a novel LLaMA-based model that has been supervised fine-tuning using generated QA (Question-Answer) instances for the HPC domain. To evaluate its effectiveness, we concentrate on two HPC tasks: managing AI models and datasets for HPC, and data race detection. By employing HPC-GPT, we demonstrate comparable performance with existing methods on both tasks, exemplifying its excellence in HPC-related scenarios. Our experiments on open-source benchmarks yield extensive results, underscoring HPC-GPT’s potential to bridge the performance gap between LLMs and HPC-specific tasks. With HPC-GPT, we aim to pave the way for LLMs to excel in HPC domains, simplifying the utilization of language models in complex computing applications.},
}

@article{Xi2024,
  title = {Parameter-free Zero-shot 3D Model Classification Based on Multi-view Representation},
  author = {Xi, Shuting and Bai, Jing and Yan, Hao},
  year = {2024},
  pages = {134–140},
  doi = {10.1145/3700035.3700057},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3700035.3700057},
  abstract = {The large-scale pre-trained models have demonstrated excellent performance in processing visual and language tasks in open-world scenarios. However, recent studies have shown that there are limitations in using comparative language-vision pretrained models for zero-shot 3D model classification, mainly in ignoring the multi-view independent information of 3D models, and using only a single template for text prompts, which fails to exploit the rich contextual information provided by the language, and restricts the matching of visual and semantic information. In order to solve these problems, this paper proposes a new framework of Parameter-free zero-shot 3D model classification based on “interaction first, fusion later”, which utilizes a pre-trained visual encoder to extract multi-view visual features, and then explores contextual information from multiple perspectives with the help of semantic comprehension of a large language model, establishes fine-grained associations between independent views and prompts through view-by-view interaction with text features, and integrates the independent decision results among views through decision-level fusion strategy to realize zero-shot classification. Without any 3D training, the proposed method achieves 78.8% accuracy on the ZS3D dataset, and 53.2%, 40.3%, and 26.3% accuracy on the three sub-datasets of Ali, which verifies the generalizability and effectiveness of the proposed algorithm.},
}

@article{Liu2025,
  title = {Public Datasets for Cloud Computing: A Comprehensive Survey},
  author = {Liu, Guozhi and Lin, Weiwei and Zhang, Haotong and Lin, Jianpeng and Peng, Shaoliang and Li, Keqin},
  year = {2025},
  journal = {ACM Comput. Surv.},
  volume = {57},
  doi = {10.1145/3719003},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3719003},
  abstract = {Publicly available datasets are vital to researchers because they permit the testing of new algorithms under a variety of conditions and ensure the verifiability and reproducibility of scientific experiments. In cloud computing research, there is a particular dependence on obtaining load traces and network traces from real cloud computing clusters, which are used for designing energy efficiency prediction, workload analysis, and anomaly detection solutions. To address the current lack of a comprehensive overview and thorough analysis of cloud computing datasets and to gain insight into their current status and future trends, in this article, we provide a comprehensive survey of existing publicly cloud computing datasets. First, we utilize a systematic mapping approach to analyze 968 scientific papers from 6 scientific databases, resulting in the retrieval of 42 datasets related to cloud computing. Second, we categorize these datasets based on 11 characteristics to assist researchers in quickly finding datasets suitable for their specific needs. Third, we provide detailed descriptions of each dataset to assist researchers in gaining a clearer understanding of their characteristics. Fourth, we select 12 mainstream datasets and conduct a comprehensive analysis and comparison of their characteristics. Finally, we discuss the weaknesses of existing datasets, identify challenges, provide recommendations for long-term dataset maintenance and updates, and outline directions for the future creation of new cloud computing datasets. Related resources are available at .},
}

@article{Chen2024_01,
  title = {Open Metaverse: Issues, Evolution, and Future},
  author = {Chen, Zefeng and Gan, Wensheng and Sun, Jiayi and Wu, Jiayang and Yu, Philip S.},
  year = {2024},
  pages = {1351–1360},
  doi = {10.1145/3589335.3651898},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3589335.3651898},
  abstract = {With the content evolution on the web and the Internet, there is a need for cyberspace that can be used to work, live, and play in digital worlds regardless of geography. The Metaverse provides the possibility of the future Internet, representing a future trend for the web and the Internet. In the future, the Metaverse is a dataspace where the real and the virtual are combined instead of a virtual space. In this paper, we have a comprehensive survey of the compelling Metaverse, including issues of the Metaverse and Metaverse's evolution and future. We hope this survey can provide some helpful prospects and insightful directions for the Metaverse.},
}

@article{Aldeen2024,
  title = {End-Users Know Best: Identifying Undesired Behavior of Alexa Skills Through User Review Analysis},
  author = {Aldeen, Mohammed and Young, Jeffrey and Liao, Song and Chang, Tsu-Yao and Cheng, Long and Cai, Haipeng and Luo, Xiapu and Hu, Hongxin},
  year = {2024},
  journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  volume = {8},
  doi = {10.1145/3678517},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3678517},
  abstract = {The Amazon Alexa marketplace has grown rapidly in recent years due to third-party developers creating large amounts of content and publishing directly to a skills store. Despite the growth of the Amazon Alexa skills store, there have been several reported security and usability concerns, which may not be identified during the vetting phase. However, user reviews can offer valuable insights into the security &amp; privacy, quality, and usability of the skills. To better understand the effects of these problematic skills on end-users, we introduce ReviewTracker, a tool capable of discerning and classifying semantically negative user reviews to identify likely malicious, policy violating, or malfunctioning behavior on Alexa skills. ReviewTracker employs a pre-trained FastText classifier to identify different undesired skill behaviors. We collected over 700,000 user reviews spanning 6 years with more than 200,000 negative sentiment reviews. ReviewTracker was able to identify 17,820 reviews reporting violations related to Alexa policy requirements across 2,813 skills, and 131,855 reviews highlighting different types of user frustrations associated with 9,294 skills. In addition, we developed a dynamic skill testing framework using ChatGPT to conduct two distinct types of tests on Alexa skills: one using a software-based simulation for interaction to explore the actual behaviors of skills and another through actual voice commands to understand the potential factors causing discrepancies between intended skill functionalities and user experiences. Based on the number of the undesired skill behavior reviews, we tested the top identified problematic skills and detected more than 228 skills violating at least one policy requirement. Our results demonstrate that user reviews could serve as a valuable means to identify undesired skill behaviors.},
}

@article{Giner-Miguelez2023,
  title = {DataDoc Analyzer: A Tool for Analyzing the Documentation of Scientific Datasets},
  author = {Giner-Miguelez, Joan and G\'{o}mez, Abel and Cabot, Jordi},
  year = {2023},
  pages = {5046–5050},
  doi = {10.1145/3583780.3614737},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3583780.3614737},
  abstract = {Recent public regulatory initiatives and relevant voices in the ML community have identified the need to document datasets according to several dimensions to ensure the fairness and trustworthiness of machine learning systems. In this sense, the data-sharing practices in the scientific field have been quickly evolving in the last years, with more and more research works publishing technical documentation together with the data for replicability purposes. However, this documentation is written in natural language, and its structure, content focus, and composition vary, making them challenging to analyze.We present DataDoc Analyzer, a tool for analyzing the documentation of scientific datasets by extracting the details of the main dimensions required to analyze the fairness and potential biases. We believe that our tool could help improve the quality of scientific datasets, aid dataset curators during its documentation process, and be a helpful tool for empirical studies on the overall quality of the datasets used in the ML field. The tool implements an ML pipeline that uses Large Language Models at its core for information retrieval. DataDoc is open-source, and a public demo is published online.},
}

@article{Ma2024,
  title = {API Misuse Detection via Probabilistic Graphical Model},
  author = {Ma, Yunlong and Tian, Wentong and Gao, Xiang and Sun, Hailong and Li, Li},
  year = {2024},
  pages = {88–99},
  doi = {10.1145/3650212.3652112},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3650212.3652112},
  abstract = {API misuses can cause a range of issues in software development, including program crashes, bugs, and vulnerabilities. Different approaches have been developed to automatically detect API misuses by checking the program against usage rules extracted from extensive codebase or API documents. However, these mined rules may not be precise or complete, leading to high false positive/negative rates. In this paper, we propose a novel solution to this problem by representing the mined API usage rules as a probabilistic graphical model, where each rule's probability value represents its trustworthiness of being correct.   Our approach automatically constructs probabilistic usage rules by mining codebase and documents, and aggregating knowledge from different sources.   Here, the usage rules obtained from the codebase initialize the probabilistic model, while the knowledge from the documents serves as a supplement for adjusting and complementing the probabilities accordingly.   We evaluate our approach on the MuBench benchmark.   Experimental results show that our approach achieves 42.0% precision and 54.5% recall, significantly outperforming state-of-the-art approaches.},
}

@article{Abbasi2025,
  title = {A Longitudinal Study of Child Wellbeing Assessment via Online Interactions with a Social Robot},
  author = {Abbasi, Nida Itrat and Laban, Guy and Ford, Tamsin and Jones, Peter B. and Gunes, Hatice},
  year = {2025},
  journal = {J. Hum.-Robot Interact.},
  doi = {10.1145/3722123},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3722123},
  abstract = {Socially Assistive Robots are studied in different Child-Robot Interaction settings. However, logistical constraints limit accessibility, particularly affecting timely support for mental wellbeing. In this work, we have investigated whether online interactions with a robot can be used for the assessment of mental wellbeing in children. The children (N=40, 20 girls and 20 boys; 8-13 years) interacted with the Nao robot (30-45 mins) over three sessions, at least a week apart. Audio-visual recordings were collected throughout the sessions that concluded with the children answering user perception questionnaires pertaining to their anxiety towards the robot, and the robot's abilities. We divided the participants into three wellbeing clusters (low, med and high tertiles) using their responses to the Short Moods and Feelings Questionnaire (SMFQ) and further analysed how their wellbeing and their perceptions of the robot changed over the wellbeing tertiles, across sessions and across participants’ gender. Our primary findings suggest that (I) online mediated-interactions with robots can be effective in assessing children's mental wellbeing over time, and (II) children's overall perception of the robot either improved or remained consistent across time. Supplementary exploratory analyses have also revealed that the gender of the children affected their wellbeing assessments with interactions effectively distinguishing between varying levels of wellbeing for both boys and girls for the first session and only for boys during the second session. The analyses have also revealed that girls have a higher opinion of the robot as a confidante as compared with boys. Findings from this work affirm the potential of using online mediated interactions with robots for the assessment of the mental wellbeing of children.},
}

@article{Thiagarajan2025,
  title = {The Aleph: Decoding Geographic Information from DNS PTR Records Using Large Language Models},
  author = {Thiagarajan, Kedar and Carisimo, Esteban and Bustamante, Fabi\'{a}n E.},
  year = {2025},
  journal = {Proc. ACM Netw.},
  volume = {3},
  doi = {10.1145/3709374},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3709374},
  abstract = {Geolocating network devices is essential for various research areas. Yet, despite notable advancements, it continues to be one of the most challenging issues for experimentalists. An approach for geolocating that has proved effective is leveraging geolocating hints in PTR records associated with network devices. Extracting and interpreting geo-hints from PTR records is challenging because the labels are primarily intended for human interpretation rather than computational processing. Additionally, a lack of standardization across operators -- and even within a single operator, due to factors like rebranding, mergers, and acquisitions -- complicates the process. We argue that Large Language Models (LLMs), rather than humans, are better equipped to identify patterns in DNS PTR records, and significantly scale the coverage of tools like Hoiho. We introduce The Aleph, an approach and system for network device geolocation that utilizes information embedded in PTR records. The Aleph leverages LLMs to classify PTR records, generate regular expressions for these classes, and establish hint-to-location mapping per operator. We present results showing the applicability of using LLMs as a scalable approach to leverage PTR records for infrastructure geolocation.},
}

@article{Zmigrod2024,
  title = {Translating between SQL Dialects for Cloud Migration},
  author = {Zmigrod, Ran and Alamir, Salwa and Liu, Xiaomo},
  year = {2024},
  pages = {189–191},
  doi = {10.1145/3639477.3639727},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3639477.3639727},
  abstract = {Migrations of systems from on-site premises to the cloud has been a fundamental endeavor by many industrial institutions. A crucial component of such cloud migrations is the transition of databases to be hosted online. In this work, we consider the difficulties of this migration for SQL databases. While SQL is one of the prominent methods for storing database procedures, there are a plethora of different SQL dialects (e.g., MySQL, Postgres, etc.) which can complicate migrations when the on-premise SQL dialect differs to the dialect hosted on the cloud. Tools exist by common cloud provides such as AWS and Azure to aid in translating between dialects in order to mitigate the majority of the difficulties. However, these tools do not successfully translate 100% of the code. Consequently, software engineers must manually convert the remainder of the untranslated database. For large organizations, this task quickly becomes intractable and so more innovative solutions are required. We consider this challenge a novel yet vital industrial research problem for any large corporation that is considering cloud migrations. Furthermore, we introduce potential avenues of research to tackle this challenge that have yielded promising preliminary results.},
}

@article{Jin2024,
  title = {SpeechCraft: A Fine-Grained Expressive Speech Dataset with Natural Language Description},
  author = {Jin, Zeyu and Jia, Jia and Wang, Qixin and Li, Kehan and Zhou, Shuoyi and Zhou, Songtao and Qin, Xiaoyu and Wu, Zhiyong},
  year = {2024},
  pages = {1255–1264},
  doi = {10.1145/3664647.3681674},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3664647.3681674},
  abstract = {Speech-language multi-modal learning presents a significant challenge due to the fine nuanced information inherent in speech styles. Therefore, a large-scale dataset providing elaborate comprehension of speech style is urgently needed to facilitate insightful interplay between speech audio and natural language. However, constructing such datasets presents a major trade-off between large-scale data collection and high-quality annotation. To tackle this challenge, we propose an automatic speech annotation system for expressiveness interpretation that annotates in-the-wild speech clips with expressive and vivid human language descriptions. Initially, speech audios are processed by a series of expert classifiers and captioning models to capture diverse speech characteristics, followed by a fine-tuned LlaMA for customized annotation generation. Unlike previous tag/templet-based annotation frameworks with limited information and diversity, our system provides in-depth understandings of speech style through tailored natural language descriptions, thereby enabling accurate and voluminous data generation for large model training. With this system, we create SpeechCraft, a fine-grained bilingual expressive speech dataset. It is distinguished by highly descriptive natural language style prompts, containing approximately 2,000 hours of audio data and encompassing over two million speech clips. Extensive experiments demonstrate that the proposed dataset significantly boosts speech-language task performance in stylist speech synthesis and speech style understanding.},
}

@article{Razniewski2024,
  title = {Completeness, Recall, and Negation in Open-world Knowledge Bases: A Survey},
  author = {Razniewski, Simon and Arnaout, Hiba and Ghosh, Shrestha and Suchanek, Fabian},
  year = {2024},
  journal = {ACM Comput. Surv.},
  volume = {56},
  doi = {10.1145/3639563},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3639563},
  abstract = {General-purpose knowledge bases (KBs) are a cornerstone of knowledge-centric AI. Many of them are constructed pragmatically from web sources and are thus far from complete. This poses challenges for the consumption as well as the curation of their content. While several surveys target the problem of completing incomplete KBs, the first problem is arguably to know whether and where the KB is incomplete in the first place, and to which degree. In this survey, we discuss how knowledge about completeness, recall, and negation in KBs can be expressed, extracted, and inferred. We cover (i) the logical foundations of knowledge representation and querying under partial closed-world semantics; (ii) the estimation of this information via statistical patterns; (iii) the extraction of information about recall from KBs and text; (iv) the identification of interesting negative statements; and (v) relaxed notions of relative recall. This survey is targeted at two types of audiences: (1) practitioners who are interested in tracking KB quality, focusing extraction efforts, and building quality-aware downstream applications; and (2) data management, knowledge base, and semantic web researchers who wish to understand the state-of-the-art of knowledge bases beyond the open-world assumption. Consequently, our survey presents both fundamental methodologies and the results that they have produced, and gives practice-oriented recommendations on how to choose between different approaches for a problem at hand.},
}

@article{Rula2023,
  title = {Procedural Text Mining with Large Language Models},
  author = {Rula, Anisa and D'Souza, Jennifer},
  year = {2023},
  pages = {9–16},
  doi = {10.1145/3587259.3627572},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3587259.3627572},
  abstract = {Recent advancements in the field of Natural Language Processing, particularly the development of large-scale language models that are pretrained on vast amounts of knowledge, are creating novel opportunities within the realm of Knowledge Engineering. In this paper, we investigate the usage of large language models (LLMs) in both zero-shot and in-context learning settings to tackle the problem of extracting procedures from unstructured PDF text in an incremental question-answering fashion. In particular, we leverage the current state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model, accompanied by two variations of in-context learning that involve an ontology with definitions of procedures and steps and a limited number of samples of few-shot learning. The findings highlight both the promise of this approach and the value of the in-context learning customisations. These modifications have the potential to significantly address the challenge of obtaining sufficient training data, a hurdle often encountered in deep learning-based Natural Language Processing techniques for procedure extraction.},
}

@article{Deng2024,
  title = {LakeBench: A Benchmark for Discovering Joinable and Unionable Tables in Data Lakes},
  author = {Deng, Yuhao and Chai, Chengliang and Cao, Lei and Yuan, Qin and Chen, Siyuan and Yu, Yanrui and Sun, Zhaoze and Wang, Junyi and Li, Jiajun and Cao, Ziqi and Jin, Kaisen and Zhang, Chi and Jiang, Yuqing and Zhang, Yuanfang and Wang, Yuping and Yuan, Ye and Wang, Guoren and Tang, Nan},
  year = {2024},
  journal = {Proc. VLDB Endow.},
  volume = {17},
  pages = {1925–1938},
  doi = {10.14778/3659437.3659448},
  publisher = {VLDB Endowment},
  url = {https://doi.org/10.14778/3659437.3659448},
  abstract = {Discovering tables from poorly maintained data lakes is a significant challenge in data management. Two key tasks are identifying joinable and unionable tables, crucial for data integration, analysis, and machine learning. However, there's a lack of a comprehensive benchmark for evaluating existing methods. To address this, we introduce LakeBench, a large-scale table discovery benchmark. It evaluates effectiveness, efficiency, and scalability of table join &amp; union search methods. With over 16 million real tables, LakeBench is 1,600X larger than existing datasets and 100X larger in storage size. It includes synthesized and real queries with ground truth, totaling more than 10 thousand queries - 10X more than used in any existing evaluation. We spent over 7,500 human hours labeling these queries and constructing diverse query categories for thorough evaluation. Our benchmark thoroughly evaluates state-of-the-art table discovery methods, providing insights into their performance and highlighting research opportunities.},
}

@article{Miskell2024,
  title = {Automated Framework to Extract Software Requirements from Source Code},
  author = {Miskell, Cameron and Diaz, Richard and Ganeriwala, Parth and Slhoub, Khaled and Nembhard, Fitzroy},
  year = {2024},
  pages = {130–134},
  doi = {10.1145/3639233.3639242},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3639233.3639242},
  abstract = {Software maintenance and innovation are constant challenges across industries, especially as programming languages evolve with technology. Similarly, poor lexicon quality degrades program comprehension, increasing the effort required by developers to improve existing software products. To address these challenges, we propose a novel automated framework that extracts software requirements directly from source code using a baseline AI language model applied to a Java code base. Leveraging natural language processing techniques, the framework validates programs and generates easily readable requirements by analyzing file contents. The framework enhances agility and flexibility by providing comprehensive documentation for existing software systems. It caters to both experienced and less-experienced developers, offering an intuitive graphical user interface and enabling efficient identification and resolution of errors. The resulting output facilitates natural interaction through language processing. By automating the extraction process, the framework allows developers to better understand software systems, make informed decisions, and adapt to evolving needs.},
}

@article{Vijayan2024,
  title = {A Prompt Engineering Approach for Structured Data Extraction from Unstructured Text Using Conversational LLMs},
  author = {Vijayan, Aishwarya},
  year = {2024},
  pages = {183–189},
  doi = {10.1145/3639631.3639663},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3639631.3639663},
  abstract = {This paper aims to extract structured information from unstructured text written in natural language. This extracted information can then be stored in a database and queried using database access languages such as SQL to derive meaningful answers to questions that might arise around a specific activity related to that unstructured text. For example, an email chain discussing a planned trip could be used to extract a record with fields such as who is taking the trip, where they are departing from, when they are departing, where they are going, and when they are arriving. This information could then be used to create a relation of trips that could be queried by SQL. This paper uses a prompt engineering approach using conversational LLMs for extracting the relevant information related to travel and store the information into relational databases which can then be queried using SQL or any other query language. This initiative holds the promise to revolutionize the storage and retrieval of information with minimal effort. Currently, unstructured text is difficult to query and analyze. By extracting structured information from this text, it becomes much easier to work with.},
}

@article{Zhang2024_02,
  title = {Automated Mining of Structured Knowledge from Text in the Era of Large Language Models},
  author = {Zhang, Yunyi and Zhong, Ming and Ouyang, Siru and Jiao, Yizhu and Zhou, Sizhe and Ding, Linyi and Han, Jiawei},
  year = {2024},
  pages = {6644–6654},
  doi = {10.1145/3637528.3671469},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3637528.3671469},
  abstract = {Massive amount of unstructured text data are generated daily, ranging from news articles to scientific papers. How to mine structured knowledge from the text data remains a crucial research question. Recently, large language models (LLMs) have shed light on the text mining field with their superior text understanding and instruction-following ability. There are typically two ways of utilizing LLMs: fine-tune the LLMs with human-annotated training data, which is labor intensive and hard to scale; prompt the LLMs in a zero-shot or few-shot way, which cannot take advantage of the useful information in the massive text data. Therefore, it remains a challenge on automated mining of structured knowledge from massive text data in the era of large language models. In this tutorial, we cover the recent advancements in mining structured knowledge using language models with very weak supervision. We will introduce the following topics in this tutorial: (1) introduction to large language models, which serves as the foundation for recent text mining tasks, (2) ontology construction, which automatically enriches an ontology from a massive corpus, (3) weakly-supervised text classification in flat and hierarchical label space, (4) weakly-supervised information extraction, which extracts entity and relation structures.},
}

@article{Cheng2024,
  title = {Why Do Customers Return Products? Using Customer Reviews to Predict Product Return Behaviors},
  author = {Cheng, Hao-Fei and Krikon, Eyal and Murdock, Vanessa},
  year = {2024},
  pages = {12–22},
  doi = {10.1145/3627508.3638326},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3627508.3638326},
  abstract = {Product returns are an increasing environmental problem, as an estimated 25% of returned products end up as landfill&nbsp;[10]. Returns are expensive for retailers as well, and it is estimated that 15-40% of all online purchases are returned&nbsp;[34]. The problem could be mitigated by identifying issues with a product that are likely to lead to its return, before many have sold. Understanding and predicting return reasons can help identify manufacturing defects, misleading information in the product description or reviews, issues with a seller or shipping company, and customers who are habitual returners. While there has been much work to identify and predict return volume, little attention has been given to the reasons for the return. In this paper we explore how customer reviews could be used as signals to identify return reasons. We developed a multi-class classifier to predict return reasons, with a fine-tuned BERT-based model to encode customer review text as features. The classifier with customer review text yields an increase of more than 20% average precision over the baseline classifier with no reviews text. We also showed that we can use aggregated review information to predict product return in case the customer returning the product did not write a review. Lastly we show that reviews can be used to identify nuanced return reasons beyond what the customer indicated.},
}

@article{Hertling2023,
  title = {OLaLa: Ontology Matching with Large Language Models},
  author = {Hertling, Sven and Paulheim, Heiko},
  year = {2023},
  pages = {131–139},
  doi = {10.1145/3587259.3627571},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3587259.3627571},
  abstract = {Ontology (and more generally: Knowledge Graph) Matching is a challenging task where information in natural language is one of the most important signals to process. With the rise of Large Language Models, it is possible to incorporate this knowledge in a better way into the matching pipeline. A number of decisions still need to be taken, e.g., how to generate a prompt that is useful to the model, how information in the KG can be formulated in prompts, which Large Language Model to choose, how to provide existing correspondences to the model, how to generate candidates, etc. In this paper, we present a prototype that explores these questions by applying zero-shot and few-shot prompting with multiple open Large Language Models to different tasks of the Ontology Alignment Evaluation Initiative (OAEI). We show that with only a handful of examples and a well-designed prompt, it is possible to achieve results that are en par with supervised matching systems which use a much larger portion of the ground truth.},
}

@article{Liu2024,
  title = {MFTCoder: Boosting Code LLMs with Multitask Fine-Tuning},
  author = {Liu, Bingchang and Chen, Chaoyu and Gong, Zi and Liao, Cong and Wang, Huan and Lei, Zhichao and Liang, Ming and Chen, Dajun and Shen, Min and Zhou, Hailian and Jiang, Wei and Yu, Hang and Li, Jianguo},
  year = {2024},
  pages = {5430–5441},
  doi = {10.1145/3637528.3671609},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3637528.3671609},
  abstract = {Code LLMs have emerged as a specialized research field, with remarkable studies dedicated to enhancing model's coding capabilities through fine-tuning on pre-trained models. Previous fine-tuning approaches were typically tailored to specific downstream tasks or scenarios, which meant separate fine-tuning for each task, requiring extensive training resources and posing challenges in terms of deployment and maintenance. Furthermore, these approaches failed to leverage the inherent interconnectedness among different code-related tasks. To overcome these limitations, we present a multi-task fine-tuning framework, MFTCoder, that enables simultaneous and parallel fine-tuning on multiple tasks. By incorporating various loss functions, we effectively address common challenges in multi-task learning, such as data imbalance, varying difficulty levels, and inconsistent convergence speeds. Extensive experiments have conclusively demonstrated that our multi-task fine-tuning approach outperforms both individual fine-tuning on single tasks and fine-tuning on a mixed ensemble of tasks. Moreover, MFTCoder offers efficient training capabilities, including efficient data tokenization modes and parameter efficient fine-tuning (PEFT) techniques, resulting in significantly improved speed compared to traditional fine-tuning methods. MFTCoder seamlessly integrates with several mainstream open-source LLMs, such as CodeLLama and Qwen. Our MFTCoder fine-tuned CodeFuse-DeepSeek-33B claimed the top spot on the Big Code Models Leaderboard ranked by WinRate as of January 30, 2024. MFTCoder is open-sourced at https://github.com/codefuse-ai/MFTCOder},
}

@article{Atif2023,
  title = {BeamQA: Multi-hop Knowledge Graph Question Answering with Sequence-to-Sequence Prediction and Beam Search},
  author = {Atif, Farah and El Khatib, Ola and Difallah, Djellel},
  year = {2023},
  pages = {781–790},
  doi = {10.1145/3539618.3591698},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3539618.3591698},
  abstract = {Knowledge Graph Question Answering (KGQA) is a task that aims to answer natural language queries by extracting facts from a knowledge graph. Current state-of-the-art techniques for KGQA rely on text-based information from graph entity and relations labels, as well as external textual corpora. By reasoning over multiple edges in the graph, these can accurately rank and return the most relevant entities. However, one of the limitations of these methods is that they cannot handle the inherent incompleteness of real-world knowledge graphs and may lead to inaccurate answers due to missing edges. To address this issue, recent advances in graph representation learning have led to the development of systems that can use link prediction techniques to handle missing edges probabilistically, allowing the system to reason with incomplete information. However, existing KGQA frameworks that use such techniques often depend on learning a transformation from the query representation to the graph embedding space, which requires access to a large training dataset. We present BeamQA, an approach that overcomes these limitations by combining a sequence-to-sequence prediction model with beam search execution in the embedding space. Our model uses a pre-trained large language model and synthetic question generation. Our experiments demonstrate the effectiveness of BeamQA when compared to other KGQA methods on two knowledge graph question-answering datasets.},
}

@article{Cheng2024_01,
  title = {Neural-Symbolic Methods for Knowledge Graph Reasoning: A Survey},
  author = {Cheng, Kewei and Ahmed, Nesreen K. and Rossi, Ryan A. and Willke, Theodore and Sun, Yizhou},
  year = {2024},
  journal = {ACM Trans. Knowl. Discov. Data},
  volume = {18},
  doi = {10.1145/3686806},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3686806},
  abstract = {Neural symbolic knowledge graph (KG) reasoning offers a promising approach that combines the expressive power of symbolic reasoning with the learning capabilities inherent in neural networks. This survey provides a comprehensive overview of advancements, techniques, and challenges in the field of neural symbolic KG reasoning. The survey introduces the fundamental concepts of KGs and symbolic logic, followed by an exploration of three significant KG reasoning tasks: KG completion, complex query answering, and logical rule learning. For each task, we thoroughly discuss three distinct categories of methods: pure symbolic methods, pure neural approaches, and the integration of neural networks and symbolic reasoning methods known as neural-symbolic. We carefully analyze and compare the strengths and limitations of each category of methods to provide a comprehensive understanding. By synthesizing recent research contributions and identifying open research directions, this survey aims to equip researchers and practitioners with a comprehensive understanding of the state-of-the-art in neural symbolic KG reasoning, fostering future advancements in this interdisciplinary domain.},
}

@article{Lee2025,
  title = {Counselor-AI Collaborative Transcription and Editing System for Child Counseling Analysis},
  author = {Lee, Hyungjung and Lee, Jiyeon and Yang, Migyeong and Lee, Daeun and Song, Hayeon and Han, Youjin and Han, Jinyoung},
  year = {2025},
  pages = {425–445},
  doi = {10.1145/3708359.3712081},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3708359.3712081},
  abstract = {Psychological counseling, especially for children, heavily relies on capturing both verbal and non-verbal cues to understand and support each child’s emotional and developmental needs. Therefore, creating a detailed and accurate transcription for a child’s counseling session is crucial but often labor-intensive and time-consuming, which makes it challenging to maintain the consistency of counseling quality. Despite advancements in AI, current session analysis practices rely primarily on manual clinical assessments and struggle to accurately capture children’s verbal and non-verbal expressions. To address these challenges, we propose an AI-based expert support system designed to enhance child counseling analysis. The system comprises two key components: (i) a transcription generation model and (ii) an editable dashboard. The transcription generation model extracts verbal expressions from both children and counselors, verifies speakers’ identities, and objectively captures non-verbal cues using a Multimodal Large Language Model. The editable dashboard facilitates Counselor &amp; AI collaboration, where AI reduces human bias by providing objectivity, and counselors mitigate the risk of over-reliance on AI while maintaining oversight. This collaboration ultimately enhances workflow efficiency and leads to accurate counseling analyses. An evaluation with 48 child counselors demonstrates the system’s superior effectiveness and usability compared to existing services, with a majority expressing a strong intent to continue using our system. The system not only improves transcription accuracy but also supports more precise analysis of counseling sessions, enabling counselors to focus more on therapeutic engagements. These findings highlight the system’s potential to reduce the workload of child counselors, improve the quality of counseling services, and provide valuable resources for both individual counseling and counselor training. To the best of our knowledge, our study is the first to propose an AI-based expert support system optimized for generating transcriptions for child counseling analysis.},
}

@article{Setlur2023,
  title = {Olio: A Semantic Search Interface for Data Repositories},
  author = {Setlur, Vidya and Kanyuka, Andriy and Srinivasan, Arjun},
  year = {2023},
  doi = {10.1145/3586183.3606806},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3586183.3606806},
  abstract = {Search and information retrieval systems are becoming more expressive in interpreting user queries beyond the traditional weighted bag-of-words model of document retrieval. For example, searching for a flight status or a game score returns a dynamically generated response along with supporting, pre-authored documents contextually relevant to the query. In this paper, we extend this hybrid search paradigm to data repositories that contain curated data sources and visualization content. We introduce a semantic search interface, Olio, that provides a hybrid set of results comprising both auto-generated visualization responses and pre-authored charts to blend analytical question-answering with content discovery search goals. We specifically explore three search scenarios - question-and-answering, exploratory search, and design search over data repositories. The interface also provides faceted search support for users to refine and filter the conventional best-first search results based on parameters such as author name, time, and chart type. A preliminary user evaluation of the system demonstrates that Olio’s interface and the hybrid search paradigm collectively afford greater expressivity in how users discover insights and visualization content in data repositories.},
}

@article{Arbaoui2024,
  title = {Federated Learning Survey: A Multi-Level Taxonomy of Aggregation Techniques, Experimental Insights, and Future Frontiers},
  author = {Arbaoui, Meriem and Brahmia, Mohamed-el-Amine and Rahmoun, Abdellatif and Zghal, Mourad},
  year = {2024},
  journal = {ACM Trans. Intell. Syst. Technol.},
  volume = {15},
  doi = {10.1145/3678182},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3678182},
  abstract = {The emerging integration of Internet of Things (IoT) and AI has unlocked numerous opportunities for innovation across diverse industries. However, growing privacy concerns and data isolation issues have inhibited this promising advancement. Unfortunately, traditional centralized Machine Learning (ML) methods have demonstrated their limitations in addressing these hurdles. In response to this ever-evolving landscape, Federated Learning (FL) has surfaced as a cutting-edge ML paradigm, enabling collaborative training across decentralized devices. FL allows users to jointly construct AI models without sharing their local raw data, ensuring data privacy, network scalability, and minimal data transfer. One essential aspect of FL revolves around proficient knowledge aggregation within a heterogeneous environment. Yet, the inherent characteristics of FL have amplified the complexity of its practical implementation compared to centralized ML. This survey delves into three prominent clusters of FL research contributions: personalization, optimization, and robustness. The objective is to provide a well-structured and fine-grained classification scheme related to these research areas through a unique methodology for selecting related work. Unlike other survey papers, we employed a hybrid approach that amalgamates bibliometric analysis and systematic scrutinizing to find the most influential work in the literature. Therefore, we examine challenges and contemporary techniques related to heterogeneity, efficiency, security, and privacy. Another valuable asset of this study is its comprehensive coverage of FL aggregation strategies, encompassing architectural features, synchronization methods, and several federation motivations. To further enrich our investigation, we provide practical insights into evaluating novel FL proposals and conduct experiments to assess and compare aggregation methods under IID and non-IID data distributions. Finally, we present a compelling set of research avenues that call for further exploration to open up a treasure of advancement.},
}

@article{Jones2024,
  title = {What do we know about Hugging Face? A systematic literature review and quantitative validation of qualitative claims},
  author = {Jones, Jason and Jiang, Wenxin and Synovic, Nicholas and Thiruvathukal, George and Davis, James},
  year = {2024},
  pages = {13–24},
  doi = {10.1145/3674805.3686665},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3674805.3686665},
  abstract = {Background: Software Package Registries (SPRs) are an integral part of the software supply chain. These collaborative platforms unite contributors, users, and code for streamlined package management. Prior work has characterized the SPRs associated with traditional software, such as NPM (JavaScript) and PyPI (Python). Pre-Trained Model (PTM) Registries are an emerging class of SPR of increasing importance, because they support the deep learning supply chain. A growing body of empirical research has examined PTM registries from various angles, such as vulnerabilities, reuse processes, and evolution. However, no synthesis provides a systematic understanding of current knowledge. Furthermore, much of the existing research includes non-quantified qualitative observations. Aims: First, we aim to provide a systematic knowledge synthesis. Second, we quantify qualitative claims. Methods: We conducted a systematic literature review (SLR). We then observed that some of the claims are qualitative, lacking quantitative evidence. We identify quantifiable metrics associated with those claims, and measure in order to substantiate these claims. Results: We identify 12 claims about PTM reuse on the HuggingFace platform, 4 of which lack quantitative support. We tested 3 of these claims through a quantitative analysis, and directly compare the fourth with traditional software. Our most notable findings are: (1) PTMs have a significantly higher turnover rate than traditional software, indicating more rapid evolution; and (2) There is a strong correlation between documentation quality and PTM popularity. Conclusions: Our findings validate several qualitative research claims with concrete metrics, confirming prior research. Our measures motivate further research on the dynamics of PTM reuse.},
}

@article{Mussa2024,
  title = {ForestQB: Enhancing Linked Data Exploration through Graphical and Conversational UIs Integration},
  author = {Mussa, Omar and Rana, Omer and Goossens, Benoit and Orozco Ter wengel, Pablo and Perera, Charith},
  year = {2024},
  journal = {ACM J. Comput. Sustain. Soc.},
  volume = {2},
  doi = {10.1145/3675759},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3675759},
  abstract = {This article introduces the Forest Query Builder (ForestQB), an innovative toolkit designed to enhance the exploration and application of observational Linked Data (LD) within the field of wildlife research and conservation. Addressing the challenges faced by non-experts in navigating Resource Description Framework (RDF) triplestores and executing SPARQL queries, ForestQB employs a novel integrated approach. This approach combines a graphical user interface (GUI) with a conversational user interface (CUI), thereby greatly simplifying the process of query formulation and making observational LD accessible to users without expertise in RDF or SPARQL. Developed through insights derived from a comprehensive ethnographic study involving wildlife researchers, ForestQB is specifically designed to improve the accessibility of SPARQL endpoints and facilitate the exploration of observational LD in wildlife research contexts. To evaluate the effectiveness of our approach, we conducted a user experiment. The results of this evaluation affirm that ForestQB is not only efficient and user-friendly but also plays a crucial role in eliminating barriers for users, facilitating the effective use of observational LD in wildlife conservation and extending its benefits to wider domains. (GitHub Link: github.com/i3omar/ForestQB).},
}

@article{Zhang2025_01,
  title = {Failure Diagnosis in Microservice Systems: A Comprehensive Survey and Analysis},
  author = {Zhang, Shenglin and Xia, Sibo and Fan, Wenzhao and Shi, Binpeng and Xiong, Xiao and Zhong, Zhenyu and Ma, Minghua and Sun, Yongqian and Pei, Dan},
  year = {2025},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  doi = {10.1145/3715005},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3715005},
  abstract = {Widely adopted for their scalability and flexibility, modern microservice systems present unique failure diagnosis challenges due to their independent deployment and dynamic interactions. This complexity can lead to cascading failures that negatively impact operational efficiency and user experience. Recognizing the critical role of fault diagnosis in improving the stability and reliability of microservice systems, researchers have conducted extensive studies and achieved a number of significant results. This survey provides an exhaustive review of 98 scientific papers from 2003 to the present, including a thorough examination and elucidation of the fundamental concepts, system architecture, and problem statement. It also includes a qualitative analysis of the dimensions, providing an in-depth discussion of current best practices and future directions, aiming to further its development and application. In addition, this survey compiles publicly available datasets, toolkits, and evaluation metrics to facilitate the selection and validation of techniques for practitioners.},
}

@article{Bartoldson2023,
  title = {Compute-efficient deep learning: algorithmic trends and opportunities},
  author = {Bartoldson, Brian R. and Kailkhura, Bhavya and Blalock, Davis},
  year = {2023},
  journal = {J. Mach. Learn. Res.},
  volume = {24},
  publisher = {JMLR.org},
  abstract = {Although deep learning has made great progress in recent years, the exploding economic and environmental costs of training neural networks are becoming unsustainable. To address this problem, there has been a great deal of research on algorithmically-efficient deep learning, which seeks to reduce training costs not at the hardware or implementation level, but through changes in the semantics of the training program. In this paper, we present a structured and comprehensive overview of the research in this field. First, we formalize the algorithmic speedup problem, then we use fundamental building blocks of algorithmically efficient training to develop a taxonomy. Our taxonomy highlights commonalities of seemingly disparate methods and reveals current research gaps. Next, we present evaluation best practices to enable comprehensive, fair, and reliable comparisons of speedup techniques. To further aid research and applications, we discuss common bottlenecks in the training pipeline (illustrated via experiments) and offer taxonomic mitigation strategies for them. Finally, we highlight some unsolved research challenges and present promising future directions.},
}

@article{Alzoubi2024,
  title = {Green artificial intelligence initiatives: Potentials and challenges},
  author = {Yehia Ibrahim Alzoubi and Alok Mishra},
  year = {2024},
  journal = {Journal of Cleaner Production},
  volume = {468},
  pages = {143090},
  doi = {https://doi.org/10.1016/j.jclepro.2024.143090},
  url = {https://www.sciencedirect.com/science/article/pii/S0959652624025393},
  abstract = {Recently, the widespread adoption of artificial intelligence, particularly generative AI technology, has surged across various industries. However, a notable drawback of this technology is its significant energy consumption during model training and operation, which poses challenges to sustainability goals and the environment. Consequently, various initiatives have emerged to promote what is termed "green artificial intelligence," aiming to mitigate these environmental impacts. Nevertheless, research discussing these initiatives remains scarce. Hence, this study aims to identify green artificial intelligence initiatives that contribute to environmental friendliness. This paper has comprehensively reviewed the existing literature, professional websites, and expert blogs to identify and analyze available green AI initiatives. This paper has identified 55 such initiatives, broadly categorized into six themes: cloud optimization, model efficiency, carbon footprinting, sustainability-focused AI development, open-source initiatives, and green AI research and community. This study discusses the strengths and limitations of each initiative to offer a comprehensive overview. The findings provide valuable insights, particularly for industries interested in green artificial intelligence and green technology in general. While some tools have been recognized and studied, comprehensive research and analysis are still required to empirically evaluate the majority of other tools due to their early stages of development in this field.},
}

@article{Masso2025,
  title = {Research ethics committees as knowledge gatekeepers: The impact of emerging technologies on social science research},
  author = {Anu Masso and Jevgenia Gerassimenko and Tayfun Kasapoglu and Mai Beilmann},
  year = {2025},
  journal = {Journal of Responsible Technology},
  volume = {21},
  pages = {100112},
  doi = {https://doi.org/10.1016/j.jrt.2025.100112},
  url = {https://www.sciencedirect.com/science/article/pii/S2666659625000083},
  abstract = {This article investigates the evolution of research ethics within the social sciences, emphasising the shift from procedural norms borrowed from medical and natural sciences to social scientific discipline-specific and method-based principles. This transformation acknowledges the unique challenges and opportunities in social science research, particularly in the context of emerging data technologies such as digital data, algorithms, and artificial intelligence. Our empirical analysis, based on a survey conducted among international social scientists (N = 214), highlights the precariousness researchers face regarding these technological shifts. Traditional methods remain prevalent, despite the recognition of new digital methodologies that necessitate new ethical principles. We discuss the role of ethics committees as influential gatekeepers, examining power dynamics and access to knowledge within the research landscape. The findings underscore the need for tailored ethical guidelines that accommodate diverse methodological approaches, advocate for interdisciplinary dialogue, and address inequalities in knowledge production. This article contributes to the broader understanding of evolving research ethics in an increasingly data-driven world.},
}

@article{Atzori2024,
  title = {Evaluating password strength based on information spread on social networks: A combined approach relying on data reconstruction and generative models},
  author = {Maurizio Atzori and Eleonora Calò and Loredana Caruccio and Stefano Cirillo and Giuseppe Polese and Giandomenico Solimando},
  year = {2024},
  journal = {Online Social Networks and Media},
  volume = {42},
  pages = {100278},
  doi = {https://doi.org/10.1016/j.osnem.2024.100278},
  url = {https://www.sciencedirect.com/science/article/pii/S246869642400003X},
  abstract = {Ensuring the security of personal accounts has become a key concern due to the widespread password attack techniques. Although passwords are the primary defense against unauthorized access, the practice of reusing easy-to-remember passwords increases security risks for people. Traditional methods for evaluating password strength are often insufficient since they overlook the public personal information that users frequently share on social networks. In addition, while users tend to limit access to their data on single profiles, personal data is often unintentionally shared across multiple profiles, exposing users to password threats. In this paper, we present an extension of a data reconstruction tool, namely soda advance, which incorporates a new module to evaluate password strength based on publicly available data across multiple social networks. It relies on a new metric to provide a comprehensive evaluation of password strength. Moreover, we investigate the capabilities and risks associated with emerging Large Language Models (LLMs) in evaluating and generating passwords, respectively. Specifically, by exploiting the proliferation of LLMs, it has been possible to interact with many LLMs through Automated Template Learning methodologies. Experimental evaluations, performed with 100 real users, demonstrate the effectiveness of LLMs in generating strong passwords with respect to data associated with users’ profiles. Furthermore, LLMs have proved to be effective also in evaluation tasks, but the combined usage of LLMs and soda advance guaranteed better classifications up to more than 10% in terms of F1-score.},
}

@article{Lu2025,
  title = {LLM-infused bi-level semantic enhancement for corporate credit risk prediction},
  author = {Sichong Lu and Yi Su and Xiaoming Zhang and Jiahui Chai and Lean Yu},
  year = {2025},
  journal = {Information Processing & Management},
  volume = {62},
  pages = {104091},
  doi = {https://doi.org/10.1016/j.ipm.2025.104091},
  url = {https://www.sciencedirect.com/science/article/pii/S0306457325000330},
  abstract = {Corporate credit risk (CCR) prediction enables investors, governments, and companies to make informed financial decisions. Existing research primarily focuses solely on the tabular feature values, yet it often overlooks the rich inherent semantic information. In this paper, a novel bi-level semantic enhancement framework for CCR prediction is proposed. Firstly, at the data-level, a large language model (LLM) generates detailed textual descriptions of companies’ financial conditions, infusing raw tabular training data with semantic information and domain knowledge. Secondly, to enable semantic perception during inference when only tabular data is available, a contrastive multimodal multitask learning model (CMML) is proposed at the model level. CMML leverages the semantically enhanced data from the previous level to acquire semantic perception capabilities during the training phase, requiring only tabular data during prediction. It aligns the representations of tabular data with textual data, enabling extracting semantically rich features from tabular data. Furthermore, a semantic alignment classifier and an MLP classifier are integrated into a weighted ensemble learner within a multitask learning architecture to enhance robustness. Empirical verification on two datasets demonstrates that CMML surpasses benchmark models in key metrics, particularly in scenarios with limited samples and high proportions of unseen corporations, implying its effectiveness in CCR prediction through bi-level semantic enhancement.},
}

@article{Matteo}2024,
  title = {Tax incentives and environmental performance: The pay-as-you-throw policy in Emilia-Romagna, Italy},
  author = {Dante {Di Matteo} and Eleonora Guadagno},
  year = {2024},
  journal = {Journal of Cleaner Production},
  volume = {475},
  pages = {143659},
  doi = {https://doi.org/10.1016/j.jclepro.2024.143659},
  url = {https://www.sciencedirect.com/science/article/pii/S0959652624031081},
  abstract = {This study investigates the impact of implementing the pay-as-you-throw policy as a fiscal incentive mechanism for municipal waste management across municipalities in the Emilia-Romagna region, Italy. Utilizing panel data from 2010 to 2022 and employing a synthetic control with staggered treatment adoption, we evaluate the policy's effects on environmental and local public finance performances. Our findings reveal that treated municipalities witness an average 10% increase in separate waste collection alongside a parallel reduction in municipal waste production since the policy outset, equating to approximately 60 kg per capita annually. Furthermore, our analysis indicates that the policy enables local governments to reduce current energy and environmental expenditures by roughly €130 per capita annually in our optimal model. Notably, results hold robust across three recently developed estimators using difference-in-differences with staggered treatment adoption over time. The policy proves overall effectiveness in promoting circularity within municipal waste management systems, leading to better environmental performances, and decreased environmental management expenses. These implications underscore the potential value for local governments to adopt similar strategies where not yet envisaged.},
}

@article{Wang2025,
  title = {Climate change impacts on city-scale building energy performance based on GIS-informed urban building energy modelling},
  author = {Meng Wang and Jingfeng Zhou and Yujing Liang and Hang Yu and Rui Jing},
  year = {2025},
  journal = {Sustainable Cities and Society},
  pages = {106331},
  doi = {https://doi.org/10.1016/j.scs.2025.106331},
  url = {https://www.sciencedirect.com/science/article/pii/S2210670725002082},
  abstract = {Climate change increasingly affects urban building energy performance, leading to varying energy demand patterns and increased maintenance costs. This study assesses the impact of climate change on city-scale building energy performance by integrating future climatic variations into urban building energy modelling (UBEM). Extensive building information is firstly collected using geographic information system (GIS) technologies. Then, a morphing statistical method is employed to generate future meteorological data considering different Shared Socio‐Economic Pathways (SSPs) scenarios. An automated workflow is further proposed to generate prototype building models, facilitating the bottom-up development of the city-scale UBEM from building-level to city-scale, capturing various building types, geometric features, and construction years. Lastly, the UBEM is validated against measured building energy data. A case study of Fuzhou City, China, encompassing over 75,000 buildings, reveals that climate change is projected to increase the whole city's total electricity demand by up to 3.1% by 2060, among which commercial buildings are the most affected category, with total electricity demand rising by up to 4.1%. The whole city's HVAC electricity demand is projected to increase up to 14.9% with transport buildings experiencing the most significant impact by rising up to 21.2% by 2060.},
}

@article{Guo2023,
  title = {AIGC challenges and opportunities related to public safety: A case study of ChatGPT},
  author = {Danhuai Guo and Huixuan Chen and Ruoling Wu and Yangang Wang},
  year = {2023},
  journal = {Journal of Safety Science and Resilience},
  volume = {4},
  pages = {329-339},
  doi = {https://doi.org/10.1016/j.jnlssr.2023.08.001},
  url = {https://www.sciencedirect.com/science/article/pii/S2666449623000397},
  abstract = {Artificial intelligence generated content (AIGC) is a production method based on artificial intelligence (AI) technology that finds rules through data and automatically generates content. In contrast to computational intelligence, generative AI, as exemplified by ChatGPT, exhibits characteristics that increasingly resemble human-level comprehension and creation processes. This paper provides a detailed technical framework and history of ChatGPT, followed by an examination of the challenges posed to political security, military security, economic security, cultural security, social security, ethical security, legal security, machine escape problems, and information leakage. Finally, this paper discusses the potential opportunities that AIGC presents in the realms of politics, military, cybersecurity, society, and public safety education.},
}

@article{Jin2024_01,
  title = {Large language model as parking planning agent in the context of mixed period of autonomous vehicles and Human-Driven vehicles},
  author = {Yuping Jin and Jun Ma},
  year = {2024},
  journal = {Sustainable Cities and Society},
  volume = {117},
  pages = {105940},
  doi = {https://doi.org/10.1016/j.scs.2024.105940},
  url = {https://www.sciencedirect.com/science/article/pii/S2210670724007649},
  abstract = {Autonomous vehicles (AVs) are anticipated to revolutionize future transportation, necessitating updates to traffic infrastructure, particularly parking facilities, due to the unique characteristics of AVs compared to Human-Driven Vehicles (HDVs). During the transition period in which AVs and HDVs coexist, adaptable infrastructure is essential to accommodate both vehicle types. Traditional research, typically reliant on complex mathematical models and simulations, faces challenges in adapting to diverse urban settings, requiring substantial time and resources. To address these challenges, a government-level framework was developed, enabling urban planners to quickly and accurately evaluate and optimize existing parking facilities for future AV and HDV coexistence scenarios. The framework integrates a Large Language Model (LLM) to enhance flexibility and efficiency in parking planning throughout the transitional period. Structured guidance is incorporated to enhance decision-making precision and reduce LLM hallucination risks. The flexibility, robustness, and accuracy of the framework were validated through step-by-step and end-to-end testing using real-world datasets. Specifically, the framework achieved 91.1 % comprehensiveness and 70.2 % consistency in Indicator Selection Module testing, a 68.9 % success rate in the Single Indicator Calculation Module, and a 66.7 % success rate in end-to-end testing, demonstrating its practical value in supporting cities during AV integration. Finally, the success rates of different LLM agent modules were further explored, along with a comparison of multiple LLMs and an analysis of key issues related to LLM trustworthiness in urban planning applications. The research highlights the potential of LLMs in advancing urban planning processes and optimizing existing infrastructure, contributing to smarter and more adaptable urban environments.},
}

@article{Wu2024,
  title = {Analyzing K-12 AI education: A large language model study of classroom instruction on learning theories, pedagogy, tools, and AI literacy},
  author = {Di Wu and Meng Chen and Xu Chen and Xing Liu},
  year = {2024},
  journal = {Computers and Education: Artificial Intelligence},
  volume = {7},
  pages = {100295},
  doi = {https://doi.org/10.1016/j.caeai.2024.100295},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000985},
  abstract = {There is growing recognition among researchers and stakeholders about the significant impact of artificial intelligence (AI) technology on classroom instruction. As a crucial element in developing AI literacy, AI education in K-12 schools is increasingly gaining attention. However, most existing research on K-12 AI education relies on experiential methodologies and suffers from a lack of quantitative analysis based on extensive classroom data, hindering a comprehensive depiction of AI education's current state at these educational levels. To address this gap, this article employs the advanced semantic understanding capabilities of large language models (LLMs) to create an intelligent analysis framework that identifies learning theories, pedagogical approaches, learning tools, and levels of AI literacy in AI classroom instruction. Compared with the results of manual analysis, analysis based on LLMs can achieve more than 90% consistency. Our findings, based on the analysis of 98 classroom instruction videos in central Chinese cities, reveal that current AI classroom instruction insufficiently foster AI literacy, with only 35.71% addressing higher-level skills such as evaluating and creating AI. AI ethics are even less commonly addressed, featured in just 5.1% of classroom instruction. We classified AI classroom instruction into three categories: conceptual (50%), heuristic (18.37%), and experimental (31.63%). Correlation analysis suggests a significant relationship between the adoption of pedagogical approaches and the development of advanced AI literacy. Specifically, integrating Project-based/Problem-based learning (PBL) with Collaborative learning appears effective in cultivating the capacity to evaluate and create AI.},
}

@article{Nwapi2021,
  title = {Developments in Beneficial Ownership Disclosure in the Extractive Industries in Nigeria},
  author = {Chilenye Nwapi and Chinwe Ezeigbo and Oluwakemi Oke},
  year = {2021},
  journal = {The Extractive Industries and Society},
  volume = {8},
  pages = {443-456},
  doi = {https://doi.org/10.1016/j.exis.2020.12.012},
  url = {https://www.sciencedirect.com/science/article/pii/S2214790X20303294},
  abstract = {Nigeria has been an active participant in the promotion of beneficial ownership disclosure in the extractive industries despite the absence of a solid domestic legal framework supporting it. Among other things, it has established a beneficial ownership register and has extended beneficial ownership disclosure reporting to commodity traders to address secrecy in the ownership of intermediary companies in the oil and gas sector. In mid-2020, a new company law was enacted with beneficial ownership disclosure provisions covering private companies. Though a significant step, the new law, like its predecessor, focuses mainly on legal ownership/control and fails to fully address the issue of ultimate ownership/control critical to beneficial ownership secrecy. This will pose a significant challenge to the effective implementation of beneficial ownership disclosure in Nigeria. Other challenges include the difficulties of collecting beneficial ownership information, the costs of maintaining a secure beneficial ownership register, and the political will to enforce compliance with beneficial ownership requirements. Of note also are the impacts of COVID-19 especially on the government's ability to raise revenue to implement beneficial ownership disclosure. Addressing these challenges requires a whole-of-government approach in the implementation of policies and laws as well as continued public engagement, for beneficial ownership disclosure represents a significant cultural shift in extractive industries business in Nigeria.},
}

@article{Pandey2025,
  title = {Digital Sovereignty and AI: Developing India’s National AI Stack for Strategic Autonomy},
  author = {Pankaj Pandey},
  year = {2025},
  journal = {Procedia Computer Science},
  volume = {254},
  pages = {250-259},
  doi = {https://doi.org/10.1016/j.procs.2025.02.084},
  url = {https://www.sciencedirect.com/science/article/pii/S187705092500434X},
  abstract = {India’s pursuit of digital sovereignty has gained urgency as global dependencies in Artificial Intelligence (AI) technologies pose risks to national security, data privacy, and economic independence. This paper presents the concept of a National AI Stack, a comprehensive AI infrastructure tailored to India’s unique socio-economic and technological needs. The National AI Stack aims to reduce reliance on foreign AI frameworks by fostering indigenous AI development across critical sectors. The paper presents the key components of this stack, consisting of seven layers, namely the Strategic Integration Layer, Hardware Layer, Compute Layer, Data Layer, Model Layer, API & Applications Layer, and Security and Compliance Layer. By leveraging India’s vast and diverse datasets and focusing on building domestic capacity, the National AI Stack is envisioned as a strategic initiative for achieving digital autonomy, ensuring the ethical and secure deployment of AI systems. However, this proposal faces multiple challenges and risks that must be tackled to succeed. Issues such as data privacy, ethical AI practices, and navigating the regulatory landscape require careful attention to preserve public trust and ensure the responsible implementation of AI systems.},
}

@article{Hernández}2023,
  title = {Cities in the times of COVID-19: Trends, impacts, and challenges for urban sustainability and resilience},
  author = {Jhon Ricardo {Escorcia Hernández} and Sara {Torabi Moghadam} and Ayyoob Sharifi and Patrizia Lombardi},
  year = {2023},
  journal = {Journal of Cleaner Production},
  volume = {432},
  pages = {139735},
  doi = {https://doi.org/10.1016/j.jclepro.2023.139735},
  url = {https://www.sciencedirect.com/science/article/pii/S0959652623038933},
  abstract = {Since the beginning of the COVID-19 outbreak, understanding its impacts on cities has received much attention in science and policy circles. This paper systematically reviews the literature on the interface of the pandemic and urban sustainability. The objective is to portray the impacts brought by the COVID-19 outbreak in urban environments within the sustainability framework and to detect trends and challenges for future research. The paper follows a methodology that integrates both bibliometric and systematic review approaches. The first approach relies on bibliometric analysis to provide an overview of the landscape of main trends on this nexus. The second approach presents a content analysis that deepens the work by outlining the impacts of the pandemic and the challenges that emerged on five different key topics for urban sustainability. The role of resilient urban planning is discussed as an integrative concept to face diverse challenges in the construction of sustainable cities in a post-pandemic scenario. Likewise, the study deliberates on future research topics related to resilient urban planning, social equity, healthy urban environments, sustainable mobility, and circular economy. This review serves as a guide for researchers and urban planners to understand emerging challenges and future research trends in urban sustainability.},
}

@article{Sønvisen2025,
  title = {Data sharing in the fisheries: Exploring the willingness to share data in the Norwegian fishing fleet},
  author = {Signe A. Sønvisen and Grethe Lilleng and Tore Syversen and Dorthea Mathilde Kristin Vatn},
  year = {2025},
  journal = {Marine Policy},
  volume = {175},
  pages = {106620},
  doi = {https://doi.org/10.1016/j.marpol.2025.106620},
  url = {https://www.sciencedirect.com/science/article/pii/S0308597X25000351},
  abstract = {This study examines the motivations and barriers to data sharing in the Norwegian fishing fleet, where data-sharing practices are increasingly recognized for their potential benefits for both business and resource management. Based on semi-structured interviews and an online survey of Norwegian fishers, findings reveal widespread reluctance among fishers to share data openly due to fears of loss of competitive advantage, free-riding, congestion and conflicts on fishing grounds, and overharvesting. However, conditions such as trust in the system, reciprocal data exchange, anonymity, publication delays, and restricted access to data were identified as critical balancing factors for fostering willingness to share data. While some fishers recognize the benefits of data sharing, particularly for compliance and knowledge transfer, concerns about power imbalances and the inequitable distribution of benefits persist. This study underscores the need for tailored initiatives that address fishers' diverse needs and contexts, balancing transparency with privacy and promoting trust.},
}

@article{Davis2025,
  title = {Open Educational Resources},
  author = {Sabrina Davis},
  year = {2025},
  pages = {542-553},
  doi = {https://doi.org/10.1016/B978-0-323-95689-5.00166-8},
  publisher = {Academic Press},
  url = {https://www.sciencedirect.com/science/article/pii/B9780323956895001668},
  abstract = {Open Educational Resources (OER) and their place within libraries have grown rapidly. As more and more libraries are adding positions that include OER promotion to campus communities, it is imperative that those studying library and information science have a basic understanding of what OER is, how it fits within academic librarianship, as well as some of the current challenges and future trends facing this new facet of librarianship. This entry also provides a general overview of main topics related to OER, such as textbook affordability and Creative Commons, and provides lists of resources to illustrate the global support behind OER efforts, as well as a list of resources that is helpful regarding OER creation.},
}

@article{Kahachi2024,
  title = {Future cities' theories for sustainable future: A systematic literature review},
  author = {Hussaen A.H. Kahachi and Maria Abreu and Mufeed Ehsan},
  year = {2024},
  journal = {Futures},
  volume = {164},
  pages = {103494},
  doi = {https://doi.org/10.1016/j.futures.2024.103494},
  url = {https://www.sciencedirect.com/science/article/pii/S0016328724001769},
  abstract = {In recent decades, sustainability has surged as a pivotal topic in academic discourse, driven by heightened global awareness of its critical role in addressing climate change, biodiversity loss, and social inequities. Consequently, cities and local authorities are increasingly prioritizing sustainability integration into urban development strategies. However, the realization of the 2015 Sustainable Development Goals faces challenges, with only 12–15 % of targets estimated to be met by the 2030 deadline. Scholars attribute this under-performance to the absence of a comprehensive approach to sustainable urban development. This study investigates the framework and mechanisms of mature future cities’ theories in achieving sustainability, focusing on sustainable urban development principles. Through a systematic review utilizing Natural Language Processing and statistical analysis of over 27,000 SCOPUS documents, the research evaluates 43 future cities’ theories from 1900 to 2023. Criteria such as comprehensiveness, consistency, empirical support, and applicability to sustainable development are analyzed. The study identifies mature theories like Smart City, Resilient City, and Sustainable City as promising avenues for sustainable urban development, albeit requiring refinement. The findings offer valuable insights to guide research and practice towards achieving sustainable urban development in cities.},
}

@article{Perini2025,
  title = {Modelling COVID-19 in the North American region with a metapopulation network and Kalman filter},
  author = {Matteo Perini and Teresa K. Yamana and Marta Galanti and Jiyeon Suh and Roselyn Kaondera-Shava and Jeffrey Shaman},
  year = {2025},
  journal = {Epidemics},
  volume = {50},
  pages = {100818},
  doi = {https://doi.org/10.1016/j.epidem.2025.100818},
  url = {https://www.sciencedirect.com/science/article/pii/S1755436525000064},
  abstract = {Background Understanding the dynamics of infectious disease spread and predicting clinical outcomes are critical for managing large-scale epidemics and pandemics, such as COVID-19. Effective modeling of disease transmission in interconnected populations helps inform public health responses and interventions across regions. Methods We developed a novel metapopulation model for simulating respiratory virus transmission in the North America region, specifically for the 96 states, provinces, and territories of Canada, Mexico, and the United States. The model is informed by COVID-19 case data, which are assimilated using the Ensemble Adjustment Kalman filter (EAKF), a Bayesian inference algorithm. Additionally, commuting and mobility data are used to build and adjust the network and movement across locations on a daily basis. Results This model-inference system provides estimates of transmission dynamics, infection rates, and ascertainment rates for each of the 96 locations from January 2020 to March 2021. The results highlight differences in disease dynamics and ascertainment among the three countries. Conclusions The metapopulation structure enables rapid simulation at a large scale, and the data assimilation method makes the system responsive to changes in system dynamics. This model can serve as a versatile platform for modeling other infectious diseases across the North American region.},
}

@article{Hafferty2024,
  title = {Engagement in the digital age: Understanding “what works” for participatory technologies in environmental decision-making},
  author = {Caitlin Hafferty and Mark S. Reed and Beth F.T. Brockett and Scott Orford and Robert Berry and Chris Short and Joshua Davis},
  year = {2024},
  journal = {Journal of Environmental Management},
  volume = {365},
  pages = {121365},
  doi = {https://doi.org/10.1016/j.jenvman.2024.121365},
  url = {https://www.sciencedirect.com/science/article/pii/S0301479724013513},
  abstract = {Effective engagement is crucial for enhancing environmental decision-making processes, fostering more sustainable and equitable outcomes. However, the success of engagement is highly variable and context-dependent. While theoretical frameworks have been developed to explain outcome variance in engagement in environmental decision-making, they have not yet been tested in digital contexts, leaving their applicability to digital engagement processes unclear. More broadly, there are unanswered questions about the effectiveness of digital tools in achieving the goals of engagement, which have become increasingly pertinent amidst growing concerns about the potential of digital technologies for exacerbating exclusions, ethical issues, and systematically undermining democratic progress. This paper addresses this evidence gap by presenting findings from interviews with practitioners in UK public, private, and third sector organisations. Our results provide empirical insights into the technical, ethical, and inclusivity debates surrounding digital tools and their effectiveness in promoting accessible engagement, high-quality social interaction, place-based decision-making, and more trustworthy and credible outcomes. Our findings indicate that while current engagement theories are applicable to digital environments, the key explanatory factors acquire new dimensions in digital compared to in-person contexts. Drawing on the findings, this study contributes novel insights to expand current theory for explaining “what works” in engagement in environmental decisions, enhancing its relevance and applicability in the digital age. The paper concludes with evidence-led recommendations for environmental practitioners to improve engagement processes in digital and remote settings.},
}

@article{Bohnsack2024,
  title = {Profiting from innovation when digital business ecosystems emerge: A control point perspective},
  author = {René Bohnsack and Michael Rennings and Carolin Block and Stefanie Bröring},
  year = {2024},
  journal = {Research Policy},
  volume = {53},
  pages = {104961},
  doi = {https://doi.org/10.1016/j.respol.2024.104961},
  url = {https://www.sciencedirect.com/science/article/pii/S0048733324000106},
  abstract = {The digital transformation of industrial-age sectors changes product architectures and industry architectures, influencing how value is created and captured in emerging digital business ecosystems. In the industrial era, products were designed around modular architectures and complementary assets, and bottlenecks determined who profits from innovation. In the digital era, products emerge on a layered modular architecture, and profiting from innovation is shifting to those who own control points. Despite the centrality of the interplay between the product architecture and industry architecture for value creation and value capture in the digital age, the effects on competitiveness and industry dynamics remain unclear. To fill this void, we draw on the concept of control points, a novel lens to reflect bargaining positions on a layered modular architecture in digital business ecosystems. Based on a case study of 19 companies, industry associations, and consulting firms in the digital business ecosystem of smart farming, we identify strategic control points, technical control points, generic control points, and institutional boundaries as instrumental in determining value creation and value capture positions. We find that actors (i.e., incumbents, diversifying entrants, and new entrants) in emerging digital business ecosystems follow a seesaw pattern in setting control points and acquiring bargaining positions, and propose a framework that allows to analyze the dynamics within digital business ecosystems. Our study offers managerial implications for firms seeking to optimize their ecosystem strategy and policy makers to support the effective development of the institutional context.},
}

@article{Hernández-Peñaloza2024,
  title = {General context and relevant public datasets available for improving pathways in Paediatric Cancer applying Artificial Intelligence. A review},
  author = {Gustavo Hernández-Peñaloza and Silvia Uribe and Francisco Moreno García and Norbert Graf and Federico Álvarez},
  year = {2024},
  journal = {EJC Paediatric Oncology},
  volume = {4},
  pages = {100196},
  doi = {https://doi.org/10.1016/j.ejcped.2024.100196},
  url = {https://www.sciencedirect.com/science/article/pii/S2772610X24000564},
  abstract = {Due to the promise of transforming healthcare and medicine that Artificial Intelligence (AI) has posed, the number of applications has increased exponentially. These applications range from screening and disease diagnosis to prognosis, treatment planning, and follow-up. In complex topics such as childhood cancer, these techniques are being expanded with the ambition of improving the quality of care by allowing healthcare professionals to make more informed decisions. However, the adequate application of such techniques heavily depends on the data, which creates a set of challenges including collection, bias, and scarcity among others. Furthermore, ethical, legal, and regulatory frameworks increase even more the difficulties to develop AI-powered solutions. In this paper, we present an exhaustive literature review to identify and analyse public datasets targeting two common childhood cancer types, such as neuroblastoma and nephroblastoma. Moreover, the complex context for the development of AI- based software solutions is outlined. It includes the description of the most relevant techniques to address problems associated with data sharing and training. Finally, a set of code snippets is provided to perform exploratory analysis for the available data.},
}

@article{Lai2024,
  title = {A systematic review of conversational AI tools in ELT: Publication trends, tools, research methods, learning outcomes, and antecedents},
  author = {Wan Yee Winsy Lai and Ju Seong Lee},
  year = {2024},
  journal = {Computers and Education: Artificial Intelligence},
  volume = {7},
  pages = {100291},
  doi = {https://doi.org/10.1016/j.caeai.2024.100291},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000948},
  abstract = {This review analyzed the trends in conversational AI tools in ELT from January 2013 to November 2023. The study examined 32 papers, focusing on publication trends, tool types, research methods, learning outcomes, and factors influencing their use. Findings revealed a gradual increase in publications, with 4 (12%) from 2013 to 2021, 13 (41%) in 2022, and 15 (47%) in 2023. All studies (100%) were conducted in Asian EFL contexts. Among the AI chatbots, Google Assistant (25%) was the most widely used. Quasi-experimental (45%) and cross-section (41%) research designs were commonly employed. Mixed-method (50%) approaches were prevalent for data collection and analysis. Conversational AI yielded positive outcomes in affective (43%) and cognitive skills (41%). The main factors influencing user perceptions or behaviors were individual (47%) and microsystem layers (31%). Future studies should (a) include diverse contexts beyond Asia, (b) consider the use of up-to-date tools (e.g., ChatGPT), (c) employ rigorous experimental designs, (d) explore behavioral learning outcomes, and (e) investigate broader environmental factors. The systematic review enhances current knowledge of recent research trends, identifies environmental factors influencing conversational AI tools concentrating in ELT, and provides insights for future research and practice in this rapidly evolving field.},
}

@article{Okoche}2025,
  title = {AI adoption in crowdsourcing},
  author = {John Michael {Maxel Okoche} and Marcia Mkansi and Godfrey Mugurusi and Wellington Chakuzira},
  year = {2025},
  journal = {Procedia Computer Science},
  volume = {253},
  pages = {2508-2521},
  doi = {https://doi.org/10.1016/j.procs.2025.01.311},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050925003199},
  abstract = {Despite significant technology advances especially in artificial intelligence (AI), crowdsourcing platforms still struggle with issues such as data overload and data quality problems, which hinder their full potential. This study addresses a critical gap in the literature how the integration of AI technologies in crowdsourcing could help overcome some these challenges. Using a systematic literature review of 77 journal papers, we identify the key limitations of current crowdsourcing platforms that included issues of quality control, scalability, bias, and privacy. Our research highlights how different forms of AI including from machine learning (ML), deep learning (DL), natural language processing (NLP), automatic speech recognition (ASR), and natural language generation techniques (NLG) can address the challenges most crowdsourcing platforms face. This paper offers knowledge to support the integration of AI first by identifying types of crowdsourcing applications, their challenges and the solutions AI offers for improvement of crowdsourcing.},
}

@article{Woods2024,
  title = {Territorialising the cloud or clouding the territory? Volumetric vulnerabilities and the militarised conjunctures of Singapore’s smart city-state},
  author = {Orlando Woods and Tim Bunnell and Lily Kong},
  year = {2024},
  journal = {Political Geography},
  volume = {115},
  pages = {103211},
  doi = {https://doi.org/10.1016/j.polgeo.2024.103211},
  url = {https://www.sciencedirect.com/science/article/pii/S0962629824001604},
  abstract = {This article explores how the volumetric characteristics of cloud computing can create new expressions of territoriality, which in turn can reveal new axes of vulnerability and threat. Whilst recent work in political geography has sought to “locate” the cloud through analyses of data centre geographies and data-driven processes of smart urbanism, we look beyond the material plane and consider the amorphous territorialities of voluminous data instead. As much as these data are acted on by the legal-regulatory mechanics of the state in a bid to territorialise them, so too do these data volumes serve to cloud, and thus obscure, territory. Processes of territorialising and clouding exist in a state of dialectical tension with each other, and reveal the volumetric vulnerabilities of cloud computing. We validate these theoretical claims through an analysis of in-depth interviews with senior stakeholders in Singapore's Smart Nation initiative. In Singapore, defending the city is equivalent to defending the nation, which causes the military to play an outsized role in securing the city-state. We consider how the attack surface of the city becomes a more voluminous construct with cloud computing, how strategies of geofencing attempt to secure the cloud, and how these processes reveal the increasingly militarised conjunctures of everyday life. Overall, these insights reveal a need for political geography to continually evolve its theoretical premises in line with the rapid digitalisation of the world.},
}

@article{Wu2024_01,
  title = {Pathway to a fully data-driven geotechnics: Lessons from materials informatics},
  author = {Stephen Wu and Yu Otake and Yosuke Higo and Ikumasa Yoshida},
  year = {2024},
  journal = {Soils and Foundations},
  volume = {64},
  pages = {101471},
  doi = {https://doi.org/10.1016/j.sandf.2024.101471},
  url = {https://www.sciencedirect.com/science/article/pii/S0038080624000490},
  abstract = {This paper elucidates the challenges and opportunities inherent in integrating data-driven methodologies into geotechnics, drawing inspiration from the success of materials informatics. Highlighting the intricacies of soil complexity, heterogeneity, and the lack of comprehensive data, the discussion underscores the pressing need for community-driven database initiatives and open science movements. By leveraging the transformative power of deep learning, particularly in feature extraction from high-dimensional data and the potential of transfer learning, we envision a paradigm shift towards a more collaborative and innovative geotechnics field. The paper concludes with a forward-looking stance, emphasizing the revolutionary potential brought about by advanced computational tools like large language models in reshaping geotechnics informatics.},
}

@article{Atkinson2025,
  title = {Health Libraries: Characteristics, and Evolving Services},
  author = {Loretta Atkinson and Jo Whitcombe},
  year = {2025},
  pages = {175-186},
  doi = {https://doi.org/10.1016/B978-0-323-95689-5.00107-3},
  publisher = {Academic Press},
  url = {https://www.sciencedirect.com/science/article/pii/B9780323956895001073},
  abstract = {Health libraries are essential for supporting health professionals to ultimately provide quality healthcare and positive patient outcomes. Librarians use their professional expertise to provide access to high-quality information and resources to support evidence-based practice in the provision of care across all sectors of health. The continually evolving nature of healthcare along with the rapidly changing digital environment, has impacted on the delivery of library services, spaces and facilities delivered by health libraries. Health librarians have responded to these ongoing challenges, adapting and transforming services to meet the current needs of clinicians, researchers, students, consumers, and patients.},
}

@article{Jones2024_01,
  title = {Rewriting the textbook for pharma: how to adapt and thrive in a digital, personalized and collaborative world},
  author = {Charles H. Jones and Subha Madhavan and Kannan Natarajan and Michael Corbo and Jane M. True and Mikael Dolsten},
  year = {2024},
  journal = {Drug Discovery Today},
  volume = {29},
  pages = {104112},
  doi = {https://doi.org/10.1016/j.drudis.2024.104112},
  url = {https://www.sciencedirect.com/science/article/pii/S135964462400237X},
  abstract = {The pharmaceutical industry is undergoing a sweeping transformation, driven by technological innovations, demographic shifts, regulatory changes and consumer expectations. For adaptive players in pharma to excel in this rapidly changing landscape, which will be markedly different from today by 2030 and beyond, they will require a different set of skills, capabilities and mindsets, as well as a willingness to collaborate and co-create value with multiple stakeholders. The industry needs to rewrite the textbook for pharma by embracing and implementing four key dimensions of change: digitalization, personalization, collaboration and innovation. In this article, we will examine how these dimensions of change are reshaping the industry, and provide practical and strategic guidance based on best practices and examples. Specifically, adaptive pharma companies should embrace the use of advanced digital technologies, such as artificial intelligence and machine learning, to streamline processes and solve challenges rapidly. Personalization, both in medicine and patient engagement, will also be key to success in the ‘digital revolution’, and a collaborative approach involving partnerships with tech start-ups, health-care providers and regulatory bodies will also be essential to create an integrated and responsive health-care ecosystem. Using these ideas for a rewritten textbook for pharma, adaptive players in pharma will evolve to be personalized and digitized health-focused organizations that provide comprehensive solutions which go beyond drugs and devices.},
}

@article{Gong2024,
  title = {Pharmacist-initiated interventions using RxChange message communication with prescribers for electronic prescriptions: A retrospective descriptive study},
  author = {Jun Gong and Yifan Zheng and Corey A. Lester},
  year = {2024},
  journal = {Journal of the American Pharmacists Association},
  volume = {64},
  pages = {102188},
  doi = {https://doi.org/10.1016/j.japh.2024.102188},
  url = {https://www.sciencedirect.com/science/article/pii/S1544319124002085},
  abstract = {Background RxChange messages improve patient medication management by enhancing pharmacist-prescriber communication, but their usage patterns in the United States are not well-documented. Objective To determine intervention characteristics by pharmacists and prescribers using RxChange messages. Methods A retrospective analysis of electronic prescription and RxChange messages from 2022 to 2023, using data from Surescripts, LLC, was conducted. This included NewRx messages and RxChange Responses, categorized by 7 RxChange use cases and Anatomical Therapeutic Chemical level 4 medication classes. Descriptive statistics and nonparametric tests were used for statistical analysis. Results The study analyzed 1,361,528 RxChange messages. Therapeutic interchange was the predominant use case (76.14%). Direct approvals accounted for 10.44% of requests, approvals with changes for 42.55%, and denials for 47.01%. Script clarification had the highest approval rate (64.21%), while prior authorization faced the most frequent denials (73.38%). The top denial reason was "Request addressed through alternate methods such as phone or fax (41.50%).” The most frequent drug classes observed in the data were selective beta-2 adrenoreceptor agonists, extended-spectrum penicillins, selective serotonin reuptake inhibitors, and glucagon-like peptide 1 analogues. Time from new e-prescription issuance to RxChange request submission was longer than from request to response, with a significant statistical difference (median 1.57 vs. 0.27 days, P value < 0.05). Conclusion This study highlights interventions pharmacists make using RxChange with electronic prescriptions to improve patient care and medication safety. It underlined the need for improved RxChange message content and data on the effectiveness of RxChange messages in improving medication use.},
}

@article{Jacob2024,
  title = {Approach to a GPT-based Early Detection Tool to Evaluate Heterogeneous Data Sources and Identify Reconfiguration Needs of SMEs in the Production Sector},
  author = {Adrian Jacob and Anas Ben Achour and Uwe Teicher and Steffen Ihlenfeldt},
  year = {2024},
  journal = {Procedia CIRP},
  volume = {130},
  pages = {631-636},
  doi = {https://doi.org/10.1016/j.procir.2024.10.140},
  url = {https://www.sciencedirect.com/science/article/pii/S2212827124012976},
  abstract = {In the face of a rapidly evolving commercial ecosystem, small and medium-sized enterprises (SMEs) must ensure their process chains are capable of swift reconfiguration to mitigate disruptions from supply chain volatility, regulatory changes, and demand shifts. This study addresses the need for reconfiguration in the context of data-driven opportunities for early identification of reconfiguration needs in SMEs, with a focus on the production sector. We identify a variety of diverse and heterogeneous data sources that are critical for tracking these disruptions, such as public news, economic reports, market analyses, legal documents, and internal corporate records. To make use of these data streams, we propose an innovative early detection tool that employs advanced machine learning techniques and private, locally operated Generative Pretrained Transformers (GPTs). This system is intended to process and analyze the various data formats that SMEs encounter including HTML, PDF and plain text as well as quantitative data. By combining public and company internal data while addressing privacy concerns, the tool aims to provide SMEs with a comprehensive tracking of their commercial environments. The tool also addresses the challenges posed by the limited availability of large datasets within SMEs, which are typically required for effective use of machine learning techniques, allowing for effective operation with smaller datasets. As these technologies continue to advance, their impact across various industries, including legal, manufacturing, and supply chain management, is set to expand significantly. The proposed tool offers SMEs enhanced decision-making, compliance assurance, and the ability to proactively adapt to the unpredictable global business landscape.},
}

@article{Toorajipour2024,
  title = {Data ecosystem business models: Value propositions and value capture with Artificial Intelligence of Things},
  author = {Reza Toorajipour and Pejvak Oghazi and Maximilian Palmié},
  year = {2024},
  journal = {International Journal of Information Management},
  volume = {78},
  pages = {102804},
  doi = {https://doi.org/10.1016/j.ijinfomgt.2024.102804},
  url = {https://www.sciencedirect.com/science/article/pii/S0268401224000525},
  abstract = {The emergence of data as a critical asset and the prevalence of technologies such as the Artificial Intelligence of Things (AIoT) on the one hand, and the importance of collaborations for value creation on the other hand have given rise to a new breed of ecosystems known as data ecosystems. While data ecosystems provide new business opportunities, proposing and capturing value in those ecosystems is challenging, and the extant literature provides little guidance in this regard. Our research encompasses two studies that address this limitation and establish a framework for business-model archetypes in the context of AIoT data ecosystems. In the first study, exploratory qualitative research on 28 leading AIoT data ecosystem actors leads to the identification of value propositions and value-capture mechanisms in these ecosystems. We identify eight possible value propositions and eight possible value-capture mechanisms. The second, qualitative study centers on 19 expert interviews. Our analysis leads to the identification of two dimensions – control and customization – that guide the conceptualization and formation of business-model archetypes. Using these dimensions, we develop a framework for business-model archetypes in AIoT data ecosystems. Our findings contribute to the discourse on data ecosystems and offer new perspectives valuable for both researchers and industry practitioners.},
}

@article{Vomberg2023,
  title = {The cold-start problem in nascent AI strategy: Kickstarting data network effects},
  author = {Arnd Vomberg and Nico Schauerte and Sebastian Krakowski and Claire {Ingram Bogusz} and Maarten J. Gijsenberg and Alexander Bleier},
  year = {2023},
  journal = {Journal of Business Research},
  volume = {168},
  pages = {114236},
  doi = {https://doi.org/10.1016/j.jbusres.2023.114236},
  url = {https://www.sciencedirect.com/science/article/pii/S0148296323005957},
  abstract = {While many artificial intelligence (AI) strategies are successful, countless others fail. Why do some strategies succeed while others fail? We adopt a network effects (NEs) perspective to conceptualize AI strategies, highlighting the AI context’s specifics. We argue that nascent AI strategies’ success depends on data NEs: companies establishing a functional “running system” to capitalize on these effects. However, this presents a challenge known as the cold-start problem (CSP), which involves initiating and accelerating a virtuous cycle: more data benefits the AI system, enhancing performance, which then attracts more data. In this paper, we examine the CSP in nascent AI strategy, exploring how it can be understood in terms of its technological and business dimensions and ultimately be overcome to kick-start a virtuous cycle of data NEs. By drawing insights from existing literature and practitioner interviews, we present a research agenda to encourage further investigation into overcoming the CSP.},
}

@article{Liu2024_01,
  title = {A review of digital twin capabilities, technologies, and applications based on the maturity model},
  author = {Yang Liu and Jun Feng and Jiamin Lu and Siyuan Zhou},
  year = {2024},
  journal = {Advanced Engineering Informatics},
  volume = {62},
  pages = {102592},
  doi = {https://doi.org/10.1016/j.aei.2024.102592},
  url = {https://www.sciencedirect.com/science/article/pii/S1474034624002404},
  abstract = {The advanced stage of Industry 4.0 is characterized by the integration and interaction between physical and virtual spaces, and Digital Twin (DT) technology, congruent with this vision, has garnered extensive attention and has undergone large-scale implementation. Yet, in the practical implementation of Digital Twin projects, several issues persist: ① How to formulate reasonable task objectives and action plans before project implementation? ② how to determine and assess the development level of the digital twin during project implementation? ③ How to evaluate the effectiveness of digital twin after project completion and how to enhance improvement in the next steps? Consequently, a methodological model is urgently needed to evaluate the development process of Digital Twins, offering a benchmark for their design, development, and appraisal. To address these issues, this paper introduces a five-level Digital Twin Maturity Model (DTMM), which systematically aligns DT capabilities, phased objectives, and technical requirements within a unified framework, creating a theoretical system capable of assessing DT’s developmental level and specifying its construction trajectory. Further, this paper catalogs supporting tools aligned with the technical specifications stipulated in DTMM’s functional capabilities, aiding developers in devising implementation strategies. Additionally, it scrutinizes the application status across six DT vertical sectors, conducts maturity evaluations, and confirms the efficacy of the proposed model. The conclusion can be drawn that DT is still in its embryonic phase. This work aspires to assist project managers and public policymakers gain a more objective understanding of Digital Twin, offering references to facilitate their positive development and broader implementation.},
}

@article{Montenarh2024,
  title = {Unmasking the oligarchs – Using open source data to detect sanctions violations},
  author = {Jonas Montenarh and Simon Marsden},
  year = {2024},
  journal = {Journal of Economic Criminology},
  volume = {3},
  pages = {100055},
  doi = {https://doi.org/10.1016/j.jeconc.2024.100055},
  url = {https://www.sciencedirect.com/science/article/pii/S2949791424000071},
  abstract = {Russia’s invasion of Ukraine in February 2022 led Western governments to impose extensive sanctions on the Russian economy and the people who maintain the regime - the so-called oligarchs. The enforcement of these sanctions and the manoeuvres of these oligarchs to evade them have created new challenges for the authorities, industry and society at large. This study aims to unmask sanctioned oligarchs who are beneficial owners of companies in the UK. The approach is based on the assumption that the actions of oligarchs leave detectable traces in the UK company register. Inspired by simple tools developed by volunteers for citizens doing their own intelligence work, software has been developed to achieve three different research objectives: to develop indicators of sanctions evasion, to demonstrate that it is possible to do useful intelligence work using only the resources available to the average citizen, and to develop a software tool to identify officers in the UK company register who are suspected of being linked to sanctioned oligarchs. The findings underscore the importance of open source investigations, reflecting a burgeoning intelligence phenomenon introduced as ‘citizen intelligence’, which has the potential to enhance the protection of society by building a more democratic and diverse intelligence community. This study opens the door for further research into the actions and tactics of sanctioned individuals, providing insights to protect global financial integrity and security.},
}

@article{Muñoz2025,
  title = {Multisectoral decarbonisation strategies in Punta Arenas, Chile: A multi-renewable technologies approach},
  author = {Iván Muñoz and Francisco Fuentes},
  year = {2025},
  journal = {Solar Energy Advances},
  volume = {5},
  pages = {100087},
  doi = {https://doi.org/10.1016/j.seja.2024.100087},
  url = {https://www.sciencedirect.com/science/article/pii/S2667113124000378},
  abstract = {This study evaluates multisectoral energy planning to decarbonize Punta Arenas, Chile, transitioning from fossil fuels to renewable energies by 2050. Scenarios aligned with the Carbon Neutral 2050 (CN2050) plan, developed under the Long-Term Energy Planning (PELP) program by the Chilean Ministry of Energy, were assessed using EnergyPLAN. In 2019, Punta Arenas emitted 1.32 million tonnes of carbon dioxide equivalent (CO₂eq), projected to rise to 2.2 million tonnes by 2050 under a business-as-usual (BAU) scenario, with annual costs of 947 million euros. Implementing PELP CN2050 measures reduces emissions to 1.2 million tonnes and costs to 560 million euros, demonstrating that decarbonization can be achieved alongside economic savings. This validates the PELP CN2050 plan's effectiveness, highlighting that renewable energy integration supports sustainability and economic benefits. Reductions are achieved through energy efficiency, technological changes, and integrating renewable energies—particularly wind and solar thermal—in industrial, transport, and residential sectors. Electrification and green hydrogen for motive, thermal, and transport applications are crucial. Two cases were evaluated: importing green hydrogen or producing it locally via renewable-powered electrolysers. Sensitivity analyses increasing wind capacity from 13 MW to 310 MW showed that higher renewable integration reduces CO₂eq emissions and costs, indicating a negative abatement cost. Further decarbonization could be achieved by incorporating cogeneration, district heating, and synthetic fuels.},
}

@article{Yuan2025,
  title = {Trends and changes in academic libraries' data management functions: A topic modeling analysis of job advertisements},
  author = {Ye Yuan and A.M.K. {Yanti Idaya} and A. Noorhidawati and Guan Wang},
  year = {2025},
  journal = {The Journal of Academic Librarianship},
  volume = {51},
  pages = {103017},
  doi = {https://doi.org/10.1016/j.acalib.2025.103017},
  url = {https://www.sciencedirect.com/science/article/pii/S0099133325000138},
  abstract = {In the era of open science, academic libraries have transitioned from traditional resource providers to proactive platforms that drive data integration and knowledge innovation. This shift has led to the continuous evolution and expansion of their data management functions. This study aims to (i) track trends in academic library data management positions, (ii) identify key themes in job advertisements related to data management, and (iii) examine how these themes have evolved. Using text mining techniques, this study applied Latent Dirichlet Allocation (LDA) and TF-IDF vectorization to systematically analyze 803 job advertisements related to data management posted on the IFLA LIBJOBS platform from 1996 to 2023. The findings reveal that the development of these positions has undergone three phases: exploration, growth, and adjustment. Four core themes in data management functions emerged: “Cataloging and Metadata Management,” “Data Services and Support,” “Research Data Management,” and “Systems Management and Maintenance.” Over time, these themes have evolved from distinct roles to a more balanced distribution. Technological advancements, political initiatives, and shifts in the global data environment have influenced these trends. Notably, the rising demand for “Systems Management and Maintenance” highlights its critical role in ensuring data security, while the sustained need for “Cataloging and Metadata Management” underscores its foundational place in data management strategies. Meanwhile, the steady growth of “Data Services and Support” and “Research Data Management” reflects the adaptability and strategic adjustments of academic libraries in response to the rapidly changing information landscape. These insights offer valuable empirical evidence for library leaders and policymakers in strategic planning and capacity development, ensuring that libraries can effectively navigate the challenges of a dynamic research environment.},
}

@article{Cui2025,
  title = {Research on the mechanism of organizing and managing mainstream integrated media information resources in the era of big data},
  author = {Jindong Cui and Chenyu Li and Chenrui Bao and Guoli Qu},
  year = {2025},
  journal = {Expert Systems with Applications},
  volume = {266},
  pages = {126128},
  doi = {https://doi.org/10.1016/j.eswa.2024.126128},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417424029956},
  abstract = {As mainstream integrated media assumes increasing importance in guiding public opinion, achieving efficient organization and management of its information resources within the context of big data and the information economy has become a fundamental cornerstone for its development and value creation. This paper focuses on the content and dissemination characteristics of mainstream integrated media information, constructs a model for its organization and management, and examines various aspects such as decentralized information collection, multimodal resource processing, semantic feature extraction, information unit construction and association, information chain traceability, and simulation results. Additionally, it proposes management strategies and countermeasures from a comprehensive perspective of the entire information organization chain. The developed organization and management mechanism aligns with the evolving requirements of mainstream integrated media in the big data era, providing a foundation for the deep utilization and enhancement of information value.},
}

@article{Bauer2024,
  title = {What if? Numerical weather prediction at the crossroads},
  author = {Peter Bauer},
  year = {2024},
  journal = {Journal of the European Meteorological Society},
  volume = {1},
  pages = {100002},
  doi = {https://doi.org/10.1016/j.jemets.2024.100002},
  url = {https://www.sciencedirect.com/science/article/pii/S2950630124000024},
  abstract = {This paper provides an outlook on the future of operational weather prediction given the recent evolution in science, computing and machine learning. In many parts, this evolution strongly deviates from the strategy operational centres have formulated only several years ago. New opportunities in digital technology have greatly accelerated progress, and the full integration of computational science in numerical weather prediction centres is common knowledge now. Within the last few years, a vast machine learning research community has emerged for creating new and tailor-made products, accelerating processing and – most of all – creating emulators for the entire production of global forecasts that outperform traditional systems at the spatial resolution of the training data. In this context, the role of both numerical models and observations is changing from being equation to data driven. Model simulations and reanalyses are becoming the new currency for training machine learning, and operational centres are in a powerful position as they generate these datasets based on decades worth of experience. This environment creates incredible opportunities to progress much faster than in the past but also uncertainties about what the strategic implications on defining cost-effective and sustainable research and operations are, and how to achieve sufficient high-performance computing and data handling capacities. It will take individual national public services a while to understand what to focus on and how to coordinate their substantial investments in staff and infrastructure at institutional, national and international level. This paper addresses this new situation operational weather prediction finds itself in through formulating the most likely “what if?” scenarios for the near future. It also provides an outline for how weather centres could adapt.},
}

@article{Zabala-López2024,
  title = {A survey of data-centric technologies supporting decision-making before deploying military assets},
  author = {Alexandra Zabala-López and Mario Linares-Vásquez and Sonia Haiduc and Yezid Donoso},
  year = {2024},
  journal = {Defence Technology},
  volume = {42},
  pages = {226-246},
  doi = {https://doi.org/10.1016/j.dt.2024.07.012},
  url = {https://www.sciencedirect.com/science/article/pii/S221491472400182X},
  abstract = {In a time characterized by the availability of vast amounts of data, the effective utilization of information is critical for timely decision-making in military operations. However, processing large amounts of data requires computational resources and time. Therefore, decision makers have used data-centric technologies to take advantage of public and private data sources to support military operations. This survey explores the integration and application of data-centric technologies, such as data analytics, data science, and machine learning, to optimize decision-making workflows within military contexts supporting the deployment of military assets and resources. To address the information gap, this article presents a literature review, specifically a survey. Our survey examines the use of the mentioned technologies to process and analyze information that contributes to the phases of situational awareness, and planning in military environments. We then introduce a taxonomy of the approaches associated with implementing these technologies in military scenarios. Furthermore, we discuss relevant factors for the seamless integration of data-centric technologies into military decision-making processes, and reveal the importance of specialized personnel, architectures, and cybersecurity issues in the task of developing prototypes and models. The findings of this paper aim to provide valuable insights for military institutions, offering a deeper understanding of the use of data-centric technologies as innovative practices to enhance the effectiveness of military decision-making.},
}

@article{Kapustina2024,
  title = {User-friendly and industry-integrated AI for medicinal chemists and pharmaceuticals},
  author = {Olga Kapustina and Polina Burmakina and Nina Gubina and Nikita Serov and Vladimir Vinogradov},
  year = {2024},
  journal = {Artificial Intelligence Chemistry},
  volume = {2},
  pages = {100072},
  doi = {https://doi.org/10.1016/j.aichem.2024.100072},
  url = {https://www.sciencedirect.com/science/article/pii/S2949747724000307},
  abstract = {Artificial intelligence has brought crucial changes to the whole field of natural sciences. Myriads of machine learning algorithms have been developed to facilitate the work of experimental scientists. Molecular property prediction and drug synthesis planning become routine tasks. Moreover, inverse design of compounds with tunable properties as well as on-the-fly autonomous process optimization and chemical space exploration became possible in silico. Affordable robotic platforms exist able to perform thousands of experiments every day, analyzing the results and tuning the protocols. Despite this, most of these developments get trapped at the stage of code or overlooked, limiting their use by experimental scientists. Meanwhile, visibility and the number of user-friendly tools and technologies available to date is too low to compensate for this fact, rendering the development of novel therapeutic compounds inefficient. In this Review, we set the goal to bridge the gap between modern technologies and experimental scientists to improve drug development efficacy. Here we survey advanced and easy-to-use technologies able to help medical chemists at every stage of their research, including those integrated in technological processes during COVID-19 pandemic motivated by the need for fast yet precise solutions. Moreover, we review how these technologies are integrated by industry and clinics to streamline drug development and production. These technologies already transform the current paradigm of scientific thinking and revolutionize not only medicinal chemistry, but the whole field of natural sciences.},
}

@article{Aghaabbasi2025,
  title = {Potentials of digital twin system for analyzing travel behavior decisions},
  author = {Mahdi Aghaabbasi and Soheil Sabri},
  year = {2025},
  journal = {Travel Behaviour and Society},
  volume = {38},
  pages = {100902},
  doi = {https://doi.org/10.1016/j.tbs.2024.100902},
  url = {https://www.sciencedirect.com/science/article/pii/S2214367X24001650},
  abstract = {This review explores the potential of digital twin systems to provide a more holistic representation of travel behavior and support transportation planning and policymaking. The paper introduces the concept of digital twins, their key characteristics, and their applications in various domains, including transportation. It discusses the traditional methods used in travel behavior analysis and their limitations, as well as the potential advantages of digital twin systems, such as the integration of heterogeneous data sources, real-time monitoring and prediction, and the ability to simulate and evaluate various policy scenarios. The review also identifies the key components of digital twin systems, the challenges associated with their implementation, and the current state of research on digital twins and related methods in travel behavior analysis. The paper highlights research gaps and future directions, emphasizing the need for privacy-preserving techniques, real-world case studies, and the integration of digital twins with decision support systems. Finally, the review discusses the broader implications of digital twin systems for transportation planning and policymaking, concluding by emphasizing the need for interdisciplinary collaboration and stakeholder engagement to fully realize the potential of digital twins in analyzing travel behavior decisions and shaping the future of transportation systems.},
}

@article{Halford2024,
  title = {Using chat GPT to evaluate police threats, risk and harm},
  author = {Eric Halford and Andrew Webster},
  year = {2024},
  journal = {International Journal of Law, Crime and Justice},
  volume = {78},
  pages = {100686},
  doi = {https://doi.org/10.1016/j.ijlcj.2024.100686},
  url = {https://www.sciencedirect.com/science/article/pii/S1756061624000387},
  abstract = {General purpose artificial intelligence (GPAI) is a form of advanced AI system that includes the recently introduced ChatGPT. GPAI is known for its capacity to understand and emulate human responses, and potentially offers an opportunity to reduce human error when conducting tasks that involve analysis, judgement, and reasoning. To support officers to do this, the police presently use a range of decision-making support tools, one of which is called THRIVE (Threat, Harm, Risk, Investigation, Vulnerability, and Engagement). THRIVE is designed to provide police practitioners with a model to improve their identification and response to vulnerability. Despite the existence of such decision models, a 2020 meta-analysis of police cases resulting in death or serious injury identified contributory failures that included poor risk identification, risk management, failure to adhere to evidentiary processes, poor criminal investigations, and inadequate police engagement with victims, including the level of care and assistance provided (Allnock, et al, 2020). Importantly, this report outlined human error as being a major underpinning factor of the failures. Although GPAI offers an opportunity to improve analysis, judgement, and reasoning, such systems have not yet been tested in policing, a field where any reduction in human error, particularly in the assessment of threat, harm, risk, and vulnerability can potentially save lives. This study is the first attempt to do this by using the chain-of-thought prompt methodology to test the GPAI ChatGPT (3.5 vs 4) in a controlled environment using 30 life-like police scenarios, crafted, and analyzed by expert practitioners. In doing so, we identify that ChatGPT 4 significantly outperforms its 3.5 predecessor, indicating that GPAI presents considerable opportunity in policing. However, systems that use this technology require extensive directional prompting to ensure outputs that can be considered accurate, and therefore, potentially safe to utilize in an operational setting. The article concludes by discussing how practitioners and researchers can further refine police related chain-of-thought prompts or use application programming interfaces (APIs) to improve responses provided by such GPAI.},
}

@article{Bello2025,
  title = {Cloud computing for chatbot in the construction industry: An implementation framework for conversational-BIM voice assistant},
  author = {Sururah A. Bello and Lukumon O. Oyedele and Lukman A. Akanbi and Abdul-Lateef Bello},
  year = {2025},
  journal = {Digital Engineering},
  volume = {5},
  pages = {100031},
  doi = {https://doi.org/10.1016/j.dte.2024.100031},
  url = {https://www.sciencedirect.com/science/article/pii/S2950550X24000311},
  abstract = {This study presents a structural framework for selecting cloud services for the Conversational AI system implementation in the construction industry using Design Thinking Methodology. A focus group discussion approach was used to obtain user requirements from construction workers to implement the Conversational AI for BIM. This resulted in five factors: finance, speed of operation, privacy, estimation, and interface. The user specifications were mapped into technical modules, which were used to select cloud services employed to implement the virtual assistant for the construction industry. The study thus presented the comprehensive requirements for the different categories of construction workers to implement the Conversational-BIM Chatbot (Conversational-BIM) system. Furthermore, the study presented the architecture of Conversational-BIM using Amazon Web Services. The study is useful to researchers and IT developers in implementing chatbots for the construction industry as it presents the relevant considerations for conversational AI applications in the industry.},
}

@article{Hernández}2024,
  title = {Urban sustainability in social housing environments: A spatial impact assessment in Bogotá, Colombia},
  author = {Jhon Ricardo {Escorcia Hernández} and Sara {Torabi Moghadam} and Patrizia Lombardi},
  year = {2024},
  journal = {Cities},
  volume = {154},
  pages = {105392},
  doi = {https://doi.org/10.1016/j.cities.2024.105392},
  url = {https://www.sciencedirect.com/science/article/pii/S0264275124006061},
  abstract = {As cities adapt to the challenges of rapid urbanization, sustainable development goals, and post-pandemic realities, understanding the intricacies of urban sustainability is vital. This study presents a spatial impact assessment of urban sustainability, focusing on the distinctive requirements of social housing urban environments in Bogota, Colombia. By contextualizing this study in the ever-evolving post-pandemic paradigm, it depicts the current conditions of the city towards future development scenarios. The sustainability assessment relies on a set of 11 Key Performance Indicators (KPIs), emphasizing the particular needs of social housing and offering a comprehensive analysis that links the conditions of the urban environment, environmental quality, and social well-being, all under the umbrella of Sustainable Development Goal 11. Specific spatial analysis using GIS tools (e.g. spatial interpolation, network analysis, multispectral imagery analysis), statistical analysis, and data engineering were employed to evaluate the selected KPIs. This evaluation revealed distinct spatial patterns of socioeconomic inequity, primarily impacting areas with a higher concentration of social housing within the city. These findings highlight critical areas requiring strategic attention to promote the city's sustainability transition. Key insights indicate that areas lacking sufficient access to green spaces and essential urban services, alongside higher pollution levels, are associated with lower social well-being, reduced perceptions of security, and are characterized by high population density and a significant presence of social housing. These results, available on an interactive web-GIS app, underscore the imperative for policies and planning that not only address the provision of social housing but also ensure a sustainable built environment, shaping a sustainable, inclusive, safe, and resilient future. The methodology and insights presented in this study are also applicable and scalable to similar urban contexts globally.},
}

@article{Tsvetkova2025,
  title = {Smart port city: Digital interfaces for enhancing RoPax port and city co-existence},
  author = {Anastasia Tsvetkova and Irina Wahlström and Kristel Edelman and Riikka Franzén and Yiran {Chen Zhou} and Magnus Hellström},
  year = {2025},
  journal = {Cities},
  volume = {161},
  pages = {105936},
  doi = {https://doi.org/10.1016/j.cities.2025.105936},
  url = {https://www.sciencedirect.com/science/article/pii/S0264275125002367},
  abstract = {The transition towards smart cities demands a multifaceted approach, which becomes particularly challenging in port cities, where urban life intersects with global logistics. While ports serve as critical logistic nodes, they hold the potential for diverse urban uses beyond mere ship traffic. This study investigates the integration of physical and digital infrastructures in port cities, focusing on the area where urban space expands towards RoPax terminals that combine wheeled cargo and passenger transport. The study is designed as a case study of a Baltic Sea port city undergoing significant infrastructural changes, including the construction of a new passenger terminal and rearrangement of the area surrounding the port. We explore the development of digital infrastructure to facilitate the coexistence of a liveable city and efficient transport connections. Our analysis is based on the dimensions of a smart city and related interfaces. The findings identify key digital interfaces between the city and its port, highlighting five types of such interfaces and the unique challenges and opportunities in balancing the smart city goals of urban and port authorities. This study contributes to the literature on smart cities by demonstrating the critical role of ports in the formation of smart cities, with implications for similar urban contexts.},
}

@article{Hang2025,
  title = {Evolving biomaterials design from trial and error to intelligent innovation},
  author = {Ruiyue Hang and Xiaohong Yao and Long Bai and Ruiqiang Hang},
  year = {2025},
  journal = {Acta Biomaterialia},
  doi = {https://doi.org/10.1016/j.actbio.2025.03.013},
  url = {https://www.sciencedirect.com/science/article/pii/S174270612500176X},
  abstract = {The design and exploration of biomaterials plays a pivotal role in many fields, including medical and engineering. The prevailing approach to biomaterials discovery relies on orthogonal experiments, with repeated attempts to optimize experimental conditions. This method has proven invaluable in gaining experience, but it is also inefficient and challenging to predict the behavior of complex systems. The advent of high-throughput screening (HTS) techniques has led to a notable enhancement in the efficiency of biomaterials development, enabling researchers to assess a vast array of material combinations within a relatively short timeframe. Nevertheless, the emergence of artificial intelligence (AI) has been the catalyst for a new era in biomaterials design. AI has markedly accelerated the development of new materials by enabling the prediction of material properties through machine learning (ML) and deep learning models, as well as optimizing the design pipeline. This review will present a systematic overview of the development of biomaterials design technology. It will also explore the integration of AI with HTS technology and envisage the potential of AI-driven materials design in biomaterials for the future. Statement of significance The design and synthesis of biomaterials have undergone substantial shifts, reflecting evolving research paradigms. High-throughput screening has emerged as a broad and efficient alternative to traditional free-form combination methods in biomaterial design. The advent of artificial intelligence (AI) enables personalized biomaterial design and, as a transformative tool in biomaterial development, is poised to redefine the field and offer long-term solutions for its advancement. Building on these advancements, this review systematically summarizes the evolution of biomaterial design, offering insights into the future trajectory of the field.},
}

@article{Mavlutova2025,
  title = {The role of green digital investments in promoting sustainable development goals and green energy consumption},
  author = {Inese Mavlutova and Aivars Spilbergs and Inna Romanova and Jekaterina Kuzmina and Andris Fomins and Atis Verdenhofs and Andris Natrins},
  year = {2025},
  journal = {Journal of Open Innovation: Technology, Market, and Complexity},
  volume = {11},
  pages = {100518},
  doi = {https://doi.org/10.1016/j.joitmc.2025.100518},
  url = {https://www.sciencedirect.com/science/article/pii/S2199853125000538},
  abstract = {The study investigates how digitalization in the financial sector, implementing innovative technologies and green finance solutions, leads to achieving sustainable development goals in general and environmental sustainability that promotes the transition to green energy in particular. The study results will increase the knowledge of digital financial technologies, open IC technology innovations and green finance in achieving sustainable development goals (SDGs). The authors examined issues related to digital technologies and green finance and their impact on CO2 emissions and green energy consumption in European countries: an extensive literature review of publications on green finance, technologies, innovation and green energy was carried out. Tested hypotheses revealed a statistically significant impact of green digital finance investments on primary energy consumption from renewable sources increase and the carbon intensity of electricity generation downfall due to digitalization and climate bonds.},
}

@article{Pampus2025,
  title = {Pattern-based Requirements Elicitation for Sovereign Data Sharing},
  author = {Julia Pampus and Maritta Heisel},
  year = {2025},
  journal = {Procedia Computer Science},
  volume = {254},
  pages = {147-156},
  doi = {https://doi.org/10.1016/j.procs.2025.02.073},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050925004235},
  abstract = {The growing data economy increasingly focuses on self-determined and autonomous data sharing, supported by infrastructures that create a trustful and secure environment. A key feature in this context is the offering, negotiation, and enforcement of so-called data usage conditions (DUCs), also known as policies. The design of involved software systems requires a structured and stakeholder-specific elicitation of technical requirements. To address this issue, we define a requirements model, consisting of actor, dataset, and condition entities, for sovereign data sharing and present a method for requirements elicitation in form of a five-step agenda with 13 validation conditions (VCs). The application of this method produces a set of instantiated requirements templates that provide descriptive information about involved actors, identified datasets, and applied DUCs. We demonstrate our method using an established use case from the automotive industry and evaluate it in qualitative expert interviews.},
}

@article{Adorno2025,
  title = {Combining spatial clustering and spatial regression models to understand distributional inequities in access to urban green spaces},
  author = {Bruno Vargas Adorno and Rafael H.M. Pereira and Silvana Amaral},
  year = {2025},
  journal = {Landscape and Urban Planning},
  volume = {256},
  pages = {105297},
  doi = {https://doi.org/10.1016/j.landurbplan.2025.105297},
  url = {https://www.sciencedirect.com/science/article/pii/S0169204625000040},
  abstract = {Proximity to urban green spaces offers numerous benefits, sparking increased research and policy interest in equitable access for different population groups. While spatial analyses evaluate access to urban green space, previous studies overlook fine-grained spatial disparities, needed for targeted urban planning. Spatial clustering models (Local Indicators of Spatial Association – LISA) group values significantly higher and lower than the average in the geographic space. In turn, spatial regression (Geographically Wheigted Regression – GWR) reveals the strength and direction of the correlation between variables across space. Here, we investigate whether and how the combination of both types of models helps examine distributional green equity. We show how combining LISA and GWR gives a more nuanced understanding of distributional green equity. We apply this approach to Goiânia, Brazil, with an empirical analysis of access to three categories of green spaces: tree cover, herb-shrub, and public green spaces. Using open-source methods and tools, we examine variations in accessibility for black people, women, and people of different age, literacy, and income groups. We used a new accessibility metric accounting for the size/area of green spaces, walking times and competition for accessing green spaces. The analyses revealed access disparities by population group and green space category identifying specific regions in the city and population groups with consistently limited access to urban green spaces, guiding planners with refined information to prioritize green space interventions where they are most likely needed. This method enables targeted, equitable urban planning that fosters inclusive access to green spaces for diverse communities.},
}

@article{Fosch-Villaronga2023,
  title = {How can ISO 13482:2014 account for the ethical and social considerations of robotic exoskeletons?},
  author = {Eduard Fosch-Villaronga and Carlos José Calleja and Hadassah Drukarch and Diego Torricelli},
  year = {2023},
  journal = {Technology in Society},
  volume = {75},
  pages = {102387},
  doi = {https://doi.org/10.1016/j.techsoc.2023.102387},
  url = {https://www.sciencedirect.com/science/article/pii/S0160791X23001926},
  abstract = {This paper analyzes and classifies regulatory gaps and inconsistencies in ISO 13482:2014 (‘Safety Requirements for Personal Care Robots'), specifically regarding robotic lower-limb exoskeletons, being personal care robots, for everyday activities. Following a systematic literature review, our findings support the conclusion that, even though ISO 13482:2014 has proven to be a substantial step towards regulating that type of wearable robot, it fails to address safety sufficiently and comprehensively. That failure results in a general overlook of critical legal, ethical, and social considerations when designing robots, with the consequence that seemingly safe systems might nonetheless harm end-users. Notwithstanding those limitations and impediments to the development of safe technologies, to date, there has been no thorough assessment of how the standard regulates the development of exoskeletons and whether it requires any improvement in light of ethical, legal, and societal considerations. To bridge this gap, we compile relevant areas for improvement concerning ISO 13482:2014 fueled by these considerations. We do so in an accessible manner and provide concrete recommendations to help decision-makers overcome the standard's drawbacks.},
}

@article{Qiu2024,
  title = {Semantic information extraction and search of mineral exploration data using text mining and deep learning methods},
  author = {Qinjun Qiu and Miao Tian and Liufeng Tao and Zhong Xie and Kai Ma},
  year = {2024},
  journal = {Ore Geology Reviews},
  volume = {165},
  pages = {105863},
  doi = {https://doi.org/10.1016/j.oregeorev.2023.105863},
  url = {https://www.sciencedirect.com/science/article/pii/S0169136823005796},
  abstract = {Large-scale mineral resource reports offer a wealth of information for geological knowledge mining and mineral explorers’ knowledge discovery. The geological conditions in which mineral deposits develop may be learned a great deal from these mineral exploration reports. Mineral exploration data may be queried and aggregated to effectively mitigate future exploration risks and reduce costs. However, due to the reports being presented in unstructured textual format, it becomes challenging to extract valuable geological data without manually scanning through a vast number of reports. This laborious process poses difficulties for geologists. To address this issue, this paper proposes a system that extracts a set of geologically relevant keywords/keyphrases of each chapter from each mineral exploration reposts using latent Dirichlet allocation (LDA), develops a topic graph, recognizes the geological entity and related relations for constructing a knowledge graph, and uses visualization of those graphs (e.g., topic graphs and knowledge graphs) to explore the contents of the report. The text mining and machine learning technique described here serves as the foundation for future research into incorporating semantic analysis into geological information extraction. The findings of this study show how automated text analysis may help with the quick processing of huge quantities of reports in order to identify target mineral systems and their related geological location and rock mineral composition. The suggested approaches can quickly and reliably convert mineral exploration data (e.g., text, figure, and table) into a structured form, which is a hitherto untouched field in geological knowledge mining.},
}

@article{Ikäheimo2023,
  title = {Detecting pitfall systems in the Suomenselkä watershed, Finland, with airborne laser scanning and artificial intelligence},
  author = {Janne Ikäheimo},
  year = {2023},
  journal = {Journal of Archaeological Science: Reports},
  volume = {51},
  pages = {104216},
  doi = {https://doi.org/10.1016/j.jasrep.2023.104216},
  url = {https://www.sciencedirect.com/science/article/pii/S2352409X23003917},
  abstract = {This article examines the use of airborne laser scanning data and semi-automatic detection algorithms to identify pitfall sites in the northern part of the Suomenselkä watershed in Finland. The results show that new sites can be effectively detected with these methods, even in areas recently surveyed archaeologically. Most of the previously known pitfall sites were also easily distinguishable from the data. The geographic location of the newly discovered sites confirmed previous interpretations of the prehistoric and historic hunting of cervids with pitfalls in the research area. Yet, further research is needed to refine the interpretations concerning the use and temporal sequence of pitfall rows both in Finland and elsewhere in Fennoscandia.},
}

@article{Dabrock2025,
  title = {Generating a nationwide residential building types dataset using machine learning},
  author = {Kristina Dabrock and Jens Ulken and Noah Pflugradt and Jann Michael Weinand and Detlef Stolten},
  year = {2025},
  journal = {Building and Environment},
  volume = {274},
  pages = {112782},
  doi = {https://doi.org/10.1016/j.buildenv.2025.112782},
  url = {https://www.sciencedirect.com/science/article/pii/S0360132325002641},
  abstract = {The lack of high-resolution building data is an obstacle to the development of detailed, spatially explicit recommendations for decarbonization measures. In an effort to fill this gap, this study outlines the creation of a building level dataset based on standardized building archetypes for all German residential buildings. A machine learning approach using XGBoost is used to train models to predict the size class and construction year of individual buildings. Refurbishment states are assigned based on federal state level statistics. Based on these characteristics, TABULA building archetypes are assigned. The training data generation is primarily based on the grid dataset of the German census. The data is enriched with morphological features of buildings and neighborhoods, as well as socio-economic characteristics. The machine learning models perform with accuracies of 97.4 % and 73.9 %, respectively, on a test set at the individual building level. The distribution of size classes and construction years in the resulting dataset shows a high degree of agreement with official statistics at the federal state level, but also a tendency to overrepresent majority classes. This study proves that the chosen methodology is suitable for generating a complete nationwide dataset. By providing spatially resolved, individual building data that can serve as a proxy for the energetic properties of buildings, the resulting dataset can facilitate building-related energy transition analyses.},
}

@article{Dörr2024,
  title = {Beyond direct stakeholders: The extensive scope of Societal Corporate Digital Responsibility (CDR)},
  author = {Saskia Dörr and Christian Lautermann},
  year = {2024},
  journal = {Organizational Dynamics},
  volume = {53},
  pages = {101057},
  doi = {https://doi.org/10.1016/j.orgdyn.2024.101057},
  url = {https://www.sciencedirect.com/science/article/pii/S0090261624000305},
  abstract = {This paper delves into the concept of Societal Corporate Digital Responsibility (CDR), expanding the traditional focus of CDR from direct stakeholders to a broader societal perspective. Societal CDR is defined is defined as the responsibility of companies to develop their digital business strategies considering the impacts on societal stakeholders and institutions. This novel approach emphasizes the indirect, yet significant effects of digital technologies on various societal domains such as economic, social, and political spheres. It underscores the importance of addressing passive stakeholder groups and societal institutions that do not have a direct relationship with businesses but are nevertheless impacted by digitalization. The paper discusses challenges in managing Societal CDR, such as measuring societal impact and influencing indirect stakeholders. It also explores the roles and responsibilities of businesses in fostering a thriving digital society by examining the vitality factors across economic, social, and political domains. The paper concludes with practical recommendations for businesses to integrate Societal CDR into their strategies, highlighting the importance of inclusivity, ethical practices, and transparency in the digital era},
}

@article{Fang2024,
  title = {Unlocking the potential of inventory management: Integrating digital transformation with firm practices},
  author = {Yuan Fang and Qian Zhou and Xiandeng Jiang and Chao Li},
  year = {2024},
  journal = {Economic Modelling},
  volume = {139},
  pages = {106841},
  doi = {https://doi.org/10.1016/j.econmod.2024.106841},
  url = {https://www.sciencedirect.com/science/article/pii/S0264999324001986},
  abstract = {Digitalization has brought a substantial economic impact on firms' production and operation. This study utilizes firm-level data from the China National Tax Statistics Database to investigate the impact of the Broadband China Strategy on firms’ inventory levels, which are an important strategic resource for firms. We find that Internet access and development result in a sharp decrease in inventory levels, particularly in small and medium-sized enterprises. Furthermore, digital infrastructure can mitigate the negative effects of an insufficient supply of transportation infrastructure. This paper also shows that the enhancements in inventory management are attributable to the increased adoption and improved affordability of broadband Internet among firms. We discuss the role of inventory, providing a thorough understanding of the logic behind the enhanced productivity improvements brought about by digitalization.},
}

@article{Chao2023,
  title = {The impact of air pollution on startups and structural transformation: Evidence from newly registered enterprises in China},
  author = {Songlei Chao and Chengfeng Huang and Wenxuan Chen},
  year = {2023},
  journal = {Journal of Cleaner Production},
  volume = {422},
  pages = {138537},
  doi = {https://doi.org/10.1016/j.jclepro.2023.138537},
  url = {https://www.sciencedirect.com/science/article/pii/S0959652623026951},
  abstract = {This paper examines the effects of air pollution on the number of startups and industry structures in China, using data on startups in various industries and air pollution at the county level. The research shows that air pollution has a significant negative impact on the number of startups, with a 1 μg/m3 rise in PM2.5 causing a 1.83% drop in the number of startups. The tertiary industry is most negatively impacted by air pollution, with a 1 μg/m3 rise in PM2.5 resulting in a 2.70% decline in the number of startups in this sector. Heterogeneity analysis reveals that sub-industries including scientific research and technical services, education, health and public services, and culture and entertainment are those that are most negatively impacted by air pollution. Nonlinear analysis shows that the harmful consequences of air pollution only become noticeable when the concentration of pollutants exceeds a particular point. Further research reveals that air pollution drives the relocation of entrepreneurial activity, which may be a significant influencing factor for local startups. This paper fills an academic gap on air pollution's effects on local economies and industrial structures. The results show that the natural environment is a crucial component of the business environment and has a significant practical impact on boosting regional economic vitality and promoting the transformation of industrial structures.},
}

@article{Clopin2025,
  title = {Integrated models of nutrient dynamics in lake and reservoir watersheds: A systematic review and integrated modelling decision pathway},
  author = {Floran Clopin and Ilaria Micella and Jorrit P. Mesman and Ma Cristina Paule-Mercado and Marina Amadori and Shuqi Lin and Lisette N. {de Senerpont Domis} and Jeroen J.M. {de Klein}},
  year = {2025},
  journal = {Environmental Modelling & Software},
  volume = {185},
  pages = {106321},
  doi = {https://doi.org/10.1016/j.envsoft.2025.106321},
  url = {https://www.sciencedirect.com/science/article/pii/S1364815225000052},
  abstract = {Eutrophication of inland water bodies is a serious environmental threat. This review explores current integrated models for lake and reservoir ecosystems that focus on nutrient dynamics at a catchment scale. Many studies applied either watershed or lake/reservoir models, however, 49 studies were finally selected that combined both. We derived a list of 21 watershed models, 23 lake/reservoir models, and 6 hybrid models in different sets of combinations, with a range of objectives (e.g. understanding the natural processes, predicting, and analysing climate change and land-use scenarios, or evaluating the different management options). Some integrated models had multiple applications whereas others were only applied once, with an uneven global geographical distribution. To aid model selection by future users, we present a support tool discriminating the models by their features and application fields. This study encourages the development of open-source tools aiding interdisciplinary collaborations and further research in the field of integrated modelling.},
}

@article{Pentland2025,
  title = {Better job application systems: Objectively assessing measures of job performance from asynchronous video interviews},
  author = {Steven J. Pentland and Xinran Wang and Nathan W. Twyman},
  year = {2025},
  journal = {Information & Management},
  volume = {62},
  pages = {104077},
  doi = {https://doi.org/10.1016/j.im.2024.104077},
  url = {https://www.sciencedirect.com/science/article/pii/S0378720624001599},
  abstract = {When selecting top candidates for a job, organizations would prefer to not accidentally filter out the highest quality candidates. But an unbiased, detailed assessment of every applicant in a large candidate pool has been prohibitively costly. Asynchronous video interviews (AVIs) are inspiring new ideas for predicting job performance early in the hiring process by providing a rich source of signals. We propose that automatic analysis of interview data can improve candidate filtering at the application stage. The potential of this approach is clear, but there is a need for a structured framework and benchmarks to develop effective and valid application systems. We therefore propose a design framework that enhances the objectiveness of automated candidate assessment using AVIs through principles such as using behavioral cues that are hard to fake and using unbiased, validated labels in training sets. We demonstrate the implementation of this framework and evaluate its potential by building a prototype for automatically assessing general mental ability, an important and generalizable indicator of job performance. Results show that if new application systems adhere to this framework, more objective measures of job performance can be assessed automatically from AVI recordings. More generally, the study guides advancement of automated AVI platforms with a focus on efficacy and fairness.},
}

@article{Long2025,
  title = {A comprehensive assessment of connected and automated vehicle analytical, modeling, and simulation tools},
  author = {Keke Long and Ke Ma and Qianwen Li and Xiaopeng Li and Zhitong Huang and Rachel James and Amir Ghiasi},
  year = {2025},
  journal = {Transportation Research Part E: Logistics and Transportation Review},
  volume = {196},
  pages = {104007},
  doi = {https://doi.org/10.1016/j.tre.2025.104007},
  url = {https://www.sciencedirect.com/science/article/pii/S1366554525000481},
  abstract = {Connected and Automated Vehicles (CAVs) promise to redefine the future of transportation. Infrastructure Owners and Operators (IOOs require advanced analytical, modeling, and simulation (AMS) tools) to grasp the impact of CAVs on their strategic goals, such as improving safety, enhancing mobility, and advancing equity, and assess policy modifications to steer the deployment of this technology effectively. Although recent research has made strides in developing models that characterize CAV behavior in mixed-traffic scenarios, significant modeling gaps persist. These gaps hinder decision-makers and policymakers from fully understanding how CAVs can serve as an instrumental force in driving desired improvements in transportation system performance. This study aims to identify these gaps by organizing two stakeholder webinars that focus on CAV AMS tools. The discussions from these webinars are categorized into three primary areas: CAV technologies, road user behaviors, and system-level modeling. This categorization helps structure an in-depth literature review designed to pinpoint existing shortcomings in CAV research. The study also provides an overview of current datasets that are both representative and capable of addressing some of these research gaps. Ultimately, this study seeks to act as a valuable reference for directing future research efforts in CAVs.},
}

@article{Thé}2023,
  title = {Transforming drug discovery with a high-throughput AI-powered platform: A 5-year experience with Patrimony},
  author = {François-Xavier {Blaudin de Thé} and Claire Baudier and Renan {Andrade Pereira} and Céline Lefebvre and Philippe Moingeon},
  year = {2023},
  journal = {Drug Discovery Today},
  volume = {28},
  pages = {103772},
  doi = {https://doi.org/10.1016/j.drudis.2023.103772},
  url = {https://www.sciencedirect.com/science/article/pii/S135964462300288X},
  abstract = {High-throughput computational platforms are being established to accelerate drug discovery. Servier launched the Patrimony platform to harness computational sciences and artificial intelligence (AI) to integrate massive multimodal data from internal and external sources. Patrimony has enabled researchers to prioritize therapeutic targets based on a deep understanding of the pathophysiology of immuno-inflammatory diseases. Herein, we share our experience regarding main challenges and critical success factors faced when industrializing the platform and broadening its applications to neurological diseases. We emphasize the importance of integrating such platforms in an end-to-end drug discovery process and engaging human experts early on to ensure a transforming impact.},
}

@article{Kim2024,
  title = {Enhancing water management and urban flood resilience using Hazard Capacity Factor Design (HCFD) model: Case study of Eco-Delta city, Busan},
  author = {Jaekyoung Kim and Jongpyo Park and Samuel Park and Junsuk Kang},
  year = {2024},
  journal = {Sustainable Cities and Society},
  volume = {115},
  pages = {105851},
  doi = {https://doi.org/10.1016/j.scs.2024.105851},
  url = {https://www.sciencedirect.com/science/article/pii/S2210670724006759},
  abstract = {This study evaluated future climate scenarios and the changes in urban flood resistance capacity in Busan Eco-Delta City using the hazard capacity factor design (HCFD) model. It analyzed the flood reduction effects of both gray and green infrastructure. Despite existing flood safety systems, there is a growing need to enhance urban flood resilience due to increasing heavy rainfall, unpredictable precipitation, and frequent typhoons driven by climate change. The HCFD model predicted urban flood volumes of Busan Eco-Delta City and analyzed the effectiveness of gray and green infrastructure in flood control. A stormwater management model (SWMM) simulated urban flood resistance capacity under the SSP1–2.6 climate change scenario. Results indicate that for an anticipated 500 mm rainfall over 3 h, green infrastructure can mitigate floods by 9 % to 17.6 %, while gray infrastructure can reduce flooding by 24 % to 32.1 %. The integration of gray and green infrastructure leads to an overall flood mitigation ranging from 47.1 % to 63.3 %. A notable contribution of this research is its predictive analysis of future flood scenarios using model-based scenario analysis and decision support algorithms, offering valuable insights into changes in urban flood resistance capacity and strategies for effective flood control decision-making.},
}

@article{Radunović2025,
  title = {The efficiency of ICT suppliers' product security incident response teams in reducing the risk of exploitation of vulnerabilities in the wild},
  author = {Vladimir Radunović and Mladen Veinović and Aleksandar Jevremović},
  year = {2025},
  journal = {Computers & Security},
  volume = {152},
  pages = {104388},
  doi = {https://doi.org/10.1016/j.cose.2025.104388},
  url = {https://www.sciencedirect.com/science/article/pii/S016740482500077X},
  abstract = {Exploitation of vulnerabilities in digital products is among the key components of cyberattacks. Suppliers of digital products use different security-by-design practices, such as a product security incident response team (PSIRT), to respond to discovered vulnerabilities and minimise the cybersecurity risk. However, the efficiency of such practices, including PSIRT, remains underexplored. This paper evaluates the efficiency of PSIRT in reducing risks of exploitation of vulnerabilities 'in the wild' (i.e. their active use in real-world cyberattacks) using a customised model based on randomised matched case-control design with data from authoritative public sources. Results show that PSIRT reduces the likelihood of exploitation by 17 % (absolute risk reduction). Additionally, factors like the availability of proof of concept for vulnerability exploitation, type of supplier's industry, and the open-source nature of its products influence the risk altering the absolute risk reduction by 10 %, 3.6 % and 2.2 % respectively. The study confirms PSIRT as a good practice that cybersecurity practitioners – particularly large suppliers and suppliers to critical infrastructure – should consider in order to reduce risk of vulnerability exploitation in the wild. It recommends coupling PSIRT with other security-by-design practices to maximise risk reduction. The proposed model allows researchers and practitioners to assess the efficiency of similar practices in reducing the risk of vulnerability exploitation.},
}

@article{Löwgren}2025,
  title = {Towards sustainable chemical process design: Revisiting the integration of life cycle assessment},
  author = {Bartolomeus {Häussling Löwgren} and Christian Hoffmann and Martina G. Vijver and Bernhard Steubing and Giuseppe Cardellini},
  year = {2025},
  journal = {Journal of Cleaner Production},
  volume = {491},
  pages = {144831},
  doi = {https://doi.org/10.1016/j.jclepro.2025.144831},
  url = {https://www.sciencedirect.com/science/article/pii/S0959652625001817},
  abstract = {Life-cycle assessment (LCA) is essential for sustainable chemical process design. However, current integrations treat LCA as a top-level environmental assessment tool, risking superficial integration and perpetuating conventional process design assumptions. This contribution reviews how LCA is integrated with model-based chemical process design. It focuses on current practices, challenges in goal and scope, modelling, computational, and interpretation integration, and discusses the integration of LCA's core features alongside profit-driven assumptions. The contribution identified more than 100 articles via a hybrid search method and reviewed 53 based on a saturation curve, resulting in 25 metrics to assess process design and LCA integration. To assist practitioners, a comprehensive classification of computational integrations is provided. The review highlights the following gaps: most studies (74%) focus only on cradle-to-gate phases, neglect use and end-of-life phases (89%) and do not define the function (92%). However, including a user function perspective could enhance the integration of use and end-of-life scenarios, supporting circular strategies and sufficiency measures. Additionally, environmental externalities are systematically excluded during model linkage, and most studies concentrate on energy utilities (75%) and material inputs (70%), whereas emissions (26%), waste, and wastewater (25%) are frequently overlooked, which emphasises the dominance of economic factors in current design studies.},
}

@article{Kim2024_01,
  title = {QSOX2 Upregulated in triple-negative breast cancer exacerbates patient prognosis by stabilizing integrin β1},
  author = {A-In Kim and Ji Hoon Oh and Je-Yoel Cho},
  year = {2024},
  journal = {Heliyon},
  volume = {10},
  pages = {e27148},
  doi = {https://doi.org/10.1016/j.heliyon.2024.e27148},
  url = {https://www.sciencedirect.com/science/article/pii/S2405844024031797},
  abstract = {Breast cancer (BC) remains a significant global health threat, with triple-negative breast cancer (TNBC) standing out as a particularly aggressive subtype lacking targeted therapies. Addressing this gap, we propose Quiescin Q6 sulfhydryl oxidase 2 (QSOX2) as a potential therapeutic target, a disulfide bond-forming enzyme implicated in cancer progression. Using publicly available datasets, we conducted a comprehensive analysis of QSOX2 expression in BC tumor and non-tumor tissues, assessing its specificity across different molecular subtypes. We further explored correlations between QSOX2 expression and patient outcomes, utilizing datasets like TCGA and METABRIC. In addition, we performed in vitro experiments to evaluate QSOX2 expression in BC cell lines and investigate the effects of QSOX2 knockdown on various TNBC cellular processes, including cell proliferation, apoptosis resistance, migration, and the epithelial-to-mesenchymal transition (EMT). Our results reveal significantly elevated QSOX2 expression in BC tumor tissues, particularly in TNBC, and establish an association between high QSOX2 expression and increased patient mortality, cancer progression, and recurrence across various BC subtypes. Notably, QSOX2 knockdown in TNBC cell lines reduces cell proliferation, enhances apoptosis, and suppresses migration, potentially mediated through its influence on the EMT process. Furthermore, we identify a significant link between QSOX2 and integrin β1 (ITGB1), suggesting that QSOX2 enhances ITGB1 stability, subsequently exacerbating the malignancy of TNBC. In conclusion, elevated QSOX2 expression emerges as a key factor associated with adverse patient outcomes in BC, particularly in TNBC, contributing to disease progression through various mechanisms, including the modulation of ITGB1 stability. Our findings underscore the potential of targeting QSOX2 as a therapeutic strategy for improving patient prognoses not only in TNBC but also in other BC subtypes.},
}

@article{Piffoux2024,
  title = {Carbon footprint of oral medicines using hybrid life cycle assessment},
  author = {Max Piffoux and Antoine {Le Tellier} and Zoé Taillemite and Coline Ducrot and Sébastien Taillemite},
  year = {2024},
  journal = {Journal of Cleaner Production},
  volume = {475},
  pages = {143576},
  doi = {https://doi.org/10.1016/j.jclepro.2024.143576},
  url = {https://www.sciencedirect.com/science/article/pii/S0959652624030257},
  abstract = {Background Healthcare represents 3–8% of a country's carbon footprint, and medicines are estimated to represent 20–55% of healthcare's carbon footprint. Unfortunately, only scarce and partial medicine life cycle assessments (LCAs) are reported due to the limited availability of needed data to perform them. Methods We describe a method to estimate the cradle-to-pharmacy gate LCA of all oral medicines from the French pharmacopeia (n = 12,316 medicines) that includes the entire medicine-related carbon footprint, encompassing active pharmaceutical ingredient (API), excipients and packaging production, transport, medicine manufacturing, and associated corporate emissions using a hybrid LCA/environmentally extended input-output model. The uncertainty surrounding this estimation is modeled using bootstrap. Findings Although the API carbon footprint is correlated with synthesis yield, its number of steps, presence of chiral center(s), and process mass intensity, the API carbon footprint is better predicted by its wholesale cost. Corporate emissions (34.5%), API production (28.5%), and medicine manufacturing (25.5%) are the most impactful contributors to medicine carbon footprints, while medicine packaging (5.3%), transport (3.6%), and excipients (2.7%) are less significant. Variations from one medicine to another are substantial. The mean carbon footprint of a medicine box is 8.47 kgCO2eq/box (median 1.46 kgCO2eq, 95% CI 0.34–73.98). Medicines' carbon footprint is correlated with their price but not linearly, as low-cost medicines have significantly higher emission factors of 0.2–0.3 kgCO2/€ versus 0.05–0.1 kgCO2/€ for high-cost drugs. Orphan and innovative medicines tend to have higher carbon footprints. Interpretation Medicine carbon footprints are highly variable. This database allows for a better understanding of the carbon footprint associated with medicines, in order to better eco-design care pathways.},
}

@article{Schedler2024,
  title = {A new bottom-up method for classifying a building portfolio by building type, self-sufficiency rate, and access to local grid infrastructure for storage demand analysis},
  author = {Steffen Schedler and Stefanie Meilinger and Tanja Clees},
  year = {2024},
  journal = {Applied Energy},
  volume = {371},
  pages = {123502},
  doi = {https://doi.org/10.1016/j.apenergy.2024.123502},
  url = {https://www.sciencedirect.com/science/article/pii/S0306261924008857},
  abstract = {A building’s energy storage demand depends on a variety of factors related to the specific local conditions such as building type, self-sufficiency-rate, and grid connection. Here, a newly developed bottom-up procedure is presented for classifying buildings in an urban building portfolio according to specific criteria. The algorithm uses publicly available building data such as building use, ground floor area, roof ridge height, solar roof potential, and population statistics. In addition, it considers the local gas grid (GG) as well as the district heating (DH) network. The building classification is developed for identifying typical building situations that can be used to estimate the demand for residential energy storage capacity. The developed algorithm is used to identify potential implementation of private photovoltaic(PV)-metal-hydride-storage (MHS) systems, for three scenarios, into the urban infrastructure for the city of Cologne. As result the statistical confidence interval of all analyzed buildings regarding their classification as well as corresponding maps is shown. Since similar data sets as used are available for many German or European metropolitan areas, the method developed with the assumptions presented in this work, can be used for classification of other urban and semi-urban areas including the assessment of their grid infrastructure.},
}

@article{Peng2025,
  title = {Factors affecting households’ adaptive energy-efficient upgrades in response to the energy crisis: The Norwegian case},
  author = {Yechennan Peng and Christian A. Klöckner},
  year = {2025},
  journal = {Energy and Buildings},
  volume = {326},
  pages = {115054},
  doi = {https://doi.org/10.1016/j.enbuild.2024.115054},
  url = {https://www.sciencedirect.com/science/article/pii/S0378778824011708},
  abstract = {Individual household energy-efficiency upgrading behaviours in relation to buildings are crucial in mitigating carbon emissions, yet understanding the predictors of these behaviours in the Norwegian context presents a research challenge. The principal aim of this study is to investigate the adoption patterns of energy-efficiency upgrades by Norwegian households, particularly in response to the energy crisis. It seeks to determine how socio-demographic, dwelling-related, household contextual, and psychological factors affect household behaviours concerning three upgrading measures – namely, private photovoltaic (PV) installation, flexible electricity use, and dwelling insulation. Based on survey data (N = 3514) collected in 2023 from Norwegian households, the study delineates a significant upsurge in all these three energy-efficiency upgrades within the past three years, driven by social norms, self-efficacy, and increased motivation to support the energy system after the energy crisis. The study also anticipates rapid growth in private PV systems and flexible electricity use over the next three years, influenced by social pressures, wider technology adoption, and a stronger desire to cut energy costs. The adoption of private PV systems is predominantly seen among high-income households, which is influenced by high fixed costs, inadequate institutional incentives, substantial rises in electricity prices, and a commitment to supporting the energy system post-crisis. The quick uptake of flexible electricity use is significantly influenced by social norms and technology’s compatibility with daily routines. Key factors driving insulation actions include the building age, the presence of younger household members with the necessary physical capability and skills, and the right timing perceptions. In addition, residential duration influences adoption patterns, with newer residents prioritizing personalization through high-tech measures and long-term residents focusing on maintaining insulation. This study will also discuss potential implications for policymakers in designing incentives tailored to households’ profiles and for investors in improving market strategies.},
}

@article{Castagna2024,
  title = {Benchmarking the efficiency of European metros from a production perspective},
  author = {Luigi Castagna and António Lobo and Pierluigi Coppola and António Couto},
  year = {2024},
  journal = {Research in Transportation Business & Management},
  volume = {53},
  pages = {101102},
  doi = {https://doi.org/10.1016/j.rtbm.2024.101102},
  url = {https://www.sciencedirect.com/science/article/pii/S221053952400004X},
  abstract = {This paper deals with the benchmarking of the technical efficiency of 23 metro systems in Europe. Since financial data reflecting the operating costs and revenues are not often made available to the public, the aim of this work is to develop a method based on production variables to enable large-scale analysis at the European level. The methodology consists of two stages. In the first stage, a gross value of effectiveness is estimated by means of a stochastic frontier regression based on the Cobb-Douglas production function. The results show about half of the considered firms reaching scores higher than 80%. However, these gross effectiveness estimates could be influenced and constrained by long term and external factors that go beyond the control of firms' day-to-day management. For this reason, in the second stage, an exponential multiple regression is estimated to determine the effects of these factors on gross effectiveness. The elasticities obtained through a multiple regression are used for evaluating the net effectiveness, by removing positive or negative contributions to the gross effectiveness that come from the identified “long term” factors. The results show that transit firms operating smaller networks tend to have higher net effectiveness scores in the short-term compared to larger transit firms.},
}

@article{Maharjan2024,
  title = {Deriving experience curves: A structured and critical approach applied to PV sector},
  author = {Prapti Maharjan and Mara Hauck and Arjan Kirkels and Benjamin Buettner and Heleen {de Coninck}},
  year = {2024},
  journal = {Technological Forecasting and Social Change},
  volume = {209},
  pages = {123795},
  doi = {https://doi.org/10.1016/j.techfore.2024.123795},
  url = {https://www.sciencedirect.com/science/article/pii/S0040162524005936},
  abstract = {Experience curves are widely used for cost estimates in energy-economy models and are proposed as a forecasting tool for projecting the future environmental impact of emerging technologies. However, further application is limited by data availability and methodological challenges related to modelling the dynamic relationship between cost, different kinds of learning, and scale effects. This paper systematically compares existing experience curves using empirical data from the PV sector. We compare the cost forecast of the assessed experience curves, derive the learning rates over different periods, and draw parallels to the environmental experience curve. Our results show that the single-factor experience curve (SEFC) is the most stable model, showing consistent performance across different technological eras, train-test splits and validation methods. Two-factor and multi-factor experience curves exhibit higher sensitivity, with their performance metrics varying significantly based on the data subsets used. Diagnostic tests are important to examine the robustness of the results. For the environmental experience curve, data quality and model explanatory power are lower, yet there is potential for its applicability in projecting environmental impact and energy use. Policymakers and modellers should consider the specific technological era when using learning rates for decision-making. Our findings indicate that learning-by-doing provides a steady learning rate across all experience curves. In the early stages of technological maturity, cost reductions in the PV industry are driven by learning-by-innovation, which is later dominated by economies of scale.},
}

@article{Parte}2024,
  title = {Spatio-temporal semantic data management systems for IoT in agriculture 5.0: Challenges and future directions},
  author = {Mario {San Emeterio de la Parte} and José-Fernán Martínez-Ortega and Pedro Castillejo and Néstor Lucas-Martínez},
  year = {2024},
  journal = {Internet of Things},
  volume = {25},
  pages = {101030},
  doi = {https://doi.org/10.1016/j.iot.2023.101030},
  url = {https://www.sciencedirect.com/science/article/pii/S2542660523003530},
  abstract = {The Agri-Food sector is in a stressful situation due to the high demand for food from the growing population around the world. The agricultural sector is facing a challenging situation; it must increase production and reduce its impact on the environment by appropriately allocating resources, adapting to climate change, and avoiding food waste. Agriculture 5.0, as the fifth agricultural evolution, aims to offer a perfect symbiosis between agriculture, advanced technologies, and sustainability. The most advanced technologies in automation, monitoring, and decision support are driven by the collection and processing of large volumes of agricultural data, such as weather information, farm machinery, soil and crop conditions, and marketing demand for higher profits. Taking advantage of the technological paradigm of the Internet of Things, agricultural data provides information on spatial, temporal, and semantic dimensions. Spatio-temporal semantic data management systems have become the cornerstone for the achievement of Agriculture 5.0 through advanced Internet of Things technologies. This paper aims to review the current literature on spatio-temporal semantic data management systems for Agriculture 5.0. This paper uses a systematic literature review technique to study eleven representative spatio-temporal semantic data management systems. A comprehensive evaluation of the aspects of interoperability, accessibility, scalability, real-time operation capability, etc. is carried out. Based on the evaluation results, future challenges are detected and development trends and possible improvements are proposed for future research. Finally, a distributed architecture capable of satisfying the above needs and challenges is proposed. The paper aims to inspire further research and development efforts to improve the efficiency, accessibility, and performance of spatio-temporal semantic data management systems.},
}

@article{Hadir2025,
  title = {Comparative study of agricultural parcel delineation deep learning methods using satellite images: Validation through parcels complexity},
  author = {Amine Hadir and Mohamed Adjou and Olga Assainova and Gaëtan Palka and Marwa Elbouz},
  year = {2025},
  journal = {Smart Agricultural Technology},
  volume = {10},
  pages = {100833},
  doi = {https://doi.org/10.1016/j.atech.2025.100833},
  url = {https://www.sciencedirect.com/science/article/pii/S2772375525000668},
  abstract = {Accurate delineation of agricultural parcels is crucial for applications ranging from resource management to policy decisions, with a direct impact on agricultural productivity and sustainability. Parcel delineation is the subject of numerous studies, most of which focus on the development of more efficient methods or ones better adapted to specific cases. In addition, various methods exist in the literature for delineating agricultural fields from satellite images. Deep learning, in particular, has revolutionized the field. Many state-of-the-art methods now utilize deep learning, often incorporating segmentation and classification techniques to define agricultural parcel boundaries. While recent research has led to a surge in deep learning methods for this task, evaluating their effectiveness goes beyond simply comparing outputs. This paper emphasizes the critical role of parcel complexity as a powerful lens for assessing the performance of deep learning methods in agricultural parcel delineation. We categorize 14 evaluation metrics into three main groups, global, boundary, and structure metrics, respectively. Global metrics assess the overall accuracy of the delineated parcels, boundary metrics focus on the precision of the parcel boundaries, and structure metrics examine the topological relationships between the parcels. Our goal is to compare these deep learning methods based on these metrics and their performance across varying levels of parcel complexity. We systematically evaluate nine state-of-the-art methods using a public database, explicitly analyzing how their strengths and weaknesses are affected by different levels of parcel complexity. This approach ensures that future deep learning techniques are robust and accurate enough to meet the demands of accurately defining the agricultural landscape and provides important insights for the development and refinement of future deep learning techniques.},
}

@article{Jia2024,
  title = {Persistent yet limited impact of protected areas on coastal wetland restoration in megacity cores},
  author = {Kai Jia and Ailin Huang and Liming Deng and Xiaoling Yin and Yue Deng and Zhiwei Hou and Zhao Li and Ying Liu and Jie Shen and Ji Yang},
  year = {2024},
  journal = {Global Ecology and Conservation},
  volume = {56},
  pages = {e03270},
  doi = {https://doi.org/10.1016/j.gecco.2024.e03270},
  url = {https://www.sciencedirect.com/science/article/pii/S2351989424004748},
  abstract = {Coastal wetlands, vital hubs of economic activities, are under significant pressure, leading to the establishment of protected areas as a crucial conservation tool. Yet, uncertainties remain regarding the long-term effectiveness of these areas, especially in megacity cores subject to intense human disturbances. This study utilized long-term Landsat satellites monitoring to track the annual impact of protected areas on coastal wetland variations in the Guangzhou-Hong Kong-Macau Greater Bay Area from 1990 to 2020. Using annual classification datasets featuring 8 land cover/land use types with an average overall accuracy of 94.0±0.4 %, the research identified a significant increase in wetland areas before 2002, followed by a slight decline due to the exploitation of aquaculture and water bodies. Protected areas established for over seven years demonstrated a significantly positive impact on wetland restoration, underscoring the value of long-term conservation efforts. While mangrove protected areas showed strong recovery trends, other wetlands in developing regions continued to decline despite protection. This analysis indicated that the overall benefits of protected areas diminished when considering broader spatial contexts. These insights emphasized the need for adaptive management strategies to enhance the effectiveness of protected areas in urban environment.},
}

@article{Pelser2024,
  title = {Reviewing accuracy & reproducibility of large-scale wind resource assessments},
  author = {Tristan Pelser and Jann Michael Weinand and Patrick Kuckertz and Russell McKenna and Jochen Linssen and Detlef Stolten},
  year = {2024},
  journal = {Advances in Applied Energy},
  volume = {13},
  pages = {100158},
  doi = {https://doi.org/10.1016/j.adapen.2023.100158},
  url = {https://www.sciencedirect.com/science/article/pii/S2666792423000379},
  abstract = {The accurate quantification and assessment of available renewable energy resources has emerged as a research topic with high relevance to policymakers and industry. Motivated by the need for a contemporary review on the methodologies and practices prevalent in wind resource assessments, we employ a systematic analysis of 195 articles that describe large-scale wind assessments. Our review reveals significant heterogeneity in global and continental-scale potentials and geographical bias of research towards the Northern Hemisphere, despite electrification needs in regions like Africa and Latin America. A fraction of the literature attempts to explicitly include social and political barriers to wind power development, thereby defining ‘feasible’ potentials. We delve into advancements in this domain, focusing on innovative methodologies that encapsulate the viewpoints of subject experts and stakeholders in the assessment process. Our analysis underscores pressing challenges relating to data sharing and scientific reproducibility, with our findings revealing a mere 10 % of studies that offer openly available data for download. This highlights a pervasive insufficiency in the reproducibility of wind assessments. Additionally, we tackle notable hurdles concerning wind data and meteorological characterization, including an over-reliance on single-source wind data and a deficit in adequately characterizing temporal wind variability. Relatedly, we uncover a highly heterogenous approach to turbine siting and characterizing wake-related losses. These methods are frequently simplistic, potentially leading to an overestimation of wind potentials by assuming an overly optimistic capacity density. In each of these domains, we discuss the state of the art for modern wind resource assessments, propose best practices, and pinpoint crucial areas warranting future research.},
}

@article{Topal}2025,
  title = {The evolution of Turkish solar energy research network in three periods},
  author = {Yelda {Erden Topal} and Özgür Kadir Özer and Gülşah Karakaya},
  year = {2025},
  journal = {Renewable Energy},
  volume = {244},
  pages = {122623},
  doi = {https://doi.org/10.1016/j.renene.2025.122623},
  url = {https://www.sciencedirect.com/science/article/pii/S096014812500285X},
  abstract = {This study explores the evolution of Türkiye's solar energy research network from 1991 to 2020 using bibliometric and project data focusing on its growth and international collaboration. Results reveal a significant increase in solar energy publications, with Türkiye's global output share increasing from 0.46 % in 1991–2000 to 1.38 % in 2011–2020. The period covering 2001–2010 emerges as a critical fermentation phase, marked by expanding collaborations and the foundation of key partnerships. Türkiye's strategic location and active participation in the EU Framework Programmes have enabled it to serve as a bridge for knowledge exchange between Europe and the Middle East and North Africa region. While universities remain central, private-sector participation has grown. The network has become more democratized, incorporating diverse actors and reducing dependence on a few dominant institutions. Individual researchers with dual affiliations have played key roles in fostering international ties for certain countries. This research provides insights into the dynamics of solar energy collaboration and identifies key factors that influence such partnerships at various levels to enhance Türkiye's role in global energy transitions.},
}

@article{Stecher2024,
  title = {Creating a labeled district heating data set: From anomaly detection towards fault detection},
  author = {Dominik Stecher and Martin Neumayer and Adithya Ramachandran and Anastasia Hort and Andreas Maier and Dominikus Bücker and Jochen Schmidt},
  year = {2024},
  journal = {Energy},
  volume = {313},
  pages = {134016},
  doi = {https://doi.org/10.1016/j.energy.2024.134016},
  url = {https://www.sciencedirect.com/science/article/pii/S0360544224037940},
  abstract = {For an efficient operation of district heating systems, being able to detect anomalies and faults at an early stage is highly desirable. Here, data-driven machine learning methods can be a cornerstone, particularly for fault detection in district heating substations, where the availability of heat meter data keeps increasing. However, the creation of data sets suitable for training such machine learning models poses challenges to researchers and practitioners alike. To address this problem, we propose a systematic and domain-specific process for data set creation for fault detection in the form of practical guidelines. This process concretizes the data science and data mining cross-industry standard CRISP-DM for the district heating domain and focuses on the process steps of goal definition, data acquisition and understanding, and data curation. We aim to enable researchers and practitioners to create data sets for fault detection in the district heating domain and therefore also enable the creation or improvement of machine learning models in this domain. In addition, we propose a minimum viable feature set for fault detection in district heating networks with the goal of enabling better cooperation between researchers and easier transfer of the resulting machine learning models, to better proliferate new progress in the field.},
}

@article{Shahriar2025,
  title = {A comprehensive review of current trends, challenges, and opportunities in text data privacy},
  author = {Sakib Shahriar and Rozita Dara and Rajen Akalu},
  year = {2025},
  journal = {Computers & Security},
  volume = {151},
  pages = {104358},
  doi = {https://doi.org/10.1016/j.cose.2025.104358},
  url = {https://www.sciencedirect.com/science/article/pii/S0167404825000471},
  abstract = {The emergence of smartphones and internet accessibility around the globe have enabled billions of people to be connected to the digital world. Due to the popularity of instant messaging applications and social media, a large quantity of personal data is in text format, and processing text data in a privacy-preserving manner poses unique challenges. While existing reviews focus on privacy concerns from specific algorithmic perspectives or target only a particular domain, such as healthcare or smart metering, they fail to provide a comprehensive view that addresses the multi-layered privacy risks inherent to text data processing. Existing works often limit their scope to specialized solutions like differential privacy, anonymization, or federated learning, neglecting a broader spectrum of challenges. To fill this gap, we present a comprehensive review of privacy-enhancing solutions for text data processing in the present literature and classify the works into six categories of privacy risks: (i) unintentional memorability, (ii) membership inference, (iii) exposure and re-identification, (iv) language models and word embeddings, (v) authorship attribution, and (vi) collaborative processing. We then analyze existing privacy-enhancing solutions for text data by considering the aforementioned privacy risks. Finally, we identified several research gaps, including the need for comprehensive privacy metrics, explainable algorithms, and privacy in social media analytics.},
}

@article{Castonguay2024,
  title = {AI maturity in health care: An overview of 10 OECD countries},
  author = {Alexandre Castonguay and Gerit Wagner and Aude Motulsky and Guy Paré},
  year = {2024},
  journal = {Health Policy},
  volume = {140},
  pages = {104938},
  doi = {https://doi.org/10.1016/j.healthpol.2023.104938},
  url = {https://www.sciencedirect.com/science/article/pii/S0168851023002233},
  abstract = {Background Artificial Intelligence (AI) and its applications in health care are on the agenda of policymakers around the world, but a major challenge remains, namely, to set policies that will ensure wide acceptance and capture the value of AI while mitigating associated risks. Objective This study aims to provide an overview of how OECD countries strategize about how to integrate AI into health care and to determine their actual level of AI maturity. Methods A scan of government-based AI strategies and initiatives adopted in 10 proactive OECD countries was conducted. Available documentation was analyzed, using the Broadband Commission for Sustainable Development's roadmap to AI maturity as a conceptual framework. Results The findings reveal that most selected OECD countries are at the Emerging stage (Level 2) of AI in health maturity. Despite considerable funding and a variety of approaches to the development of an AI in health supporting ecosystem, only the United Kingdom and United States have reached the highest level of maturity, an integrated and collaborative AI in health ecosystem (Level 3). Conclusion Despite policymakers looking for opportunities to expedite efforts related to AI, there is no one-size-fits-all approach to ensure the sustainable development and safe use of AI in health. The principles of equifinality and mindfulness must thus guide policymaking in the development of AI in health care.},
}

@article{Frattini2024,
  title = {Requirements quality research artifacts: Recovery, analysis, and management guideline},
  author = {Julian Frattini and Lloyd Montgomery and Davide Fucci and Michael Unterkalmsteiner and Daniel Mendez and Jannik Fischbach},
  year = {2024},
  journal = {Journal of Systems and Software},
  volume = {216},
  pages = {112120},
  doi = {https://doi.org/10.1016/j.jss.2024.112120},
  url = {https://www.sciencedirect.com/science/article/pii/S0164121224001651},
  abstract = {Requirements quality research, which is dedicated to assessing and improving the quality of requirements specifications, is dependent on research artifacts like data sets (containing information about quality defects) and implementations (automatically detecting and removing these defects). However, recent research exposed that the majority of these research artifacts have become unavailable or have never been disclosed, which inhibits progress in the research domain. In this work, we aim to improve the availability of research artifacts in requirements quality research. To this end, we (1) extend an artifact recovery initiative, (2) empirically evaluate the reasons for artifact unavailability using Bayesian data analysis, and (3) compile a concise guideline for open science artifact disclosure. Our results include 10 recovered data sets and 7 recovered implementations, empirical support for artifact availability improving over time and the positive effect of public hosting services, and a pragmatic artifact management guideline open for community comments. With this work, we hope to encourage and support adherence to open science principles and improve the availability of research artifacts for the requirements research quality community.},
}

@article{Stock2025,
  title = {Investigating the research output of institutions},
  author = {Wolfgang G. Stock and Gerhard Reichmann and Christian Schlögl},
  year = {2025},
  journal = {Journal of Informetrics},
  volume = {19},
  pages = {101638},
  doi = {https://doi.org/10.1016/j.joi.2025.101638},
  url = {https://www.sciencedirect.com/science/article/pii/S1751157725000021},
  abstract = {Describing, analyzing, and evaluating research institutions are among the main tasks of scientometrics and research evaluation. But how can we optimally search for an institution's research output? Possible search arguments include institution names, affiliations, addresses, and affiliated authors’ names. Prerequisites of these search tasks are complete lists (or at least good approximations) of the institutions’ publications, and—in later steps—their citations, and topics. When searching for the publications of research institutions in an information service, there are two options, namely (1) searching directly for the name of the institution and (2) searching for all authors affiliated with the institution in a defined time interval. Which strategy is more effective? More specifically, do informetric indicators such as recall and precision, search recall and search precision, and relative visibility change depending on the search strategy? What are the reasons for differences? To illustrate our approach, we conducted an illustrative study on two information science institutions and identified all staff members. The search was performed using the Web of Science Core Collection (WoS CC). As a performance indicator, applying fractional counting and considering co-affiliations of authors, we used the institution's relative visibility in an information service. We also calculated two variants of recall and precision at the institution level, namely search recall and search precision as informetric measures of performance differences between different search strategies (here: author search versus institution search) on the same information service (here: WoS CC) and recall and precision in relation to the complete set of an institution's publications. For all our calculations, there is a clear result: Searches for affiliated authors outperform searches for institutions in WoS. However, especially for large institutions it is difficult to determine all the staff members in the time interval of research. Additionally, information services (including WoS) are incomplete and there are variants for the names of institutions in the services. Therefore, searching for institutions and the publication-based quantitative evaluation of institutions are very critical issues.},
}

@article{Offenhuber2023,
  title = {Reconsidering Representation in College Design Curricula},
  author = {Dietmar Offenhuber and Joy Mountford},
  year = {2023},
  journal = {She Ji: The Journal of Design, Economics, and Innovation},
  volume = {9},
  pages = {264-282},
  doi = {https://doi.org/10.1016/j.sheji.2023.04.005},
  url = {https://www.sciencedirect.com/science/article/pii/S2405872623000394},
  abstract = {The Future of Design Education working group on representation addressed the roles of data, maps, models, and interfaces as a continuum from representation to action. The article traces historical ideas of representation grounded by a linguistic paradigm to more recent approaches based on performance, embodiment, and sensory modalities other than vision. Discussions include the use of representations in the design process. Designers are able to use traditional forms of representation in the design of artifacts, such as sketches. These forms of representation are not sufficient for the design of systems. System design requires models that allow stakeholders to negotiate their view of a situation and design teams to iterate how things might work. Core ideas in the working group recommendations address issues of, substitution, formal rules, motivation, context dependency, materiality, provisionality, latency, performance, externalization, facilitation and negotiation, mediation, and measurement and evaluation. Discussions address the socio-political implications of representation and the expanding role of computing and data that call for a systems view.},
}

@article{Thebault2025,
  title = {A comprehensive building-wise rooftop photovoltaic system detection in heterogeneous urban and rural areas: application to French territories},
  author = {Martin Thebault and Boris Nerot and Benjamin Govehovitch and Christophe Ménézo},
  year = {2025},
  journal = {Applied Energy},
  volume = {388},
  pages = {125630},
  doi = {https://doi.org/10.1016/j.apenergy.2025.125630},
  url = {https://www.sciencedirect.com/science/article/pii/S0306261925003605},
  abstract = {With the rapid expansion of Rooftop Photovoltaic (RPV) systems, accurately identifying the location of these installations has become essential for urban planning, grid management, and socio-economic analysis. However, existing European datasets of RPV systems are often limited in both spatial coverage and precision, especially in regions with diverse architectural styles. This study presents a novel methodology for identifying RPV systems by employing a convolutional neural network (CNN) trained on high-resolution aerial imagery and building registry data. Alternatively to traditional tile-based methods, we propose a building-by-building approach, ensuring that each building is individually assessed. The model was trained and validated on five French departments representing a variety of roofing materials and urban typologies. It demonstrates a high correlation between predicted and registered RPV systems, though detection performance varies with roofing materials—achieving better accuracy on tiled roofs than slate roofs. When applied to the entire metropolitan French territory, the model processed images of more than 40 million buildings, identifying approximately 600,000 RPV systems. The results’ accuracy is evaluated, taking into account factors such as data quality and local urban characteristics. All data and the model are publicly available for further research and applications.},
}

@article{Ramezani2025,
  title = {Sentiment analysis applications using deep learning advancements in social networks: A systematic review},
  author = {Erfan Bakhtiari Ramezani},
  year = {2025},
  journal = {Neurocomputing},
  volume = {634},
  pages = {129862},
  doi = {https://doi.org/10.1016/j.neucom.2025.129862},
  url = {https://www.sciencedirect.com/science/article/pii/S092523122500534X},
  abstract = {Sentiment analysis is required to extract insights from social media content affecting decision-making and personalized services. The enormous volume of social network information has to be technically processed to extract relevant knowledge. Sentiment analysis is the most widely used method for this purpose. The current techniques of sentiment analysis have made significant progress in various fields. However, the potential of social networks to better understand human emotions and the recent advancements in deep learning necessitate the review and use of advanced sentiment analysis techniques that still require more attention from researchers in this field. In this regard, this review presents a systematic literature review (SLR) on the advancements of sentiment analysis using deep learning techniques in social networks from 2019 to May 2024. Furthermore, this review emphasizes that sentiment analysis can provide meaningful insights into information extracted from large and diverse datasets such as social media, which is extremely important for decision-making and personalized services. It also highlights mental health concerns as one of the windows into the emotional atmosphere of social networks. In addition, this SLR provides a technical taxonomy and comparison of various deep learning approaches. This SLR not only provides a comprehensive overview of the most advanced techniques and methodologies now used in sentiment analysis but also highlights forthcoming challenges and open issues that need to be addressed in the future. This study helps researchers and practitioners use deep learning to improve sentiment analysis applications and digital social well-being.},
}

@article{Leach2024,
  title = {Translation of nonclinical to clinical safety findings for 27 biotherapeutics},
  author = {Michael W. Leach and Payal Rana and Wenyue Hu and Rajendar K. Mittapalli and Jason Pinkstaff and David Potter and Xing Min Qiu and Lila Ramaiah and Cynthia Rohde and Feng Xia and K. Nasir Khan},
  year = {2024},
  journal = {Toxicology and Applied Pharmacology},
  volume = {484},
  pages = {116854},
  doi = {https://doi.org/10.1016/j.taap.2024.116854},
  url = {https://www.sciencedirect.com/science/article/pii/S0041008X24000528},
  abstract = {Human adverse drug reactions (ADRs), and in vivo nonclinical adverse and nonadverse findings, were identified in 27 biotherapeutic programs and placed into organ categories to determine translation. The sensitivity of detecting human ADRs was 30.8% with a positive predictive value (PPV) of 53.3% for nonclinical adverse findings; sensitivity increased to 67.3% and PPV fell to 35.0% when including nonadverse findings. Nonclinical findings were associated with a greater likelihood of a human ADR in that organ category, especially for adverse findings [positive likelihood ratio (LR+) >10 (lower 95% confidence interval [CI] of >5)]. The specificity and negative predictive value (NPV) were very high (>85%). A lack of nonclinical findings in an organ category was associated with a lower likelihood of a human ADR in that organ category. About 40–50% of human ADRs and nonclinical adverse findings, and about 30% of nonclinical nonadverse findings, were attributed to pharmacology. Slightly more than half of the human ADRs with a translating nonclinical finding had findings in animals that could be considered very similar. Overall, 38% of nonclinical findings translated to a human ADR at the organ category level. When nonclinical findings did not translate to humans, the cause was usually higher exposures or longer dosing in animals. All programs with human ADRs attributed to immunogenicity also had nonclinical adverse or nonadverse findings related to immunogenicity. Overall, nonclinical adverse and nonadverse findings were useful in predicting human ADRs, especially at an organ category level, and the majority of human ADRs were predicted by nonclinical toxicity studies.},
}

@article{Yao2025,
  title = {Open source oriented cross-platform survey},
  author = {Simeng Yao and Xunhui Zhang and Yang Zhang and Tao Wang},
  year = {2025},
  journal = {Information and Software Technology},
  volume = {182},
  pages = {107704},
  doi = {https://doi.org/10.1016/j.infsof.2025.107704},
  url = {https://www.sciencedirect.com/science/article/pii/S0950584925000436},
  abstract = {Context: Open-source software development has become a widely adopted approach to software creation. However, developers’ activities extend beyond social coding platforms (e.g., GitHub), encompassing social Q&A platforms (e.g., StackOverflow) and social media platforms (e.g., Twitter). Therefore, cross-platform research is essential for a deeper understanding of the nature of software development activities. Objective: This paper focuses on open-source platforms and systematically summarizes relevant cross-platform research. It aims to assess the current state of cross-platform research and provide insights into the challenges and future developments in this field. Method: This paper reviews 69 cross-platform research papers related to open-source software from 2013 to 2024, with a focus on several key areas, including platform interconnections, research themes, experimental design methods, challenges and research opportunities. Results: Through the analysis of 69 papers, we found that cross-platform research primarily involves platforms such as social coding, social Q&A, and social media. Researchers typically rely on information traces, including user personal info, technical info, project/post/bug report metadata, interaction info, to facilitate connections between platforms. Cross-platform research in the open-source domain mainly focuses on problem classification and feature extraction. The predominant research methods include data-driven approaches, qualitative studies, modeling and machine learning, and tool development and implementation. Despite these advancements, common challenges remain, such as subjective evaluation bias in manual data classification, insufficient data source coverage, and inaccurate data recognition. Future research opportunities may focus on increasing the diversity of data sources, improving data recognition accuracy, optimizing data classification methods, and clarifying user skill requirements. Conclusions: Based on our findings, we propose six future directions for cross-platform research in the open-source domain and provide corresponding recommendations for developers, researchers, and service/tool providers.},
}

@article{Phua2024,
  title = {Fostering urban resilience and accessibility in cities: A dynamic knowledge graph approach},
  author = {Shin Zert Phua and Markus Hofmeister and Yi-Kai Tsai and Oisín Peppard and Kok Foong Lee and Seán Courtney and Sebastian Mosbach and Jethro Akroyd and Markus Kraft},
  year = {2024},
  journal = {Sustainable Cities and Society},
  volume = {113},
  pages = {105708},
  doi = {https://doi.org/10.1016/j.scs.2024.105708},
  url = {https://www.sciencedirect.com/science/article/pii/S221067072400533X},
  abstract = {This paper explores the utilisation of knowledge graphs and an agent-based implementation to enhance urban resilience and accessibility in city planning. We expand The World Avatar (TWA) dynamic knowledge graph to support decision-making in disaster response and urban planning. By employing a set of connected agents and integrating diverse data sources — including flood data, geospatial building information, land plots, and open-source data — through sets of ontologies, we demonstrate disaster response in a coastal town in the UK and various aspects relevant to city planning for a mid-sized town in Germany using TWA. In King’s Lynn, our agent-based approach facilitates holistic disaster response by calculating optimal routes, avoiding flooded segments dynamically, assessing infrastructure accessibility before and during a flood using isochrones, identifying inaccessible population areas, guiding infrastructure restoration, and conducting critical path analysis. In Pirmasens, for city planning purposes, the knowledge graph-driven isochrone generation provides evidence-based insights into current amenity coverage and enables scenario planning for future amenities while adhering to land regulations. The implementation of agents and knowledge graphs achieves interoperability and enhances urban resilience and accessibility by enabling cross-domain correlation analysis that extends various areas including geospatial buildings, population demographics, accessibility coverage, and land use regulations.},
}

@article{Wang2025_01,
  title = {Can Chinese household consumption become more energy efficient? Analysis based on input–output and demand system models},
  author = {Libo Wang and Hongxia Zhang and Ming Xia and Jianhong Ma},
  year = {2025},
  journal = {Energy Economics},
  volume = {141},
  pages = {108116},
  doi = {https://doi.org/10.1016/j.eneco.2024.108116},
  url = {https://www.sciencedirect.com/science/article/pii/S0140988324008259},
  abstract = {To gain a comprehensive understanding of the role that household consumption has in the transition to a low-carbon economy, analyses of household energy use (HEU) should focus on total HEU that includes direct and indirect energy use. We examine the major factors of total HEU efficiency using input–output and Quadratic Almost Ideal Demand System models. The analyses are based on time-series non-competitive input–output tables at constant prices for 1986—2018 compiled by this study, industrial energy satellite accounts, and the data from the Chinese Household Income Project. First, the findings reveal that while total HEU is increasing rapidly, HEU intensity has declined, suggesting that household consumption has become more energy efficient. However, the primary cause is the reduction of energy intensity in production sectors rather than the household consumption structure. Second, HEU will continue to rise with advancing urbanization and expected income increasing in the future. Furthermore, changing consumption patterns may increase urban and rural HEU, with an increasing share of household facilities, transportation, and communication further driving energy use in upstream industries. Therefore, the key to improving HEU efficiency is more strongly related to technological advances on the production side than changing consumption patterns. Energy policy should primarily focus on promoting industrial technological advances and measures to advance circular economy development.},
}

@article{Chang2024,
  title = {Chapter 1 - Introduction to artificial intelligence for cardiovascular clinicians},
  author = {Anthony C. Chang and Alfonso Limon},
  year = {2024},
  pages = {3-120},
  doi = {https://doi.org/10.1016/B978-0-323-90534-3.00010-X},
  publisher = {Academic Press},
  url = {https://www.sciencedirect.com/science/article/pii/B978032390534300010X},
  abstract = {The impressive gains in deep learning (DL) started in 2012 and its successful utilization in image interpretation have led to the current momentum for artificial intelligence (AI) awareness and adoption. In 2016, Google DeepMind's AlphaGo software soundly defeated the best human Go champion Lee Sedol to introduce the capability of DL outside of image interpretation. More recently, there have been impressive exponential advances in natural language processing with transformer tools such as GPT-3, GPT-4, and now ChatGPT. DeepMind and its AlphaFold AI tool has been able to predict the three-dimensional (3D) structure of proteins since 2021 and was Science magazine's “Breakthrough of the Year.” All of these AI accomplishments heralded the recent new era in AI. Major universities with AI departments (such as Stanford, MIT, and Carnegie Mellon) and technology giants (such as IBM, Apple, Facebook, and Microsoft in the United States as well as other large companies such as Baidu, Alibaba, and Tencent [BAT] in China) are all fervidly exploring real-life applications of AI. There is also a movement to democratize AI so that “no-code platforms” can accommodate people who do not know how to code [1].},
}

@article{Alfredo2024,
  title = {Human-centred learning analytics and AI in education: A systematic literature review},
  author = {Riordan Alfredo and Vanessa Echeverria and Yueqiao Jin and Lixiang Yan and Zachari Swiecki and Dragan Gašević and Roberto Martinez-Maldonado},
  year = {2024},
  journal = {Computers and Education: Artificial Intelligence},
  volume = {6},
  pages = {100215},
  doi = {https://doi.org/10.1016/j.caeai.2024.100215},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X2400016X},
  abstract = {The rapid expansion of Learning Analytics (LA) and Artificial Intelligence in Education (AIED) offers new scalable, data-intensive systems but raises concerns about data privacy and agency. Excluding stakeholders—like students and teachers—from the design process can potentially lead to mistrust and inadequately aligned tools. Despite a shift towards human-centred design in recent LA and AIED research, there remain gaps in our understanding of the importance of human control, safety, reliability, and trustworthiness in the design and implementation of these systems. We conducted a systematic literature review to explore these concerns and gaps. We analysed 108 papers to provide insights about i) the current state of human-centred LA/AIED research; ii) the extent to which educational stakeholders have contributed to the design process of human-centred LA/AIED systems; iii) the current balance between human control and computer automation of such systems; and iv) the extent to which safety, reliability and trustworthiness have been considered in the literature. Results indicate some consideration of human control in LA/AIED system design, but limited end-user involvement in actual design. Based on these findings, we recommend: 1) carefully balancing stakeholders' involvement in designing and deploying LA/AIED systems throughout all design phases 2) actively involving target end-users, especially students, to delineate the balance between human control and automation, and 3) exploring safety, reliability, and trustworthiness as principles in future human-centred LA/AIED systems.},
}

@article{Fetene2025,
  title = {Remote sensing analysis of urban heat island dynamics in Bahir Dar and Hawassa, Ethiopia: The role of vegetation, urbanization, and climate},
  author = {Aramde Fetene},
  year = {2025},
  journal = {Environmental Challenges},
  volume = {19},
  pages = {101139},
  doi = {https://doi.org/10.1016/j.envc.2025.101139},
  url = {https://www.sciencedirect.com/science/article/pii/S2667010025000587},
  abstract = {The rapid urbanization within most developing nations has resulted in the Urban Heat Island (UHI) effect. Yet, there is a lack of substantial research on local environmental and climatic variables that define the UHI intensity. This study investigated the relationship between urbanization, vegetation dynamics, and UHI intensity in Bahir Dar and Hawassa, Ethiopia, from 2000 to 2022, using Landsat 7, Landsat 8, and climatic variables such as temperature, humidity, wind speed, and rainfall. The analysis was computed with Python and ArcMap. Accordingly, the results demonstrated a slight reduction in Land Surface Temperature (LST) for both cities, with variation in Bahir Dar at (R² = 0.50) compared to Hawassa at (R² = 0.07), showing other factors controlling the temperature in Hawassa. Indeed, Bahir Dar in the Ethiopian Highlands exhibits higher temperatures than Hawassa in the Rift Valley. Changes in the Normalized Built-up Index (NDBI) have recorded weak negative trends, (R² = 0.20) for both cities, which means the expansion of the built-up areas is inconsistent. While the increase of Normalized Difference Vegetation Index (NDVI) in Bahir Dar was significant (p < 0.01), that of Hawassa was not significant (p = 0.107). UHII in Bahir Dar has significantly declined, with (R² = 0.68; p < 0.01) while in Hawassa UHII characterizes variations with a downward trend (R2 =0.42; p < 0.01) including even negative values recording, indicating cooler urban than the surroundings. This calls for deep attention to local specificities for any climate adaptation strategy. Further studies should be conducted with multisource remote sensing, socioeconomic factors, and predictive modeling to strengthen evidence-based urban planning and decision-making.},
}

@article{Feng2024,
  title = {Pathogenomics for accurate diagnosis, treatment, prognosis of oncology: a cutting edge overview},
  author = {Xiaobing Feng and Wen Shu and Mingya Li and Junyu Li and Junyao Xu and Min He},
  year = {2024},
  journal = {Journal of Translational Medicine},
  volume = {22},
  pages = {131},
  doi = {10.1186/s12967-024-04915-3},
  url = {https://doi.org/10.1186/s12967-024-04915-3},
  abstract = {The capability to gather heterogeneous data, alongside the increasing power of artificial intelligence to examine it, leading a revolution in harnessing multimodal data in the life sciences. However, most approaches are limited to unimodal data, leaving integrated approaches across modalities relatively underdeveloped in computational pathology. Pathogenomics, as an invasive method to integrate advanced molecular diagnostics from genomic data, morphological information from histopathological imaging, and codified clinical data enable the discovery of new multimodal cancer biomarkers to propel the field of precision oncology in the coming decade. In this perspective, we offer our opinions on synthesizing complementary modalities of data with emerging multimodal artificial intelligence methods in pathogenomics. It includes correlation between the pathological and genomic profile of cancer, fusion of histology, and genomics profile of cancer. We also present challenges, opportunities, and avenues for future work.},
}

@article{Shimizu2023,
  title = {AI-driven molecular generation of not-patented pharmaceutical compounds using world open patent data},
  author = {Yugo Shimizu and Masateru Ohta and Shoichi Ishida and Kei Terayama and Masanori Osawa and Teruki Honma and Kazuyoshi Ikeda},
  year = {2023},
  journal = {Journal of Cheminformatics},
  volume = {15},
  pages = {120},
  doi = {10.1186/s13321-023-00791-z},
  url = {https://doi.org/10.1186/s13321-023-00791-z},
  abstract = {Developing compounds with novel structures is important for the production of new drugs. From an intellectual perspective, confirming the patent status of newly developed compounds is essential, particularly for pharmaceutical companies. The generation of a large number of compounds has been made possible because of the recent advances in artificial intelligence (AI). However, confirming the patent status of these generated molecules has been a challenge because there are no free and easy-to-use tools that can be used to determine the novelty of the generated compounds in terms of patents in a timely manner; additionally, there are no appropriate reference databases for pharmaceutical patents in the world. In this study, two public databases, SureChEMBL and Google Patents Public Datasets, were used to create a reference database of drug-related patented compounds using international patent classification. An exact structure search system was constructed using InChIKey and a relational database system to rapidly search for compounds in the reference database. Because drug-related patented compounds are a good source for generative AI to learn useful chemical structures, they were used as the training data. Furthermore, molecule generation was successfully directed by increasing and decreasing the number of generated patented compounds through incorporation of patent status (i.e., patented or not) into learning. The use of patent status enabled generation of novel molecules with high drug-likeness. The generation using generative AI with patent information would help efficiently propose novel compounds in terms of pharmaceutical patents. Scientific contribution: In this study, a new molecule-generation method that takes into account the patent status of molecules, which has rarely been considered but is an important feature in drug discovery, was developed. The method enables the generation of novel molecules based on pharmaceutical patents with high drug-likeness and will help in the efficient development of effective drug compounds.},
}

@article{Snyder2024,
  title = {The Goldilocks paradigm: comparing classical machine learning, large language models, and few-shot learning for drug discovery applications},
  author = {Scott H Snyder and Patricia A Vignaux and Mustafa Kemal Ozalp and Jacob Gerlach and Ana C Puhl and Thomas R Lane and John Corbett and Fabio Urbina and Sean Ekins},
  year = {2024},
  journal = {Communications Chemistry},
  volume = {7},
  pages = {134},
  doi = {10.1038/s42004-024-01220-4},
  url = {https://doi.org/10.1038/s42004-024-01220-4},
  abstract = {Recent advances in machine learning (ML) have led to newer model architectures including transformers (large language models, LLMs) showing state of the art results in text generation and image analysis as well as few-shot learning (FSLC) models which offer predictive power with extremely small datasets. These new architectures may offer promise, yet the ‘no-free lunch’ theorem suggests that no single model algorithm can outperform at all possible tasks. Here, we explore the capabilities of classical (SVR), FSLC, and transformer models (MolBART) over a range of dataset tasks and show a ‘goldilocks zone’ for each model type, in which dataset size and feature distribution (i.e. dataset “diversity”) determines the optimal algorithm strategy. When datasets are small ( < 50 molecules), FSLC tend to outperform both classical ML and transformers. When datasets are small-to-medium sized (50-240 molecules) and diverse, transformers outperform both classical models and few-shot learning. Finally, when datasets are of larger and of sufficient size, classical models then perform the best, suggesting that the optimal model to choose likely depends on the dataset available, its size and diversity. These findings may help to answer the perennial question of which ML algorithm is to be used when faced with a new dataset.},
}

@article{Chng2025,
  title = {Ethical considerations in AI for child health and recommendations for child-centered medical AI},
  author = {Seo Yi Chng and Mark Jun Wen Tern and Yung Seng Lee and Lionel Tim-Ee Cheng and Jeevesh Kapur and Johan Gunnar Eriksson and Yap Seng Chong and Julian Savulescu},
  year = {2025},
  journal = {npj Digital Medicine},
  volume = {8},
  pages = {152},
  doi = {10.1038/s41746-025-01541-1},
  url = {https://doi.org/10.1038/s41746-025-01541-1},
  abstract = {There does not exist any previous comprehensive review on AI ethics in child health or any guidelines for management, unlike in adult medicine. This review describes ethical principles in AI for child health and provides recommendations for child-centered medical AI. We also introduce the Pediatrics EthicAl Recommendations List for AI (PEARL-AI) framework for clinicians and AI developers to ensure ethical AI enabled systems in healthcare for children.},
}

@article{Fraisl2025,
  title = {Leveraging the collaborative power of AI and citizen science for sustainable development},
  author = {Dilek Fraisl and Linda See and Steffen Fritz and Mordechai Haklay and Ian McCallum},
  year = {2025},
  journal = {Nature Sustainability},
  volume = {8},
  pages = {125-132},
  doi = {10.1038/s41893-024-01489-2},
  url = {https://doi.org/10.1038/s41893-024-01489-2},
  abstract = {Both artificial intelligence (AI) and citizen science hold immense potential for addressing major sustainability challenges from health to climate change. Alongside their individual benefits, when combined, they offer considerable synergies that can aid in both better monitoring of, and achieving, sustainable development. While AI has already been integrated into citizen science projects such as through automated classification and identification, the integration of citizen science approaches into AI is lacking. This integration has, however, the potential to address some of the major challenges associated with AI such as social bias, which could accelerate progress towards achieving sustainable development.},
}

@article{Mangalik2024,
  title = {Robust language-based mental health assessments in time and space through social media},
  author = {Siddharth Mangalik and Johannes C Eichstaedt and Salvatore Giorgi and Jihu Mun and Farhan Ahmed and Gilvir Gill and Adithya V. Ganesan and Shashanka Subrahmanya and Nikita Soni and Sean A P Clouston and H Andrew Schwartz},
  year = {2024},
  journal = {npj Digital Medicine},
  volume = {7},
  pages = {109},
  doi = {10.1038/s41746-024-01100-0},
  url = {https://doi.org/10.1038/s41746-024-01100-0},
  abstract = {In the most comprehensive population surveys, mental health is only broadly captured through questionnaires asking about “mentally unhealthy days” or feelings of “sadness.” Further, population mental health estimates are predominantly consolidated to yearly estimates at the state level, which is considerably coarser than the best estimates of physical health. Through the large-scale analysis of social media, robust estimation of population mental health is feasible at finer resolutions. In this study, we created a pipeline that used ~1 billion Tweets from 2 million geo-located users to estimate mental health levels and changes for depression and anxiety, the two leading mental health conditions. Language-based mental health assessments (LBMHAs) had substantially higher levels of reliability across space and time than available survey measures. This work presents reliable assessments of depression and anxiety down to the county-weeks level. Where surveys were available, we found moderate to strong associations between the LBMHAs and survey scores for multiple levels of granularity, from the national level down to weekly county measurements (fixed effects β = 0.34 to 1.82; p < 0.001). LBMHAs demonstrated temporal validity, showing clear absolute increases after a list of major societal events (+23% absolute change for depression assessments). LBMHAs showed improved external validity, evidenced by stronger correlations with measures of health and socioeconomic status than population surveys. This study shows that the careful aggregation of social media data yields spatiotemporal estimates of population mental health that exceed the granularity achievable by existing population surveys, and does so with generally greater reliability and validity.},
}

@article{Khalighi2024,
  title = {Artificial intelligence in neuro-oncology: advances and challenges in brain tumor diagnosis, prognosis, and precision treatment},
  author = {Sirvan Khalighi and Kartik Reddy and Abhishek Midya and Krunal Balvantbhai Pandav and Anant Madabhushi and Malak Abedalthagafi},
  year = {2024},
  journal = {npj Precision Oncology},
  volume = {8},
  pages = {80},
  doi = {10.1038/s41698-024-00575-0},
  url = {https://doi.org/10.1038/s41698-024-00575-0},
  abstract = {This review delves into the most recent advancements in applying artificial intelligence (AI) within neuro-oncology, specifically emphasizing work on gliomas, a class of brain tumors that represent a significant global health issue. AI has brought transformative innovations to brain tumor management, utilizing imaging, histopathological, and genomic tools for efficient detection, categorization, outcome prediction, and treatment planning. Assessing its influence across all facets of malignant brain tumor management- diagnosis, prognosis, and therapy- AI models outperform human evaluations in terms of accuracy and specificity. Their ability to discern molecular aspects from imaging may reduce reliance on invasive diagnostics and may accelerate the time to molecular diagnoses. The review covers AI techniques, from classical machine learning to deep learning, highlighting current applications and challenges. Promising directions for future research include multimodal data integration, generative AI, large medical language models, precise tumor delineation and characterization, and addressing racial and gender disparities. Adaptive personalized treatment strategies are also emphasized for optimizing clinical outcomes. Ethical, legal, and social implications are discussed, advocating for transparency and fairness in AI integration for neuro-oncology and providing a holistic understanding of its transformative impact on patient care.},
}

@article{Wang2025_02,
  title = {Preliminary evaluation of ChatGPT model iterations in emergency department diagnostics},
  author = {Jinge Wang and Kenneth Shue and Li Liu and Gangqing Hu},
  year = {2025},
  journal = {Scientific Reports},
  volume = {15},
  pages = {10426},
  doi = {10.1038/s41598-025-95233-1},
  url = {https://doi.org/10.1038/s41598-025-95233-1},
  abstract = {Large language model chatbots such as ChatGPT have shown the potential in assisting health professionals in emergency departments (EDs). However, the diagnostic accuracy of newer ChatGPT models remains unclear. This retrospective study evaluated the diagnostic performance of various ChatGPT models—including GPT-3.5, GPT-4, GPT-4o, and o1 series—in predicting diagnoses for ED patients (n = 30) and examined the impact of explicitly invoking reasoning (thoughts). Earlier models, such as GPT-3.5, demonstrated high accuracy for top-three differential diagnoses (80.0% in accuracy) but underperformed in identifying leading diagnoses (47.8%) compared to newer models such as chatgpt-4o-latest (60%, p < 0.01) and o1-preview (60%, p < 0.01). Asking for thoughts to be provided significantly enhanced the performance on predicting leading diagnosis for 4o models such as 4o-2024-0513 (from 45.6 to 56.7%; p = 0.03) and 4o-mini-2024-07-18 (from 54.4 to 60.0%; p = 0.04) but had minimal impact on o1-mini and o1-preview. In challenging cases, such as pneumonia without fever, all models generally failed to predict the correct diagnosis, indicating atypical presentations as a major limitation for ED application of current ChatGPT models.},
}

@article{Koirala2025,
  title = {Evaluating AI performance in nephrology triage and subspecialty referrals},
  author = {Priscilla Koirala and Charat Thongprayoon and Jing Miao and Oscar A Garcia Valencia and Mohammad S Sheikh and Supawadee Suppadungsuk and Michael A Mao and Justin H Pham and Iasmina M Craici and Wisit Cheungpasitporn},
  year = {2025},
  journal = {Scientific Reports},
  volume = {15},
  pages = {3455},
  doi = {10.1038/s41598-025-88074-5},
  url = {https://doi.org/10.1038/s41598-025-88074-5},
  abstract = {Artificial intelligence (AI) has shown promise in revolutionizing medical triage, particularly in the context of the rising prevalence of kidney-related conditions with the aging global population. This study evaluates the utility of ChatGPT, a large language model, in triaging nephrology cases through simulated real-world scenarios. Two nephrologists created 100 patient cases that encompassed various aspects of nephrology. ChatGPT’s performance in determining the appropriateness of nephrology consultations and identifying suitable nephrology subspecialties was assessed. The results demonstrated high accuracy; ChatGPT correctly determined the need for nephrology in 99–100% of cases, and it accurately identified the most suitable nephrology subspecialty triage in 96–99% of cases across two evaluation rounds. The agreement between the two rounds was 97%. While ChatGPT showed promise in improving medical triage efficiency and accuracy, the study also identified areas for refinement. This included the need for better integration of multidisciplinary care for patients with complex, intersecting medical conditions. This study’s findings highlight the potential of AI in enhancing decision-making processes in clinical workflow, and it can inform the development of AI-assisted triage systems tailored to institution-specific practices including multidisciplinary approaches.},
}

@article{Hardy2023,
  title = {Improving nonalcoholic fatty liver disease classification performance with latent diffusion models},
  author = {Romain Hardy and Joe Klepich and Ryan Mitchell and Steve Hall and Jericho Villareal and Cornelia Ilin},
  year = {2023},
  journal = {Scientific Reports},
  volume = {13},
  pages = {21619},
  doi = {10.1038/s41598-023-48062-z},
  url = {https://doi.org/10.1038/s41598-023-48062-z},
  abstract = {Integrating deep learning with clinical expertise holds great potential for addressing healthcare challenges and empowering medical professionals with improved diagnostic tools. However, the need for annotated medical images is often an obstacle to leveraging the full power of machine learning models. Our research demonstrates that by combining synthetic images, generated using diffusion models, with real images, we can enhance nonalcoholic fatty liver disease (NAFLD) classification performance even in low-data regime settings. We evaluate the quality of the synthetic images by comparing two metrics: Inception Score (IS) and Fréchet Inception Distance (FID), computed on diffusion- and generative adversarial network (GAN)-generated images. Our results show superior performance for the diffusion-generated images, with a maximum IS score of 1.90 compared to 1.67 for GANs, and a minimum FID score of 69.45 compared to 100.05 for GANs. Utilizing a partially frozen CNN backbone (EfficientNet v1), our synthetic augmentation method achieves a maximum image-level ROC AUC of 0.904 on a NAFLD prediction task.},
}

@article{Hager2024,
  title = {Evaluation and mitigation of the limitations of large language models in clinical decision-making},
  author = {Paul Hager and Friederike Jungmann and Robbie Holland and Kunal Bhagat and Inga Hubrecht and Manuel Knauer and Jakob Vielhauer and Marcus Makowski and Rickmer Braren and Georgios Kaissis and Daniel Rueckert},
  year = {2024},
  journal = {Nature Medicine},
  volume = {30},
  pages = {2613-2622},
  doi = {10.1038/s41591-024-03097-1},
  url = {https://doi.org/10.1038/s41591-024-03097-1},
  abstract = {Clinical decision-making is one of the most impactful parts of a physician’s responsibilities and stands to benefit greatly from artificial intelligence solutions and large language models (LLMs) in particular. However, while LLMs have achieved excellent performance on medical licensing exams, these tests fail to assess many skills necessary for deployment in a realistic clinical decision-making environment, including gathering information, adhering to guidelines, and integrating into clinical workflows. Here we have created a curated dataset based on the Medical Information Mart for Intensive Care database spanning 2,400 real patient cases and four common abdominal pathologies as well as a framework to simulate a realistic clinical setting. We show that current state-of-the-art LLMs do not accurately diagnose patients across all pathologies (performing significantly worse than physicians), follow neither diagnostic nor treatment guidelines, and cannot interpret laboratory results, thus posing a serious risk to the health of patients. Furthermore, we move beyond diagnostic accuracy and demonstrate that they cannot be easily integrated into existing workflows because they often fail to follow instructions and are sensitive to both the quantity and order of information. Overall, our analysis reveals that LLMs are currently not ready for autonomous clinical decision-making while providing a dataset and framework to guide future studies.},
}

@article{Lachowycz2024,
  title = {Utility of artificial intelligence in geoscience},
  author = {Stefan Lachowycz},
  year = {2024},
  journal = {Nature Geoscience},
  volume = {17},
  pages = {953-955},
  doi = {10.1038/s41561-024-01548-5},
  url = {https://doi.org/10.1038/s41561-024-01548-5},
  abstract = {Nature Geoscience spoke with Dr Mariana Clare, a machine learning scientist at the European Centre for Medium-Range Weather Forecasts; Prof. Haifeng Qian, an environmental scientist at Zhejiang University of Technology; and Dr Theresa Sawi, a seismologist at the US Geological Survey, about using artificial intelligence (AI) in their research and in geoscience generally.},
}

@article{Carli2025,
  title = {Learning and actioning general principles of cancer cell drug sensitivity},
  author = {Francesco Carli and Pierluigi Di Chiaro and Mariangela Morelli and Chakit Arora and Luisa Bisceglia and Natalia De Oliveira Rosa and Alice Cortesi and Sara Franceschi and Francesca Lessi and Anna Luisa Di Stefano and Orazio Santo Santonocito and Francesco Pasqualetti and Paolo Aretini and Pasquale Miglionico and Giuseppe R Diaferia and Fosca Giannotti and Pietro Liò and Miquel Duran-Frigola and Chiara Maria Mazzanti and Gioacchino Natoli and Francesco Raimondi},
  year = {2025},
  journal = {Nature Communications},
  volume = {16},
  pages = {1654},
  doi = {10.1038/s41467-025-56827-5},
  url = {https://doi.org/10.1038/s41467-025-56827-5},
  abstract = {High-throughput screening of drug sensitivity of cancer cell lines (CCLs) holds the potential to unlock anti-tumor therapies. In this study, we leverage such datasets to predict drug response using cell line transcriptomics, focusing on models’ interpretability and deployment on patients’ data. We use large language models (LLMs) to match drug to mechanisms of action (MOA)-related pathways. Genes crucial for prediction are enriched in drug-MOAs, suggesting that our models learn the molecular determinants of response. Furthermore, by using only LLM-curated, MOA-genes, we enhance the predictive accuracy of our models. To enhance translatability, we align RNAseq data from CCLs, used for training, to those from patient samples, used for inference. We validated our approach on TCGA samples, where patients’ best scoring drugs match those prescribed for their cancer type. We further predict and experimentally validate effective drugs for the patients of two highly lethal solid tumors, i.e., pancreatic cancer and glioblastoma.},
}

@article{Maity2025,
  title = {Can large language models meet the challenge of generating school-level questions?},
  author = {Subhankar Maity and Aniket Deroy and Sudeshna Sarkar},
  year = {2025},
  journal = {Computers and Education: Artificial Intelligence},
  volume = {8},
  pages = {100370},
  doi = {https://doi.org/10.1016/j.caeai.2025.100370},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000104},
  abstract = {In the realm of education, crafting appropriate questions for examinations is a meticulous and time-consuming task that is crucial for assessing students' understanding of the subject matter. This paper explores the potential of leveraging large language models (LLMs) to automate question generation in the educational domain. Specifically, we focus on generating educational questions from contexts extracted from school-level textbooks. Our study aims to prompt LLMs such as GPT-4 Turbo, GPT-3.5 Turbo, Llama-2-70B, Llama-3.1-405B, and Gemini Pro to generate a complete set of questions for each context, potentially streamlining the question generation process for educators. We performed a human evaluation of the generated questions, assessing their coverage, grammaticality, usefulness, answerability, and relevance. Additionally, we prompted LLMs to generate questions based on Bloom's revised taxonomy, categorizing and evaluating these questions according to their cognitive complexity and learning objectives. We applied both zero-shot and eight-shot prompting techniques. These efforts provide insight into the efficacy of LLMs in automated question generation and their potential in assessing students' cognitive abilities across various school-level subjects. The results show that employing an eight-shot technique improves the performance of human evaluation metrics for the generated complete set of questions and helps generate questions that are better aligned with Bloom's revised taxonomy.},
}

@article{Malas2024,
  title = {The future of artificial intelligence: evaluating ChatGPT's performance in X sentiment prediction},
  author = {Laila Malas and Ahmad Shawaqfeh and Ahmad Abushakra},
  year = {2024},
  journal = {Discover Artificial Intelligence},
  volume = {4},
  pages = {105},
  doi = {10.1007/s44163-024-00218-0},
  url = {https://doi.org/10.1007/s44163-024-00218-0},
  abstract = {Technological advancements have significantly progressed from the inception of computers and the internet to the rise of artificial intelligence (AI), profoundly impacting various sectors such as business, education, and healthcare. Among these advancements, ChatGPT has emerged as a prominent conversational AI tool, facilitating tasks such as sentiment prediction and sector-specific decision-making. This study aims to evaluate ChatGPT's performance in analyzing public sentiment using data from X (formerly Twitter) and to propose an ethical framework for its sector-specific applications. By combining the BERT transformer model with BiLSTM, Random Forest, and K-Nearest Neighbor algorithms, the study achieves a hybrid approach to sentiment prediction, demonstrating superior performance with an accuracy of 86.7%. Furthermore, the study explores ethical considerations, such as fairness, accountability, and transparency, to address ChatGPT's adoption across industries. The findings offer insights into both the technical and ethical dimensions of ChatGPT’s integration into business, education, and healthcare sectors, emphasizing its potential for sustainable and responsible use.},
}

@article{Erdélyi2024,
  title = {AI regulation: still a to-do item on New Zealand’s political agenda},
  author = {Olivia J Erdélyi and Gábor Erdélyi},
  year = {2024},
  journal = {Discover Artificial Intelligence},
  volume = {4},
  pages = {106},
  doi = {10.1007/s44163-024-00210-8},
  url = {https://doi.org/10.1007/s44163-024-00210-8},
  abstract = {The body of AI regulations and country strategies has been steadily growing in the past few years. The EU recently adopted the world’s first comprehensive AI regulation, the AI Act. The US has published several high-level guidance documents, such as the 2023 Executive Order on AI, and has begun to introduce AI-specific laws and amendments to existing laws at the state and substate levels. China has also adopted several laws in the AI space. Many other countries have at least developed and published AI strategies. New Zealand (NZ) is a latecomer to AI regulation. As of yet, there is no overarching AI strategy, AI-specific legislation, much less a comprehensive AI regulatory framework in place. This paper surveys key domestic actors and regulatory initiatives that will form the basis of the country’s future AI regulatory landscape.},
}

@article{Maity2024,
  title = {Investigating Large Language Models for Prompt-Based Open-Ended Question Generation in the Technical Domain},
  author = {Subhankar Maity and Aniket Deroy and Sudeshna Sarkar},
  year = {2024},
  journal = {SN Computer Science},
  volume = {5},
  pages = {1128},
  doi = {10.1007/s42979-024-03464-2},
  url = {https://doi.org/10.1007/s42979-024-03464-2},
  abstract = {We explore the automated generation of open-ended questions from technical domain textbooks. These questions are more diverse than those typically examined in the field of question generation (QG) for reading comprehension. To facilitate this endeavor, we curate EngineeringQ, a prompt-based QG dataset that contains triples of (1) Context: a segment from which the questions are formed; (2) Prompt: a prompt is a concise and specific keyphrase that serves as a short guiding reference to generate a question based on a given context; (3) Question: a question that aligns with the context and is coherent with the prompt. We evaluate the performance of several fine-tuned encoder-decoder based large language models (LLMs), including Pegasus, BART, Flan-T5, and T5 on EngineeringQ. We introduce a novel prompt-tuning method for these encoder-decoder based LLMs. We also investigate the potential of general-purpose decoder-only LLMs such as GPT$$-$$3.5 Turbo, text-davinci-003, and GPT-4 in zero-shot setting. Evaluation involves automated metrics and human evaluation by domain experts. Extending our study beyond EngineeringQ, we apply these methods to subjects such as biology, chemistry, earth science, and physics. Furthermore, we investigate the potential for domain adaptation by fine-tuning the best-performing LLM on school-level subjects and assessing its effectiveness on undergraduate-level computer science and information technology subjects for zero-shot and few-shot QG. To gain insights into generated question complexity, we also utilize Bloom’s revised taxonomy to categorize questions into different levels, enhancing our understanding of their educational value. Experimental results highlight T5LARGE’s superiority in automated evaluation metrics and text-davinci-003’s excellence in human evaluation metrics in EngineeringQ. For subjects such as biology, chemistry, earth science, and physics, Flan-T5BASE excels in automated metrics, while GPT-4 leads in human evaluation. Notably, human baseline methods consistently outperform all AQG approaches, including fine-tuned LLMs, prompt-tuned LLMs, and zero-shot GPT models.},
}

@article{Mahale2025,
  title = {A comprehensive review on artificial intelligence driven predictive maintenance in vehicles: technologies, challenges and future research directions},
  author = {Yashashree Mahale and Shrikrishna Kolhar and Anjali S More},
  year = {2025},
  journal = {Discover Applied Sciences},
  volume = {7},
  pages = {243},
  doi = {10.1007/s42452-025-06681-3},
  url = {https://doi.org/10.1007/s42452-025-06681-3},
  abstract = {Predictive maintenance has rapidly grown in automotive industries with the advancements in artificial intelligence (AI) technologies like machine learning, deep learning, and now generative AI. The amount of data extracted from machines with sensors and other network technologies can be valuable and useful for building advanced solutions in predictive maintenance tasks. This, in turn, helps improve vehicle up-time and reliability. This paper comprehensively reviews the different technologies and methods used for predictive maintenance. A systematic literature review of 94 papers was conducted from renowned databases such as Scopus and Web of Science. The paper reviews various techniques applied for predictive maintenance, highlighting the role of techniques in AI and the importance of explainable AI for predictive analytics. This review examines AI applications in vehicle maintenance strategies and diagnostics to reduce costs, maintenance schedules, remaining useful life predictions, and effective monitoring of health conditions. In addition, publicly available data sets relevant to predictive maintenance tasks are discussed, which play a crucial role in research and model development. The paper also identifies various challenges in predictive maintenance related to data quality, scalability, and integration of AI technology. In addition, emerging research topics within the domain are highlighted with future directions to address these challenges, thus optimizing maintenance strategies in the automotive industry.},
}

@article{Stanojević2024,
  title = {Rockfish: A transformer-based model for accurate 5-methylcytosine prediction from nanopore sequencing},
  author = {Dominik Stanojević and Zhe Li and Sara Bakić and Roger Foo and Mile Šikić},
  year = {2024},
  journal = {Nature Communications},
  volume = {15},
  pages = {5580},
  doi = {10.1038/s41467-024-49847-0},
  url = {https://doi.org/10.1038/s41467-024-49847-0},
  abstract = {DNA methylation plays an important role in various biological processes, including cell differentiation, ageing, and cancer development. The most important methylation in mammals is 5-methylcytosine mostly occurring in the context of CpG dinucleotides. Sequencing methods such as whole-genome bisulfite sequencing successfully detect 5-methylcytosine DNA modifications. However, they suffer from the serious drawbacks of short read lengths and might introduce an amplification bias. Here we present Rockfish, a deep learning algorithm that significantly improves read-level 5-methylcytosine detection by using Nanopore sequencing. Rockfish is compared with other methods based on Nanopore sequencing on R9.4.1 and R10.4.1 datasets. There is an increase in the single-base accuracy and the F1 measure of up to 5 percentage points on R.9.4.1 datasets, and up to 0.82 percentage points on R10.4.1 datasets. Moreover, Rockfish shows a high correlation with whole-genome bisulfite sequencing, requires lower read depth, and achieves higher confidence in biologically important regions such as CpG-rich promoters while being computationally efficient. Its superior performance in human and mouse samples highlights its versatility for studying 5-methylcytosine methylation across varied organisms and diseases. Finally, its adaptable architecture ensures compatibility with new versions of pores and chemistry as well as modification types.},
}

@article{Kim2024_02,
  title = {A feasibility study on the adoption of a generative denoising diffusion model for the synthesis of fundus photographs using a small dataset},
  author = {Hong Kyu Kim and Ik Hee Ryu and Joon Yul Choi and Tae Keun Yoo},
  year = {2024},
  journal = {Discover Applied Sciences},
  volume = {6},
  pages = {188},
  doi = {10.1007/s42452-024-05871-9},
  url = {https://doi.org/10.1007/s42452-024-05871-9},
  abstract = {The generative diffusion model has been highlighted as a state-of-the-art artificial intelligence technique for image synthesis. Here, we show that a denoising diffusion probabilistic model (DDPM) can be used for a domain-specific task generating fundus photographs based on a limited training dataset in an unconditional manner. We trained the DDPM based on U-Net backbone architecture, which is the most popular form of the generative diffusion model. After training, serial multiple denoising U-Nets can generate FPs using random noise seeds. A thousand healthy retinal images were used to train the diffusion model. The input image size was set to a pixel resolution of 128 × 128. The trained DDPM successfully generated synthetic fundus photographs with a resolution of 128 × 128 pixels using our small dataset. We failed to train the DDPM for 256-by-256-pixel images due to the limited computation capacity using a personal cloud platform. In a comparative analysis, the progressive growing generative adversarial network (PGGAN) model synthesized more sharpened images than the DDPM in the retinal vessels and optic discs. The PGGAN (Frechet inception distance [FID] score: 41.761) achieved a better FID score than the DDPM (FID score: 65.605). We used a domain-specific generative diffusion model to synthesize fundus photographs based on a relatively small dataset. Because the DDPM has disadvantages with a small dataset, including difficulty in training and low image quality compared with generative adversarial networks such as PGGAN, further studies are needed to improve diffusion models for domain-specific medical tasks with small numbers of samples.},
}

@article{Greif2024,
  title = {A systematic review of current AI techniques used in the context of the SDGs},
  author = {Lucas Greif and Fabian Röckel and Andreas Kimmig and Jivka Ovtcharova},
  year = {2024},
  journal = {International Journal of Environmental Research},
  volume = {19},
  pages = {1},
  doi = {10.1007/s41742-024-00668-5},
  url = {https://doi.org/10.1007/s41742-024-00668-5},
  abstract = {This study aims to explore the application of artificial intelligence (AI) in the resolution of sustainability challenges, with a specific focus on environmental studies. Given the rapidly evolving nature of this field, there is an urgent need for more frequent and dynamic reviews to keep pace with the innovative applications of AI. Through a systematic analysis of 191 research articles, we classified AI techniques applied in the field of sustainability. Our review found that 65% of the studies applied supervised learning methods, 18% employed unsupervised learning, and 17% utilized reinforcement learning approaches. The review highlights that artificial neural networks (ANN), are the most commonly applied AI techniques in sustainability contexts, accounting for 23% of the reviewed methods. This comprehensive overview of AI techniques identifies key trends and proposes new research avenues to address the complex issue of achieving the Sustainable Development Goals (SDGs).},
}

@article{Allec2024,
  title = {A Case Study of Multimodal, Multi-institutional Data Management for the Combinatorial Materials Science Community},
  author = {Sarah I Allec and Eric S Muckley and Nathan S Johnson and Christopher K H Borg and Dylan J Kirsch and Joshua Martin and Rohit Pant and Ichiro Takeuchi and Andrew S Lee and James E Saal and Logan Ward and Apurva Mehta},
  year = {2024},
  journal = {Integrating Materials and Manufacturing Innovation},
  volume = {13},
  pages = {406-419},
  doi = {10.1007/s40192-024-00345-7},
  url = {https://doi.org/10.1007/s40192-024-00345-7},
  abstract = {Although the convergence of high-performance computing, automation, and machine learning has significantly altered the materials design timeline, transformative advances in functional materials and acceleration of their design will require addressing the deficiencies that currently exist in materials informatics, particularly a lack of standardized experimental data management. The challenges associated with experimental data management are especially true for combinatorial materials science, where advancements in automation of experimental workflows have produced datasets that are often too large and too complex for human reasoning. The data management challenge is further compounded by the multimodal and multi-institutional nature of these datasets, as they tend to be distributed across multiple institutions and can vary substantially in format, size, and content. Furthermore, modern materials engineering requires the tuning of not only composition but also of phase and microstructure to elucidate processing–structure–property–performance relationships. To adequately map a materials design space from such datasets, an ideal materials data infrastructure would contain data and metadata describing (i) synthesis and processing conditions, (ii) characterization results, and (iii) property and performance measurements. Here, we present a case study for the low-barrier development of such a dashboard that enables standardized organization, analysis, and visualization of a large data lake consisting of combinatorial datasets of synthesis and processing conditions, X-ray diffraction patterns, and materials property measurements generated at several different institutions. While this dashboard was developed specifically for data-driven thermoelectric materials discovery, we envision the adaptation of this prototype to other materials applications, and, more ambitiously, future integration into an all-encompassing materials data management infrastructure.},
}

@article{Hossain2024,
  title = {Academic integrity and copyright literacy policy and instruction in K-12 schools: a global study from the perspective of school library professionals},
  author = {Zakir Hossain and Özgür Çelik and Corinne Hertel},
  year = {2024},
  journal = {International Journal for Educational Integrity},
  volume = {20},
  pages = {4},
  doi = {10.1007/s40979-024-00150-x},
  url = {https://doi.org/10.1007/s40979-024-00150-x},
  abstract = {This study examined the policies and instructional practices related to academic integrity and copyright literacy in K-12 schools through the lens of school library professionals. Since school librarians play a key role in promoting academic integrity and copyright literacy in schools, they were chosen. An online survey was administered to school library professionals in 85 countries using a mixed methods approach, yielding 569 responses. The results revealed that many K-12 schools lack policies on academic integrity and copyright, and there is variability in the perceived value, implementation and teaching of these literacies. While most school library professionals reported teaching academic integrity and copyright literacy in their schools, implementing effective pedagogies remains challenging. Collaboration between school library professionals and teachers, along with the use of multimedia resources, were identified as potential strategies for practical education and instruction. The study highlights the need for greater attention to be given to these literacies in K-12 education and calls for the development of policies, necessary support and effective teaching methods to ensure students are knowledgeable and well-prepared for higher education.},
}

@article{Durantaye2025,
  title = {Control and Compensation. A Comparative Analysis of Copyright Exceptions for Training Generative AI},
  author = {Katharina de la Durantaye},
  year = {2025},
  journal = {IIC - International Review of Intellectual Property and Competition Law},
  doi = {10.1007/s40319-025-01569-6},
  url = {https://doi.org/10.1007/s40319-025-01569-6},
  abstract = {Lawmakers and administrative agencies around the globe are debating whether the use of copyrighted content for AI training does or should require the rights holder’s consent. This article examines legislation and policy debates in the U.S., Canada, the UK, the EU, Israel, China, Singapore, and Japan. Issues of control, compensation, transparency and legal certainty dominate the discussion. Countries are trying to recalibrate the balance of interests, either in favour of AI companies, or by supporting rights holders – for example, with copyright-related transparency obligations. EU copyright law offers a relatively favourable environment for AI companies. Ultimately, however, copyright law is not the decisive factor when AI companies choose the location of their training facilities.},
}

@article{Jha2025,
  title = {Agricultural supply chain management using hyperledger and AIOT},
  author = {Anurag Kumar Jha and Aparna Raj and Ashish Kumar Jha and Sujala D Shetty},
  year = {2025},
  journal = {Journal of Ambient Intelligence and Humanized Computing},
  doi = {10.1007/s12652-024-04948-y},
  url = {https://doi.org/10.1007/s12652-024-04948-y},
  abstract = {Supply chain management and Hyperledger are two interconnected domains. They leverage blockchain technology to enhance efficiency, transparency, and security in supply chain operations. Together, they provide a decentralized, traceable, and real-time platform for recording and managing transactions. This combination is particularly valuable for industries dealing with sensitive goods, as it provides accurate traceability and real-time information. This paper explores the integration of supply chain management with Hyperledger blockchain technology to enhance efficiency, transparency, and security in supply chain operations. We propose a decentralized Hyperledger Fabric blockchain network to improve traceability, security, and efficiency by monitoring environmental conditions. This approach is particularly beneficial for transporting sensitive goods, such as medical supplies and perishable items, by ensuring optimal conditions and real-time data accessibility. The integration of Artificial Intelligence (AI) further enhances insights, reduces waste, and improves overall efficiency. By utilizing a distributed network free from third-party intermediaries, the system ensures immutability and remote accessibility, addressing challenges related to transporting heat and humidity sensitive products. Our experimental assessment demonstrates the benefits of private blockchain technologies, including enhanced security, regulatory compliance, compatibility, flexibility, and scalability. This study presents a detailed methodology for developing a traceable, efficient, and sustainable agricultural supply chain.},
}

@article{Xing2025,
  title = {Artificial intelligence in landscape architecture: a survey},
  author = {Yue Xing and Wensheng Gan and Qidi Chen},
  year = {2025},
  journal = {International Journal of Machine Learning and Cybernetics},
  doi = {10.1007/s13042-025-02536-w},
  url = {https://doi.org/10.1007/s13042-025-02536-w},
  abstract = {The development history of landscape architecture (LA) reflects the human pursuit of environmental beautification and ecological balance. With the advancement of artificial intelligence (AI) technologies that simulate and extend human intelligence, immense opportunities have been provided for LA, offering scientific and technological support throughout the entire workflow. In this article, we comprehensively review the applications of AI technology in the field of LA. First, we introduce the many potential benefits that AI brings to the design, planning, and management aspects of LA. Secondly, we discuss how AI can assist the LA field in solving its current development problems, including urbanization, environmental degradation and ecological decline, irrational planning, insufficient management and maintenance, and lack of public participation. Furthermore, we summarize the key technologies and practical cases of applying AI in the LA domain, from design assistance to intelligent management, all of which provide innovative solutions for the planning, design, and maintenance of LA. Finally, we look ahead to the problems and opportunities in LA, emphasizing the need to combine human expertise and judgment for rational decision-making. This article provides both theoretical and practical guidance for LA designers, researchers, and technology developers. The successful integration of AI technology into LA holds great promise for enhancing the field’s capabilities and achieving more sustainable, efficient, and user-friendly outcomes.},
}

@article{Chamola2024,
  title = {A Comprehensive Survey on Generative AI for Metaverse: Enabling Immersive Experience},
  author = {Vinay Chamola and Siva Sai and Animesh Bhargava and Ashis Sahu and Wenchao Jiang and Zehui Xiong and Dusit Niyato and Amir Hussain},
  year = {2024},
  journal = {Cognitive Computation},
  volume = {16},
  pages = {3286-3315},
  doi = {10.1007/s12559-024-10342-9},
  url = {https://doi.org/10.1007/s12559-024-10342-9},
  abstract = {Generative Artificial Intelligence models are Artificial Intelligence models that generate new content based on a prompt or input. The output content can be in various forms, including text, images, and video. Metaverse refers to a virtual world where users can interact with each other, objects and events in an immersive, realistic, and dynamic manner. A critical and foremost step in realizing the Metaverse is content creation for its different realms. Given Metaverse’s need for enormous content, Generative AI is a perfect technology for content creation. This paper explores how Generative AI models can help fulfil the potential of the Metaverse by assisting in the design and production of various aspects of the Metaverse and attracting users not just by creating dynamic, interactive, and personalised content at scale but also by producing various revenue-generating opportunities for users and organisations in the Metaverse. The paper analyses the Generative AI models by grouping them according to the type of content they generate, namely text, image, video, 3D visual, audio, and gaming. Various use cases in the Metaverse are explored and listed according to each type of AI Generated Content (AIGC). This paper also presents several applications and scenarios where the mixture of different Generative AI (GAI) models benefits the Metaverse. Further, this paper also enumerates the limitations and challenges of Generative AI models and the areas of future work. Despite the obstacles, Generative AI can realise the potential of the Metaverse by making it much more functional and interactive owing to the vast use cases of different types of AIGC in the Metaverse, and the age of virtual reality may not be too distant.},
}

@article{Devunuri2024,
  title = {ChatGPT for GTFS: benchmarking LLMs on GTFS semantics... and retrieval},
  author = {Saipraneeth Devunuri and Shirin Qiam and Lewis J Lehe},
  year = {2024},
  journal = {Public Transport},
  volume = {16},
  pages = {333-357},
  doi = {10.1007/s12469-024-00354-x},
  url = {https://doi.org/10.1007/s12469-024-00354-x},
  abstract = {The General Transit Feed Specification (GTFS) standard for publishing transit data is ubiquitous. With the advent of LLMs being used widely, this research explores the possibility of extracting transit information from GTFS through natural language instructions. To evaluate the capabilities and limitations of LLMs, we introduce two benchmarks, namely “GTFS Semantics” and “GTFS Retrieval” that test how well LLMs can “understand” GTFS standards and retrieve relevant transit information. We benchmark OpenAI’s GPT-3.5 Turbo and GPT-4 LLMs, which are backends for the ChatGPT interface. In particular, we use zero-shot, one-shot, chain of thought, and program synthesis techniques with prompt engineering. For our multiple questions, GPT-3.5 Turbo answers 59.7% correctly and GPT-4 answers 73.3% correctly, but they do worse when one of the multiple choice options is replaced by “None of these”. Furthermore, we evaluate how well the LLMs can extract information from a filtered GTFS feed containing four bus routes from the Chicago Transit Authority. Program synthesis techniques outperformed zero-shot approaches, achieving up to 93% (90%) accuracy for simple queries and 61% (41%) for complex ones using GPT-4 (GPT-3.5 Turbo).},
}

@article{Xu2024,
  title = {Large language models for generative information extraction: a survey},
  author = {Derong Xu and Wei Chen and Wenjun Peng and Chao Zhang and Tong Xu and Xiangyu Zhao and Xian Wu and Yefeng Zheng and Yang Wang and Enhong Chen},
  year = {2024},
  journal = {Frontiers of Computer Science},
  volume = {18},
  pages = {186357},
  doi = {10.1007/s11704-024-40555-y},
  url = {https://doi.org/10.1007/s11704-024-40555-y},
  abstract = {Information Extraction (IE) aims to extract structural knowledge from plain natural language texts. Recently, generative Large Language Models (LLMs) have demonstrated remarkable capabilities in text understanding and generation. As a result, numerous works have been proposed to integrate LLMs for IE tasks based on a generative paradigm. To conduct a comprehensive systematic review and exploration of LLM efforts for IE tasks, in this study, we survey the most recent advancements in this field. We first present an extensive overview by categorizing these works in terms of various IE subtasks and techniques, and then we empirically analyze the most advanced methods and discover the emerging trend of IE tasks with LLMs. Based on a thorough review conducted, we identify several insights in technique and promising research directions that deserve further exploration in future studies. We maintain a public repository and consistently update related works and resources on GitHub (LLM4IE repository).},
}

@article{Baro2025,
  title = {Predicting hospitalization with LLMs from health insurance data},
  author = {Everton F Baro and Luiz S Oliveira and Alceu de Souza Britto},
  year = {2025},
  journal = {Medical & Biological Engineering & Computing},
  volume = {63},
  pages = {1215-1226},
  doi = {10.1007/s11517-024-03251-4},
  url = {https://doi.org/10.1007/s11517-024-03251-4},
  abstract = {Predictions of hospitalizations can help in the development of applications for health insurance, hospitals, and medicine. The data collected by health insurance has potential that is not always explored, and extracting features from it for use in machine learning applications requires demanding processes and specialized knowledge. With the emergence of large language models (LLM) there are possibilities to use this data for a wide range of applications requiring little specialized knowledge. To do this, it is necessary to organize and prepare this data to be used by these models. Therefore, in this work, an approach is presented for using data from health insurance in LLMs with the objective of predict hospitalizations. As a result, pre-trained models were generated in Portuguese and English with health insurance data that can be used in several applications. To prove the effectiveness of the models, tests were carried out to predict hospitalizations in general and due to stroke. For hospitalizations in general, F1-Score = 87.8 and AUC = 0.955 were achieved, and for hospitalizations due to stroke, the best model achieved F1-Score = 88.7 and AUC of 0.964. Considering the potential for use, the models were made available to the scientific community.},
}

@article{Gao2024,
  title = {Fairness in machine learning: definition, testing, debugging, and application},
  author = {Xuanqi Gao and Chao Shen and Weipeng Jiang and Chenhao Lin and Qian Li and Qian Wang and Qi Li and Xiaohong Guan},
  year = {2024},
  journal = {Science China Information Sciences},
  volume = {67},
  pages = {191201},
  doi = {10.1007/s11432-023-4060-x},
  url = {https://doi.org/10.1007/s11432-023-4060-x},
  abstract = {In recent years, artificial intelligence technology has been widely used in many fields, such as computer vision, natural language processing and autonomous driving. Machine learning algorithms, as the core technique of AI, have significantly facilitated people’s lives. However, underlying fairness issues in machine learning systems can pose risks to individual fairness and social security. Studying fairness definitions, sources of problems, and testing and debugging methods of fairness can help ensure the fairness of machine learning systems and promote the wide application of artificial intelligence technology in various fields. This paper introduces relevant definitions of machine learning fairness and analyzes the sources of fairness problems. Besides, it provides guidance on fairness testing and debugging methods and summarizes popular datasets. This paper also discusses the technical advancements in machine learning fairness and highlights future challenges in this area.},
}

@article{Fan2025_01,
  title = {On the Trustworthiness Landscape of State-of-the-art Generative Models: A Survey and Outlook},
  author = {Mingyuan Fan and Chengyu Wang and Cen Chen and Yang Liu and Jun Huang},
  year = {2025},
  journal = {International Journal of Computer Vision},
  doi = {10.1007/s11263-025-02375-w},
  url = {https://doi.org/10.1007/s11263-025-02375-w},
  abstract = {Diffusion models and large language models have emerged as leading-edge generative models, revolutionizing various aspects of human life. However, their practical implementation has also exposed inherent risks, bringing to light their potential downsides and sparking concerns about their trustworthiness. Despite the wealth of literature on this subject, a comprehensive survey that specifically delves into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, this paper investigates both long-standing and emerging threats associated with these models across four fundamental dimensions: 1) privacy, 2) security, 3) fairness, and 4) responsibility. Based on our investigation results, we develop an extensive survey that outlines the trustworthiness of large generative models. Following that, we provide practical recommendations and identify promising research directions for generative AI, ultimately promoting the trustworthiness of these models and benefiting society as a whole.},
}

@article{Perbal2023,
  title = {Farewell Springer… Hello Wiley},
  author = {Bernard Perbal},
  year = {2023},
  journal = {Journal of Cell Communication and Signaling},
  volume = {17},
  pages = {1123-1129},
  doi = {10.1007/s12079-023-00796-1},
  url = {https://doi.org/10.1007/s12079-023-00796-1},
  abstract = {Academic publishing is the support for dissemination of research findings that constitute the grounds upon which new orientations and improvements are based on sharing breaking ideas, critical analyses of data, and argumentations that sustain the development of collaborative research projects. The wide diffusion of new scientific findings is pivotal to the progress of medical sciences, a salient feature of human societal fullness and intellectual welfare. In a practical way, the value of academic publishing can be ascertained by its capacity to reach a wide number of readers from different fields that may provide the soil for interactive projects. The challenges are numerous (Zul in Challenges in Academic Publishing; Navigating the Obstacles, 2023). An examination of the means developed to survey the individual performances of scientists, based on their publications, has led me to comment in this editorial on pitfalls that muddle the way to upstanding evaluations mainly based on irrelevant metrics.},
}

@article{Lee2024_01,
  title = {GPT Prompt Engineering for a Large Language Model-Based Process Improvement Generation System},
  author = {Donghyeon Lee and Jaewook Lee and Dongil Shin},
  year = {2024},
  journal = {Korean Journal of Chemical Engineering},
  volume = {41},
  pages = {3263-3286},
  doi = {10.1007/s11814-024-00276-1},
  url = {https://doi.org/10.1007/s11814-024-00276-1},
  abstract = {Process design improvements require extensive knowledge, considerable time, and huge human resources due to the complexity of chemical processes and their diverse objective functions. However, machine learning-based approaches using vast accumulated data are limited in low versatility, applicable only to specific processes, and unable to understand the basis of model decisions. This study proposes the GPT-based Improved Process Hybrid Transformer (GIPHT), a process design improvement generation system utilizing Large Language Model (LLM). LLMs, being natural language-based, allow for understanding the basis of model decisions without need of explainable AI analysis. GIPHT is composed of multi-agent to enhance versatility and performance for diverse chemical processes. We also propose the Detailed Simplified Flowsheet Input Line Entry System format to express process diagrams in natural language, including enhanced information about process conditions. A structured prompt system is employed and validated in the LLM domain through prompt engineering. GIPHT searches and extracts data based on its proposed improvement methodology, providing explanations for the decision-making process and the basis, overcoming limitations of the traditional black-box AI models. It offers directional ideas to design engineers in the early stages of process design and would be used for training of process engineers, supporting improvement of outdated processes and transformation into more environmentally friendly processes.},
}

@article{Goyanes2024,
  title = {Automatic gender detection: a methodological procedure and recommendations to computationally infer the gender from names with ChatGPT and gender APIs},
  author = {Manuel Goyanes and Luis de-Marcos and Adrián Domínguez-Díaz},
  year = {2024},
  journal = {Scientometrics},
  volume = {129},
  pages = {6867-6888},
  doi = {10.1007/s11192-024-05149-2},
  url = {https://doi.org/10.1007/s11192-024-05149-2},
  abstract = {Both computational social scientists and scientometric scholars alike, interested in gender-related research questions, need to classify the gender of observations. However, in most public and private databases, this information is typically unavailable, making it difficult to design studies aimed at understanding the role of gender in influencing citizens’ perceptions, attitudes, and behaviors. Against this backdrop, it is essential to design methodological procedures to infer the gender automatically and computationally from data already provided, thus facilitating the exploration and examination of gender-related research questions or hypotheses. Researchers can use automatic gender detection tools like Namsor or Gender-API, which are already on the market. However, recent developments in conversational bots offer a new, still relatively underexplored, alternative. This study offers a step-by-step research guide, with relevant examples and detailed clarifications, to automatically classify the gender from names through ChatGPT and two partially free gender detection tool (Namsor and Gender-API). In addition, the study provides methodological suggestions and recommendations on how to gather, interpret, and report results coming from both platforms. The study methodologically contributes to the scientometric literature by describing an easy-to-execute methodological procedure that enables the computational codification of gender from names. This procedure could be implemented by scholars without advanced computing skills.},
}

@article{Moreno-Delgado2024,
  title = {Mapping scientific mobility in leading Eurozone economies: insights from ORCID data analysis},
  author = {Alicia Moreno-Delgado and Marlon Cárdenas-Bonett and Óscar de Gregorio-Vicente and Julio Montero-Díaz},
  year = {2024},
  journal = {Scientometrics},
  volume = {129},
  pages = {6889-6907},
  doi = {10.1007/s11192-024-05153-6},
  url = {https://doi.org/10.1007/s11192-024-05153-6},
  abstract = {Research into the mobility of researchers has garnered increasing interest among institutions and governments. In this study, we use ORCID as a data source to analyse the mobility of researchers trained in Germany, France, Spain, the Netherlands, and Italy, the main economies of the Eurozone according to Gross Domestic Product (GDP). Our approach focuses on the connection between the place of education and employment, identifying graduates and their countries of employment through profiles on ORCID. We conduct a comparative analysis of preferred destinations, considering various levels of education, and develop a migration rate for researchers from these countries. The results reveal a clear preference for the United States and Great Britain among graduates, influenced by linguistic affinities and historical cultural relations. Regarding the migration rate, we observe that all countries retain more graduates than those who emigrate. France leads in emigration, followed by the Netherlands, Germany, Italy, and Spain. This analysis of researcher mobility in the Eurozone allows us to track migratory flows, identifying both sending and receiving countries. These findings are essential for the formulation of scientific and migration policies and contribute to understanding individual behaviour in building academic and professional careers.},
}

@article{Norbäck2024,
  title = {Why generative AI can make creative destruction more creative but less destructive},
  author = {Pehr-Johan Norbäck and Lars Persson},
  year = {2024},
  journal = {Small Business Economics},
  volume = {63},
  pages = {349-377},
  doi = {10.1007/s11187-023-00829-4},
  url = {https://doi.org/10.1007/s11187-023-00829-4},
  abstract = {The application of machine learning (ML) to operational data is becoming increasingly important with the rapid development of artificial intelligence (AI). We propose a model where incumbents have an initial advantage in ML technology and access to (historical) operational data. We show that the increased application of ML for operational data raises entrepreneurial barriers that make the creative destruction process less destructive (less business stealing) if entrepreneurs have only limited access to the incumbent’s data. However, this situation induces entrepreneurs to take on more risk and to be more creative. Policies making data generally available may therefore be suboptimal. A complementary policy is one that supports entrepreneurs’ access to ML, such as open source initiatives, since doing so would stimulate creative entrepreneurship.},
}

@article{Revalde2025,
  title = {Can ChatGPT Pass a Physics Test?},
  author = {Gita Revalde and Madi Zholdakhmet and Anda Abola and Aliya Murzagaliyeva},
  year = {2025},
  journal = {Technology, Knowledge and Learning},
  doi = {10.1007/s10758-025-09814-0},
  url = {https://doi.org/10.1007/s10758-025-09814-0},
  abstract = {Since its launch in November 2022, chatbot ChatGPT has gained significant popularity worldwide. It performs the task of a search engine, analyzes the information, and generates the required output. ChatGPT is already recognized as a useful tool for educational purposes, but it also comes with some limitations and potential risks. In this case study, we tested and analyzed ChatGPT’s ability to solve typical physics tests and problems in four languages: Kazakh, Latvian, Russian, and English. We collected over 400 questions, including problem-solving tasks, to evaluate ChatGPT’s performance. Compared to students' results, ChatGPT performed slightly better on multiple-choice theory questions but struggled with practical problem-solving, achieving only 17% correct answers. The highest accuracy was observed in English, with an 83% success rate, while Kazakh had the lowest at just 33%, showing the problems of using ChatGPT in non-English languages. The potential for unfair use of ChatGPT and measures to address this issue are also briefly discussed.},
}

@article{Delfani2024,
  title = {Integrative approaches in modern agriculture: IoT, ML and AI for disease forecasting amidst climate change},
  author = {Payam Delfani and Vishnukiran Thuraga and Bikram Banerjee and Aakash Chawade},
  year = {2024},
  journal = {Precision Agriculture},
  volume = {25},
  pages = {2589-2613},
  doi = {10.1007/s11119-024-10164-7},
  url = {https://doi.org/10.1007/s11119-024-10164-7},
  abstract = {Plant disease forecasting models, driven by concurrent data and advanced technologies, are reliable tools for accurate prediction of disease outbreaks in achieving sustainable and productive agricultural systems. Optimal integration of Internet of Things (IoTs), machine learning (ML) techniques and artificial intelligence (AI), further augment the capabilities of these models in empowering farmers with proactive disease control measures towards modern agriculture manifested by efficient resource management, reduced diseases and higher crop yields. This article summarizes the role of disease forecasting models in crop management, emphasizing the advancements and applications of AI and ML in disease prediction, challenges and future directions in the field via (a) The technological foundations and need for validation testing of models, (b) The advancements in disease forecasting with the importance of high-quality publicly available data and (c) The challenges and future directions for the development of transparent and interpretable open-source AI models. Further improvement of these models needs investment in continuous innovative research with collaboration and data sharing among agricultural stakeholders.},
}

@article{Kim2024_03,
  title = {Leading teachers' perspective on teacher-AI collaboration in education},
  author = {Jinhee Kim},
  year = {2024},
  journal = {Education and Information Technologies},
  volume = {29},
  pages = {8693-8724},
  doi = {10.1007/s10639-023-12109-5},
  url = {https://doi.org/10.1007/s10639-023-12109-5},
  abstract = {Moving beyond the direct support all alone by a human teacher or an Artificial Intelligence (AI) system, optimizing the complementary strengths of the two has aroused great expectations and educational innovation potential. Yet, the conceptual guidance of how best to structure and implement teacher-AI collaboration (TAC) while ensuring teachers' instructional roles to support students learning remains limited. This study, therefore, aims what (1) curriculum, (2) teacher-AI interaction, (3) learning environment would be required as well as how TAC would evolve by reflecting teachers' views. Through in-depth interviews with 20 Chinese leading teachers in AI in Education (AIED), the study found that teachers aimed to improve students' subject-matter knowledge and build their capacity as the desired goals for TAC and these can be carried out by data-driven problem-based learning and case-based reasoning in tandem with growth-focused and reflective assessment. While teachers highlighted that developing teachers' data literacy and collegiality with AI are essential, they expected AI to be equipped with Technological Pedagogical and Content Knowledge (TPACK) knowledge and conflict resolution skills. In addition, teachers expressed that Internet of Things (IoT)-based classrooms, systematic AIED curriculum, school-based continuing professional development, research-practice-policy partnerships as well as creating a continuous learning and AI-ready culture are significant. Furthermore, teachers envision TAC would develop into three stages: (1) teachers as passive AI recipients, (2) teachers as active AI users (3) teachers-AI as constructive partners. These findings build a more holistic and in-depth understanding of the AIED and offer implications for the educational AI design and teachers' education.},
}

@article{Schneider2024,
  title = {Explainable Generative AI (GenXAI): a survey, conceptualization, and research agenda},
  author = {Johannes Schneider},
  year = {2024},
  journal = {Artificial Intelligence Review},
  volume = {57},
  pages = {289},
  doi = {10.1007/s10462-024-10916-x},
  url = {https://doi.org/10.1007/s10462-024-10916-x},
  abstract = {Generative AI (GenAI) represents a shift from AI’s ability to “recognize” to its ability to “generate” solutions for a wide range of tasks. As generated solutions and applications grow more complex and multi-faceted, new needs, objectives, and possibilities for explainability (XAI) have emerged. This work elaborates on why XAI has gained importance with the rise of GenAI and the challenges it poses for explainability research. We also highlight new and emerging criteria that explanations should meet, such as verifiability, interactivity, security, and cost considerations. To achieve this, we focus on surveying existing literature. Additionally, we provide a taxonomy of relevant dimensions to better characterize existing XAI mechanisms and methods for GenAI. We explore various approaches to ensure XAI, ranging from training data to prompting. Our paper provides a concise technical background of GenAI for non-technical readers, focusing on text and images to help them understand new or adapted XAI techniques for GenAI. However, due to the extensive body of work on GenAI, we chose not to delve into detailed aspects of XAI related to the evaluation and usage of explanations. Consequently, the manuscript appeals to both technical experts and professionals from other fields, such as social scientists and information systems researchers. Our research roadmap outlines over ten directions for future investigation.},
}

@article{Blanco-Justicia2025,
  title = {Digital forgetting in large language models: a survey of unlearning methods},
  author = {Alberto Blanco-Justicia and Najeeb Jebreel and Benet Manzanares-Salor and David Sánchez and Josep Domingo-Ferrer and Guillem Collell and Kuan Eeik Tan},
  year = {2025},
  journal = {Artificial Intelligence Review},
  volume = {58},
  pages = {90},
  doi = {10.1007/s10462-024-11078-6},
  url = {https://doi.org/10.1007/s10462-024-11078-6},
  abstract = {Large language models (LLMs) have become the state of the art in natural language processing. The massive adoption of generative LLMs and the capabilities they have shown have prompted public concerns regarding their impact on the labor market, privacy, the use of copyrighted work, and how these models align with human ethics and the rule of law. As a response, new regulations are being pushed, which require developers and service providers to evaluate, monitor, and forestall or at least mitigate the risks posed by their models. One mitigation strategy is digital forgetting: given a model with undesirable knowledge or behavior, the goal is to obtain a new model where the detected issues are no longer present. Digital forgetting is usually enforced via machine unlearning techniques, which modify trained machine learning models for them to behave as models trained on a subset of the original training data. In this work, we describe the motivations and desirable properties of digital forgetting when applied to LLMs, and we survey recent works on machine unlearning. Specifically, we propose a taxonomy of unlearning methods based on the reach and depth of the modifications done on the models, we discuss and compare the effectiveness of machine unlearning methods for LLMs proposed so far, and we survey their evaluation. Finally, we describe open problems of machine unlearning applied to LLMs and we put forward recommendations for developers and practitioners.},
}

@article{Lyu2025,
  title = {An exploratory analysis of the DPRK cyber threat landscape using publicly available reports},
  author = {Jeonggak Lyu and Ahyun Song and Euiseong Seo and Gibum Kim},
  year = {2025},
  journal = {International Journal of Information Security},
  volume = {24},
  pages = {66},
  doi = {10.1007/s10207-025-00980-x},
  url = {https://doi.org/10.1007/s10207-025-00980-x},
  abstract = {Cyber activities have evolved to mirror real-world operations, prompting state-sponsored intelligence agencies to pivot swiftly to cyberspace. Notably, Democratic People’s Republic of Korea (DPRK) state-sponsored threat actors have emerged as significant global players, targeted not only the Republic of Korea but also engaged in espionage activities worldwide. Their activities have expanded to include ransomware distribution and cryptocurrency heists, indicating a pursuit of financial gain. To comprehensively understand and track their activities, the research utilized exploratory analysis of publicly available reports. This research involved meticulous analysis of over 2000 publicly available reports spanning a significant period from 2009 to May 2024. Our analysis focused on identifying the code names employed in these reports to denote DPRK state-sponsored threat actors. By analyzing the naming conventions used by cyber threat intelligence companies, the study clustered groups believed to represent the same entity. This approach identified 160 distinct code names for these actors. Additionally, the threat actors were categorized into seven widely recognized groups in the threat intelligence industry. Furthermore, 154 notable incidents attributed to these actors were extracted and documented. Detailed analysis of these incidents, including motivations, targeted sectors, and related factors, provided valuable insights into the evolving tactics of DPRK state-sponsored threat actors. In a concerted effort to contribute to the cybersecurity community, our findings have been openly shared as a dataset and presented through a dedicated website for easy access. This initiative aims to significantly enhance the understanding of researchers interested in their activities. The dataset, now publicly available, serves as a valuable resource for researchers seeking comprehensive material on their activities. Openly sharing the findings aims to foster collaboration and further research in the cybersecurity community to effectively combat emerging threats.},
}

@article{Jamarani2024,
  title = {Big data and predictive analytics: A systematic review of applications},
  author = {Amirhossein Jamarani and Saeid Haddadi and Raheleh Sarvizadeh and Mostafa Haghi Kashani and Mohammad Akbari and Saeed Moradi},
  year = {2024},
  journal = {Artificial Intelligence Review},
  volume = {57},
  pages = {176},
  doi = {10.1007/s10462-024-10811-5},
  url = {https://doi.org/10.1007/s10462-024-10811-5},
  abstract = {Big data involves processing vast amounts of data using advanced techniques. Its potential is harnessed for predictive analytics, a sophisticated branch that anticipates unknown future events by discerning patterns observed in historical data. Various techniques obtained from modeling, data mining, statistics, artificial intelligence, and machine learning are employed to analyze available history to extract discriminative patterns for predictors. This study aims to analyze the main research approaches on Big Data Predictive Analytics (BDPA) based on very up-to-date published articles from 2014 to 2023. In this article, we fully concentrate on predictive analytics using big data mining techniques, where we perform a Systematic Literature Review (SLR) by reviewing 109 articles. Based on the application and content of current studies, we introduce taxonomy including seven major categories of industrial, e-commerce, smart healthcare, smart agriculture, smart city, Information and Communications Technologies (ICT), and weather. The benefits and weaknesses of each approach, potentially important changes, and open issues, in addition to future paths, are discussed. The compiled SLR not only extends on BDPA’s strengths, open issues, and future works but also detects the need for optimizing the insufficient metrics in big data applications, such as timeliness, accuracy, and scalability, which would enable organizations to apply big data to shift from retrospective analytics to prospective predictive if fulfilled.},
}

@article{Saha Biswas2024,
  title = {A survey on artificial intelligence-based approaches for personality analysis from handwritten documents},
  author = {Suparna Saha Biswas and Himadri Mukherjee and Ankita Dhar and Obaidullah Sk Md and Kaushik Roy},
  year = {2024},
  journal = {International Journal on Document Analysis and Recognition (IJDAR)},
  doi = {10.1007/s10032-024-00496-5},
  url = {https://doi.org/10.1007/s10032-024-00496-5},
  abstract = {Human personality is a blend of different traits and virtues. It’s modeling is challenging due to its inherent complexity. There are multitudinous cues to predict personality and handwriting is one of them. This is because it is distinctive to a large extent and varies at the individual level. The allied field of science which deals with the analysis of handwriting for understanding personality is known as Graphology. Researchers have discovered disparate features of handwriting that can reveal the personality traits of an individual. Several attempts have been made to model personality from handwriting in different languages but significant advancement is required for commercialization. In this paper, we present the reported aspects of handwriting, techniques for processing handwritten documents and evaluation measures for personality identification to draw a horizon and aid in further advancement of research in this field.},
}

@article{Annepaka2025,
  title = {Large language models: a survey of their development, capabilities, and applications},
  author = {Yadagiri Annepaka and Partha Pakray},
  year = {2025},
  journal = {Knowledge and Information Systems},
  volume = {67},
  pages = {2967-3022},
  doi = {10.1007/s10115-024-02310-4},
  url = {https://doi.org/10.1007/s10115-024-02310-4},
  abstract = {Large language models can generate text, respond to queries, and translate between languages, as recent research demonstrates. As a new and essential part of computational linguistics, LLMs can understand complex speech patterns and respond appropriately and rationally in the given context. Research contributions have significantly increased as a result of LLMs growing popularity. Nevertheless, the rate of increase has been so quick that evaluating the cumulative impact of these developments is challenging. Keeping abreast with all the LLM research that has surfaced in such a short time and gaining a thorough understanding of the present state of the field’s study has become challenging. The scientific community would gain from a succinct but thorough LLM summary covering their history, architecture, applications, issues, influence, and resources. This paper covers many model types while reviewing LLMs’ fundamental principles and notions. It summarizes earlier research, the evolution of LLMs throughout time, transformer history, and the range of tools and training methods applied to these models. The study also looks at the many fields in which LLMs are used, including business, social work, education, healthcare, and agriculture. Furthermore, it illustrates how LLMs impact society, mold AI’s future, and find valuable uses in problem-solving. This review article aims to provide a comprehensive overview of the history of LLMs, pre-trained architectures, applications, problems, and future directions for practitioners, researchers, and experts.},
}

@article{Wang2024_02,
  title = {Fueling a net-zero future: The influence of government-funded research on climate change mitigation inventions},
  author = {Jieshu Wang and José Lobo and Shade T. Shutters and Deborah Strumsky},
  year = {2024},
  journal = {Environmental Innovation and Societal Transitions},
  volume = {51},
  pages = {100836},
  doi = {https://doi.org/10.1016/j.eist.2024.100836},
  url = {https://www.sciencedirect.com/science/article/pii/S2210422424000273},
  abstract = {This study examines the pace and content of Climate Change Mitigation Technology (CCMT) inventions, focusing on the influence of government-funded research on patent characteristics. Utilizing data from the USPTO, we analyze the trends in CCMT patenting from 1988 to 2017 and reveal a significant increase in CCMT inventions. However, patents in hydrogen technology and Carbon Capture and Storage (CCS) are comparatively low, suggesting these fields are still in the early development stages. CCMT inventions rely heavily on government-funded research, particularly in CCS and hydrogen technology. CCMT inventions relying on government research are more complex and generate larger and more pervasive knowledge spillovers than their counterparts. However, they are less likely to be novel and tend to consolidate rather than destabilize existing technologies. Interestingly, the effect of government research reducing the likelihood of novelty is only observed in CCMT inventions and does not extend to utility patents. These findings highlight the role of government-funded research in facilitating high-quality CCMT inventions through knowledge spillovers. Our study underscores the importance of sustained and targeted public investment in CCMT R&D.},
}

@article{Sioumalas-Christodoulou2025,
  title = {AI metrics and policymaking: assumptions and challenges in the shaping of AI},
  author = {Konstantinos Sioumalas-Christodoulou and Aristotle Tympas},
  year = {2025},
  journal = {AI & SOCIETY},
  doi = {10.1007/s00146-025-02181-5},
  url = {https://doi.org/10.1007/s00146-025-02181-5},
  abstract = {This paper explores the interplay between AI metrics and policymaking by examining the conceptual and methodological frameworks of global AI metrics and their alignment with National Artificial Intelligence Strategies (NAIS). Through topic modeling and qualitative content analysis, key thematic areas in NAIS are identified. The findings suggest a misalignment between the technical and economic focus of global AI metrics and the broader societal and ethical priorities emphasized in NAIS. This highlights the need to recalibrate AI evaluation frameworks to include ethical and other social considerations, aligning AI advancements with the United Nations Sustainable Development Goals (SDGs) for an inclusive, ethical, and sustainable future.},
}

@article{Dodick2024,
  title = {Localizing AIED: moving beyond North–South narratives to serve contextual needs},
  author = {David Dodick},
  year = {2024},
  journal = {AI & SOCIETY},
  doi = {10.1007/s00146-024-02047-2},
  url = {https://doi.org/10.1007/s00146-024-02047-2},
  abstract = {This article problematizes simplistic Global North–South binaries in artificial intelligence in education discourse and implementation. The author draws on dual teaching experiences in Canada and Paraguay to demonstrate the diversity within and across regions, and challenges notions of a homogeneous “Global South.” The analysis emphasizes the importance of incorporating local actors’ perspectives when introducing new technologies rather than centering outside entities. It advocates examining the specific causes, conditions, and complexities within particular countries to develop tailored AIED solutions. Using Paraguay as a case study, the article explores the ethical and contextual considerations necessary for developing AIED solutions aligned with local realities. The conclusion argues that real progress in equitable AIED requires looking past technological solutionism and deficit narratives, empowering communities to guide integration based on their own needs and priorities. Responsible AIED must give voice to the underserved through small-scale, transparent implementations customized to local contexts.},
}

@article{Ward2024,
  title = {What Is Azure SQL?},
  author = {Bob Ward},
  year = {2024},
  pages = {35-77},
  doi = {10.1007/979-8-8688-0974-3_2},
  publisher = {Apress},
  url = {https://doi.org/10.1007/979-8-8688-0974-3_2},
  abstract = {“Bob, what is the cloud?” I vividly remember my beautiful and talented wife Ginger asking me this question when I walked in our kitchen one evening. I paused for a second, getting ready to present my incredible and thoughtful answer, and said “Well, you see the cloud is….” Fifteen minutes later (as Ginger recalls – I thought it was just a few minutes), Ginger politely interrupted me and said “Uh…I was kind of look for the CliffsNotes answer?” I was taken aback. How can anyone define the cloud as something so simple when it is such a complex topic and provides so much. I couldn’t let this go, so I spent the next few days researching a simpler answer that still defined the cloud. But I also wanted to make sure Ginger knew the answer to the question “What is Azure?”},
}

@article{Li2024,
  title = {A Digital Twinning Process for Metaverse Cities},
  author = {Xiangming Samuel Li},
  year = {2024},
  pages = {35-50},
  doi = {10.1007/979-8-8688-0811-1_6},
  publisher = {Apress},
  url = {https://doi.org/10.1007/979-8-8688-0811-1_6},
  abstract = {In this chapter, I will focus on the data-driven digital twinning process (DDTP) for metaverse cities, an extension of the architectural framework from Chapter 5. The process, energized by the Data Analytics Flywheel (DAFW), includes a series of steps: understanding the problem, collecting and preparing data, visualizing it, modeling, evaluating models, and deploying the best solution. This cycle ensures the continuous alignment and updating of both the physical and digital aspects of a city. The chapter will wrap up with Virtual Singapore as a case study, underscoring the importance of systematic execution and research in the development of smart metaverse cities.},
}

@article{Pulapaka2024,
  title = {Program Management, Business Intelligence, and Reporting},
  author = {Sanjeev Pulapaka and Srinath Godavarthi and Dr. Sherry Ding},
  year = {2024},
  pages = {205-226},
  doi = {10.1007/979-8-8688-0473-1_8},
  publisher = {Apress},
  url = {https://doi.org/10.1007/979-8-8688-0473-1_8},
  abstract = {Public sector organizations (PSOs) are subject to a myriad of federal, state, and local regulations, necessitating effective program management and compliance reporting. They must provide transparency in their operations to maintain public trust. Providing a high level of transparency requires accurate reporting of program metrics and key performance indicators in alignment with mission outcomes. Failure to comply with reporting requirements can result in the loss of funding, underscoring the importance of robust reporting capabilities. As an example, the US Congress authorized the Center for Medicare and Medicaid Services (CMS) to withhold funds from states that do not satisfy certain reporting requirements.},
}

@article{Gupta2024,
  title = {The Databricks Data Intelligence Platform},
  author = {Nikhil Gupta and Jason Yip},
  year = {2024},
  pages = {331-352},
  doi = {10.1007/979-8-8688-0444-1_14},
  publisher = {Apress},
  url = {https://doi.org/10.1007/979-8-8688-0444-1_14},
  abstract = {In the previous chapters, we learned about the Databricks lakehouse, which essentially means storing all your data in open storage in an open format with Unity Catalog providing a single governance layer and Databricks providing features to enable all use cases such as data engineering, data science, streaming, and warehousing. With the advent and popularity of GenAI and LLMs since 2023, Databricks has integrated them into its platform. The Databricks data intelligence platform (see Figure 14-1) combines the lakehouse platform and AI/LLMs to add the “data intelligence” engine that understands the uniqueness of your data and uses that understanding across everything in the platform.},
}

@article{Nulkar2024,
  title = {Greening Urban Spaces},
  author = {Gurudas Nulkar},
  year = {2024},
  pages = {447-504},
  doi = {10.1007/978-981-99-7379-8_11},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-99-7379-8_11},
  abstract = {The rapid urbanization of the twenty-first century has presented both challenges and opportunities for creating more livable and environmentally-friendly cities. United Nations Sustainable Development Goal number 11 is about Sustainable Cities and Communities. In this SDG the UN website observes that presently a majority of the global population resides in urban areas. They estimate that by the year 2050, almost 70% of the global population would reside in metropolitan regions. Urban areas play a key role in economic development and account for over 80% of the world's gross domestic product (GDP). On the other hand, UN observes that they contribute to about 70% of the total worldwide greenhouse gas emissions (Nations, Making Cities and Human settlements more inclusive, safe, resilient and sustainable, 2023). When urban development is carefully strategized and effectively administered, it has the potential to be sustainable and foster equitable economic growth.},
}

@article{Cohan2024,
  title = {Generative AI Software},
  author = {Peter Cohan},
  year = {2024},
  pages = {111-166},
  doi = {10.1007/979-8-8688-0318-5_5},
  publisher = {Apress},
  url = {https://doi.org/10.1007/979-8-8688-0318-5_5},
  abstract = {In the first four chapters, we explored generative AI from the perspective of people and organizations who aim to benefit from the technology. In the next three chapters, we look at the supply side, examining how technology companies compete for their share of the generative AI ecosystem and which suppliers could potentially be profitable investments. This chapter answers questions such as:},
}

@article{Stathis2024,
  title = {The Value of Proactive Data for Intelligent Contracts},
  author = {Georgios Stathis and Giulia Biagioni and Klaas Andries de Graaf and Athanasios Trantas and Jaap van den Herik},
  year = {2024},
  pages = {107-125},
  publisher = {Springer Nature Singapore},
  abstract = {Intelligent Contracts (iContracts) is a new branch of research at the intersection of AI and law. It has many challenges, among which including the quality of data used. In our research we focus on generating and including quality Proactive Control Data (PCD) to improve iContracts, which is a novel research scope in literature. Our scope is defined by the main challenge in regards to emerging legal technologies. Currently, the legal system is more reactive than proactive, leading to high consequential legal costs. By shifting the focus to proactiveness, we discuss and improve upon the available methodologies (Bow-Tie Method and Logocratic Method) and technologies (Ontology Engineering, Software Engineering and Large Language Models [LLMs]) to demonstrate a higher degree of proactiveness in iContracts. Our results are threefold. First, we prove that the generation of PCD is possible with the development of a prototype that leverages the foundations of the Bow-Tie Method. Second, we demonstrate that the impact of PCD on contract drafting is significant, as the explicit inclusion of PCD in prompt engineering alters significantly the content of an LLM-drafted contract. Third, we show how the quality of PCD can be assessed and improved upon with the application of the Logocratic Method. The discussion highlights the feasibility of the research with available technologies. Ultimately, the implementation of our research depends on organisational considerations and resource allocation. We conclude that the generation of PCD is feasible, their impact on contract drafting is significant and their quality assessment is both possible and novel.},
}

@article{Li2025,
  title = {Part-of-Speech and Confusion-Set Constrained Language Model for Vietnamese Spelling Correction Corpus Construction},
  author = {Ying Li and Xin Chen and Xiao Liu and Ling Dong and Zhengtao Yu and Cunli Mao},
  year = {2025},
  pages = {184-197},
  publisher = {Springer Nature Singapore},
  abstract = {Supervised spelling error correction models have achieved outstanding performances on rich-source languages. However, these models are difficult to directly apply to Vietnamese spelling correction due to the corpus scarcity. To address this issue, we first construct a basic high-quality Vietnamese Spelling Correction (ViSC) corpus via automatic speech recognition (ASR) generation and human annotation. Then, we propose a part-of-speech and confusion-set double-constrained method to mimic the practical error distribution and use them as external knowledge to guide the large language models (LLMs) to construct diverse pseudo data. Finally, we exploit pseudo corpora to pre-train and ViSC corpus to fine-tune spelling error correction models. Experiments on the benchmark dataset show that our proposed corpus construction method consistently outperforms various baselines, leading to state-of-the-art results on all Vietnamese-specific pre-trained language model-enhanced spelling correction models. Detailed analysis demonstrates that part-of-speech and confusion-set are complementary and significant in controlling a stable and diverse corpus generation. In-depth comparison experiments reveal that the proper utilization of pseudo corpus is essential for improving Vietnamese spelling error correction. Besides, we release our codes and constructed corpus at https://github.com/DarkFanta3y/VSEC_corpusto facilitate future research.},
}

@article{Shaban2024,
  title = {Digitalization, Cultural Production, Exchange, and Consumption},
  author = {Abdul Shaban},
  year = {2024},
  pages = {319-598},
  doi = {10.1007/978-981-97-9278-8_5},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-97-9278-8_5},
  abstract = {These were the sociologists from the Frankfurt School, Theodor Adorno (1903–1969), and Max Horkheimer (1895–1973) (Adorno & Horkheimer, 1944), who coined the term “cultural industries.” They were worried about commercialization, mechanical production, and trade in cultural products. However, by 1970s, the culture became an accepted commodity to be traded and produced. The rise of the electronic and digital industry further provided a flip to it, and music, plays, cinemas, arts, designs, etc., started being produced and traded on a mass scale. Added to it were instrumental music, video games, and virtual reality shows. Since the 1990s, with the commercialization of the Internet, e-commerce has led to sizeable trade in cultural and creative goods and trade.},
}

@article{Wu2025,
  title = {Charting the Uncharted: Building and Analyzing a Multifaceted Chart Question Answering Dataset for Complex Logical Reasoning Process},
  author = {Anran Wu and Shuwen Yang and Yujia Xia and Xingjiao Wu and Tianlong Ma and Liang He},
  year = {2025},
  pages = {18-33},
  publisher = {Springer Nature Singapore},
  abstract = {Charts, as a vital part of visualization language, are omnipresent in real-world. Understanding charts is crucial for unveiling implicit data insights. The evolution of large-scale models has marked significant milestones in chart comprehension. However, comprehending multiple charts jointly remains challenging due to the complexities of multi-chart reasoning and the intricate dataset construction involving multiple charts. In this study, we introduce DGE, a sophisticated logic-based multi-chart question-answering dataset generation engine that, with only simple data input, generates diverse joint charts and questions with complex logic. It employs logical templates to guide question generation, ensuring excellent scalability. Leveraging the DGE engine, we propose MCQA, the inaugural large-scale dataset for joint reasoning question-answering involving multiple charts, which includes 22,860 chart pairs and 100,331 complex questions, each annotated with an inference process. Finally, we evaluate several baselines on the MCQA dataset, establishing a research foundation for the chart question answering community. The MCQA dataset is available at github (https://github.com/ICALK-CVU/MCQA).},
}

@article{Schwarz2025,
  title = {Harnessing OSINT in Disaster Management: Transforming Microblogging Posts Into Insightful Data Through Text Embeddings},
  author = {Klaus Schwarz and Reiner Creutzburg and Michael Hartmann and Daniel Arias-Aranda},
  year = {2025},
  pages = {195-213},
  publisher = {Springer Nature Singapore},
  abstract = {This paper introduces a novel method for leveraging Open-Source Intelligence (OSINT) in disaster management by analyzing social media data, notably from microblogging sites. Utilizing text embedding, event detection, and knowledge graphs, our technique converts Twitter/X messages into actionable insights, filtering out irrelevant content. Addressing the challenge of large-scale social media data’s volume and hate speech, we employ an approach for sub-event detection by utilizing techniques like text embedding combined with clustering to categorize messages, simplifying data analysis, and enhancing pattern detection. Our approach surpasses traditional methods by automating data filtering and cleansing, reducing manual effort, ensuring data quality, and facilitating efficient resource utilization. It aids in identifying event-related clusters, tracking their development, and generating knowledge graphs for swift, accurate disaster management. This paper details our clustering, cleansing, and processing strategies, setting the stage for future discussions on their application in comprehensive situation report generation.},
}

@article{Khan2025,
  title = {Impact of Artificial Intelligence on the Global Economy and Technology Advancements},
  author = {Muhammad Abbas Khan and Habib Khan and Muhammad Faizan Omer and Inam Ullah and Muhammad Yasir},
  year = {2025},
  pages = {147-180},
  doi = {10.1007/978-981-97-3222-7_7},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-97-3222-7_7},
  abstract = {The profound impact of Artificial Intelligence (AI) on the global economy spans a diverse spectrum of consequences, with Cognitive Computing emerging as a catalyst for significant economic advancement. This transformative force notably augments productivity nurtures innovation, and contributes to worldwide economic expansion. Its influence is conspicuous across healthcare, finance, and manufacturing sectors, where intelligent systems optimize processes and ignite imaginative thinking. Moreover, AI plays a pivotal role in globalization, fortifying international competitiveness and enabling seamless worldwide economic integration. This chapter embarks on a journey to elaborate on the extensive economic impact of AI advancements, deftly navigating the intricate implications of cutting-edge technologies. We unravel AI’s role as a catalyst for heightened productivity and enhanced decision-making, skillfully addressing ethical and regulatory challenges. Shifting focus to economic growth and productivity, our exploration delves deeply into AI’s transformative influence in improving operational efficiency and fostering innovation on a global scale. The narrative then seamlessly transitions to labor market dynamics, unveiling evolving employment patterns and emphasizing the collaborative potential between humans and AI while thoughtfully integrating ethical considerations. Adopting a sector-specific lens, our narrative investigates AI’s pervasive influence across diverse industries, showcasing its impact from manufacturing automation to healthcare innovations and sustainable practices. This chapter is a holistic guide, offering nuanced insights into the multifaceted dimensions of AI’s economic impact. It ensures a comprehensive understanding of the evolving economic landscape shaped by artificial intelligence in this scientific survey.},
}

@article{Dhoni2024,
  title = {An Economical, Time Bound, Scalable Data Platform Designed for Advanced Analytics and AI},
  author = {Pan Singh Dhoni},
  year = {2024},
  pages = {543-558},
  publisher = {Springer Nature Singapore},
  abstract = {Data-driven innovation has served as a pivotal element in technological advancement for more than a decade. However, the recent pandemic and the subsequent post-pandemic period have underscored the diverse needs facing industries today, from addressing supply chain challenges to adapting to hybrid work cultures. Additionally, the recent surge in generative AI has steered companies toward the development of enterprise data platforms. This shift underscores the urgent need to harness data more effectively, empowering businesses to navigate the evolving landscape and tackle these critical challenges. However, recent research in this direction—aiming to develop economical, timely, and scalable data platforms—has been notably scarce. This research presents the development of a scalable, cost-effective, and time-efficient data platform designed to support various data domains such as Data Analytics, Marketing, Data Science, and Artificial Intelligence. Utilizing a case study with a simulation within a real-world organizational context, in the practical organizational setting of over the past four years, we integrated data into a central Lakehouse using Spark Delta to assess the platform’s capabilities. We have collected data using metadata-driven ingestion patterns using near real-time, batch, and Application programming interface (API). The platform leverages advanced technologies like Spark, Databricks, Kafka, Power BI, and cloud infrastructure to meet the high-performance demands of data management and analysis. The implementation results revealed the platform’s proficiency in processing data, performing intelligent analytics, and generating actionable insights efficiently. Its cost-effectiveness and impressive performance outcomes highlight the platform’s potential as an invaluable resource for organizations optimizing data assets. This platform also builds in short duration. This paper details the design, implementation, and benefits of our proposed data platform, illustrating its significance as a robust solution for enterprises leveraging data-driven decision-making. This approach will help organizations to get project success and highlighted positive outcomes.},
}

@article{Zhou2025,
  title = {The Application of the MDATA Cognitive Model in Open Source Intelligence Analysis},
  author = {Bin Zhou and Binxing Fang and Ye Wang},
  year = {2025},
  pages = {148-180},
  doi = {10.1007/978-981-96-3528-3_6},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-96-3528-3_6},
  abstract = {Open source intelligence (OSINT) refers to information that is collected, identified, refined, and utilized from any publicly available sources to meet intelligence needs. With the advent of the internet, particularly the widespread use of social media, OSINT information in many fields is distributed across various online platforms, fundamentally altering its value, status, and impact. Generally, OSINT has characteristics such as low value density (a small proportion of valuable intelligence relative to total data), frequent spatiotemporal evolution (the content and connections of the information can change over time), and complex multidimensional relationships (intelligence and its related information can be embedded in any dimension and appear in any form).},
}

@article{Li2025_01,
  title = {Blockchain-Enabled Large Language Models for Prognostics and Health Management Framework in Industrial Internet of Things},
  author = {Dun Li and Hongzhi Li and Jing Li and Hung-Wei Li and Huan Wang and Roberto Minerva and Noel Crespi and Kuan-Ching Li},
  year = {2025},
  pages = {3-16},
  publisher = {Springer Nature Singapore},
  abstract = {The Industrial Internet of Things (IIoT) emphasizes the importance of equipment health and reliability, which is critical to maintaining operational efficiency and preventing costly downtime. This article introduces an innovative prognostics and health management (PHM) framework that synergistically combines blockchain technology with large language models (LLM) to pioneer safe, reliable, cutting-edge health monitoring and failure prediction services for IIoT devices new era. By leveraging the immutable and transparent properties of blockchain, the proposed framework ensures data integrity and security throughout the IIoT ecosystem. In addition, the solution employs advanced LLM for in-depth data analysis and prediction of potential failures, thereby facilitating pre-emptive maintenance actions. This dual approach enhances the safety and reliability of health monitoring data while simultaneously utilising the predictive power of LLM to analyse complex patterns and predict faults with high accuracy. Experimental results show that the framework is effective in improving the accuracy of fault prediction and the overall resilience of IIoT systems against cyber-physical threats.},
}

@article{Mussa2025,
  title = {Towards Enhancing Linked Data Retrieval in Conversational UIs Using Large Language Models},
  author = {Omar Mussa and Omer Rana and Benoît Goossens and Pablo Orozco-terWengel and Charith Perera},
  year = {2025},
  pages = {246-261},
  publisher = {Springer Nature Singapore},
  abstract = {Despite the recent broad adoption of Large Language Models (LLMs) across various domains, their potential for enriching information systems in extracting and exploring Linked Data (LD) and Resource Description Framework (RDF) triplestores has not been extensively explored. This paper examines the integration of LLMs within existing systems, emphasising the enhancement of conversational user interfaces (UIs) and their capabilities for data extraction by producing more accurate SPARQL queries without the requirement for model retraining. Typically, conversational UI models necessitate retraining with the introduction of new datasets or updates, limiting their functionality as general-purpose extraction tools. Our approach addresses this limitation by incorporating LLMs into the conversational UI workflow, significantly enhancing their ability to comprehend and process user queries effectively. By leveraging the advanced natural language understanding capabilities of LLMs, our method improves RDF entity extraction within web systems employing conventional chatbots. This integration facilitates a more nuanced and context-aware interaction model, critical for handling the complex query patterns often encountered in RDF datasets and Linked Open Data (LOD) endpoints. The evaluation of this methodology shows a marked enhancement in system expressivity and the accuracy of responses to user queries, indicating a promising direction for future research in this area. This investigation not only underscores the versatility of LLMs in enhancing existing information systems but also sets the stage for further explorations into their potential applications within more specialised domains of web information systems.},
}

@article{Dar2024_01,
  title = {Effect of Training Epoch Number on Patient Data Memorization in Unconditional Latent Diffusion Models},
  author = {Salman U Hassan Dar and Isabelle Ayx and Marie Kapusta and Theano Papavassiliu and Stefan O Schoenberg and Sandy Engelhardt},
  year = {2024},
  pages = {88-93},
  publisher = {Springer Fachmedien Wiesbaden},
  abstract = {Deep diffusion models hold great promise for open data sharing while preserving patient privacy by utilizing synthetic high quality data as surrogates for real patient data. Despite the promise, such models are also prone to patient data memorization, where generative models synthesize patient data copies instead of novel samples. This can compromise patient privacy and further lead to patient re-identification. Given the risks, it is of considerable importance to investigate the reasons underlying memorization in such models. One aspect that is typically ignored is number of epochs while training, and over-training a model can lead to memorization. Here, we evaluate the effect of over-training on memorization. We train diffusion models on a publicly available chest X-ray dataset for varying number of epochs and detect patient data copies among synthesized samples using self-supervised models. Our results suggest that over-training can result in enhanced data memorization and it is an important aspect that should be considered while training generative models.},
}

@article{Troussas2025,
  title = {The Role of Augmented Intelligence and Pedagogical Theories in Digital Learning},
  author = {Christos Troussas and Akrivi Krouska and Cleo Sgouropoulou},
  year = {2025},
  pages = {39-93},
  doi = {10.1007/978-3-031-84453-9_2},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-84453-9_2},
  abstract = {This chapter explores the pivotal role of augmented intelligence and pedagogical theories in reshaping digital learning. Augmented intelligence, a symbiotic collaboration between human and machine intelligence, is positioned as a transformative force in education, enhancing cognitive processes and fostering creativity without replacing human capabilities. This paradigm allows educators and learners to leverage AI tools that amplify human potential, facilitating personalized, adaptive learning experiences that foster innovation and collaboration. Pedagogical theories provide the foundational framework for integrating these AI systems into educational environments, ensuring that technological advancements align with proven instructional strategies. The chapter discusses the fusion of augmented intelligence with pedagogical principles such as constructivism and connectivism, which guide the design of learning experiences that are responsive to individual needs. By prioritizing human–machine collaboration, augmented intelligence in education not only enhances engagement but also ensures inclusivity, addressing diverse learner requirements. Concluding, the chapter outlines how the intersection of augmented intelligence and pedagogical theories promises to revolutionize digital learning, providing a comprehensive understanding of their transformative potential in the educational landscape.},
}

@article{Zentgraf2025,
  title = {Enhancing Information Extraction from Building Standards with ChatGPT-4: A Multimodal Approach},
  author = {Sven Zentgraf and Markus König},
  year = {2025},
  pages = {234-247},
  publisher = {Springer Nature Switzerland},
  abstract = {The AECO (Architectural, Engineering, Construction, and Operations) industry is increasingly benefiting from the advantages of advanced information management, particularly through the use of Building Information Modeling (BIM) methods. BIM integrates geometric and alphanumeric data, such as dimensions and spatial relationships, into digital building models. These elements are subject to specific constraints defined by industry standards and guidelines. The issue with these constraints embedded in regulatory documents is that they are mostly only available in non-machine readable formats which hinders the exchange and usage of data within the building’s lifecycle. Currently, the required rules and requirements are extracted manually by experts from the regulatory documents and integrated manually and labor-intensively into corresponding BIM processes. Thus, many research projects are currently addressing the issue of how the textual components of regulatory documents can be automatically analyzed to extract rules and requirements using Natural Language Processing or other approaches. In addition to this textual information, many standards also contain figures and illustrations that explain the information described in the text in more detail or contain additional knowledge. Converting this graphical content into machine-readable information is just as challenging as analyzing the textual components. This paper aims to develop an approach for extracting information related to building information requirements from figures contained in standards using the two state-of-the-art MLLMs Kosmos 2 and Chat GPT-4. An MLLM is an AI system that can process text, images, audio, and video, unlike traditional models that are limited to text. MLLMs trained on large data sets recognize complex patterns in different types of data. With the help of the two MLLMs, the images, and their corresponding texts and captions from the standards will first be analyzed, and then the information obtained will be transferred into a structured data format to make it efficiently usable within BIM processes.},
}

@article{Melo2025,
  title = {Scaling and Adapting Large Language Models for Portuguese Open Information Extraction: A Comparative Study of Fine-Tuning and LoRA},
  author = {Alan Melo and Bruno Cabral and Daniela Barreiro Claro},
  year = {2025},
  pages = {427-441},
  publisher = {Springer Nature Switzerland},
  abstract = {This paper comprehensively investigates the efficacy of different adaptation techniques for Large Language Models (LLMs) in the context of Open Information Extraction (OpenIE) for Portuguese. We compare Full Fine-Tuning (FFT) and Low-Rank Adaptation (LoRA) across a model with 0.5B parameters. Our study evaluates the impact of model size and adaptation method on OpenIE performance, considering precision, recall, and F1 scores, as well as computational efficiency during training and inference phases. We contribute to a high-performing LLM and novel insights into the trade-offs between model scale, adaptation technique, and cross-lingual transferability in the OpenIE task. Our findings reveal significant performance variations across different configurations, with LoRA demonstrating competitive results. We also analyze the linguistic nuances in the Portuguese OpenIE that pose challenges for models primarily trained on English data. This research advances our understanding of LLM adaptation for specialized NLP tasks and provides practical guidelines for deploying these models in resource-constrained and multilingual scenarios. Our work has implications for the broader cross-lingual open information extraction field and contributes to the ongoing discourse on efficient fine-tuning strategies for large pre-trained models.},
}

@article{Paschalides2025,
  title = {PARALLAX: Leveraging Polarization Knowledge for Misinformation Detection},
  author = {Demetris Paschalides and George Pallis and Marios D Dikaiakos},
  year = {2025},
  pages = {86-105},
  publisher = {Springer Nature Switzerland},
  abstract = {Recent techniques for the automated detection of online misinformation typically rely on ML models trained with features extracted from content analysis and/or general-purpose Knowledge Graphs (KGs). These techniques often fail to consider the interplay between misinformation and polarization. To bridge this gap, we introduce PARALLAX, a methodology that enhances misinformation detection by infusing polarization knowledge into existing classifiers. Polarization knowledge is represented in terms of Polarization Knowledge Graphs (PKG). PARALLAX constructs PKGs in an unsupervised way, and uses them to enrich articles with polarization knowledge. A Flexible Knowledge-aware Graph Neural Network (FlexKGNN) is trained on these enriched representations. We tested our methodology on three misinformation datasets, demonstrating that it achieves approximately a 15% improvement in performance over baseline classifiers and consistently outperforms other KGs, which typically reach baseline levels only.},
}

@article{Grundmann2025,
  title = {Regulatory Competition in the Digital Economy: The Theory, the Framework and a Few Applications},
  author = {Stefan Grundmann and Arthur Winter},
  year = {2025},
  pages = {37-53},
  doi = {10.1007/978-3-031-81089-3_2},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-81089-3_2},
  abstract = {This paper analyses regulatory competition in the digital economy through three interconnected lenses: theoretical foundations, EU framework conditions and practical applications in recent EU digital regulations. It demonstrates how the interplay between centralised and decentralised rule-making, including private ordering, shapes regulatory outcomes in digital markets. The paper finds that whilst the EU has opted for centralised regulation through the GDPR, DSA and AI Act internally, it maintains pockets of decentralised rule-making through private ordering mechanisms. Beyond the EU’s borders, the paper shows how these regulations affect global regulatory competition through two key mechanisms: the ‘effects doctrine’ and the ‘Brussels effect’. The analysis reveals that the EU has emerged as a central global rule-maker in digital regulation, capable of inducing a ‘race to the top’ through its European rights-driven regulatory model, whilst still preserving space for regulatory experimentation. This dual approach of centralised standard-setting with retained flexibility for innovation exemplifies how regulatory competition can be productively channelled in rapidly evolving digital markets.},
}

@article{Tammisto2025,
  title = {Generating and Evolving Real-Life Like Synthetic Data for e-Government Services Without Using Real-World Raw Data},
  author = {Maj-Annika Tammisto and Dietmar Pfahl and Faiz Ali Shah},
  year = {2025},
  pages = {173-178},
  publisher = {Springer Nature Switzerland},
  abstract = {Testing of applications that use data from e-Government services as input requires test data that is real-life like but where the privacy of personal information is guaranteed. Many approaches exist for creating high-quality synthetic test data, but most of them need real-life raw data as input. Our research aims to develop and evaluate an approach for generating and evolving real-life like synthetic test data without using real-world raw data. The expected benefit of our research is to enable the creation and evolvement of meaningful and real-life like synthetic test data in situations where real-life raw data is not accessible due to privacy reasons.},
}

@article{Wenger2025,
  title = {Large Language Models for Democratizing Business Process Modeling: BPMN Model Generation and Style Guide Adherence},
  author = {Seline Wenger and Maja Spahic-Bogdanovic and Andreas Martin},
  year = {2025},
  pages = {372-389},
  publisher = {Springer Nature Switzerland},
  abstract = {Business Process Management (BPM) is important for organizations, with process modeling being a key component. However, creating accurate and consistent models is challenging and time-consuming. As large language models (LLMs) offer potential solutions, this article explores using LLMs to generate business process models that follow company internal style guides. Interviews with process engineers at a company provided insights into current practices and challenges. A prototype using OpenAI’s GPT-4 API was developed based on these insights. This tool enables non-experts to generate business process models in BPMN 2.0 notation through a user-friendly interface. Evaluation using the PET dataset demonstrated that the LLM-generated models are comparable to those created by human experts regarding recall and precision.},
}

@article{Balendra2024,
  title = {Regulating Social Media Platforms},
  author = {Soorya Balendra},
  year = {2024},
  pages = {47-72},
  doi = {10.1007/978-3-031-75813-3_4},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-75813-3_4},
  abstract = {The chapter explores the regulatory need of the social media platforms, focusing on the various approaches employed by different regulatory models to manage online content and their effects on human rights, particularly free speech. It highlights the necessity for a regulatory model in social media, identifying three main models discussed within the context of social media governance by scholars and policymakers: internal, external, and co-regulation. Through meticulous examination and case studies, the chapter outlines the challenges and chances associated with each model. It aims to identify the most effective model for navigating these challenges while ensuring the protection or promotion of free speech online. Additionally, the chapter extends the discussion to intermediary liability models, which are crucial for developing co-regulation frameworks that accommodate both authorities and private platforms.},
}

@article{Chen2025,
  title = {DUNKS: Chunking and Summarizing Large and Heterogeneous Data for Dataset Search},
  author = {Qiaosheng Chen and Xiao Zhou and Zhiyang Zhang and Gong Cheng},
  year = {2025},
  pages = {78-97},
  publisher = {Springer Nature Switzerland},
  abstract = {With the vast influx of open data on the Web, dataset search has become a trending research problem which is crucial to data discovery and reuse. Existing methods for dataset search either employ only the unstructured metadata of datasets but ignore their actual data, or cater to structured data in a single format such as RDF despite the diverse formats of open data. In this paper, to address the magnitude of large datasets, we decompose RDF data into data chunks, and then, to accommodate big chunks to the limited input capacity of dense ranking models based on pre-trained language models, we propose a multi-chunk summarization method that extracts representative data from representative chunks. Moreover, to handle heterogeneous data formats beyond RDF, we transform other formats into chunks to be processed in a uniform way. Experiments on two test collections for dataset search demonstrate the effectiveness of our dense ranking over summarized data chunks.},
}

@article{Lin2025,
  title = {Relationships Are Complicated! An Analysis of Relationships Between Datasets on the Web},
  author = {Kate Lin and Tarfah Alrashed and Natasha Noy},
  year = {2025},
  pages = {47-66},
  publisher = {Springer Nature Switzerland},
  abstract = {The Web today has millions of datasets, and the number of datasets continues to grow at a rapid pace. These datasets are not standalone entities; rather, they are intricately connected through complex relationships. Semantic relationships between datasets provide critical insights for research and decision-making processes. In this paper, we study dataset relationships from the perspective of users who discover, use, and share datasets on the Web: what relationships are important for different tasks? What contextual information might users want to know? We first present a comprehensive taxonomy of relationships between datasets on the Web and map these relationships to user tasks performed during dataset discovery. We develop a series of methods to identify these relationships and compare their performance on a large corpus of datasets generated from Web pages with schema.org markup. We demonstrate that machine-learning based methods that use dataset metadata achieve multi-class classification accuracy of 90%. Finally, we highlight gaps in available semantic markup for datasets and discuss how incorporating comprehensive semantics can facilitate the identification of dataset relationships. By providing a comprehensive overview of dataset relationships at scale, this paper sets a benchmark for future research.},
}

@article{Chen2025_01,
  title = {ShareGPT4V: Improving Large Multi-modal Models with Better Captions},
  author = {Lin Chen and Jinsong Li and Xiaoyi Dong and Pan Zhang and Conghui He and Jiaqi Wang and Feng Zhao and Dahua Lin},
  year = {2025},
  pages = {370-387},
  publisher = {Springer Nature Switzerland},
  abstract = {Modality alignment serves as the cornerstone for large multi-modal models (LMMs). However, the impact of different attributes (e.g., data type, quality, and scale) of training data on facilitating effective alignment is still under-explored. In this paper, we delve into the influence of training data on LMMs, uncovering three pivotal findings: 1) Highly detailed captions enable more nuanced vision-language alignment, significantly boosting the performance of LMMs in diverse benchmarks, surpassing outcomes from brief captions or VQA data; 2) Cutting-edge LMMs can be close to the captioning capability of costly human annotators, and open-source LMMs could reach similar quality after lightweight fine-tuning; 3) The performance of LMMs scales with the number of detailed captions, exhibiting remarkable improvements across a range from thousands to millions of captions. Drawing from these findings, we introduce the ShareGPT4V series for advanced modality alignment. It includes ShareGPT4V, consisting of 100K high-quality captions curated from GPT4-Vision; ShareGPT4V-PT, containing 1.2M captions produced by our Share-Captioner that can be close to the captioning capabilities of GPT4-Vision; and ShareGPT4V-7B, a simple yet superior LMM excelling in most multi-modal benchmarks, which realized better alignment based on our large-scale high-quality captions. The project is available at https://sharegpt4v.github.io/.},
}

@article{Cherednichenko2025,
  title = {Development of Collaborative Business Intelligence Framework for Tourism Domain Analysis},
  author = {Olga Cherednichenko and Oleksandr Sutiahin},
  year = {2025},
  pages = {253-262},
  publisher = {Springer Nature Switzerland},
  abstract = {Business Intelligence (BI) is a widely recognized term encompassing analytical applications and IT systems supporting them. Collaborative Business Intelligence (CBI) represents a refinement of traditional BI, emphasizing the cooperative execution of BI tasks. The tourism industry is experiencing rapid growth, leading to heightened competition among businesses. Collaborative Business Intelligence transcends organizational boundaries, creating a virtual space where individuals contribute insights to improve decision-making. In this study, our focus is on developing an application tailored for tourists, empowering them to make well-informed decisions about destinations and activities. Our approach involves aggregating, cleansing, and structuring tourism data to offer a user-friendly interface equipped with collaborative analysis tools. This necessitates gathering data from disparate sources and consolidating it into a unified format. CubeJS serves as a tool in creating an additional layer between the database and the main application, facilitating the generation of an OLAP cube and an API for generating necessary cube slices. MongoDB, chosen as the NOSQL database, is selected for its user-friendly interface, compatibility with CubeJS, and performance capabilities. To populate the database, we utilize data from Google Maps obtained through the corresponding API. Through our experimentation with tourism data, we introduce the CBI framework tailored for the tourism domain. This framework comprises three primary components: 1) Data Exploration; 2) Shared Analysis and Insights; 3) Real-time Decision-making. By integrating these three key components, our CBI framework aims to enhance decision-making processes, foster collaboration among stakeholders, and ultimately improve the overall effectiveness and efficiency of decision making.},
}

@article{Feng2024_01,
  title = {Diversified and Structure-Realistic Fundus Image Synthesis for Diabetic Retinopathy Lesion Segmentation},
  author = {Xiaoyi Feng and Minqing Zhang and Mengxian He and Mengdi Gao and Hao Wei and Wu Yuan},
  year = {2024},
  pages = {77-86},
  publisher = {Springer Nature Switzerland},
  abstract = {Automated diabetic retinopathy (DR) lesion segmentation aids in improving the efficiency of DR detection. However, obtaining lesion annotations for model training heavily relies on domain expertise and is a labor-intensive process. In addition to classical methods for alleviating label scarcity issues, such as self-supervised and semi-supervised learning, with the rapid development of generative models, several studies have indicated that utilizing synthetic image-mask pairs as data augmentation is promising. Due to the insufficient labeled data available to train powerful generative models, however, the synthetic fundus data suffers from two drawbacks: 1) unrealistic anatomical structures, 2) limited lesion diversity. In this paper, we propose a novel framework to synthesize fundus with DR lesion masks under limited labels. To increase lesion variation, we designed a learnable module to generate anatomically plausible masks as the condition, rather than directly using lesion masks from the limited dataset. To reduce the difficulty of learning intricate structures, we avoid directly generating images solely from lesion mask conditions. Instead, we developed an inpainting strategy that enables the model to generate lesions only within the mask area based on easily accessible healthy fundus images. Subjective evaluations indicate that our approach can generate more realistic fundus images with lesions compared to other generative methods. The downstream lesion segmentation experiments demonstrate that our synthetic data resulted in the most improvement across multiple network architectures, surpassing state-of-the-art methods.},
}

@article{Rad2025,
  title = {Unleashing the Potential of AI in Sustainable Urban Planning and Design},
  author = {Arefeh Mortzavi Rad and Elsa Haagensen Karlsen and Mohammed Nazar},
  year = {2025},
  pages = {1625-1634},
  publisher = {Springer Nature Switzerland},
  abstract = {Artificial Intelligence (AI) is transforming urban planning and design, heralding a new era of sustainability and livability in rapidly expanding cities. By analyzing vast datasets, forecasting future trends, and autonomously executing tasks, AI holds the promise of significantly enhancing urban efficiency and environmental stewardship. Despite its potential, the adoption of AI faces hurdles, including concerns over complexity, privacy, and cost. This article explores the comprehensive impact of AI on urban development, emphasizing its capacity to optimize energy consumption, improve waste management, and streamline public transportation. Central to our discussion is AI’s role in the critical process of evaluating and selecting contractors and suppliers, which is pivotal for integrating sustainable solutions into urban infrastructure. By addressing these challenges, we advocate for a strategic embrace of AI technologies, aiming to unlock their full potential in crafting greener, more resilient cities worldwide. This synthesis aims to clarify the significant contributions of AI in urban development, providing a clear vision for its future role without the distraction of in-text citations.},
}

@article{Boyer2024,
  title = {Toward a Unified Cybersecurity Knowledge Graph: Leveraging Ontologies and Open Data Sources},
  author = {Adam Boyer and Erdogan Dogdu and Roya Choupani and Jason S Watson and Diego Sanchez and Alexander Ametu},
  year = {2024},
  pages = {17-33},
  publisher = {Springer Nature Switzerland},
  abstract = {The cybersecurity field is very heterogeneous with the ever-growing digital cyberspace and the increasing volume of big data produced in the field. It is therefore difficult to overcome many of the security challenges with manual solutions. Automation and intelligent cybersecurity solutions are both promising tools to overcome common security challenges, however, they require robust data and knowledge management. Today, Knowledge graphs (KG) are widely used in many intelligent system solutions, including the cybersecurity field. However, the efforts to build KGs in many cybersecurity areas are narrowly focused, and therefore it is difficult to unify and integrate these otherwise very useful solutions. Earlier efforts in unifying the cybersecurity knowledge using common ontologies have not generalized their approach to provide a unified solution for cybersecurity challenges. Here, we attempt provide a renewed approach to building a Unified Cybersecurity Knowledge Graph (UCKG), using the Unified Cybersecurity Ontology (UCO), that integrates structured and unstructured open data sources in an automated fashion. With this paper, we hope to pave the way toward a Unified Cybersecurity Knowledge Graph (UCKG) and its utilization in intelligent cybersecurity solutions.},
}

@article{Zhao2024,
  title = {Metadata for Scientific Experiment Reporting: A Case Study in Metal-Organic Frameworks},
  author = {Xintong Zhao and Kyle Langlois and Jacob Furst and Scott McClellan and Xiaohua Hu and Yuan An and Diego A Gómez-Gualdrón and Fernando J Uribe-Romo and Jane Greenberg},
  year = {2024},
  pages = {30-40},
  publisher = {Springer Nature Switzerland},
  abstract = {Research methods and procedures are core aspects of the research process. Metadata focused on these components is critical to supporting the FAIR principles, particularly reproducibility. The research reported on in this paper presents a methodological framework for metadata documentation supporting the reproducibility of research producing Metal Organic Frameworks (MOFs). The MOF case study involved natural language processing to extract key synthesis experiment information from a corpus of research literature. Following, a classification activity was performed by domain experts to identify entity-relation pairs. Results include: 1) a research framework for metadata design, 2) a metadata schema that includes nine entities and two relationships for reporting MOF synthesis experiments, and 3) a growing database of MOF synthesis reports structured by our metadata scheme. The metadata schema is intended to support discovery and reproducibility of metal-organic framework research and the FAIR principles. The paper provides background information, identifies the research goals and objectives, research design, results, a discussion, and the conclusion.},
}

@article{Gundhus2024,
  title = {Introduction to Volume II: Interrogating Cultures of Policing and Intelligence in the Big Data Era},
  author = {Helene Oppen Ingebrigtsen Gundhus and Christin Thea Wathne and Tereza Østbø Kuldova},
  year = {2024},
  pages = {1-24},
  doi = {10.1007/978-3-031-68298-8_1},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-68298-8_1},
  abstract = {Advances in artificial intelligence combined with the increasing role of private security, tech, and consulting companies, are reshaping contemporary policing and the ways in which we ensure security, enforce law, and prevent and investigate crime. This chapter introduces the volume II which zooms in on the epistemologies of data in algorithmic governance, setting the stage for the discussions across this volume, in which we interrogate the various roles, quality of, and assumptions behind data collection and how data has become an instrument in governance, management, and decision-making. Data is used for the purposes of both public and private surveillance and governance and critical attention therefore needs to be paid to how gathering, sharing, and reuse of data is both political and culturally constructed in a global context. We also need to account for the ways in which it affects knowledge production in non-transparent ways and for the human and social consequences of this development. This chapter prepares the ground for the discussions across this second volume where we delve into the imaginaries of accuracy, clean versus dirty data in South Africa, discussions about facial recognition and technopolitics in Brazil, the construction of intelligence and organisational learning in Norwegian police, data reuse in humanitarian aid and border control, ethics and broader questions of transparency, data quality, and trust in data-driven policing, as well as the very topical issues of policing of generative AI and the ways in which authoritarian and liberal democracies use biopolitics to turn social welfare into surveillance, with examples from India and China.},
}

@article{Desbois2024,
  title = {French-Style Applications of Artificial Intelligence to Human Health in a European Context},
  author = {Dominique Desbois},
  year = {2024},
  pages = {110-122},
  publisher = {Springer Nature Switzerland},
  abstract = {In this paper, we discuss the applications of artificial intelligence to human health and the problems this poses in France. We explore the field of application of weak artificial intelligence to human health problems in relation to questions of socio-economic development, law and ethics. We also look at issues arising from strong Artificial Intelligence. We present a few examples of applications of weak artificial intelligence in various fields: predictive medicine, precision medicine, diagnostic and therapeutic assistance, supportive care, computer-assisted surgery, epidemic prevention and pharmacovigilance, and access to care. Within the European regulatory context of the single digital market and the personal data protection regulation, we illustrate the societal issues surrounding the application of artificial intelligence. We conclude with the prospects for the development of artificial intelligence applications in human health in France within a European context.},
}

@article{Praveen2024,
  title = {Applications of Artificial Intelligence and Machine Learning in Antimicrobial Resistance Study},
  author = {Ayush Praveen and Nicholas Bartelo and Vijay Soni},
  year = {2024},
  pages = {359-385},
  doi = {10.1007/978-3-031-65986-7_11},
  publisher = {Springer International Publishing},
  url = {https://doi.org/10.1007/978-3-031-65986-7_11},
  abstract = {Microbes possess a natural capacity to resist antimicrobial agents (substances or compounds that can stop or slow down their growth). Due to bad administration of antibiotics uses, dearth of new development pipelines for antibiotics, poor monitoring and surveillance, lack of decision support systems warranting the right antibiotic administration, and other factors, antimicrobial resistance (AMR) is a global threat that will impact deaths, quality of life postinfection, and economic losses. Artificial Intelligence (AI) can be an emerging tool to fight AMR in different areas ranging from identifying or developing new antibiotics, slowing down AMR development due to better management guided by Machine Learning (ML) models, development of diagnostics arrangements, and supporting clinical decision systems. In this chapter, we present the applications of AI in different areas of early research, clinical setting, and decision-making for health care. Further, we highlight the potential of AI in omics studies for AMR research and diagnosis of AMR and the recent developments within these areas. The adoption of AI is strongly challenged by high accuracy requirements within the healthcare system and hence for better adoption. Therefore, we discussed the impending challenges, upon resolution of which can collectively slow down AMR development. Lastly, we also discuss the recommendations for investing focus within the current AI regime and future developments that can benefit the field.},
}

@article{Gizelis2024,
  title = {Data Monetization Opportunities and Challenges: The European Landscape by DATAMITE},
  author = {Christos A Gizelis and Maria Panagiotidou and Sotiria Petrova and Daniela Fuchs and Margit Hofer and Marcin Plociennik and Agnieszka Rausch and Santiago Cáceres Elvira and Liliana Beltrán Blanco and Gabriel Gonzales Castane and Viivi Lähteenoja and Udo Bub and Polina Petrova and Martina Bogdanova and Elisa Cauche},
  year = {2024},
  pages = {62-79},
  publisher = {Springer Nature Switzerland},
  abstract = {In Europe, data monetization presents a promising goal for new revenue streams exploiting the power of the data. Nevertheless, obstacles such as regulatory environment and internal organizational constraints create a skepticism. DATAMITE project addresses these challenges and proposes a holistic framework designed to streamline the data monetization journey. Offering modules centered on data governance, quality assurance, and security, DATAMITE equips stakeholders with the tools necessary to optimize the value of their data. By ensuring trust and fostering collaboration, DATAMITE assures the expansion and competitiveness of European industries in data market. Through its comprehensive approach, DATAMITE aims to unlock opportunities and guide European enterprises towards success in the dynamic landscape of the digital economy.},
}

@article{Perevalov2024,
  title = {Language Models as SPARQL Query Filtering for Improving the Quality of Multilingual Question Answering over Knowledge Graphs},
  author = {Aleksandr Perevalov and Aleksandr Gashkov and Maria Eltsova and Andreas Both},
  year = {2024},
  pages = {3-18},
  publisher = {Springer Nature Switzerland},
  abstract = {Question Answering systems working over Knowledge Graphs (KGQA) generate a ranked list of SPARQL query candidates for a given natural-language question. In this paper, we follow our long-term research agenda of providing trustworthy KGQA systems – here – by presenting a query filtering approach that utilizes (large) language models (LMs/LLMs), s.t., correct and incorrect queries can be distinguished. In contrast to the previous work, we address here multilingual questions represented in major languages (English, German, French, Spanish, and Russian), and confirm the generalizability of our approach by also evaluating it on low-resource languages (Ukrainian, Armenian, Lithuanian, Belarusian, and Bashkir). For our experiments, we used the following LMs: BERT, DistilBERT, Mistral, Zephyr, GPT-3.5, and GPT-4. The LMs were applied to the KGQA systems – QAnswer and MemQA – as SPARQL query filters. The approach was evaluated on the multilingual Wikidata-based dataset QALD-9-plus. The experimental results suggest that the KGQA systems achieve quality improvements for all languages when using our query-filtering approach.},
}

@article{Kasprzik2024,
  title = {The Automation of Subject Indexing at ZBW and the Role of Metadata in Times of Large Language Models},
  author = {Anna Kasprzik},
  year = {2024},
  journal = {Procedia Computer Science},
  volume = {249},
  pages = {160-166},
  doi = {https://doi.org/10.1016/j.procs.2024.11.059},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050924032708},
  abstract = {Subject indexing is one of the core activities of libraries. Due to the proliferation of digital documents it is no longer possible to annotate every single document intellectually, which is why we need to explore the potentials of automation. At ZBW the efforts to automate the subject indexing process started as early as 2000 with experiments involving external partners and commercial software. The conclusion from that first period was that supposedly shelf-ready solutions would not cover the requirements of the library. In 2014 the decision was made to start doing the necessary applied research in-house by establishing a corresponding PhD position. However, the prototypical machine learning solutions they developed were yet to be integrated into productive operations at the library. Therefore in 2020 an additional position for a software engineer was established and a 4-year pilot phase was initiated with the goal to build a software architecture that allows for real-time subject indexing with our trained models and the integration thereof into the other metadata workflows at ZBW. This paper gives an account of how we tackled the task of transferring results from applied research into a productive subject indexing service (the “AutoSE service”), including the milestones we have reached, the challenges we were facing on a strategic level, and the measures and resources (computing power, software, personnel) that were needed in order to be able to effect the transfer and get a first version going, which went live in 2021. The models used by AutoSE until now were models from classical machine learning. We therefore also touch on the question if and how the recent advent of large language models (LLMs) has changed our outlook on the task of automating subject indexing and on the role of metadata in information management and retrieval in general, and the ways in which it impacts our research and development roadmap going forward.},
}

@article{Félix2025,
  title = {Why are you traveling? Inferring trip profiles from online reviews and domain-knowledge},
  author = {Lucas G.S. Félix and Washington Cunha and Claudio M.V. {de Andrade} and Marcos André Gonçalves and Jussara M. Almeida},
  year = {2025},
  journal = {Online Social Networks and Media},
  volume = {45},
  pages = {100296},
  doi = {https://doi.org/10.1016/j.osnem.2024.100296},
  url = {https://www.sciencedirect.com/science/article/pii/S2468696424000211},
  abstract = {This paper addresses the task of inferring trip profiles (TPs), which consists of determining the profile of travelers engaged in a particular trip given a set of possible categories. TPs may include working trips, leisure journeys with friends, or family vacations. Travelers with different TPs typically have varied plans regarding destinations and timing. TP inference may provide significant insights for numerous tourism-related services, such as geo-recommender systems and tour planning. We focus on TP inference using TripAdvisor, a prominent tourism-centric social media platform, as our data source. Our goal is to evaluate how effectively we can automatically discern the TP from a user review on this platform. A user review encompasses both textual feedback and domain-specific data (such as a user’s previous visits to the location), which are crucial for accurately characterizing the trip. To achieve this, we assess various feature sets (including text and domain-specific) and implement advanced machine learning models, such as neural Transformers and open-source Large Language Models (Llama 2, Bloom). We examine two variants of the TP inference task—binary and multi-class. Surprisingly, our findings reveal that combining domain-specific features with TF-IDF-based representation in an LGBM model performs as well as more complex Transformer and LLM models, while being much more efficient and interpretable.},
}

@article{Lechner2024,
  title = {BMW: Process Intelligence for Everybody—Organizational Setup and Scaling},
  author = {Patrick Lechner},
  year = {2024},
  pages = {87-97},
  doi = {10.1007/978-3-031-61343-2_11},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-61343-2_11},
  abstract = {At BMW we believe in Process Excellence. By providing Process Intelligence tools for every single employee, we help them to understand and improve their relevant processes, driving process efficiency and value. By setting up the right organizational structures, we’re able to achieve these insights and improvements end-to-end and to scale globally and across all prime processes. This also allows us to act object based and therefore understand interactions of various processes better.},
}

@article{Sjöström2024,
  title = {Meta-requirements for LLM-Based Knowledge Exploration Tools in Information Systems Research},
  author = {Jonas Sjöström and Stefan Cronholm},
  year = {2024},
  pages = {424-439},
  publisher = {Springer Nature Switzerland},
  abstract = {The problem we address in this paper is that the potential impact of Large Language Models (LLMs) on the research practice in information systems is not well understood. The focus has been on how LLMs could support literature review processes. Therefore, this paper aims to advance knowledge on how Large Language Models (LLMs) could support knowledge exploration through literature reviews. The knowledge contribution consists of meta-requirements that inform the design of LLM-based tools assisting knowledge exploration. The meta-requirements are theoretically justified by scrutinizing established IS literature review methodologies, reported challenges of LLMs and design process experiences. Furthermore, we introduce an LLM supported literature review process model that maps the relationships between the meta-requirements and specific phases of the process model. This work contributes to the field by providing a foundation for designing transparent, controllable, and resource-efficient tools for knowledge exploration, and supporting the rigor of knowledge exploration in information systems research.},
}

@article{Sartini2024,
  title = {IICONGRAPH: Improved Iconographic and Iconological Statements in Knowledge Graphs},
  author = {Bruno Sartini},
  year = {2024},
  pages = {57-74},
  publisher = {Springer Nature Switzerland},
  abstract = {Iconography and iconology are fundamental domains when it comes to understanding artifacts of cultural heritage (CH). Iconography deals with the study and interpretation of visual elements depicted in artifacts and their symbolism, while iconology delves deeper, exploring the underlying cultural and historical meanings. Despite the advances in representing CH with Linked Open Data (LOD), recent studies show persistent gaps in the representation of iconographic and iconological statements in current knowledge graphs (KGs). To address them, this paper presents IICONGRAPH, a KG that was created by refining and extending the iconographic and iconological statements of ArCo (the Italian KG of CH) and Wikidata. The development of IICONGRAPH was also driven by a series of requirements emerging from research case studies expressed in competency questions (CQs) that were unattainable in the non-reengineered versions of the KGs. The evaluation results demonstrate that IICONGRAPH not only outperforms ArCo and Wikidata through domain-specific assessments from the literature but also serves as a robust platform for answering the formulated CQs. IICONGRAPH is released and documented in accordance with the FAIR principles to guarantee the resource’s reusability. The algorithms used to create it and assess the CQs have also been made available to ensure transparency and reproducibility. While future work focuses on ingesting more data into the KG, and on implementing it as a backbone of LLM-based question answering systems, the current version of IICONGRAPH still emerges as a valuable asset, contributing to the evolving landscape of CH representation within KGs, the Semantic Web, and beyond.},
}

@article{Gutiérrez2024,
  title = {The Effects of Class Balance on the Training Energy Consumption of Logistic Regression Models},
  author = {María Gutiérrez and Coral Calero and Félix García and Mª Ángeles Moraga},
  year = {2024},
  pages = {324-337},
  publisher = {Springer Nature Switzerland},
  abstract = {The presence of Artificial Intelligence and specifically Machine Learning (ML) has increased in all manner of software applications, and it already plays a major role in a variety of systems pertaining to Information Science such as public transport, disease diagnosis support and other medical problems. This increase in use has raised concerns about possible environmental impacts, since ML models require to be trained in datacentres that can impose a high ecological toll. With the aim of uncovering new ways of reducing the energy consumption of ML models, in this study we will explore the energetic impact of class balance for binary classification tasks by comparing a set of logistic regression models (LRMs) trained on a synthetic balanced dataset against another set trained on a synthetic, unbalanced dataset. We focus on the total energy and time required to complete the task, and discover that the order in energy efficiency of the models remained consistent regardless of class balance, but those trained on the unbalanced dataset required between 1.42 and 1.5 times more energy to complete the tasks, despite requiring only around 1 s more of runtime. We finish by analysing the results and proposing using synthetic datasets to estimate the energy cost of different hyperparameter options for LRMs.},
}

@article{Riezler2024,
  title = {Analyzing Inferential Reproducibility},
  author = {Stefan Riezler and Michael Hagmann},
  year = {2024},
  pages = {133-152},
  doi = {10.1007/978-3-031-57065-0_5},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-57065-0_5},
  abstract = {Reproducibility of experimental results is one of the fundamental pillars of scientific research. If neither a reliable nor significant evaluation result can be obtained when replicating an experiment, the whole methodological foundation of the research result becomes questionable, even casting doubt on its validity. In this chapter, we will show how to apply the reliability and significance tests introduced in the previous chapters to analyze the reproducibility of research results. The flexibility of the model-based tests allows us to give this analysis an important twist: Instead of following the current tendency to remove measurement noise in order to enforce reproducibility under the exact same training conditions, we aim at a notion of inferential reproducibility (Goodman et al., 2016). This concept embraces certain types of nondeterminism as inherent and irreducible conditions of measurement, and aims at incorporating several sources of variance, including their interaction with data properties, into an analysis of significance and reliability of machine learning evaluation, with the aim to draw inferences beyond particular instances of trained models. We will show how to incorporate arbitrary sources of noise like meta-parameter variations into statistical significance testing with GLRTs, how to use variance component analysis based on LMEMs to analyze the contribution of noise sources to overall variance, and how to compute a reliability coefficient as indicator for reproducibility.},
}

@article{Makridis2024,
  title = {From theory to practice: Harmonizing taxonomies of trustworthy AI},
  author = {Christos A. Makridis and Joshua Mueller and Theo Tiffany and Andrew A. Borkowski and John Zachary and Gil Alterovitz},
  year = {2024},
  journal = {Health Policy OPEN},
  volume = {7},
  pages = {100128},
  doi = {https://doi.org/10.1016/j.hpopen.2024.100128},
  url = {https://www.sciencedirect.com/science/article/pii/S2590229624000133},
  abstract = {The increasing capabilities of AI pose new risks and vulnerabilities for organizations and decision makers. Several trustworthy AI frameworks have been created by U.S. federal agencies and international organizations to outline the principles to which AI systems must adhere for their use to be considered responsible. Different trustworthy AI frameworks reflect the priorities and perspectives of different stakeholders, and there is no consensus on a single framework yet. We evaluate the leading frameworks and provide a holistic perspective on trustworthy AI values, allowing federal agencies to create agency-specific trustworthy AI strategies that account for unique institutional needs and priorities. We apply this approach to the Department of Veterans Affairs, an entity with largest health care system in US. Further, we contextualize our framework from the perspective of the federal government on how to leverage existing trustworthy AI frameworks to develop a set of guiding principles that can provide the foundation for an agency to design, develop, acquire, and use AI systems in a manner that simultaneously fosters trust and confidence and meets the requirements of established laws and regulations.},
}

@article{Prummer2024,
  title = {From Virtual Worlds to Real-World Impact: An Industrial Metaverse Survey},
  author = {Michael Prummer and Emanuel Regnath and Saurabh Singh and Harald Kosch},
  year = {2024},
  pages = {592-613},
  publisher = {Springer Nature Switzerland},
  abstract = {Metaverse for industrial applications has enormous potential for accelerating digital transformation. More than ever, digital transformation, especially in manufacturing and energy industries, is needed to become more sustainable and resilient. While the consumer-oriented Metaverse revolves around interoperable virtual worlds, the Industrial Metaverse focuses on Digital Twins that mirror operational technologies and complex autonomous systems, creating decentralized ecosystems for solving problems. Applying Metaverse thinking to industrial processes, we gain vast possibilities for innovation, collaboration, and more accessible systems, resulting in more efficient systems. In this paper, we discuss the aspects of decentralized Metaverse applications in an industrial context and distinguish them from similar concepts like cyber-physical systems and Industry 4.0. We examine social Metaverse concepts that can be adapted to industrial systems, facilitating virtual spaces for collaboration, knowledge sharing, and a decentralized creator economy. We present a comprehensive reference architecture that outlines the fundamental building blocks of the Industrial Metaverse. This architecture leverages the unique amalgamation of technologies, including interoperable Digital Twins, in the context of immersion, interaction, and collaboration for secure, autonomous-governed decentralized industrial applications. Following, we discuss the research status, requirements, and challenges of core technologies and the unique ecosystem characteristics that drive us toward a future in the Industrial Metaverse.},
}

@article{Shandilya2024,
  title = {Role of Artificial Intelligence and Machine Learning},
  author = {Shishir Kumar Shandilya and Agni Datta and Yash Kartik and Atulya Nagar},
  year = {2024},
  pages = {313-399},
  doi = {10.1007/978-3-031-53290-0_6},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-53290-0_6},
  abstract = {This chapter explores the development and wide array of uses of artificial intelligence (AI) and machine learning (ML) while highlighting their key roles in promoting digital resilience. Further it will describe a wide range of areas, such as agriculture, cybersecurity, healthcare, and finance, demonstrating the extensive influence of AI and ML. It explores the advantages and benefits of AI, including its ability to scale, its accuracy, and its capacity to mitigate threats. It also critically analyses the problems and issues related to integrating data, ensuring professional competency, and addressing ethical considerations. In addition, the chapter will briefly discuss the use, benefits, and drawbacks of explainable AI and responsible AI principles. The chapter concludes by highlighting the incorporation of artificial intelligence into digital resilience strategies, emphasizing the capacity of these technologies to strengthen systems against dynamic cyber threats.},
}

@article{Xu2024_01,
  title = {BigText-QA: Question Answering over a Large-Scale Hybrid Knowledge Graph},
  author = {Jingjing Xu and Maria Biryukov and Martin Theobald and Vinu Ellampallil Venugopal},
  year = {2024},
  pages = {33-48},
  publisher = {Springer Nature Switzerland},
  abstract = {Answering complex questions over textual resources remains a challenge, particularly when dealing with nuanced relationships between multiple entities expressed within natural-language sentences. To this end, curated knowledge bases (KBs) like YAGO, DBpedia, Freebase, and Wikidata have been widely used and gained great acceptance for question-answering (QA) applications in the past decade. While these KBs offer a structured knowledge representation, they lack the contextual diversity found in natural-language sources. To address this limitation, BigText-QA introduces an integrated QA approach, which is able to answer questions based on a more redundant form of a knowledge graph (KG) that organizes both structured and unstructured (i.e., “hybrid”) knowledge in a unified graphical representation. Thereby, BigText-QA is able to combine the best of both worlds—a canonical set of named entities, mapped to a structured background KB (such as YAGO or Wikidata), as well as an open set of textual clauses providing highly diversified relational paraphrases with rich context information. Our experimental results demonstrate that BigText-QA outperforms DrQA, a neural-network-based QA system, and achieves competitive results to QUEST, a graph-based unsupervised QA system.},
}

@article{Hu2025,
  title = {Artificial Intelligence in Higher Education: Applications, Challenges, and Policy Development and Further Considerations},
  author = {Shouping Hu and Fengfeng Ke and Dina Vyortkina and Pei Hu and Sam Luby and Joe O’Shea},
  year = {2025},
  pages = {1-52},
  doi = {10.1007/978-3-031-51930-7_13-2},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-51930-7_13-2},
  abstract = {In this chapter, we review the development of artificial intelligence in education in general and higher education in particular. We then describe a range of use cases of artificial intelligence in higher education and explore the efficacy of those applications on various constituents, including students, faculty, researchers, administrators, and staff members in colleges and universities. While describing various applications, we raise issues related to the challenges of artificial intelligence in higher education such as equity, security, and confidentiality, among other matters. We also document existing policy development and related initiatives from different levels (e.g., international, national, state, and institutional) regarding artificial intelligence. Finally, we explore future directions for research and policy considerations on artificial intelligence in higher education. As a rapidly changing area, there is an urgent need to build evidence base on the impacts of artificial intelligence on all constituents in higher education through rigorous research to guide policy and practice.},
}

@article{Gordon2024,
  title = {Radical Technologies},
  author = {Ian Gordon and Neil Thompson},
  year = {2024},
  pages = {239-337},
  doi = {10.1007/978-3-031-51008-3_6},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-51008-3_6},
  abstract = {Over the course of this chapter, we provide an overview of a range of technologies that deliver the application of data to the built environment. We first use complexity theory to frame the technologies described in this chapter in terms of whether they are best suited to tackling complex or merely complicated problem sets. We then discuss key types of technology that one can use to deliver digital change in built environment organisations including Building Information Modelling (BIM), Business Intelligence (BI), Data Science and Artificial Intelligence (AI), Smart Buildings and Internet of Things (IoT), Digital Rehearsals and automated parametric design, Digital Twins, and Generative AI.},
}

@article{Gordon2024_01,
  title = {Delivering Data Capability},
  author = {Ian Gordon and Neil Thompson},
  year = {2024},
  pages = {199-238},
  doi = {10.1007/978-3-031-51008-3_5},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-51008-3_5},
  abstract = {In the previous chapter, we explored the value case for applying data to the built environment, and the cultural change required for organisations to realise that value. We now dive into the practicalities of delivering better data outcomes within organisations. In doing so we touch upon three themes. Firstly, we cover the foundational data capabilities (governance, architecture, and data modelling). Secondly, we discuss how to deliver data projects within an established organisation, and how to work alongside a core IT function. Finally, we dig into how best to procure data projects to maximise their chance of success.},
}

@article{Mir2024,
  title = {Conversational Systems and Computational Intelligence, A Critical Analysis},
  author = {Yuniesky Orlando Vasconcelo Mir and Pedro Yobanis Piñero Pérez and Iliana Pérez Pupo and Luis Alvarado Acuña and Rafael Bello Pérez},
  year = {2024},
  pages = {3-28},
  doi = {10.1007/978-3-031-50495-2_1},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-50495-2_1},
  abstract = {The increase in research on conversational systems and their applications, constitutes the main motivation of this work. In this research, a critical analysis of the growth of smart chatbots and their combination with computational intelligence techniques is carried out. The analysis is oriented on three fundamental fronts. First, a review protocol is applied to identify the main schools and centers of knowledge. Then the conversational systems are characterized based on the level of inclusion of artificial intelligence techniques. Finally, the integration of conversational systems with different computational intelligence techniques is reviewed. The analysis identifies that there are many opportunities and lines open to research. In particular, the need to strengthen the application of neutrosophic theory and sets for the evaluation of conversations is identified. Also, the need to combine linguistic data summarization techniques and reinforcement learning is identified to improve training methods and reduce the computational cost of conversational systems responses.},
}

@article{Gall2023,
  title = {Approaches for Sustainable Urban Mobility Futures},
  author = {Tjark Gall and Flore Vallet and Laura Mariana Reyes Madrigal and Sebastian Hörl and Adam Abdin and Tarek Chouaki and Jakob Puchinger},
  year = {2023},
  pages = {53-102},
  doi = {10.1007/978-3-031-45795-1_3},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-45795-1_3},
  abstract = {Aside from the trends, there are some dominant—and promising—ways of addressing today’s problems whilst preparing for the developments of tomorrow. In this chapter, we present and discuss three types of approaches and some of their methods. First, we look at urban governance. Whilst not at the focus of this book, it is the foundation for many of the other approaches and a key element of urban mobility system transitions. Next, we outline people-centred design approaches, including some examples of how that can be achieved in future settings. Lastly, we look at data-driven design and decision-making with in-depth discussions of mobility system modelling and charging infrastructure management.},
}

@article{Mastoras2023,
  title = {Towards a Framework for Seismic Data},
  author = {Valadis Mastoras and Alexandros Vassiliades and Maria Rousi and Sotiris Diplaris and Thanassis Mavropoulos and Ilias Gialampoukidis and Stefanos Vrochidis and Ioannis Kompatsiaris},
  year = {2023},
  pages = {106-119},
  publisher = {Springer Nature Switzerland},
  abstract = {Over the past decade, Knowledge Graphs (KGs) gained significant attention as a powerful method for knowledge representation. Driven by increasing interest, a paradigm shift has occurred, where the technology of KGs has transitioned from the research domain to the industry and public sector, with companies and organizations increasingly representing their data as Linked Open Data, gaining in that way significant traction for this technology. This paper focuses on KGs in the context of environmental challenges. More specifically, this work concerns KGs that contain seismic event data, such as location, timestamp, magnitude, depth, target date, as well as images before and after the event occurrence. Moreover, a Natural Language Processing (NLP) module is integrated to enhance the KG. That module enables users to query for seismic events in a free-text manner, before addressing them with a relevant response through a dedicated Information Retrieval (IR) component. The KG was constructed with data retrieved from the Instituto Nazionale di Geofisica e Vulcanologia, a rich resource that comes with earthquake-related information, such as magnitude, depth, occurrence location, and timestamp. Additionally, public APIs from the Copernicus Open Access Data Hub and ONDA DIAS are leveraged to provide access to sentinel data, such as images of the event location before and after its occurrence.},
}

@article{Bughin2024,
  title = {What drives the corporate payoffs of using generative artificial intelligence?},
  author = {Jacques Bughin},
  year = {2024},
  journal = {Structural Change and Economic Dynamics},
  volume = {71},
  pages = {658-668},
  doi = {https://doi.org/10.1016/j.strueco.2024.09.011},
  url = {https://www.sciencedirect.com/science/article/pii/S0954349X24001413},
  abstract = {Artificial Intelligence, a set of technologies that aim to replicate human cognitive functions, has seen remarkable improvements over the last decade. In particular, generative AI (GenAI), a subset of AI able to generate content tasks based on Large Language Models (LLM), has recently gained momentum. Based on an extensive analysis of generative AI use cases in large enterprises, we find that Gen AI shows strong labor productivity improvements across metrics such as throughput time, unit cost, and task effectiveness. However, the distribution of gains is asymmetric in favor of a few companies. While the current distribution of gains does not provide evidence of a power law effect, the current asymmetry reflects differences in AI resources/capabilities across companies - mainly data access, AI talent, or AI governance.},
}

@article{Saheb2023,
  title = {Topical review of artificial intelligence national policies: A mixed method analysis},
  author = {Tahereh Saheb and Tayebeh Saheb},
  year = {2023},
  journal = {Technology in Society},
  volume = {74},
  pages = {102316},
  doi = {https://doi.org/10.1016/j.techsoc.2023.102316},
  url = {https://www.sciencedirect.com/science/article/pii/S0160791X23001215},
  abstract = {A number of countries have adopted national policies and directives to balance the advantages and disadvantages of innovative technologies. The purpose of this paper is to identify the most prominent topics addressed by national AI policies, as well as their relative importance across nations. This paper integrates the results of a topic modeling analysis of 30 national AI policies with a qualitative content analysis of the policies. Based on this analysis, fourteen main common themes have been identified among national AI policies, which predominantly relate to educational, technological, government, ethical/legal, and social good concerns. Following this, we conducted a co-occurrence analysis of topics across countries to determine the extent of topic prioritization in each country. In this investigation, several marginalized AI policy topics were also identified. In general, the challenges and concerns of the majority of policies pertain to education, technology, and the government. Governments refer to real-world projects and investments in AI technologies without developing shared digital governance platforms that promote responsible and sustainable AI among technology titans and mitigate the negative effects of surveillance capitalism. Although governments acknowledge the ethical and legal aspects of AI development and frequently cite the GDPR, they limit their discussion to the data level, particularly data sharing, and marginalize ethical algorithms and other phases of data and AI management and design. In addition, government policies marginalize AI startups and the API economy, even though they play a crucial role in fostering the AI ecosystem. The paper contributes to the existing literature on AI policy and will serve as a guide for AI policymakers to help them better understand the topical similarities across countries and the neglected or marginalized challenges that require further attention.},
}

@article{Siciliani2023,
  title = {AI-based decision support system for public procurement},
  author = {Lucia Siciliani and Vincenzo Taccardi and Pierpaolo Basile and Marco {Di Ciano} and Pasquale Lops},
  year = {2023},
  journal = {Information Systems},
  volume = {119},
  pages = {102284},
  doi = {https://doi.org/10.1016/j.is.2023.102284},
  url = {https://www.sciencedirect.com/science/article/pii/S0306437923001205},
  abstract = {Tenders are powerful means of investment of public funds and represent a strategic development resource. Thus, improving the efficiency of procuring entities and developing evaluation models turn out to be essential to facilitate e-procurement procedures. With this contribution, we introduce our research to create a supporting system for the decision-making and monitoring process during the entire course of investments and contracts. This system employs artificial intelligence techniques based on natural language processing, focused on providing instruments for extracting useful information from both structured and unstructured (i.e., text) data. Therefore, we developed a framework based on a web app that provides integrated tools such as a semantic search engine, a summariser, an open information extraction engine in the form of triples (subject–predicate–object) for tender documents, and dashboards for analysing tender data.},
}

@article{Jakobsen2023,
  title = {A literature review of smart technology domains with implications for research on smart rural communities},
  author = {Kine Jakobsen and Marius Mikalsen and Grethe Lilleng},
  year = {2023},
  journal = {Technology in Society},
  volume = {75},
  pages = {102397},
  doi = {https://doi.org/10.1016/j.techsoc.2023.102397},
  url = {https://www.sciencedirect.com/science/article/pii/S0160791X23002026},
  abstract = {This literature review examines empirical research on smart technologies in eight domains and seeks to analyse implications for smart rural communities research. The development and implementation of smart technologies has the potential to improve various aspects of life in rural areas, including healthcare, mobility, and governance. However, the adoption of these technologies also raises important questions about their relation to communities. The existing literature is reviewed to examine the key findings and knowledge gaps. By using a sociotechnical perspective and considering the specific characteristics of rural communities, the discussion is focused on how smart technologies can contribute to rural contexts. Further, policy implications are considered, encompassing the requirement for a holistic and inclusive approach to the implementation of smart technologies in rural areas. Implementation should address the specific needs, challenges, economic and infrastructural conditions, social structures, and cultural contexts of the respective communities. Future research and exploration of smart community concepts in rural contexts are suggested to improve smart technology implementation.},
}

@article{Lan2024,
  title = {Complementary or substitutive effects? The duality of digitalization and ESG on firm's innovation},
  author = {Lan Lan and Zhifang Zhou},
  year = {2024},
  journal = {Technology in Society},
  volume = {77},
  pages = {102567},
  doi = {https://doi.org/10.1016/j.techsoc.2024.102567},
  url = {https://www.sciencedirect.com/science/article/pii/S0160791X24001155},
  abstract = {As an increasing number of companies are recognizing digitalization and sustainability to be important components of their overall business strategies, there is a growing need to evaluate the innovative outcomes of these two strategies. This study examines the dual effects of a digital orientation and a corporate environmental, social, and governance (ESG) orientation on firm innovation under firm resource constraints. The empirical results confirm the hypothesis that digitization and ESG practices promote corporate innovation. In addition, a combined effect of ESG and digitization strategies on innovation performance is confirmed, but this effect is substitutive for resource-constrained enterprises, such as those with high levels of financing constraints and small and medium-sized enterprises. The role in supporting or hindering the synergy between digitalization and ESG strategies in different regions and industry sectors may various. Enterprises should prudently implement a dual strategy encompassing both digitalization and ESG while fully considering the inherent interaction of the two to improve the level of strategic synergy.},
}

@article{Hagmann2025,
  title = {I walk an ancient road: A straightforward methodology for analyzing intra- and inter-regional connectivity systems along Roman Frontier Zones (c. 1st—5th century AD)},
  author = {Dominik Hagmann},
  year = {2025},
  journal = {Journal of Archaeological Science},
  volume = {176},
  pages = {106151},
  doi = {https://doi.org/10.1016/j.jas.2024.106151},
  url = {https://www.sciencedirect.com/science/article/pii/S030544032400219X},
  abstract = {This paper presents a broadly reusable and straightforward methodology for examining ancient road networks in the hinterlands of Roman frontiers, with attention to both intra- and inter-regional connectivity. It employs a range of tools—including Geographic Information Systems (GIS), Least Cost Analyses (LCA), Spatial Social Network Analysis (SSNA), and Visibility Analysis (VA)—to trace and interpret the road systems that facilitated troop movements, goods transport, and social exchange. The study exemplifies this methodology through an investigation of a frontier zone within a specific area of interest at the UNESCO World Heritage Site “Danube Limes” in Northern Noricum (modern Austria), spanning the mid-1st to the 5th century CE. By integrating archaeological data, open government datasets, and advanced digital methods, the analysis reveals a variety of pathways that may have supported military logistics and civilian travel across this strategically significant region. Selected roads identified through the analyses could be subsequently verified through ground-truthing.The paper also emphasizes the need for cautious interpretation, acknowledging the hypothetical nature of certain pathways and the inherent limitations of tracing ancient infrastructure through the methods employed.},
}

@article{Wang2024_03,
  title = {Open Government Data (OGD) as a catalyst for smart city development: Empirical evidence from Chinese cities},
  author = {Ruoyun Wang and Corey Kewei Xu and Xun Wu},
  year = {2024},
  journal = {Government Information Quarterly},
  volume = {41},
  pages = {101983},
  doi = {https://doi.org/10.1016/j.giq.2024.101983},
  url = {https://www.sciencedirect.com/science/article/pii/S0740624X24000753},
  abstract = {While existing smart city models recognize the importance of data, they often overlook the specific role of Open Government Data (OGD) for urban development. This study addresses this gap by adapting the Smart City Model to explicitly include OGD as a critical component. Drawing on panel data from the 2022–2024 Chinese Cities Digitalization Evolution Index, we employ Structural Equation Modeling (SEM) to empirically examine the direct and indirect effects of OGD, digital infrastructure, and digital economy on smart city development. Our analysis identifies four key pathways, revealing that while digital infrastructure positively influences smart city development directly, the indirect pathways incorporating OGD demonstrate stronger effects. OGD plays a pivotal role by significantly enhancing the digital economy and digital infrastructure, as well as directly contributing to smart city development. This research contributes to the smart city literature by moving beyond discussions of individual components to empirically test the relationships between these elements. By positioning OGD as a catalyst, we provide a nuanced understanding of the mechanisms through which data-driven initiatives empower smart city development. Our findings offer valuable insights into the multifaceted ways OGD serves as a driving force for urban innovation, challenging the traditional view of government data as a passive resource. This study highlights the importance of OGD as a strategic asset for policymakers seeking to harness the potential of data-driven urban governance. We conclude with policy recommendations for leveraging OGD to support sustainable and efficient smart city development.},
}

@article{Pesqueira2024,
  title = {Exploring the impact of EU tendering operations on future AI governance and standards in pharmaceuticals},
  author = {Antonio Pesqueira and Andreia {de Bem Machado} and Sama Bolog and Rúben Pereira and Maria José Sousa},
  year = {2024},
  journal = {Computers & Industrial Engineering},
  volume = {198},
  pages = {110655},
  doi = {https://doi.org/10.1016/j.cie.2024.110655},
  url = {https://www.sciencedirect.com/science/article/pii/S0360835224007770},
  abstract = {This research examines the incorporation of artificial intelligence (AI) into the domain of tender management (TM) within the pharmaceutical industry, with a particular emphasis on operational efficiency, governance, and compliance with European regulatory standards. A comparative analysis of four companies—two that have adopted AI and two that have not—reveals significant discrepancies in the management of TM processes between AI-driven and traditional companies. The study employs the Delphi method to ascertain expert consensus on eight critical areas of AI governance, including data privacy, transparency, and ethical AI use. The findings indicate that companies integrating AI demonstrate enhanced decision-making capabilities, accelerated processing times, and enhanced stakeholder engagement. However, they also encounter challenges pertaining to ethical governance and regulatory compliance. The research highlights the necessity of aligning the adoption of AI with the latest European directives, such as the AI Act and General Data Protection Regulation (GDPR), to ensure both operational efficiency and adherence to ethical standards. The broader implications of the study underscore the necessity for pharmaceutical companies to develop robust governance frameworks, prioritize ethical considerations, and maintain regulatory compliance to fully leverage the potential of AI. Additionally, the study contributes to the ongoing scholarly discourse by providing empirical evidence on the interplay between AI, ethics, and governance, thereby encouraging further interdisciplinary research. This work emphasizes the critical role of strategic AI adoption in maintaining competitive advantage while safeguarding societal trust and adhering to legal requirements.},
}

@article{Xiao2024,
  title = {A review of big data technology and its application in cancer care},
  author = {Tianyun Xiao and Shanshan Kong and Zichen Zhang and Dianbo Hua and Fengchun Liu},
  year = {2024},
  journal = {Computers in Biology and Medicine},
  volume = {176},
  pages = {108577},
  doi = {https://doi.org/10.1016/j.compbiomed.2024.108577},
  url = {https://www.sciencedirect.com/science/article/pii/S0010482524006620},
  abstract = {The development of modern medical devices and information technology has led to a rapid growth in the amount of data available for health protection information, with the concept of medical big data emerging globally, along with significant advances in cancer care relying on data-driven approaches. However, outstanding issues such as fragmented data governance, low-quality data specification, and data lock-in still make sharing challenging. Big data technology provides solutions for managing massive heterogeneous data while combining artificial intelligence (AI) techniques such as machine learning (ML) and deep learning (DL) to better mine the intrinsic connections between data. This paper surveys and organizes recent articles on big data technology and its applications in cancer, dividing them into three different types to outline their primary content and summarize their critical role in assisting cancer care. It then examines the latest research directions in big data technology in cancer and evaluates the current state of development of each type of application. Finally, current challenges and opportunities are discussed, and recommendations are made for the further integration of big data technology into the medical industry in the future.},
}

@article{Li2024_01,
  title = {A privacy risk identification framework of open government data: A mixed-method study in China},
  author = {Ying Li and Rui Yang and Yikun Lu},
  year = {2024},
  journal = {Government Information Quarterly},
  volume = {41},
  pages = {101916},
  doi = {https://doi.org/10.1016/j.giq.2024.101916},
  url = {https://www.sciencedirect.com/science/article/pii/S0740624X2400008X},
  abstract = {Open government data (OGD) has great potential to promote economic growth, stimulate innovation, and improve service efficiency. However, as more and more private information is collected by government information systems, private data become increasingly vulnerable. Thus, governments must monitor the privacy risks of OGD. The focus of this study is to identify privacy risk factors in the process of developing OGD. Using a mixed-method design, we developed a privacy risk identification framework based on evidence from China. According to the results of qualitative interviews, the privacy risk identification framework mainly includes five risk dimensions: data risk, institutional risk, technical risk, structural risk, and behavioral risk. We identified 17 risk factors under these five dimensions. We further developed the measurement items for each risk factor and verified the indicator framework through quantitative methods. Our research provides a theoretical basis for identifying the privacy risks in OGD, supporting governments in discovering and dealing with them accordingly. Future research can continuously explore potential privacy risks arising from merging technologies such as generative artificial intelligence when applied to OGD.},
}

@article{Phippen2025,
  title = {Artificial Intelligence},
  author = {Andy Phippen},
  year = {2025},
  pages = {3-11},
  doi = {https://doi.org/10.1016/B978-0-323-95689-5.00098-5},
  publisher = {Academic Press},
  url = {https://www.sciencedirect.com/science/article/pii/B9780323956895000985},
  abstract = {Artificial Intelligence (AI) is attracting considerable, and justified, attention about its potential and impact on information systems. However, it is important to look at this evolution against its history. AI’s historical evolution has been beset with underperformance and ethical concerns in data training and responsible deployment. Information science has undergone significant changes with AI׳s integration, impacting information retrieval, classification, and library automation. More specifically Machine Learning plays a crucial role in understanding human requirements for information and processing large data set, but challenges like bias persist. Large Language Models (LLMs) like ChatGPT represent the vanguard of public adoption of AI driven information systems and have exhibited remarkable performance in natural language processing. While they enhance information searching and content creation, users must understand limitations, biases, and practice critical thinking for responsible utilisation in a digital age.},
}

@article{Ahmed2024,
  title = {BRYT: Automated keyword extraction for open datasets},
  author = {Umair Ahmed and Charalampos Alexopoulos and Marco Piangerelli and Andrea Polini},
  year = {2024},
  journal = {Intelligent Systems with Applications},
  volume = {23},
  pages = {200421},
  doi = {https://doi.org/10.1016/j.iswa.2024.200421},
  url = {https://www.sciencedirect.com/science/article/pii/S2667305324000954},
  abstract = {In today’s information-driven world, open data is crucial in making valuable structured data freely accessible to the public. However, the absence of quality metadata often hinders the findability and representation of this data. In this study we specifically focus on keywords, proposing a strategy for their automatic generation. In particular, we employed five existing keyword extraction methodologies (BERT, RAKE, YAKE, TEXTRANK, and ChatGPT) and proposed a novel hybrid methodology, named BRYT (read as bright). Our evaluation of these algorithms was conducted using Gestalt String Matching and Jaccard Similarity techniques. We validated our study using a selection of datasets from the EU data portal, specifically choosing those that exhibited potentially high-quality metadata. This included datasets that contained a substantial number of keywords and had comprehensive, relevant metadata. The results showed that 69.1% of the dataset keywords majorly matched (more than 50% or 5 keywords), 24.7% minorly matched (up to 50% or 5 keywords), and 6.2% did not match. The proposed hybrid model, BRYT, outperformed other algorithms in the major matches, while ChatGPT was a close second. YAKE outperformed the others in minor matches, and ChatGPT was again a close second. The evaluations concluded that BRYT consistently extracted more representative keywords in major matches, highlighting its effectiveness in improving findability. This study sets up a favorable field for further representative metadata extraction and population, making the data more findable, discoverable, and accessible.},
}

@article{Germani2024,
  title = {The Dual Nature of AI in Information Dissemination: Ethical Considerations},
  author = {Federico Germani and Giovanni Spitale and Nikola Biller-Andorno},
  year = {2024},
  journal = {JMIR AI},
  volume = {3},
  doi = {https://doi.org/10.2196/53505},
  url = {https://www.sciencedirect.com/science/article/pii/S2817170524000590},
  abstract = {Infodemics pose significant dangers to public health and to the societal fabric, as the spread of misinformation can have far-reaching consequences. While artificial intelligence (AI) systems have the potential to craft compelling and valuable information campaigns with positive repercussions for public health and democracy, concerns have arisen regarding the potential use of AI systems to generate convincing disinformation. The consequences of this dual nature of AI, capable of both illuminating and obscuring the information landscape, are complex and multifaceted. We contend that the rapid integration of AI into society demands a comprehensive understanding of its ethical implications and the development of strategies to harness its potential for the greater good while mitigating harm. Thus, in this paper we explore the ethical dimensions of AI’s role in information dissemination and impact on public health, arguing that potential strategies to deal with AI and disinformation encompass generating regulated and transparent data sets used to train AI models, regulating content outputs, and promoting information literacy.},
}

@article{Zhao2025,
  title = {Decentralized governance in action: A governance framework of digital responsibility in startups},
  author = {Yangyang Zhao and Jiajun Qiu},
  year = {2025},
  journal = {Journal of Responsible Technology},
  volume = {21},
  pages = {100107},
  doi = {https://doi.org/10.1016/j.jrt.2025.100107},
  url = {https://www.sciencedirect.com/science/article/pii/S2666659625000034},
  abstract = {The rise of digital technologies has fueled the emergence of decentralized governance among startups. However, this trend imposes new challenges in digitally responsible governance, such as technology usage, business accountability, and many other issues, particularly in the absence of clear guidelines. This paper explores two types of digital startups with decentralized governance: digitally transformed (e.g., DAO) and IT-enabled decentralized startups. We adapt the previously described Corporate Digital Responsibility model into a streamlined seven-cluster governance framework that is more directly applicable to these novel organizations. Through a case study, we illustrate the practical value of the conceptual framework and find key points vital for digitally responsible governance by decentralized startups. Our findings lay a conceptual and empirical groundwork for in-depth and cross-disciplinary future inquiries into digital responsibility issues in decentralized settings.},
}

@article{Kazakov2023,
  title = {ESGify: Automated Classification of Environmental, Social, and Corporate Governance Risks},
  author = {A Kazakov and S Denisova and I Barsola and E Kalugina and I Molchanova and I Egorov and A Kosterina and E Tereshchenko and L Shutikhina and I Doroshchenko and N Sotiriadi and S Budennyy},
  year = {2023},
  journal = {Doklady Mathematics},
  volume = {108},
  pages = {S529-S540},
  doi = {10.1134/S1064562423701673},
  url = {https://doi.org/10.1134/S1064562423701673},
  abstract = {The growing recognition of environmental, social, and governance (ESG) factors in financial decision-making has spurred the need for effective and comprehensive ESG risk assessment tools. In this study, we introduce an open-source Natural Language Processing (NLP) model, “ESGify”1,2, based on MPNet-base architecture and aimed to classify texts within the frames of ESG risks. We also present a hierarchical and detailed methodology for ESG risk classification, leveraging the expertise of ESG professionals and global best practices. Anchored by a manually annotated multilabel dataset of 2000 news articles and domain adaptation with texts of sustainability reports, ESGify is developed to automate ESG risk classification following the established methodology. We compare augmentation techniques based on back translation and Large Language Models (LLMs) to improve the model quality and achieve 0.5 F1-weighted model quality in the dataset with 47 classes. This result outperforms ChatGPT 3.5 with a simple prompt. The model weights and documentation is hosted on Github https://github.com/sb-ai-lab/ESGifyunder the Apache 2.0 license.},
}

@article{Chetty2024,
  title = {AI as a Catalyst for Good Governance: Transforming South Africa’s Fight Against Corruption},
  author = {Krish Chetty and Petronella Saal and Nothando Ntshayintshayi and Nondumiso Masuku and Tahiya Moosa},
  year = {2024},
  journal = {Development},
  volume = {67},
  pages = {50-60},
  doi = {10.1057/s41301-024-00404-8},
  url = {https://doi.org/10.1057/s41301-024-00404-8},
  abstract = {Artificial Intelligence (AI) offers opportunities to advance good governance and combat corruption in South Africa, presenting a path to restore public confidence. This systematic review explores AI’s potential to enhance good governance, detect fraud, mitigate procurement risks, improve transparency, and address corruption, cybercrime, and disinformation challenges in South Africa. This article argues that the Presidency can lead a cultural shift by embracing technology as a critical enabler of good governance through AI-powered audit tools.},
}

@article{Lippolis2025,
  title = {The Water Health Open Knowledge Graph},
  author = {Anna Sofia Lippolis and Giorgia Lodi and Andrea Giovanni Nuzzolese},
  year = {2025},
  journal = {Scientific Data},
  volume = {12},
  pages = {274},
  doi = {10.1038/s41597-025-04537-4},
  url = {https://doi.org/10.1038/s41597-025-04537-4},
  abstract = {Global sustainability challenges have recently led to an increasing interest in the management of water and health resources. Thus, the availability of effective, meaningful and open data is crucial to address those issues in the broader context of the Sustainable Development Goals of clean water and sanitation as targeted by the United Nations. In this paper, we present the Water Health Open Knowledge Graph (WHOW-KG) along with its design methodology and analysis on impact. Developed in the context of the EU-funded WHOW (Water Health Open Knowledge) project, the WHOW-KG is a semantic knowledge graph that models data on water consumption, pollution, extreme weather events, infectious disease rates and drug distribution. Indeed, it aims at supporting a wide range of applications: from knowledge discovery to decision-making, making it a valuable resource for researchers, policymakers, and practitioners in the water and health domains. The WHOW-KG consists of a network of five ontologies and related linked open data, modelled according to those ontologies. As a fully distributed system, it is sustainable over time, can handle large datasets, and allows data providers full control, establishing it as a vital European asset in the fields of water consumption and pollution.},
}

@article{Eke2024,
  title = {Ethics in the Governance of Data and Digital Technology: An Analysis of European Data Regulations and Policies},
  author = {Damian Eke and Bernd Stahl},
  year = {2024},
  journal = {Digital Society},
  volume = {3},
  pages = {11},
  doi = {10.1007/s44206-024-00101-6},
  url = {https://doi.org/10.1007/s44206-024-00101-6},
  abstract = {Addressing ethical concerns is among the fundamental motivations for the development of policies and regulations for data and digital technologies. In the last few years, the European Commission has issued a number of policies, regulations and legislative proposals for socially desirable and legally compliant data governance for technologies which have ethical implications. What is not obvious, however, is whether and in what way ethics are included explicitly in the way these policies and regulations are created and implemented to address data governance challenges. Given the increasing amount of available digital data, its use for AI and other purposes and the growing amount of regulatory activity around data, this paper explores the role ethics plays in these documents. We examined eight of these documents to map the ethical concerns and justifications underlining their provisions, the ethical principles they promote and the implementation approaches recommended. Our analysis shows that the current EU data governance policy landscape can be read from an ethical perspective as being grounded in ethical thinking, typically expressed in terms of human rights, aware of likely concerns, based on well-established principles and in the process of being codified in regulation, legislation and institutions. However, the practical implementation of these principles, for instance how conflicts among these principles can be resolved, remain unclear.},
}

@article{Ford2025,
  title = {What is the patient re-identification risk from using de-identified clinical free text data for health research?},
  author = {Elizabeth Ford and Simon Pillinger and Robert Stewart and Kerina Jones and Angus Roberts and Arlene Casey and Katie Goddard and Goran Nenadic},
  year = {2025},
  journal = {AI and Ethics},
  doi = {10.1007/s43681-025-00681-0},
  url = {https://doi.org/10.1007/s43681-025-00681-0},
  abstract = {Important clinical information is recorded in free text in patients’ records, notes, letters and reports in healthcare settings. This information is currently under-used for health research and innovation. Free text requires more processing for analysis than structured data, but processing natural language at scale has recently advanced, using large language models. However, data controllers are often concerned about patient privacy risks if clinical text is allowed to be used in research. Text can be de-identified, yet it is challenging to quantify the residual risk of patient re-identification. This paper presents a comprehensive review and discussion of elements for consideration when evaluating the risk of patient re-identification from free text. We consider (1) the reasons researchers want access to free text; (2) the accuracy of automated de-identification processes, identifying best practice; (3) methods previously used for re-identifying health data and their success; (4) additional protections put in place around health data, particularly focussing on the UK where “Five Safes” secure data environments are used; (5) risks of harm to patients from potential re-identification and (6) public views on free text being used for research. We present a model to conceptualise and evaluate risk of re-identification, accompanied by case studies of successful governance of free text for research in the UK. When de-identified and stored in secure data environments, the risk of patient re-identification from clinical free text is very low. More health research should be enabled by routinely storing and giving access to de-identified clinical text data.},
}

@article{Wörsdörfer2025,
  title = {AI ethics and ordoliberalism 2.0: towards a ‘Digital Bill of Rights’},
  author = {Manuel Wörsdörfer},
  year = {2025},
  journal = {AI and Ethics},
  volume = {5},
  pages = {507-525},
  doi = {10.1007/s43681-023-00367-5},
  url = {https://doi.org/10.1007/s43681-023-00367-5},
  abstract = {This article analyzes AI ethics from a distinct business ethics perspective, i.e., ‘ordoliberalism 2.0.’ It argues that the ongoing discourse on (generative) AI relies too much on corporate self-regulation and voluntary codes of conduct and thus lacks adequate governance mechanisms. To address these issues, the paper suggests not only introducing hard-law legislation with a more effective oversight structure but also merging already existing AI guidelines with an ordoliberal-inspired regulatory and competition policy. However, this link between AI ethics, regulation, and antitrust is not yet adequately discussed in the academic literature and beyond. The paper thus closes a significant gap in the academic literature and adds to the predominantly legal-political and philosophical discourse on AI governance. The paper’s research questions and goals are twofold: first, it identifies ordoliberal-inspired AI ethics principles that could serve as the foundation for a ‘digital bill of rights.’ Second, it shows how those principles could be implemented at the macro level with the help of ordoliberal competition and regulatory policy.},
}

@article{Jiang2024,
  title = {Integrating prior knowledge to build transformer models},
  author = {Pei Jiang and Takashi Obi and Yoshikazu Nakajima},
  year = {2024},
  journal = {International Journal of Information Technology},
  volume = {16},
  pages = {1279-1292},
  doi = {10.1007/s41870-023-01635-7},
  url = {https://doi.org/10.1007/s41870-023-01635-7},
  abstract = {The big Artificial General Intelligence models inspire hot topics currently. The black box problems of Artificial Intelligence (AI) models still exist and need to be solved urgently, especially in the medical area. Therefore, transparent and reliable AI models with small data are also urgently necessary. To build a trustable AI model with small data, we proposed a prior knowledge-integrated transformer model. We first acquired prior knowledge using Shapley Additive exPlanations from various pre-trained machine learning models. Then, we used the prior knowledge to construct the transformer models and compared our proposed models with the Feature Tokenization Transformer model and other classification models. We tested our proposed model on three open datasets and one non-open public dataset in Japan to confirm the feasibility of our proposed methodology. Our results certified that knowledge-integrated transformer models perform better (1%) than general transformer models. Meanwhile, our proposed methodology identified that the self-attention of factors in our proposed transformer models is nearly the same, which needs to be explored in future work. Moreover, our research inspires future endeavors in exploring transparent small AI models.},
}

@article{Androutsopoulou2024,
  title = {Leveraging AI for Enhanced eGovernment: Optimizing the Use of Open Governmental Data},
  author = {Maro Androutsopoulou and Dimitrios Askounis and Elias G Carayannis and Nikos Zotas},
  year = {2024},
  journal = {Journal of the Knowledge Economy},
  doi = {10.1007/s13132-024-02317-w},
  url = {https://doi.org/10.1007/s13132-024-02317-w},
  abstract = {Artificial intelligence (AI) is rapidly transforming various sectors and holds significant potential for improving government operations, specifically in the realm of eGovernment. This paper explores how AI can pave the way for Gov4 by optimizing the use of open governmental data. Despite AI’s limited adoption in the public sector, it presents a disruptive technology that can drive transparency, innovation, and economic growth. This study examines pioneering governments that have successfully integrated AI to handle and open governmental data, assesses the challenges they faced, and provides recommendations to achieve the desired outcomes.},
}

@article{Alexopoulos2024,
  title = {Assessing the availability and interoperability of open government data (OGD) supporting sustainable development goals (SDGs) and value creation in the gulf cooperation council (GCC)},
  author = {Charalampos Alexopoulos and Mohsan Ali and Maria Ioanna Maratsi and Nina Rizun and Yannis Charalabidis and Euripidis Loukis and Stuti Saxena},
  year = {2024},
  journal = {Quality & Quantity},
  doi = {10.1007/s11135-024-02025-2},
  url = {https://doi.org/10.1007/s11135-024-02025-2},
  abstract = {Value creation and innovation by a range of stakeholders, including citizens, analysts, journalists, non-profit entities, etc. are the hallmarks of Open Government Data (OGD) initiatives. At the same time, availability and interoperability of datasets are determined as two of the most important factors for value creation. In parallel, the United Nations’ Sustainable Development Goals (SDGs) are meant to be realized to attain quality of life through the development of initiatives based on the value creation and innovation provided by the afore-identified stakeholders. Thus, the information provided from the public sector (OGD) regarding SDGs would help the monitoring of current and the identification of next actions and initiatives. The examination of SDG datasets availability and the assessment of their interoperability would provide valuable insights regarding the extent to which the data are of high-value. In that vein, the availability and the interoperability dimensions of the OGD provisioned via the national OGD portals of the six Gulf Cooperation Council (GCC) constituents; viz., Bahrain, Qatar, Oman, Kuwait, United Arab Emirates (UAE) and Saudi Arabia, are studied based on a semi-automatic methodological approach. In this light, the present study seeks to answer the following research question: “To what extent are the national OGD portals of the GCC region catering for the interoperability dimensions, more specifically, the semantic interoperability, to facilitate value creation and innovation?” To drive home the arguments, semantic interoperability dimensions were investigated via the cosine similarity calculations in Python to understand the extent to which the availability of OGD via the national OGD portals facilitates their interoperability. Findings show that the value creation and innovation initiatives to realize the SDGs’ attainment is dependent upon both the availability and the extent of interoperability for all SDGs. GCC countries present different levels of both factors. This is suggestive of the mismatch of the OGD provision and their attributes which results in low interoperability. Findings from the study are indicators that the GCC countries should develop different strategies regarding the availability and the interoperability of SDG-related OGD in order to stimulate innovation and value creation.},
}

@article{Siciliani2024,
  title = {OIE4PA: open information extraction for the public administration},
  author = {Lucia Siciliani and Eleonora Ghizzota and Pierpaolo Basile and Pasquale Lops},
  year = {2024},
  journal = {Journal of Intelligent Information Systems},
  volume = {62},
  pages = {273-294},
  doi = {10.1007/s10844-023-00814-z},
  url = {https://doi.org/10.1007/s10844-023-00814-z},
  abstract = {Tenders are powerful means of investment of public funds and represent a strategic development resource. Despite the efforts made so far by governments at national and international levels to digitalise documents related to the Public Administration sector, most of the information is still available in an unstructured format only. With the aim of bridging this gap, we present OIE4PA, our latest study on extracting and classifying relations from tenders of the Public Administration. Our work focuses on the Italian language, where the availability of linguistic resources to perform Natural Language Processing tasks is considerably limited. Nevertheless, OIE4PA adopts a multilingual approach so it can be applied to several languages by providing appropriate training data. Rather than purely training a classifier on a portion of the extracted relations, the backbone idea of our learning strategy is to put a supervised method based on self-training to the proof and to assess whether or not it improves the performance of the classifier. For evaluation purposes, we built a dataset composed of 2,000 triples which have been manually annotated by two human experts. The in-vitro evaluation shows that OIE4PA achieves a MacroF$$_1$$equal to 0.89 and a 91$$\%$$accuracy. In addition, OIE4PA was used as the pillar of a prototype search engine, which has been evaluated through an in-vivo experiment with positive feedback from 32 final users, obtaining a SUS score equal to 83.98.},
}

@article{Mollen2024,
  title = {LLMs beyond the lab: the ethics and epistemics of real-world AI research},
  author = {Joost Mollen},
  year = {2024},
  journal = {Ethics and Information Technology},
  volume = {27},
  pages = {6},
  doi = {10.1007/s10676-024-09819-w},
  url = {https://doi.org/10.1007/s10676-024-09819-w},
  abstract = {Research under real-world conditions is crucial to the development and deployment of robust AI systems. Exposing large language models to complex use settings yields knowledge about their performance and impact, which cannot be obtained under controlled laboratory conditions or through anticipatory methods. This epistemic need for real-world research is exacerbated by large-language models’ opaque internal operations and potential for emergent behavior. However, despite its epistemic value and widespread application, the ethics of real-world AI research has received little scholarly attention. To address this gap, this paper provides an analysis of real-world research with LLMs and generative AI, assessing both its epistemic value and ethical concerns such as the potential for interpersonal and societal research harms, the increased privatization of AI learning, and the unjust distribution of benefits and risks. This paper discusses these concerns alongside four moral principles influencing research ethics standards: non-maleficence, beneficence, respect for autonomy, and distributive justice. I argue that real-world AI research faces challenges in meeting these principles and that these challenges are exacerbated by absent or imperfect current ethical governance. Finally, I chart two distinct but compatible ways forward: through ethical compliance and regulation and through moral education and cultivation.},
}

@article{Rizun2025,
  title = {Text analytics for co-creation in public sector organizations: a literature review-based research framework},
  author = {Nina Rizun and Aleksandra Revina and Noella Edelmann},
  year = {2025},
  journal = {Artificial Intelligence Review},
  volume = {58},
  pages = {125},
  doi = {10.1007/s10462-025-11112-1},
  url = {https://doi.org/10.1007/s10462-025-11112-1},
  abstract = {The public sector faces considerable challenges that stem from increasing external and internal demands, the need for diverse and complex services, and citizens’ lack of satisfaction and trust in public sector organisations (PSOs). An alternative to traditional public service delivery is the co-creation of public services. Data analytics has been fueled by the availability of immense amounts of data, including textual data, and techniques to analyze data, so it has immense potential to foster data-driven solutions for the public sector. In the paper, we systematically review the existing literature on the application of Text Analytics (TA) techniques on textual data that can support public service co-creation. In this review, we identify the TA techniques, the public services and the co-creation phase they support, as well as envisioned public values for the stakeholder groups. On the basis of the analysis, we develop a Research Framework that helps to structure the TA-enabled co-creation process in PSOs, increases awareness among public sector organizations and stakeholders on the significant potential of TA in creating value, and provides scholars with some avenues for further research.},
}

@article{Vaca2024,
  title = {Interpretability of deep learning models in analysis of Spanish financial text},
  author = {César Vaca and Manuel Astorgano and Alfonso J López-Rivero and Fernando Tejerina and Benjamín Sahelices},
  year = {2024},
  journal = {Neural Computing and Applications},
  volume = {36},
  pages = {7509-7527},
  doi = {10.1007/s00521-024-09474-8},
  url = {https://doi.org/10.1007/s00521-024-09474-8},
  abstract = {Artificial intelligence methods based on deep learning (DL) have recently made significant progress in many different areas including free text classification and sentiment analysis. We believe that corporate governance is one of these areas, where DL can generate very valuable and differential knowledge, for example, by analyzing the biographies of independent directors, which allows for qualitative modeling of their profile in an automatic way.  For this technology to be accepted it is important to be able to explain how it generates its results. In this work we have developed a six-dimensional labeled dataset of independent director biographies, implemented three recurrent DL models based on LSTM and transformers along with four ensembles, one of which is an innovative proposal based on a multi-layer perceptron (MLP), trained them using Spanish language and economics and finance terminology and performed a comprehensive test study that demonstrates the accuracy of the results. We have also performed a complete study of explainability using the SHAP methodology by comparatively analyzing the developed models. We have achieved a mean error (MAE) of 8% in the modeling of the open text biographies, which has allowed us to perform a case study of time analysis that has detected significant variations in the composition of the Standard Expertise Profile (SEP) of the boards of directors, related to the crisis of the period 2008–2013. This work shows that DL technology can be accurately applied to free text analysis in the finance and economic domain, by automatically analyzing large volumes of data to generate knowledge that would have been unattainable by other means.},
}

@article{Zehner2024,
  title = {Dreaming of AI: environmental sustainability and the promise of participation},
  author = {Nicolas Zehner and André Ullrich},
  year = {2024},
  journal = {AI & SOCIETY},
  doi = {10.1007/s00146-024-02011-0},
  url = {https://doi.org/10.1007/s00146-024-02011-0},
  abstract = {There is widespread consensus among policymakers that climate change and digitalisation constitute the most pressing global transformations shaping human life in the 21st century. Seeking to address the challenges arising at this juncture, governments, technologists and scientists alike increasingly herald artificial intelligence (AI) as a vehicle to propel climate change mitigation and adaptation. In this paper, we explore the intersection of digitalisation and climate change by examining the deployment of AI in government-led climate action. Building on participant observations conducted in the context of the “Civic Tech Lab for Green”—a government-funded public interest AI initiative—and eight expert interviews, we investigate how AI shapes the negotiation of environmental sustainability as an issue of public interest. Challenging the prescribed means–end relationship between AI and environmental protection, we argue that the unquestioned investment in AI curtails political imagination and displaces discussion of climate “problems” and possible “solutions” with “technology education”. This line of argumentation is rooted in empirical findings that illuminate three key tensions in current coproduction efforts: “AI talk vs. AI walk”, “civics washing vs. civics involvement” and “public invitation vs. public participation”. Emphasising the importance of re-exploring the innovative state in climate governance, this paper extends academic literature in science and technology studies that examines public participation in climate change adaptation by shedding light on the emergent phenomenon of public interest AI.},
}

@article{Shaban2024_01,
  title = {Digitalization and Exclusion—Digital Divides and Development},
  author = {Abdul Shaban},
  year = {2024},
  pages = {255-496},
  doi = {10.1007/978-981-97-4734-4_4},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-97-4734-4_4},
  abstract = {Despite the massive social deepening and spread of digital technologies, there are groups of the people who are facing exclusions from the same. The exclusions are often driven along some social planes, geographies, or individual attributes. Such social planes and personal attributes are class, caste, religion, language, race, education, age, gender, disabilities, etc. As digital technologies become important tools in governance processes, media, and the market, the exclusion can have massive spatial, social, and individual consequences. They may lead to educational deprivation, lack of access to health care, democratic participation, exclusion in smart cities, financial exclusion, and even from citizenship which is now being determined by digital inclusion. Additionally, the exclusions of certain groups of people and individuals can also result from digitally enabled surveillance as today digital cameras or CCTVs have become ubiquitous and often lead to recognition of those who face exclusion through monitoring.},
}

@article{Mureddu2025,
  title = {Rights and Responsibilities: Legal and Ethical Considerations in Adopting Local Digital Twin Technology},
  author = {Francesco Mureddu and Alessandro Paciaroni and Tomáš Pavelka and Annabel Pemberton and Luca Alessandro Remotti},
  year = {2025},
  pages = {291-317},
  doi = {10.1007/978-3-031-81451-8_11},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-81451-8_11},
  abstract = {As Local Digital Twin (LDT) technology becomes increasingly complex and integrated with disruptive technologies, like Artificial Intelligence (AI) and Machine Learning (ML), new ethical, legal and transparency challenges arise. Particular concerns centre around personal information protection, data usage, and the ethics of algorithmic decision-making. This paper explores these challenges, including both personal and non-personal data governance, future regulatory impact, and the implications of new and proposed legislation, including e-privacy, the Digital Markets Act, and the AI Act. Drawing on the perspectives of various scholars, as well as European digital twin projects, this paper emphasises the importance of human intelligence in balancing the impacts of AI and ML, the need for societal assessment frameworks to integrate broader social elements, and the necessity of transparency in decision-making processes. It proposes strategies for the responsible and ethical implementation of LDTs, emphasising risk mitigation measures, such as using trusted data processors, ensuring data transparency, and adopting robust data governance frameworks to protect fundamental rights and freedoms. This paper also aims to contribute to the ongoing discourse by highlighting legal frameworks, underlining ethical aspects, and facilitating the practical application of ethics and relevant legislation through gap analysis methodologies. Alternative ways to manage LDT data and information are also presented, reflecting the key role of data and information management in this domain.},
}

@article{Fan2025_02,
  title = {AI-Driven Smart City Security and Surveillance System: A Bibliometric Analysis},
  author = {Wong Qi Fan and Aalima Shaura Ismail and Fathey Mohammed and Muaadh Mukred},
  year = {2025},
  pages = {305-328},
  doi = {10.1007/978-3-031-75091-5_17},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-75091-5_17},
  abstract = {Integrating advanced technologies such as artificial intelligence (AI) has become crucial for enhancing urban management and security as urban centers evolve into smart cities. This study conducts a bibliometric analysis to explore smart cities' security and surveillance dimensions, focusing on data protection and ethical AI use. By reviewing 745 articles indexed in Scopus from 1977 to 2023, we employ co-citation analysis, co-occurrence analysis, and bibliographic coupling to identify key thematic clusters and influential publications. The findings reveal the complex interplay between technological advancements and privacy concerns, highlighting the importance of a balanced approach to AI integration. This research provides valuable insights for urban planners and policymakers to develop strategies that enhance urban safety and efficiency while respecting individual privacy and fostering public trust. The study underscores the necessity of ethical standards and robust governance frameworks in deploying AI-powered security systems in smart cities.},
}

@article{Molina-Carmona2025,
  title = {Safeguarding Knowledge: Ethical Artificial Intelligence Governance in the University Digital Transformation},
  author = {Rafael Molina-Carmona and Francisco José García-Peñalvo},
  year = {2025},
  pages = {201-220},
  doi = {10.1007/978-3-031-71530-3_14},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-71530-3_14},
  abstract = {Higher Education Institutions (HEIs) safeguard knowledge, uphold aca-demic integrity, and contribute to societal progress. They are custodians of knowledge, promoting innovation, addressing societal challenges, and disseminating research ethically. With the rise of Artificial Intelligence (AI), effective governance becomes crucial to ensure responsible use, protect rights, and foster innovation in HEIs. A proposal for a governance framework for AI in Higher Education is presented, designed to be simple, tailored to universities, and easily integrated into existing digital transformation efforts. Specific goals include examining AI's impact, evaluating governance models, suggesting adaptable principles, and defining a framework that balances innovation, ethics, and regulatory compliance. It takes into account that AI in higher education reshapes teaching, research, and administration, and makes emphasis on ethical deployment and observation of the national and international policies and regulations. The proposal sets out four fundamental principles for AI in universities to be applied to every phase of knowledge generation: the principles of legality, neutrality, transparency, and promotion of innovation. As a consequence, the AI Governance Grid is obtained, that allows the identification of 12 key aspects to consider in order to ensure that the governance proposal complies with the principles. A structure for AI Governance is also proposed so that it is efficient and also takes advantage of the expertise that universities already have, as well as being in line with the international standards for IT Governance. Finally, a set of best practices for AI governance is also proposed that aims to provide practical guidance for simple implementation.},
}

@article{Carlizzi2025,
  title = {Public Administration Reengineered Applying AI Models Towards Precision Governance},
  author = {Demetrio Naccari Carlizzi},
  year = {2025},
  pages = {61-84},
  doi = {10.1007/978-3-031-73880-7_6},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-73880-7_6},
  abstract = {The PA’s transition towards the implementation of automation tools, digital solutions and the use of artificial intelligence algorithms is progressively eroding the traditional public model. The definition of an organisational model consistent with such technological innovations is held back by the removal of the role of the Algorithm and by cultural barriers such as the formal-hierarchical ecosystem placed as a guarantee of the citizen towards the authority, as well as by the disruptive and exponential progress of artificial intelligence techniques that continuously move the horizon of a final model forward. This paper intends to contribute to the modelling of an organisational system designed on the basis of artificial intelligence (AI) and web 3.0 technologies and data science to build, from digital services and digital administrative procedure, a decision-making process in local government that maximises efficiency and public value, realising precision government. Starting from the bottom, from digital administrative processes and re-engineering of the organisational structure, we can focus on a transition model to implement digital technologies. For a true digital transformation, which maximises artificial intelligence technologies in a new administrative paradigm, it is instead necessary to holistically rethink the public model starting from native digital services, to then design coherent procedures and an organisation that is sensitive and adaptive to the PA’s programmatic objectives. Without accepting the role of the machine and recognising to the artificial intelligence algorithm a strategic role in assisting and implementing public choices, the transition will be severely limited. AI techniques can help us instead to build, from the organisation of structures, a new administrative capacity to read society. The paper, starting from the existing literature, illustrates the background and the traditional model of public administration and reports the case of the digitisation of the Urban Planning sector of the Municipality of Reggio Calabria, a city in Southern Italy, with the trade-off in terms of productivity. A path is indicated for the realisation of an intermediate model of transition towards an entirely digital paradigm that, starting from data, redesigns the organisational structure and builds a dematerialised, data-driven, effective and therefore accurate public decision-making and administrative action.},
}

@article{Gimpel2024,
  title = {Toward Open-Source AI Systems as Digital Public Goods: Definitions, Hopes and Challenges},
  author = {Lea Gimpel},
  year = {2024},
  pages = {129-142},
  doi = {10.1007/978-3-031-61187-2_8},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-61187-2_8},
  abstract = {The recent advancements in artificial intelligence (AI), especially generative AI, have ignited a vivid debate about the potential of AI for positively impacting societies at large and attaining the United Nations Sustainable Development Goals (SDGs), but also about its risks and challenges. For the global majority, this debate is also linked to pressing questions about the distribution of (economic) power and influence, representation, diversity, access, and ownership. To address these issues, the democratization of artificial intelligence’s development, use, benefits, and governance is at the core of the debate, and with it, the importance of open-source AI to provide widespread access to these technologies, including the freedom of using, studying, sharing, and modifying the underlying AI models. This text examines the risks and opportunities associated with disseminating AI models and related components openly and unpacks the current debate about the definition of open-source AI. It finds that the term “open-source” AI is not well-defined and is used in misleading ways. It then explores the intersection of open-source AI systems and digital public goods (DPGs). Contrary to open-source software, DPGs have SDG relevance at their core, adding a layer of purpose to an otherwise technical and legal definition. In a nutshell, DPGs are tools that are open and accessible and can be adapted to local contexts to contribute to inclusive development. AI DPGs will be essential in addressing urgent global challenges and attaining the SDGs by 2030. However, apart from making AI technologies available under an open-source paradigm, additional aspects must be addressed to help democratize AI in all its facets. This includes looking beyond the hype of generative AI and systematically exploring other AI technologies for under-resourced contexts, leveraging AI research innovations to improve training efficiency and safety, creating open, high-value datasets for under-represented contexts, novel data governance approaches and licenses that benefit data subjects and communities over corporations, and policies and regulations addressing market concentration and enable access to essential resources. These efforts must be complemented by enhancing human, organizational, and societal capacity to shape AI development locally and support under-represented groups to partake in AI governance decisions globally.},
}

@inproceedings{Nikiforova2024,
  title = {From the Evolution of Public Data Ecosystems to the Evolving Horizons ofthe Forward-Looking Intelligent Public Data Ecosystem Empowered byEmerging Technologies},
  author = {Nikiforova, Anastasija and Lnenicka, Martin and Milic, Petar andLuterek, Mariusz and Rodriguez Bolivar, Manuel Pedro},
  year = {2024},
  volume = {14841},
  pages = {402-418},
  doi = {10.1007/978-3-031-70274-7\_25},
  abstract = {Public Data Ecosystems (PDEs) represent complex socio-technical systems crucial for optimizing data use in the public sector and outside it. Recognizing their multifaceted nature, previous research proposed a six-generation Evolutionary Model of Public Data Ecosystems (EMPDE). Designed as a result of a systematic literature review on the topic spanning three decades, this model, while theoretically robust, necessitates empirical validation to enhance its practical applicability. This study addresses this gap by validating the theoretical model through a real-life examination in five European countries - Latvia, Serbia, Czech Republic, Spain, and Poland. This empirical validation provides insights into PDEs dynamics and variations of implementations across contexts, particularly focusing on the 6th generation of forward-looking PDE generation named ``Intelligent Public Data Generation{''} which represents a paradigm shift driven by emerging technologies such as cloud computing, Artificial Intelligence (AI), Natural Language Processing tools, Generative AI, and Large Language Models with potential to contribute to both automation and augmentation of business processes within these ecosystems. By transcending their traditional status as a mere component, evolving into both an actor and a stakeholder simultaneously, these technologies catalyse innovation and progress, enhancing PDE management strategies to align with societal, regulatory, and technical imperatives in the digital era.},
}

@article{Bratto2024,
  title = {Linguistics-based dialogue simulations to evaluate argumentative conversational recommender systems},
  author = {Martina Di Bratto and Antonio Origlia and Maria Di Maro and Sabrina Mennella},
  year = {2024},
  journal = {User Modeling and User-Adapted Interaction},
  volume = {34},
  pages = {1581-1611},
  doi = {10.1007/s11257-024-09403-3},
  url = {https://doi.org/10.1007/s11257-024-09403-3},
  abstract = {Conversational recommender systems aim at recommending the most relevant information for users based on textual or spoken dialogues, through which users can communicate their preferences to the system more efficiently. Argumentative conversational recommender systems represent a kind of deliberation dialogue in which participants share their specific beliefs in the respective representations of the common ground, to act towards a common goal. The goal of such systems is to present appropriate supporting arguments to their recommendations to show the interlocutor that a specific item corresponds to their manifested interests. Here, we present a cross-disciplinary argumentation-based conversational recommender model based on cognitive pragmatics. We also present a dialogue simulator to investigate the quality of the theoretical background. We produced a set of synthetic dialogues based on a computational model implementing the linguistic theory and we collected human evaluations about the plausibility and efficiency of these dialogues. Our results show that the synthetic dialogues obtain high scores concerning their naturalness and the selection of the supporting arguments.},
}

@article{Alam2024,
  title = {Navigating food price shocks in a pandemic: Food insecurity and coping mechanisms in Burkina Faso},
  author = {Shamma Adeeb Alam and Shi Xi Liu and Claus C. Pörtner},
  year = {2024},
  journal = {World Development},
  volume = {182},
  pages = {106714},
  doi = {https://doi.org/10.1016/j.worlddev.2024.106714},
  url = {https://www.sciencedirect.com/science/article/pii/S0305750X24001840},
  abstract = {Global food prices rose substantially after the start of the COVID-19 pandemic. This paper examines the impact of rising food prices during the pandemic on food security in Burkina Faso. We aim to answer two primary questions. First, how do food price shocks affect household food insecurity? Second, what coping strategies do households adopt in response to these price shocks? Leveraging country-wide high-frequency longitudinal data, we employ household fixed effect models to examine the effects. In the absence of direct information on local food prices, we use household-reported price shocks to capture province-level price increases and show that the results are consistent with national-level price increases. We find significant and immediate increases in food insecurity following the price shocks, and this effect persists for at least two months. The price shocks most acutely affected the poorest households. Furthermore, food insecurity increased more in rural areas than in urban areas. The higher proportion of poorer households in rural areas explains part of this difference. We find that households primarily cope with the shock by relying on increased assistance from relatives in Burkina Faso and abroad. This study is the first to use panel data with household fixed effects to examine the repercussions of the rise in food prices during the pandemic on food insecurity in a developing country and to examine the coping mechanisms employed by households. Given that food prices are likely to remain high globally for an extended period, our findings carry implications for the broader developing world. Furthermore, given the disproportionate effect on the poorest and those living in rural areas, the findings highlight the need for policies to mitigate the negative impacts of the price shocks and enhance overall food security in countries like Burkina Faso.},
}

@article{Uraguchi2024,
  title = {Pediatric otitis media in Japan: A nationwide longitudinal study of the pre- and post-pneumococcal conjugate vaccine eras born in 2001 and 2010},
  author = {Kensuke Uraguchi and Naomi Matsumoto and Toshiharu Mitsuhashi and Soshi Takao and Seiichiro Makihara and Mizuo Ando and Takashi Yorifuji},
  year = {2024},
  journal = {Vaccine},
  volume = {42},
  pages = {4081-4087},
  doi = {https://doi.org/10.1016/j.vaccine.2024.05.020},
  url = {https://www.sciencedirect.com/science/article/pii/S0264410X2400567X},
  abstract = {Background Otitis media (OM) is a prevalent respiratory disease in children and poses significant public health challenges due to its impact on child health and economic burdens. However, there have no nationwide epidemiological studies conducted in Japan. This study investigates the epidemiological trends of OM in Japan, taking into account the impact of the 7-valent pneumococcal conjugate vaccine (PCV7) introduction. Method This study was retrospective cohort study using secondary data on the nationwide longitudinal birth cohort. This survey followed two cohorts born in 2001 (pre-PCV era) and 2010 (post-PCV era) until the age of 9. Every year, parents were surveyed about their children’s health status, including occurrences of OM. The annual period prevalence and cumulative incidence of OM were assessed in this study, and the two cohorts were compared using a modified Poisson regression model adjusted environmental factors with the 2001 cohort as reference. Result The study included 47,015 children from the 2001 cohort and 38,554 from the 2010 cohort. Peak annual period prevalence of OM varied by era. Cumulative incidence was 13.8 % for the 2001 cohort and 18.5 % for the 2010 cohort by 1.5 years of age and 28.9 % and 33.3 %, respectively, by 3.5 years of age. In particular, from the fourth survey onward, covering ages 2.5–3.5 years, a shift was observed from an increased risk to a decreased risk of OM. Conclusion This nationwide longitudinal study emphasizes variations in OM epidemiology across Japan over time, with changes potentially influenced by the introduction of PCV7. In this study, due to the absence of individual PCV7 vaccination data, the effect of PCV7 was estimated based on the vaccination rate at the population level. The results suggest a notable decrease in the incidence of OM in later years, aligning with the increased uptake of PCV7.},
}

@article{Chen2024_02,
  title = {When large language models meet personalization: perspectives of challenges and opportunities},
  author = {Jin Chen and Zheng Liu and Xu Huang and Chenwang Wu and Qi Liu and Gangwei Jiang and Yuanhao Pu and Yuxuan Lei and Xiaolong Chen and Xingmei Wang and Kai Zheng and Defu Lian and Enhong Chen},
  year = {2024},
  journal = {World Wide Web},
  volume = {27},
  pages = {42},
  doi = {10.1007/s11280-024-01276-1},
  url = {https://doi.org/10.1007/s11280-024-01276-1},
  abstract = {The advent of large language models marks a revolutionary breakthrough in artificial intelligence. With the unprecedented scale of training and model parameters, the capability of large language models has been dramatically improved, leading to human-like performances in understanding, language synthesizing, common-sense reasoning, etc. Such a major leap forward in general AI capacity will fundamentally change the pattern of how personalization is conducted. For one thing, it will reform the way of interaction between humans and personalization systems. Instead of being a passive medium of information filtering, like conventional recommender systems and search engines, large language models present the foundation for active user engagement. On top of such a new foundation, users’ requests can be proactively explored, and users’ required information can be delivered in a natural, interactable, and explainable way. For another thing, it will also considerably expand the scope of personalization, making it grow from the sole function of collecting personalized information to the compound function of providing personalized services. By leveraging large language models as a general-purpose interface, the personalization systems may compile user’s requests into plans, calls the functions of external tools (e.g., search engines, calculators, service APIs, etc.) to execute the plans, and integrate the tools’ outputs to complete the end-to-end personalization tasks. Today, large language models are still being rapidly developed, whereas the application in personalization is largely unexplored. Therefore, we consider it to be right the time to review the challenges in personalization and the opportunities to address them with large language models. In particular, we dedicate this perspective paper to the discussion of the following aspects: the development and challenges for the existing personalization system, the newly emerged capabilities of large language models, and the potential ways of making use of large language models for personalization.},
}

@article{Ramos2025,
  title = {Mapping e-commerce trends in the USA: a time series and deep learning approach},
  author = {Filipe R Ramos and Luisa M Martinez and Luis F Martinez and Ricardo Abreu and Lihki Rubio},
  year = {2025},
  journal = {Journal of Marketing Analytics},
  doi = {10.1057/s41270-025-00392-9},
  url = {https://doi.org/10.1057/s41270-025-00392-9},
  abstract = {Driven by digitalization and accelerated by the COVID-19 pandemic, e-commerce has experienced strong growth, especially in the last 4 years. This transformation has reshaped consumer behavior, business models, and workplace dynamics, where digitalization, such as artificial intelligence and automation, has improved operational efficiency, personalization, and market reach. This study explores these dynamics and provides an overview of e-commerce in the U.S. through a time series approach, analyzing five key variables: sales, employment, hours worked, costs, and the producer price index. It also models and forecasts sales and the producer price index using classic, deep learning, and hybrid methods. The results show that while sales have increased, employment and labor hours have fallen, alongside stable production costs and a reduction in the producer price index over the past 2 years. In forecasting, deep neural networks offer superior predictive performance, although classic methods provide similarly accurate results in series with clear trends and seasonality, making them a more computationally efficient alternative. This research contributes to decision-making in e-commerce by exploring the relationships between sales growth and labor market dynamics, evaluating the effectiveness of different forecasting methods, and highlighting the need for strategic adaptability in a digitalized sector.},
}

@article{Park2024,
  title = {Predictors of Medical and Dental Clinic Closure by Machine Learning Methods: Cross-Sectional Study Using Empirical Data},
  author = {Young-Taek Park and Donghan Kim and Ji Soo Jeon and Kwang Gi Kim},
  year = {2024},
  journal = {Journal of Medical Internet Research},
  volume = {26},
  doi = {https://doi.org/10.2196/46608},
  url = {https://www.sciencedirect.com/science/article/pii/S1438887124005363},
  abstract = {Background Small clinics are important in providing health care in local communities. Accurately predicting their closure would help manage health care resource allocation. There have been few studies on the prediction of clinic closure using machine learning techniques. Objective This study aims to test the feasibility of predicting the closure of medical and dental clinics (MCs and DCs, respectively) and investigate important factors associated with their closure using machine running techniques. Methods The units of analysis were MCs and DCs. This study used health insurance administrative data. The participants of this study ran and closed clinics between January 1, 2020, and December 31, 2021. Using all closed clinics, closed and run clinics were selected at a ratio of 1:2 based on the locality of study participants using the propensity matching score of logistic regression. This study used 23 and 19 variables to predict the closure of MCs and DCs, respectively. Key variables were extracted using permutation importance and the sequential feature selection technique. Finally, this study used 5 and 6 variables of MCs and DCs, respectively, for model learning. Furthermore, four machine learning techniques were used: (1) logistic regression, (2) support vector machine, (3) random forest (RF), and (4) Extreme Gradient Boost. This study evaluated the modeling accuracy using the area under curve (AUC) method and presented important factors critically affecting closures. This study used SAS (version 9.4; SAS Institute Inc) and Python (version 3.7.9; Python Software Foundation). Results The best-fit model for the closure of MCs with cross-validation was the support vector machine (AUC 0.762, 95% CI 0.746-0.777; P<.001) followed by RF (AUC 0.736, 95% CI 0.720-0.752; P<.001). The best-fit model for DCs was Extreme Gradient Boost (AUC 0.700, 95% CI 0.675-0.725; P<.001) followed by RF (AUC 0.687, 95% CI 0.661-0.712; P<.001). The most significant factor associated with the closure of MCs was years of operation, followed by population growth, population, and percentage of medical specialties. In contrast, the main factor affecting the closure of DCs was the number of patients, followed by annual variation in the number of patients, year of operation, and percentage of dental specialists. Conclusions This study showed that machine running methods are useful tools for predicting the closure of small medical facilities with a moderate level of accuracy. Essential factors affecting medical facility closure also differed between MCs and DCs. Developing good models would prevent unnecessary medical facility closures at the national level.},
}

@article{Rao2024,
  title = {ChatGPT: A Conceptual Review of Applications and Utility in the Field of Medicine},
  author = {Shiavax J Rao and Ameesh Isath and Parvathy Krishnan and Jonathan A Tangsrivimol and Hafeez Ul Hassan Virk and Zhen Wang and Benjamin S Glicksberg and Chayakrit Krittanawong},
  year = {2024},
  journal = {Journal of Medical Systems},
  volume = {48},
  pages = {59},
  doi = {10.1007/s10916-024-02075-x},
  url = {https://doi.org/10.1007/s10916-024-02075-x},
  abstract = {Artificial Intelligence, specifically advanced language models such as ChatGPT, have the potential to revolutionize various aspects of healthcare, medical education, and research. In this narrative review, we evaluate the myriad applications of ChatGPT in diverse healthcare domains. We discuss its potential role in clinical decision-making, exploring how it can assist physicians by providing rapid, data-driven insights for diagnosis and treatment. We review the benefits of ChatGPT in personalized patient care, particularly in geriatric care, medication management, weight loss and nutrition, and physical activity guidance. We further delve into its potential to enhance medical research, through the analysis of large datasets, and the development of novel methodologies. In the realm of medical education, we investigate the utility of ChatGPT as an information retrieval tool and personalized learning resource for medical students and professionals. There are numerous promising applications of ChatGPT that will likely induce paradigm shifts in healthcare practice, education, and research. The use of ChatGPT may come with several benefits in areas such as clinical decision making, geriatric care, medication management, weight loss and nutrition, physical fitness, scientific research, and medical education. Nevertheless, it is important to note that issues surrounding ethics, data privacy, transparency, inaccuracy, and inadequacy persist. Prior to widespread use in medicine, it is imperative to objectively evaluate the impact of ChatGPT in a real-world setting using a risk-based approach.},
}

@article{Voigtlaender2024,
  title = {Artificial intelligence in neurology: opportunities, challenges, and policy implications},
  author = {Sebastian Voigtlaender and Johannes Pawelczyk and Mario Geiger and Eugene J Vaios and Philipp Karschnia and Merit Cudkowicz and Jorg Dietrich and Ira R J Hebold Haraldsen and Valery Feigin and Mayowa Owolabi and Tara L White and Paweł Świeboda and Nita Farahany and Vivek Natarajan and Sebastian F Winter},
  year = {2024},
  journal = {Journal of Neurology},
  volume = {271},
  pages = {2258-2273},
  doi = {10.1007/s00415-024-12220-8},
  url = {https://doi.org/10.1007/s00415-024-12220-8},
  abstract = {Neurological conditions are the leading cause of disability and mortality combined, demanding innovative, scalable, and sustainable solutions. Brain health has become a global priority with adoption of the World Health Organization’s Intersectoral Global Action Plan in 2022. Simultaneously, rapid advancements in artificial intelligence (AI) are revolutionizing neurological research and practice. This scoping review of 66 original articles explores the value of AI in neurology and brain health, systematizing the landscape for emergent clinical opportunities and future trends across the care trajectory: prevention, risk stratification, early detection, diagnosis, management, and rehabilitation. AI’s potential to advance personalized precision neurology and global brain health directives hinges on resolving core challenges across four pillars—models, data, feasibility/equity, and regulation/innovation—through concerted pursuit of targeted recommendations. Paramount actions include swift, ethical, equity-focused integration of novel technologies into clinical workflows, mitigating data-related issues, counteracting digital inequity gaps, and establishing robust governance frameworks balancing safety and innovation.},
}

@article{Al-Karkhi2024,
  title = {Statistical analysis and open innovation in economic growth of scottish business SMEs for sustainable development},
  author = {Mustafa I. Al-Karkhi},
  year = {2024},
  journal = {Journal of Open Innovation: Technology, Market, and Complexity},
  volume = {10},
  pages = {100275},
  doi = {https://doi.org/10.1016/j.joitmc.2024.100275},
  url = {https://www.sciencedirect.com/science/article/pii/S2199853124000696},
  abstract = {The propulsion of economic growth is a multifaceted construct, influenced by technology, capital, and resource management, with Small and Medium-sized Enterprises (SMEs) being a significant contributors to innovation and employment. This study examines the growth of Scottish SMEs across four sectors over a 14-year period (2008–2021) by employing a novel statistical approach to understand their development trajectories within the framework of open innovation. The paper engaged in a longitudinal analysis to assess trends and inform policy for sustainable growth by the utilization of open-access data. Theoretical metrics such as the Average Growth Rate, Compound Average Growth Rate (CAGR), and Year-Over-Year (YoY) Growth were applied to evaluate sectoral performance. The findings revealed varied growth patterns; the information technology sector exhibited a robust increase, while the telecommunications sector showed significant percentage growth. Conversely, the Information and Communication Technologies (ICT) sector experienced a decline that suggests a potential market gap. An affirmative trajectory in CAGR was observed in most sectors except ICT, which corroborates a shift in technological evolution and market needs. The study illuminates the dynamic economic landscape of Scottish SMEs, with telecoms emerging as the most prosperous sector in terms of growth rates, thus indicates a shifting economic focus. The study’s limitations are briefly discussed, in addition to the need for updated databases and further research employing predictive modeling with machine learning to enhance forecasting capabilities and foster open innovation for sustainable economic development.},
}

@article{Lnenicka2024,
  title = {Understanding big data and data protection measures in smart city strategies: An analysis of 28 cities},
  author = {Martin Lnenicka and Petr Hervert and Oldrich Horak},
  year = {2024},
  journal = {Urban Governance},
  volume = {4},
  pages = {255-273},
  doi = {https://doi.org/10.1016/j.ugj.2024.12.008},
  url = {https://www.sciencedirect.com/science/article/pii/S2664328624000615},
  abstract = {The Smart City concept aims to improve urban governance and optimize public services, ultimately enhancing the quality of life for citizens. As data generation and processing grow rapidly in volume, velocity, and variety, Smart Cities must integrate secure big data considerations into their strategic frameworks and project implementations. This paper explores how big data and data protection measures are represented in the strategies of 28 cities worldwide. To achieve this, we employed a three-phase research methodology: 1) identifying resources, 2) conducting content analysis, and 3) using the Delphi method. Our findings indicate that only half of the cities explicitly address big data in their strategies, and most lack adequate data protection measures. Additionally, the paper presents a list of recommendations for big data management and data protection, derived from measures found in Smart City strategies and validated by domain experts through the Delphi method. These recommendations aim to enhance understanding of how to effectively incorporate big data and its protection into urban planning and Smart City projects. However, it is important to note that these insights primarily apply to larger urban areas with abundant resources.},
}

@article{Auth2024,
  title = {Digitale Kompetenzanforderungen in der öffentlichen Verwaltung nach der COVID-19-Pandemie},
  author = {Gunnar Auth and Frank Bensberg and Julian P Christ},
  year = {2024},
  journal = {HMD Praxis der Wirtschaftsinformatik},
  volume = {61},
  pages = {220-233},
  doi = {10.1365/s40702-023-01023-6},
  url = {https://doi.org/10.1365/s40702-023-01023-6},
  abstract = {Digitale Kompetenzen haben sich spätestens seit den COVID-19-bedingten Lockdowns sowie auch im Kontext von New Work und KI-Anwendungen zu einem zentralen Aktionsfeld für Wirtschaft, Wissenschaft und Politik entwickelt. Die öffentliche Verwaltung befindet sich seit einigen Jahren in einer verstärkten strukturellen und technischen Transformation, hin zu einer digitalisierten und modernen Verwaltung, welche einen tiefgreifenden Auf- und Umbau von Kompetenzanforderungen bedingt. Dies führt zu massiven Veränderungen von Kompetenzprofilen und Stellenanforderungen auf Ebene der Kommunen, der Länder und des Bundes. Relevanz und Bandbreite digitaler Kompetenzen für E‑Government wurden bereits in mehreren Studien untersucht, wenngleich generalisierbare empirische Analysen noch am Anfang stehen. Unmittelbar vor Ausbruch der COVID-19-Pandemie führten die Autoren im Zeitraum von Dezember 2019 bis März 2020 eine Erhebung digitaler Kompetenzanforderungen in Stellenanzeigen der öffentlichen Verwaltung durch. Den Bezugsrahmen für diese quantitative Untersuchung bildete ein eigens entwickeltes komplementäres Modell digitaler Kompetenzen inkl. einer zweckgerichteten Operationalisierung. In diesem Beitrag werden auf Basis eines quantitativen Ansatzes Stellenanzeigen der öffentlichen Verwaltung aus dem Jahr 2023 großzahlig analysiert und mit der Referenzauswertung der Prä-Corona-Datenerhebung aus 2019/2020 verglichen. Als Datengrundlage dienen elektronische Stellenanzeigen, die automatisiert aus dem zentralen Stellenportal des Bundes extrahiert wurden. Hierdurch wird folgende Forschungsfrage beantwortet: Welche Veränderungen digitaler Kompetenzanforderungen in Folge der COVID-19-Pandemie lassen sich in Stellenanzeigen der öffentlichen Verwaltung identifizieren?},
}

@article{Harrison2018,
  title = {Beyond institutional voids and the middle-income trap: The emerging business angel market in Malaysia},
  author = {Richard Harrison and William Scheela and P C Lai and Sivapalan Vivekarajah},
  year = {2018},
  journal = {Asia Pacific Journal of Management},
  volume = {35},
  pages = {965-991},
  doi = {10.1007/s10490-017-9535-y},
  url = {https://doi.org/10.1007/s10490-017-9535-y},
  abstract = {Emerging economies are characterized by the presence of institutional voids which challenge and constrain the behavior of economic agents. In this paper we report on one set of agents, angel investors, in Malaysia, which investors fear is experiencing a middle-income trap whereby economic growth and new venture formation stalls due to persistent institutional voids. This research addresses this question through interviews with 19 Malaysian business angel investors in 2015, utilizing a mixed-methods approach. Results indicate that business angels in our sample generated strong returns, though they did find it a challenge to invest in and monitor new ventures in a highly uncertain and competitive environment where there is high political uncertainty, weak legal and financial support for investors and SMEs. In order to overcome weak institutional support, business angel investors develop informal institutions by co-investing and networking with family members and government officials. They also conduct careful due diligence before investing and closely monitor their investee companies after investing. This research provides several theory and practice contributions with respect to business-angel investing in emerging economies with weak formal institutional regimes.},
}

@article{Cordero2019,
  title = {BITS2018: the fifteenth annual meeting of the Italian Society of Bioinformatics},
  author = {Francesca Cordero and Raffaele A Calogero and Michele Caselle},
  year = {2019},
  journal = {BMC Bioinformatics},
  volume = {20},
  pages = {562},
  doi = {10.1186/s12859-019-3175-9},
  url = {https://doi.org/10.1186/s12859-019-3175-9},
  abstract = {This preface introduces the content of the BioMed Central Bioinformatics journal Supplement related to the 15th annual meeting of the Bioinformatics Italian Society, BITS2018. The Conference was held in Torino, Italy, from June 27th to 29th, 2018.},
}

@article{Zuccalà2017,
  title = {Enabling Energy Smart Cities through Urban Sharing Ecosystems},
  author = {Maurilio Zuccalà and Emiliano Sergio Verga},
  year = {2017},
  journal = {Energy Procedia},
  volume = {111},
  pages = {826-835},
  doi = {https://doi.org/10.1016/j.egypro.2017.03.245},
  url = {https://www.sciencedirect.com/science/article/pii/S1876610217302783},
  abstract = {In order to build real smart cities, heterogeneous data from different sources has to be properly collected, integrated and shared. In this paper, a real district scale example of urban sharing ecosystem based on coopetition is presented. This digital ecosystem enables data sharing that can be synergically applied to different sectors relevant to the urban context, e.g., energy and transportation, in order to create innovative solutions for energy monitoring, citizen engagement, and evaluation and monitoring at district and city level.},
}

@article{Brettell2013,
  title = {A Survey of Environmental Deterrence in China’s Evolving Regulatory Framework},
  author = {Anna Brettell},
  year = {2013},
  pages = {21-81},
  doi = {10.1057/9781137343680_2},
  publisher = {Palgrave Macmillan US},
  url = {https://doi.org/10.1057/9781137343680_2},
  abstract = {China has enjoyed strong economic growth for nearly three decades and has become the world’s second largest economy (World Bank 2012:xxi). Part of the cost of that accomplishment has been significant and widespread industrial pollution and ecological degradation (OECD 2006:5; World Bank 2012:14).},
}

@article{Olwan2013,
  title = {Voluntary Mechanisms, Copyright and Development},
  author = {Rami M Olwan},
  year = {2013},
  pages = {265-345},
  doi = {10.1007/978-3-642-27907-2_6},
  publisher = {Springer Berlin Heidelberg},
  url = {https://doi.org/10.1007/978-3-642-27907-2_6},
  abstract = {Having examined IP law, particularly copyright law, and how it needs to adopt a development perspective, it is important to consider innovative systems that can be used in combination with such laws for the benefit of developing countries. This chapter considers voluntary mechanisms, primarily Free and Open Source Software (FOSS) and Creative Commons (CC), and examines their relevance for developing countries. Voluntary mechanisms must be examined in the context of development as they are practical tools that can be localised for the benefit of developing countries. Most of the recent research on IP and development does not give proper attention to voluntary mechanisms or examine their relevance to the IP and development debate. This chapter calls on developing countries to seriously consider alternative approaches such as voluntary mechanisms and to use them in conjunction with traditional copyright laws. It examines the legal challenges and risks that those countries will face if they decide to adopt voluntary mechanisms. The chapter also looks at the civil legal system of Jordan as a case study for the adoption of FOSS and CC.},
}

@article{Venkatasubramanian2001,
  title = {Process Fault Detection and Diagnosis: Past, Present and Future},
  author = {Venkat Venkatasubramanian},
  year = {2001},
  journal = {IFAC Proceedings Volumes},
  volume = {34},
  pages = {1-13},
  doi = {https://doi.org/10.1016/S1474-6670(17)33563-2},
  url = {https://www.sciencedirect.com/science/article/pii/S1474667017335632},
  abstract = {One of the most important challenges facing control system engineers is the design and implementation of intelligent control systems that can assist operators to make supervisory control decisions such as abnormal situations management (ASM), start up and shut down, controller performance assessment and so on. Operator failure to exercise the appropriate supervisory control decisions often have an adverse effect on product quality, process safety, occupational health and environmental impact. The economic impact of such abnormal situations is enormous; about $20 billion/year in losses in the petrochemical industries alone in the US. Furthermore, process safety, occupational health and environmental hazards are ever increasing in importance in response to heightening public concern and the resultant tightening of regulations. Thus, there exists considerable incentive in developing intelligent control systems that can provide automated operator assistance for supervisory control situations for complex process plants. People in the process industries view this as the next major challenge in control systems research and application. The automation of process fault detection and diagnosis forms the first step in automating supervisory control and ASM. In this paper, I present an overview of the various approaches to fault diagnosis, challenges we face and encouraging emerging trends. The recent progress has promising implications on the use of intelligent systems for inherently safer design, operator training, abnormal situation management, process hazards analysis and optimal process operations.},
}

@article{Reinink-Smith1990,
  title = {Mineral Assemblages of Volcanic and Detrital Partings in Tertiary Coal Beds, Kenai Peninsula, Alaska},
  author = {Linda M Reinink-Smith},
  year = {1990},
  journal = {Clays and Clay Minerals},
  volume = {38},
  pages = {97-108},
  doi = {10.1346/CCMN.1990.0380113},
  url = {https://doi.org/10.1346/CCMN.1990.0380113},
  abstract = {Volcanic and non-volcanic partings are exposed in coal beds of the Tertiary Beluga and Sterling Formations along the shores of the Kenai lowland, Alaska. About two-thirds of the partings originated as air-fall tephra which fell in coal-forming swamps. The tephra partings in the Pliocene strata are unaltered or slightly altered and have a characteristic mineral assemblage of volcanic glass ± montmoriUonite ± kaolinite ± opal-CT. Miocene strata are slightly altered to totally altered, and a typical mineral assemblage consists of feldspar ± kaolinite ± montmorillonite ± quartz ± crandallite ± altered volcanic glass. Crandallite appears to have formed early in diagenesis by the replacement of volcanic glass prior to the formation of montmorillonite and kaolinite.},
}

@article{Vernadat1994,
  title = {Databases for CIMS and IMS},
  author = {F B Vernadat},
  year = {1994},
  pages = {85-114},
  doi = {10.1007/978-1-4471-2101-5_4},
  publisher = {Springer London},
  url = {https://doi.org/10.1007/978-1-4471-2101-5_4},
  abstract = {One of the trends of modern manufacturing is to progress towards Computer-Integrated Manufacturing (CIM) for better communication, co-ordination, and efficiency throughout the enterprise. Communications systems and information systems are key components for achieving true integration of distributed manufacturing systems (horizontal integration). Furthermore, information is the basic ingredient used for decision-making at the various decision levels of any manufacturing enterprise (vertical integration). Information is the “glue” which ties together all the modules of a manufacturing system while the underlying integrating infrastructure (i.e., a set of computer services connecting modules and ensuring transparent information exchange) is the backbone of its information system [2].},
}

@article{Shi2025,
  title = {Generative AI in Fashion: Overview},
  author = {Shi, Wenda and Wong, Waikeung and Zou, Xingxing},
  year = {2025},
  journal = {ACM Trans. Intell. Syst. Technol.},
  doi = {10.1145/3718098},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3718098},
  abstract = {Generative Artificial Intelligence (GenAI) has recently gained immense popularity by offering various applications for generating high-quality and aesthetically pleasing content of image, 3D, and video data format. The innovative GenAI solutions have shifted paradigms across various design-related industries, particularly fashion. In this paper, we explore the incorporation of GenAI into fashion-related tasks and applications. Our examination encompasses a thorough review of more than 470 research papers and an in-depth analysis of over 300 applications, focusing on their contributions to the field. These contributions are identified as 13 tasks within four categories: multi-modal fashion understanding, and fashion synthesis of image, 3D, and dynamic (video and animatable 3D) formats We delve into these methods, recognizing their potential to propel future endeavours toward achieving state-of-the-art (SOTA) performance. Furthermore, we present a comprehensive overview of 53 publicly available datasets suitable for training and benchmarking fashion-centric models, accompanied by the relevant evaluation metrics. Finally, we review real-world applications, unveiling existing challenges and future directions. With comprehensive investigation and in-depth analysis, this paper is targeted to serve as a useful resource for understanding the current landscape of GenAI in fashion, paving the way for future innovations in this dynamic field. Papers discussed in this paper, along with public code and datasets links are available at: .},
}

@article{Loukis2023,
  title = {ChatGPT Application vis-a-vis Open Government Data (OGD): Capabilities, Public Values, Issues and a Research Agenda},
  author = {Euripidis Loukis and Stuti Saxena and Nina Rizun and Maria Ioanna Maratsi and Mohsan Ali and Charalampos Alexopoulos},
  year = {2023},
  pages = {95-110},
  publisher = {Springer Nature Switzerland},
  abstract = {As a novel Artificial Intelligence (AI) application, ChatGPT holds pertinence not only for the academic, medicine, law, computing or other sectors, but also for the public sector-case in point being the Open Government Data (OGD) initiative. However, though there has been some limited (as this topic is quite new) research concerning the capabilities ChatGPT in these sectors, there has been no research about the capabilities it can provide to government concerning its wide range of functions and activities. This paper contributes to filling this gap by investigating the capabilities that the ChatGPT can provide concerning one of most recently initiated and novel, and at the same time most promising, activities of government that aims to fuel the emerging data economy and society: the opening of large amounts of government data; furthermore, we investigate the public values that can be promoted through the use of ChatGPT in the area of OGD by both the data publishers as well as their users. At the same time, we investigate the issues that the use of ChatGPT in the area of OGD can pose, which can reduce the capabilities identified as aforesaid as well as the benefits and public values that can be generated from them. For these purposes interviews with 12 experts have been conducted and their responses have been analyzed. Finally, based on our findings we have developed a research agenda concerning the exploitation of ChatGPT application in the OGD domain.},
}

@article{Panitz2024,
  title = {Introduction: Knowledge and Digital Technology},
  author = {Robert Panitz and Johannes Glückler},
  year = {2024},
  pages = {1-13},
  doi = {10.1007/978-3-031-39101-9_1},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-39101-9_1},
  abstract = {Development happens as a society undergoes structural transformation. Structural change in a society’s culture, institutions, and technologies is driven by new ways of thinking, new knowledge, and innovations. Although the latest wave of technological change, often referred to as the fifth Kondratieff cycle (Schumpeter, 1961), has been transforming world society since the 1990s, innovative uses of digital technology have continued to yield radical and disruptive changes. Digitization has been central to shaping new ways of observing (e.g., by collecting big data and augmenting reality), knowing (e.g., supported by machine learning), and transforming (e.g., by automation and robotics) our environment. As humanity uses its knowledge to advance technologies, which in turn have an effect on human knowledge and our ways of learning, we have dedicated this book to the reflexive relationship between knowledge and technology. In addition, geography is an important, yet frequently neglected, context for the ways in which people and organizations generate new knowledge, how they adopt and use new technologies, and how the use of these technologies affects their knowledge. Coincidently, technological advances have an immediate impact on human knowledge of geography and space. Whereas people once used maps and compasses to find their way around, today GPS-based navigation services take over all the work, with the effect of gradually diminishing both human cognition of space (Yan et al., 2022) and spatial knowledge acquisition (Brügger et al., 2019). This 19th volume in the Springer Series of Knowledge and Space has brought together leading interdisciplinary expertise, new empirical evidence, and conceptual propositions on the conditions, impact, and future potential of digital technologies for varying geographies of human society.},
}

@article{Cascini2023,
  title = {Personalized Medicine Through Artificial Intelligence: A Public Health Perspective},
  author = {Fidelia Cascini and Stefan Buttigieg and Roberta Pastorino and Walter Ricciardi and Stefania Boccia},
  year = {2023},
  pages = {3-14},
  doi = {10.1007/978-3-031-32614-1_1},
  publisher = {Springer International Publishing},
  url = {https://doi.org/10.1007/978-3-031-32614-1_1},
  abstract = {Digital technologies are leading the transformation of the healthcare sector and are bringing new models of health service delivery that are mostly focused on prevention and based on personalization and precision. From a Public Health perspective, personalized medicine implies a transformation of health systems that facilitates improved targeting of healthcare services based on specific population sub-group needs, to maximize their effectiveness and relevance. The digitalization of healthcare systems and the use of health data-driven approaches allow the development of novel, targeted Public Health solutions, which are necessary in the complexity of the healthcare ecosystem. The growing amount of health data generated every year makes big data platforms an essential tool for data management and analytics. Although various different artificial intelligence systems are being developed with the aid of such platforms, they generally share similar objectives: to assist health systems’ response to specific emerging health demands; to design healthcare services that are able to scale their provision according to the growth of populations; to improve public health resilience and responsiveness, to promptly control epidemic-related emergencies; to better differentiate patient communities through risk stratification and to inform individual decision-making, both essential for the personalized medicine movement. In this chapter, we present the development of artificial intelligence and its promising applications allowing targeted Public Health interventions, and current limitations to address as well.},
}

@article{Antinori2023,
  title = {Online Research Techniques and Methodologies in the Study of Left-Wing Extremism},
  author = {Arije Antinori},
  year = {2023},
  pages = {157-169},
  doi = {10.1007/978-3-031-30897-0_9},
  publisher = {Springer International Publishing},
  url = {https://doi.org/10.1007/978-3-031-30897-0_9},
  abstract = {Social science researchers face risks when studying deviant or criminal behavior, especially when researching violent extremism online. The author emphasizes the importance of acquiring necessary skills to operate safely and anonymously. In the (cyber-)social ecosystem characterized by the centrality of a new dimension of human experience that goes beyond the internet and social media, the Left-Wing Extremism and Violent Left-Wing and Anarchist Extremism (VLWAE) constitute an important challenge for security. The author provides guidelines for conducting online research on Left-Wing Extremism entities using Open Source Intelligence (OSINT), while ensuring digital hygiene and safety, to study their online activities, propaganda, and radicalization strategies. OSINT enables researchers to collect information from various sources, including private and encrypted messaging channels, to understand the organization and mobilization of extremist campaigns. The use of various tools and techniques is presented to highlight the most relevant elements that should be included in a research report on left-wing extremism target. The author also focuses on the importance of technological innovation to support research in this field. Moreover, a strong integration between Artificial Intelligence (AI) and OSINT is revolutionizing online investigations of left-wing extremism and other forms of extremism and terrorism. AI-based technology allows researchers to analyze large amounts of images, videos, and data with the aim for identifying patterns and trends. In the near-future scenario, AI will be used in extremist individuals and entities profiling to anticipate new onlife security threats.},
}

@article{Nikolinakos2023,
  title = {The Proposed Artificial Intelligence Act and Subsequent ‘Compromise’ Proposals: Commission, Council, Parliament},
  author = {Nikos Th. Nikolinakos},
  year = {2023},
  pages = {327-741},
  doi = {10.1007/978-3-031-27953-9_8},
  publisher = {Springer International Publishing},
  url = {https://doi.org/10.1007/978-3-031-27953-9_8},
  abstract = {This chapter provides a thorough and systematic analysis of the Commission’s proposed AI Act which set harmonised rules for the development, placement on the market and use of AI systems in the EU following a proportionate risk-based approach. The proposed AI Act must be reviewed and adopted by the European Parliament and Member States before it comes into effect and, to this end, the Commission’s proposed AI Act is being discussed by the co-legislators (the main decision-making bodies of the EU), i.e., the European Parliament (“the Parliament”) and the Council of the European Union (“the Council”). This chapter compares the Commission’s proposed AI Act with the Council’s numerous “compromise” proposals (from November 2021 to December 2022) which attempted to find a common position between Member States. The Council’s AI Act final version (general approach) of 11 November 2022 received unanimous approval from the Committee of Permanent Representatives on 18 November 2022, and its formal adoption by EU ministers took place at the Telecom Council meeting on 6 December 2022. This chapter also takes into account the European Parliament’s views on all major and controversial issues, revealing the position the European Parliament is likely to adopt in the forthcoming negotiations with the Commission and the Council on the text of the AI Act. It covers the process of preparing the Parliament’s compromise proposal, taking account of the comments/amendments which were submitted by political groups represented in the European Parliament and which set the tone for future discussions and political negotiations in order to reach a majority via compromises.},
}

@article{Shan2022,
  title = {Digital Transformation Method for Healthcare Data},
  author = {Richard Shan and Tony Shan},
  year = {2022},
  pages = {48-63},
  publisher = {Springer International Publishing},
  abstract = {More than 80% of healthcare data is unstructured. The complexity and challenges in healthcare data demands a methodical approach for digital transformation. The Process, Enablement, Tooling, and Synthesis (PETS) method is presented, which provides a holistic approach and discipline to help organizations do it right the first time on digital transformation of unstructured data in the healthcare domain. PETS establishes and evolves a comprehensive knowledgebase for the technology facilitation and implementations in the new era. Details of PETS modules and open-source solutions are discussed. Best practices and real-world PETS applications are articulated in the context.},
}

@article{Santos2024,
  title = {REAL-UP: Urban Perceptions From LBSNs Helping Moving Real-Estate Market to the Next Level},
  author = {Santos, Frances A. and Silva, Thiago H. and Villas, Leandro A.},
  year = {2024},
  pages = {1071–1074},
  doi = {10.1145/3589335.3651252},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3589335.3651252},
  abstract = {Finding the best short- or long-term accommodation is troublesome in unknown areas. Current tools provided by the real-estate market offer valuable information regarding the property, such as price, photos, and descriptions of the space; however, this market has little explored other relevant information regarding the surrounding area, such as what is nearby and users' subjective perception of the property's area. To address this gap, we propose REAL-UP, an interactive tool designed to enrich real-estate marketplaces. In addition to information commonly provided by such applications, e.g., rent price, REAL-UP also provides subjective neighborhood information based on Location-Based Social Networks (LBSNs) messages. This novel tool helps to represent complex users' subjective perceptions of urban areas, which could ease the process of finding the best accommodation.},
}

@article{Lee2024_02,
  title = {Fundamental Trends and Hot-Spots in Public Governance and Artificial Intelligence Research: A Bibliometric Study Analysis From 2013 to 2023},
  author = {Lee, Siyuan},
  year = {2024},
  pages = {787–796},
  doi = {10.1145/3671151.3671290},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3671151.3671290},
  abstract = {Background: AI has become an important theoretical and practical hotspot in public governance. This study aims to analyses the publication characteristics of AI in public governance research in terms of institutions, journals and partnerships, and to analyses the development trend of AI in public governance research. Methods: Publications regarding AI in public governance were retrieved from Web of Science core collection. Microsoft Excel 2010, VOSviewer were used to characterize the contributions of the authors, journals, and institutions. The trends, hot-spots and knowledge network were analyzed by Citespace and VOSviewer. Results: We identified 2525 papers between 2013 and 2023. The number of papers grows gradually until 2019, followed by a sharp increase between 2020 and 2023. The University of Oxford is the most active institution. Most of the papers in this field are published in Sustainability and IEEE Access. Luciano is the most productive author, and EUROPEAN COMMISSION is the most frequently co-cited author. The frontier topics are public sector and smart city. The basic themes of this field are “application scenarios, operation characteristics, algorithm characteristics, risk governance”. Conclusion: In the past three years, artificial intelligence and public governance research have attracted widespread attention. The research hotspots are gradually shifting to public service, decision-making and risk.},
}

@article{Molodtsov2024,
  title = {An Integrated Usability Framework for Evaluating Open Government Data Portals: Comparative Analysis of EU and GCC Countries},
  author = {Molodtsov, Fillip and Nikiforova, Anastasija},
  year = {2024},
  pages = {899–908},
  doi = {10.1145/3657054.3657159},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3657054.3657159},
  abstract = {This study explores the critical role of open government data (OGD) portals in fostering transparency and collaboration between diverse stakeholders. Recognizing the challenges of usability, communication with diverse populations, and strategic value creation, this paper develops an integrated framework for evaluating OGD portal effectiveness that accommodates user diversity (regardless of their data literacy and language), evaluates collaboration and participation, and opportunities to explore and understand the data provided through them. The framework is validated by applying it to 33 national portals across European Union and Gulf Cooperation Council (GCC) countries, as a result of which we rank OGD portals, identify some good practices that lower-performing portals can learn from, and common shortcomings. The study unveils the competitive and innovative nature of GCC OGD portals, pinpointing specific improvement areas such as multilingual support and data understandability. The findings underscore the growing trend of exposing data quality metrics and advocate for enhanced two-way communication channels between users and portal representatives. Overall, the study contributes to accelerating the development of user-friendly, collaborative, and sustainable OGD portals while addressing gaps identified in previous research.},
}

@article{Raoufi2024,
  title = {Deep learning applications in the Internet of Things: a review, tools, and future directions},
  author = {Parisa Raoufi and Atefeh Hemmati and Amir Masoud Rahmani},
  year = {2024},
  journal = {Evolutionary Intelligence},
  volume = {17},
  pages = {3621-3654},
  doi = {10.1007/s12065-024-00949-0},
  url = {https://doi.org/10.1007/s12065-024-00949-0},
  abstract = {The emergence of the Internet of Things (IoT) has enabled the proliferation of interconnected devices and sensors, generating vast amounts of often complex and unstructured data. Deep learning (DL), a subfield of machine learning (ML), has shown great promise in addressing the challenges of processing and analyzing such data. Considering the increasing importance of DL and data analysis, we decided to review the articles of the last few years in this field to pave the way for researchers. In this article, we used the systematic literature review (SLR) method, and in line with that, we selected and analyzed 56 articles published from 2019 to April 2024. We first discuss the DL models used in the IoT field and clarify their specific use cases. Secondly, we outline an analysis of research areas in DL-based IoT. In addition, our research extends to the tools and simulators used to evaluate studies in the DL-based IoT domain. We also examine the DL-based IoT research datasets. Finally, our review identifies future directions and open issues in DL-based IoT. We aim to contribute to an accurate understanding of the current state, challenges, and potential breakthroughs at the intersection of DL and the IoT.},
}

@article{Shata2025,
  title = {Artificial intelligence and communication technologies in academia: faculty perceptions and the adoption of generative AI},
  author = {Aya Shata and Kendall Hartley},
  year = {2025},
  journal = {International Journal of Educational Technology in Higher Education},
  volume = {22},
  pages = {14},
  doi = {10.1186/s41239-025-00511-7},
  url = {https://doi.org/10.1186/s41239-025-00511-7},
  abstract = {Artificial intelligence (AI) is ushering in an era of potential transformation in various fields, especially in educational communication technologies, with tools like ChatGPT and other generative AI (GenAI) applications. This rapid proliferation and adoption of GenAI tools have sparked significant interest and concern among college professors, who are dealing with evolving dynamics in digital communication within the classroom. Yet, the effect and implications of GenAI in education remain understudied. Therefore, this study employs the Technology Acceptance Model (TAM) and the Social Cognitive Theory (SCT) as theoretical frameworks to explore higher education faculty’s perceptions, attitudes, usage, and motivations, as the underlying factors that influence their adoption or rejection of GenAI tools. A survey was conducted among full-time higher education faculty members (N = 294) recruited from two mid-size public universities in the US. Results found that college professors’ perceived usefulness of AI predicted their attitudes and intention to use and adopt the technology, more than their perceived ease of use. Trust and social reinforcement strongly influenced college professors’ GenAI adoption decisions and acted as significant mediators to better understand the relationship between TAM and SCT. Findings emphasized the power of social dynamics in shaping professors’ self-efficacy, attitudes, and use of GenAI. Trust enhances peer influence and affects how perceived usefulness shapes users’ willingness to adopt technology, whereas self-efficacy has a minimal impact. This research provides valuable insights that inform higher education policies aimed at improving the educational experience for college students in an AI-driven workforce.},
}

@article{Chan2023,
  title = {A comprehensive AI policy education framework for university teaching and learning},
  author = {Cecilia Ka Yuk Chan},
  year = {2023},
  journal = {International Journal of Educational Technology in Higher Education},
  volume = {20},
  pages = {38},
  doi = {10.1186/s41239-023-00408-3},
  url = {https://doi.org/10.1186/s41239-023-00408-3},
  abstract = {This study aims to develop an AI education policy for higher education by examining the perceptions and implications of text generative AI technologies. Data was collected from 457 students and 180 teachers and staff across various disciplines in Hong Kong universities, using both quantitative and qualitative research methods. Based on the findings, the study proposes an AI Ecological Education Policy Framework to address the multifaceted implications of AI integration in university teaching and learning. This framework is organized into three dimensions: Pedagogical, Governance, and Operational. The Pedagogical dimension concentrates on using AI to improve teaching and learning outcomes, while the Governance dimension tackles issues related to privacy, security, and accountability. The Operational dimension addresses matters concerning infrastructure and training. The framework fosters a nuanced understanding of the implications of AI integration in academic settings, ensuring that stakeholders are aware of their responsibilities and can take appropriate actions accordingly.},
}

@article{Yang2023,
  title = {The impact of ChatGPT and LLMs on medical imaging stakeholders: Perspectives and use cases},
  author = {Yang, Jiancheng and Li, Hongwei Bran and Wei, Donglai},
  year = {2023},
  journal = {Meta-Radiology},
  volume = {1},
  doi = {10.1016/j.metrad.2023.100007},
  publisher = {KeAi Publishing Communications Ltd.},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174670809&doi=10.1016%2fj.metrad.2023.100007&partnerID=40&md5=a7ab21745f6ec6d7326b586db29690a8},
  abstract = {This study investigates the transformative potential of Large Language Models (LLMs), such as OpenAI ChatGPT, in medical imaging. With the aid of public data, these models, which possess remarkable language understanding and generation capabilities, are augmenting the interpretive skills of radiologists, enhancing patient-physician communication, and streamlining clinical workflows. The paper introduces an analytic framework for presenting the complex interactions between LLMs and the broader ecosystem of medical imaging stakeholders, including businesses, insurance entities, governments, research institutions, and hospitals (nicknamed BIGR-H). Through detailed analyses, illustrative use cases, and discussions on the broader implications and future directions, this perspective seeks to raise discussion in strategic planning and decision-making in the era of AI-enabled healthcare. © 2023 The Authors},
}

@inproceedings{Yuenyong2024,
  title = {DataDecon: Data Cleansing Tools for Large Language Model with Efficient Decontamination Techniques},
  author = {Yuenyong, Sumeth and Buppodom, Norapat and Sangkaew, Koravich and Boonmeeprakob, Konthee and Boonkwan, Prachya and Jaroenkantasima, Jillaphat and Khlaisamniang, Pitikorn and Lertpiya, Anuruth and Piyatumrong, Apivadee and Rojratchadakorn, Peerawat and Rugsujarit, Thaweewat and Saengsukhiran, Teerapol and Saetan, Kriangkrai and Sukprapa, Isada and Thavornmongkol, Thanachot and Thongthungwong, Nucharee and Triamamornwooth, Patteera and Utupon, Chanon and Viriyayudhakorn, Kobkrit and Witchutanon, Phoochit and Wongprayon, Sadit and Supnithi, Thepchai},
  year = {2024},
  journal = {19th International Joint Symposium on Artificial Intelligence and Natural Language Processing, iSAI-NLP 2024},
  doi = {10.1109/iSAI-NLP64410.2024.10799278},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216587816&doi=10.1109%2fiSAI-NLP64410.2024.10799278&partnerID=40&md5=a6dd65ea7b1911087c9355bf8c3a2797},
  abstract = {Large language models (LLMs) play an important role in modern NLP technology as they are versatile for a wide array of NLP tasks. However, constructing an LLM is challenging due to concealed construction pipelines, the lack of cleansed datasets, and hyperparameter settings, making it almost irreproducible. This paper presents an efficient pipeline for constructing an LLM tailored to a low-to-medium-sourced language with a high level of data contamination and tools to cleanse the dataset. Following our pipeline, we constructed OpenThaiGPT, an LLM for Thai, with only open-sourced datasets such as CC100, OSCAR, and mC4, and achieved the state-of-the-art accuracies on our downstream tasks. Here, we disclosed the data statistics and all hyperparameter settings for reproducibility.  © 2024 IEEE.},
}

@article{Cabral2024_01,
  title = {Open Information Extraction with LLM for the Portuguese Language; [Extração de Informação Aberta com LLM para a Língua Portuguesa]},
  author = {Cabral, Bruno and Souza, Marlo and Claro, Daniela Barreiro},
  year = {2024},
  journal = {Linguamatica},
  volume = {16},
  pages = {167 – 182},
  doi = {10.21814/lm.16.2.454},
  publisher = {Universidade do Minho},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216983339&doi=10.21814%2flm.16.2.454&partnerID=40&md5=f99debdfb0ccc4d6a76808ebe4eae52c},
  abstract = {In this study, we investigate the application of Large Language Models (LLMs) for Open Information Extraction (OpenIE) in the Portuguese language. While most OpenIE methods have been developed with a focus on the English language, few works in the literature explore multilingual and cross-linguistic scenarios. Although there is a growing interest in OpenIE methods for Portuguese, the use of LLMs specifically focused on OpenIE in this language remains underexplored. We analyze the feasibility of incorporating both open and commercial LLMs using few-shot prompt engineering for OpenIE in Portuguese. We provide a detailed analysis of the performance of these LLMs in OpenIE tasks, demonstrating that they achieve performance metrics comparable to state-of-the-art systems. Additionally, we refine and release an open LLM for OpenIE, named PortOIE-Llama, which outperforms commercial LLMs in our experiments. Our results highlight the potential of LLMs in OpenIE tasks in Portuguese and suggest that further refinement and fine-tuning of larger models can further enhance these outcomes. © 2024 Universidade do Minho. All rights reserved.},
}

@inproceedings{Nikiforova2024_01,
  title = {From the Evolution of Public Data Ecosystems to the Evolving Horizons of the Forward-Looking Intelligent Public Data Ecosystem Empowered by Emerging Technologies},
  author = {Nikiforova, Anastasija and Lnenicka, Martin and Milić, Petar and Luterek, Mariusz and Rodríguez Bolívar, Manuel Pedro},
  year = {2024},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {14841 LNCS},
  pages = {402 – 418},
  doi = {10.1007/978-3-031-70274-7_25},
  publisher = {Springer Science and Business Media Deutschland GmbH},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202634015&doi=10.1007%2f978-3-031-70274-7_25&partnerID=40&md5=92e38b3e9982a726091a73d435a1e12b},
  abstract = {Public Data Ecosystems (PDEs) represent complex socio-technical systems crucial for optimizing data use in the public sector and outside it. Recognizing their multifaceted nature, previous research proposed a six-generation Evolutionary Model of Public Data Ecosystems (EMPDE). Designed as a result of a systematic literature review on the topic spanning three decades, this model, while theoretically robust, necessitates empirical validation to enhance its practical applicability. This study addresses this gap by validating the theoretical model through a real-life examination in five European countries - Latvia, Serbia, Czech Republic, Spain, and Poland. This empirical validation provides insights into PDEs dynamics and variations of implementations across contexts, particularly focusing on the 6th generation of forward-looking PDE generation named “Intelligent Public Data Generation” which represents a paradigm shift driven by emerging technologies such as cloud computing, Artificial Intelligence (AI), Natural Language Processing tools, Generative AI, and Large Language Models with potential to contribute to both automation and augmentation of business processes within these ecosystems. By transcending their traditional status as a mere component, evolving into both an actor and a stakeholder simultaneously, these technologies catalyse innovation and progress, enhancing PDE management strategies to align with societal, regulatory, and technical imperatives in the digital era. © IFIP International Federation for Information Processing 2024.},
}

@inproceedings{Kliimask2024,
  title = {TAGIFY: LLM-powered Tagging Interface for Improved Data Findability on OGD portals},
  author = {Kliimask, Kevin and Nikiforova, Anastasija},
  year = {2024},
  journal = {2024 5th International Conference on Intelligent Data Science Technologies and Applications, IDSTA 2024},
  pages = {18 – 27},
  doi = {10.1109/IDSTA62194.2024.10746941},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211964747&doi=10.1109%2fIDSTA62194.2024.10746941&partnerID=40&md5=1749467c1026c9b8bff3e02a33c04e49},
  abstract = {Efforts directed towards promoting Open Government Data (OGD) have gained significant traction across various governmental tiers since the mid-2000s. As more datasets are published on OGD portals, finding specific data becomes harder, leading to information overload and so-called 'dark data'. Complete and accurate documentation of datasets, including association of proper tags with datasets is key to improving dataset findability and accessibility. Analysis conducted on the Estonian Open Data Portal revealed that 11% datasets have no associated tags, while 26% had only one tag assigned to them, which underscores challenges in data findability and accessibility within the portal, which, according to the recent Open Data Maturity Report, is considered trendsetter. The aim of this study is to propose an automated solution to tagging datasets to improve data findability on OGD portals. This paper presents TAGIFY - a prototype of tagging interface that employs large language models (LLM) such as GPT-3.5turbo and GPT-4 to automate dataset tagging, generating tags for datasets in English and Estonian, thereby augmenting metadata preparation by data publishers and improving data findability on OGD portals by data users. The developed solution was evaluated by users and their feedback was collected to define an agenda for future prototype improvements. © 2024 IEEE.},
}

@article{Shirazi2022,
  title = {Low Perinatal Androgens Predict Recalled Childhood Gender Nonconformityin Men},
  author = {Shirazi, Talia N. and Self, Heather and Rosenfield, Kevin A. and Dawood,Khytam and Welling, Lisa L. M. and Cardenas, Rodrigo and Bailey, J.Michael and Balasubramanian, Ravikumar and Delaney, Angela andBreedlove, S. Marc and Puts, David A.},
  year = {2022},
  journal = {PSYCHOLOGICAL SCIENCE},
  volume = {33},
  pages = {343-353},
  doi = {10.1177/09567976211036075},
  abstract = {The contributions of gonadal hormones to the development of human behavioral sex differences are subjects of intense scientific and social interest. Isolated gonadotropin-releasing-hormone deficiency (IGD) is a rare endocrine disorder that can reveal a possible role of early gonadal hormones. IGD is characterized by low or absent gonadal hormone production after the first trimester of gestation, but external genitalia and hence gender of rearing are concordant with chromosomal and gonadal sex. We investigated recalled childhood gender nonconformity in men (n = 65) and women (n = 32) with IGD and typically developing men (n = 463) and women (n = 1,207). Men with IGD showed elevated childhood gender nonconformity, particularly if they also reported undescended testes at birth, a marker of low perinatal androgens. Women with IGD did not differ from typically developing women. These results indicate that early androgen exposure after the first trimester contributes to male-typical gender-role behaviors in childhood.},
}

@inproceedings{Martinez2024,
  title = {Towards Understanding the Interplay of Generative ArtificialIntelligence and the Internet},
  author = {Martinez, Gonzalo and Watson, Lauren and Revirieg, Pedro and AlbertoHernandez, Jose and Juare, Marc and Sarka, Rik},
  year = {2024},
  volume = {14523},
  pages = {59-73},
  doi = {10.1007/978-3-031-57963-9\_5},
  abstract = {The rapid adoption of generative Artificial Intelligence (AI) tools that can generate realistic images or text, such as DALL-E, Mid-Journey, or ChatGPT, have put the societal impacts of these technologies at the center of public debate. These tools are possible due to the massive amount of data (text and images) that is publicly available through the Internet. At the same time, these generative AI tools become content creators that are already contributing to the data that is available to train future models. Therefore, future versions of generative AI tools will be trained with a mix of human-created and AI-generated content, causing a potential feedback loop between generative AI and public data repositories. This interaction raises many questions: how will future versions of generative AI tools behave when trained on a mixture of real and AI-generated data? Will they evolve and improve with the new data sets or on the contrary will they degrade? Will evolution introduce biases or reduce diversity in subsequent generations of generative AI tools? What are the societal implications of the possible degradation of these models? Can we mitigate the effects of this feedback loop? In this work, we explore the effect of this interaction and report some initial results using simple diffusion models trained with various image datasets. Our results show that the quality and diversity of the generated images can degrade over time suggesting that incorporating AI-created data can have undesired effects on future versions of generative models.},
}

@article{Kliimask2024_01,
  title = {TAGIFY: LLM-powered Tagging Interface for Improved Data Findability on OGD portals},
  author = {Kliimask, Kevin and Nikiforova, Anastasija},
  year = {2024},
  pages = {18-27},
  doi = {10.1109/IDSTA62194.2024.10746941},
  abstract = {Efforts directed towards promoting Open Government Data (OGD) have gained significant traction across various governmental tiers since the mid-2000s. As more datasets are published on OGD portals, finding specific data becomes harder, leading to information overload and so-called “dark data”. Complete and accurate documentation of datasets, including association of proper tags with datasets is key to improving dataset findability and accessibility. Analysis conducted on the Estonian Open Data Portal revealed that 11% datasets have no associated tags, while 26% had only one tag assigned to them, which underscores challenges in data findability and accessibility within the portal, which, according to the recent Open Data Maturity Report, is considered trendsetter. The aim of this study is to propose an automated solution to tagging datasets to improve data findability on OGD portals. This paper presents TAGIFY – a prototype of tagging interface that employs large language models (LLM) such as GPT-3.5turbo and GPT-4 to automate dataset tagging, generating tags for datasets in English and Estonian, thereby augmenting metadata preparation by data publishers and improving data findability on OGD portals by data users. The developed solution was evaluated by users and their feedback was collected to define an agenda for future prototype improvements.},
}

@article{Shen2025,
  title = {Get Large Language Models Ready to Speak: A Late-fusion Approach for Speech Generation},
  author = {Shen, Maohao and Zhang, Shun and Wu, Jilong and Xiu, Zhiping and AlBadawy, Ehab and Lu, Yiting and Seltzer, Mike and He, Qing},
  year = {2025},
  pages = {1-5},
  doi = {10.1109/ICASSP49660.2025.10890141},
  abstract = {Large language models (LLMs) have revolutionized natural language processing (NLP) with impressive performance across various text-based tasks. However, the extension of text-dominant LLMs to with speech generation tasks remains underexplored. In this work, we introduce a text-to-speech (TTS) system powered by a fine-tuned Llama model, named TTS-Llama, that achieves state-of-the-art speech synthesis performance. Building on TTS-Llama, we further propose MoLE-Llama, a text-and-speech multimodal LLM developed through purely late-fusion parameter-efficient fine-tuning (PEFT) and a mixture-of-expert architecture. Extensive empirical results demonstrate MoLE-Llama’s competitive performance on both text-only question-answering (QA) and TTS tasks, mitigating catastrophic forgetting issue in either modality. Finally, we further explore MoLE-Llama in text-in-speech-out QA tasks, demonstrating its great potential as a multimodal dialog system capable of speech generation.},
}

@article{Martínez-Oña2025,
  title = {NDE in Energy and Nuclear Industry},
  author = {Rafael Martínez-Oña},
  year = {2025},
  pages = {1-52},
  doi = {10.1007/978-3-030-48200-8_31-2},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-030-48200-8_31-2},
  abstract = {Nondestructive evaluation (NDE) is an important asset in the industrial world. In-service inspections (ISIs) by means of NDEs are widely applied in Nuclear Power Plants (NPPs). Implementing reliable and effective ISIs contributes to the safe operation of the NPP with satisfactory reliability and availability levels.},
}

@article{Wang2023,
  title = {Starting from “Two Suitcases”: My Journey Leading Academic and Research Libraries in America},
  author = {Xuemao Wang},
  year = {2023},
  pages = {245-257},
  doi = {10.1007/978-3-031-42379-6_22},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-42379-6_22},
  abstract = {I was born and raised in China and grew up during the tumultuous period of the Cultural Revolution. After working for an academic library during my undergraduate education in China, I came to the United States with an eagerness and curiosity to pursue the American Dream. Driven by the belief that one’s ultimate life goal is to reach one’s greatest potential, and influenced by the hardship of my early life in China, I have enjoyed pursuing my American Dream. Since coming to the United States more than 30 years ago, I earned three graduate degrees (library and information science, information technology, and business administration) and held six library positions, four of which were at R1 higher education institutions. I climbed the leadership ladder from a ranking staff position to such positions as dean, vice provost, and university librarian at large academic and research libraries. I am extremely grateful for the opportunities that this country and its people have given me.},
}

@article{Birch2023,
  title = {Introduction},
  author = {Kean Birch},
  year = {2023},
  pages = {1-17},
  doi = {10.1007/978-3-031-46402-7_1},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-46402-7_1},
  abstract = {Today, digital personal data has become the defining resource for our societies and economies. Unfortunately, our personal data are increasingly concentrated in the hands of a small number of digital technology businesses often called Big Tech. The past decade has been defined by the rise of Big Tech as the dominant social players in our societies, and much of their rise and dominance is down to their control over our personal data. Big Tech has created a series of data enclaves that entrench their power and dominance, limiting the capacity of other businesses to compete in technoscientific capitalism. In building their data enclaves, Big Tech has engaged in a parasitic form of innovation, developing digital technologies designed to limit access to resources, to undermine regulations or social conventions, to undermine or avoid competition, to exploit customers psychology, to lock customers into using a product, to stop customers fixing their own property, or to use information asymmetries to treat customers inequitably. The contention of this book is that we need to rethink data governance in order to address the growing paradoxes and problems engendered by the market and social power of Big Tech.},
}

@article{Martínez2024,
  title = {Towards Understanding the Interplay of Generative Artificial Intelligence and the Internet},
  author = {Gonzalo Martínez and Lauren Watson and Pedro Reviriego and José Alberto Hernández and Marc Juarez and Rik Sarkar},
  year = {2024},
  pages = {59-73},
  publisher = {Springer Nature Switzerland},
  abstract = {The rapid adoption of generative Artificial Intelligence (AI) tools that can generate realistic images or text, such as DALL-E, MidJourney, or ChatGPT, have put the societal impacts of these technologies at the center of public debate. These tools are possible due to the massive amount of data (text and images) that is publicly available through the Internet. At the same time, these generative AI tools become content creators that are already contributing to the data that is available to train future models. Therefore, future versions of generative AI tools will be trained with a mix of human-created and AI-generated content, causing a potential feedback loop between generative AI and public data repositories. This interaction raises many questions: how will future versions of generative AI tools behave when trained on a mixture of real and AI-generated data? Will they evolve and improve with the new data sets or on the contrary will they degrade? Will evolution introduce biases or reduce diversity in subsequent generations of generative AI tools? What are the societal implications of the possible degradation of these models? Can we mitigate the effects of this feedback loop? In this work, we explore the effect of this interaction and report some initial results using simple diffusion models trained with various image datasets. Our results show that the quality and diversity of the generated images can degrade over time suggesting that incorporating AI-created data can have undesired effects on future versions of generative models.},
}

@article{Hu2025_01,
  title = {Artificial Intelligence in Higher Education: Applications, Challenges, and Policy Development and Further Considerations},
  author = {Shouping Hu and Fengfeng Ke and Dina Vyortkina and Pei Hu and Sam Luby and Joe O’Shea},
  year = {2025},
  pages = {265-315},
  doi = {10.1007/978-3-031-58698-9_13},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-58698-9_13},
  abstract = {In this chapter, we review the development of artificial intelligence in education in general and higher education in particular. We then describe a range of use cases of artificial intelligence in higher education and explore the efficacy of those applications on various constituents, including students, faculty, researchers, administrators, and staff members in colleges and universities. While describing various applications, we raise issues related to the challenges of artificial intelligence in higher education such as equity, security, and confidentiality, among other matters. We also document existing policy development and related initiatives from different levels (e.g., international, national, state, and institutional) regarding artificial intelligence. Finally, we explore future directions for research and policy considerations on artificial intelligence in higher education. As a rapidly changing area, there is an urgent need to build evidence base on the impacts of artificial intelligence on all constituents in higher education through rigorous research to guide policy and practice.},
}

@article{Morgner2024,
  title = {Siemens: Acting Resiliently Through Hybrid Process Intelligence in the Supply Chain Metaverse},
  author = {Robert Morgner and Markus Burger},
  year = {2024},
  pages = {175-191},
  doi = {10.1007/978-3-031-61343-2_19},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-61343-2_19},
  abstract = {In the face of disruptions like COVID-19, the Ukraine conflict, or the Suez Canal blockage, Siemens Supply Chain Management is encountering various challenges. The ability to withstand such supply chain disturbances, has evolved into a competitive edge, highlighting the growing importance of supply chain resilience. The strategic component of Supply Chain Management changes. The strategic ability to successfully manage supply chains, therefore needs to be rethought. Shorter product life-cycles and times between disruptions, require a high-level of proactivity. Integrating process intelligence and artificial intelligence into supply chain operations, holds promise that both can play a crucial role in improving resilience. This chapter presents the Siemens cycle of resilience concept, that is based on both human and artificial intelligence. We show four process intelligence approaches in terms of (1) incident management, (2) crisis control, (3) agile recovery, and (4) supply chain design, where the combination of human and artificial intelligences helps us to improve supply chain resilience. We show technology- and people-related success factors, e.g., data connectivity or change management, which turned out to be crucial when implementing such approaches. This cycle of resilience concept marks a decisive step towards our mission of a supply chain metaverse.},
}

@article{Zhao2024_01,
  title = {Metadata for Scientific Experiment Reporting: A Case Study in Metal-Organic Frameworks},
  author = {Xintong Zhao and Kyle Langlois and Jacob Furst and Scott McClellan and Xiaohua Hu and Yuan An and Diego A Gómez-Gualdrón and Fernando J Uribe-Romo and Jane Greenberg},
  year = {2024},
  pages = {30-40},
  publisher = {Springer Nature Switzerland},
  abstract = {Research methods and procedures are core aspects of the research process. Metadata focused on these components is critical to supporting the FAIR principles, particularly reproducibility. The research reported on in this paper presents a methodological framework for metadata documentation supporting the reproducibility of research producing Metal Organic Frameworks (MOFs). The MOF case study involved natural language processing to extract key synthesis experiment information from a corpus of research literature. Following, a classification activity was performed by domain experts to identify entity-relation pairs. Results include: 1) a research framework for metadata design, 2) a metadata schema that includes nine entities and two relationships for reporting MOF synthesis experiments, and 3) a growing database of MOF synthesis reports structured by our metadata scheme. The metadata schema is intended to support discovery and reproducibility of metal-organic framework research and the FAIR principles. The paper provides background information, identifies the research goals and objectives, research design, results, a discussion, and the conclusion.},
}

@article{Demchenko2024,
  title = {Introduction},
  author = {Yuri Demchenko and Juan J Cuadrado-Gallego and Oleg Chertov and Marharyta Aleksandrova},
  year = {2024},
  pages = {1-43},
  doi = {10.1007/978-3-031-69366-3_1},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-69366-3_1},
  abstract = {Modern science and industry are becoming increasingly digitized and data driven. Applications are becoming distributed and require integration of multiple infrastructure components. This creates a continuous trend to use Big Data services and applications which are generally cloud based. Effective design, development and operation of modern data driven applications and services must start from the beginning and involve the whole services lifecycle. Effective architecture and design solutions will ensure services efficiency.},
}

@article{Nikiforova2024_02,
  title = {From the Evolution of Public Data Ecosystems to the Evolving Horizons of the Forward-Looking Intelligent Public Data Ecosystem Empowered by Emerging Technologies},
  author = {Anastasija Nikiforova and Martin Lnenicka and Petar Milić and Mariusz Luterek and Manuel Pedro Rodríguez Bolívar},
  year = {2024},
  pages = {402-418},
  publisher = {Springer Nature Switzerland},
  abstract = {Public Data Ecosystems (PDEs) represent complex socio-technical systems crucial for optimizing data use in the public sector and outside it. Recognizing their multifaceted nature, previous research proposed a six-generation Evolutionary Model of Public Data Ecosystems (EMPDE). Designed as a result of a systematic literature review on the topic spanning three decades, this model, while theoretically robust, necessitates empirical validation to enhance its practical applicability. This study addresses this gap by validating the theoretical model through a real-life examination in five European countries - Latvia, Serbia, Czech Republic, Spain, and Poland. This empirical validation provides insights into PDEs dynamics and variations of implementations across contexts, particularly focusing on the 6th generation of forward-looking PDE generation named “Intelligent Public Data Generation” which represents a paradigm shift driven by emerging technologies such as cloud computing, Artificial Intelligence (AI), Natural Language Processing tools, Generative AI, and Large Language Models with potential to contribute to both automation and augmentation of business processes within these ecosystems. By transcending their traditional status as a mere component, evolving into both an actor and a stakeholder simultaneously, these technologies catalyse innovation and progress, enhancing PDE management strategies to align with societal, regulatory, and technical imperatives in the digital era.},
}

@article{Štefčík2025,
  title = {Research in Translation Studies},
  author = {Jozef Štefčík},
  year = {2025},
  pages = {231-278},
  doi = {10.1007/978-3-031-87205-1_5},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-87205-1_5},
  abstract = {Research in Translation Studies explores diverse and innovative areas, reflecting the field's adaptation to technological, cultural, and global changes. It integrates artificial intelligence and machine learning to improve translation quality and efficiency. The issues addressed in the research are related to the market demands of the time, i.e. from exploring equivalence from different aspects to simultaneously exploring the integration of AI into translation from both a process and product perspective. Technological revolution has brought new elements to TS that need to be first researched and understood and then integrated into education.},
}

@article{Rodean1981,
  title = {Inelastic Processes in Seismic Wave Generation by Underground Explosions},
  author = {Howard C Rodean},
  year = {1981},
  pages = {97-189},
  publisher = {Springer Netherlands},
  abstract = {There are similarities and differences between chemical and nuclear explosions underground. Most of the differences are in the early stages of the explosions. The later stages are similar with respect to seismic wave generation. Three sources of seismic waves from explosions are coincident in space and time, or nearly so: the explosion itself, explosion-induced tectonic strain release, and (probably) spall-closure following explosion-produced spall. Cavity collapse and explosion-induced aftershocks are two sources of delayed seismic signals. Theories, computer calculations, and measurements of spherical stress waves from explosions are described and compared, with emphasis on the transition from inelastic to almost-elastic relations between stress and strain. Two aspects of nonspherical explosion geometry are considered: tectonic strain release and surface spall. Tectonic strain release affects the generation of surface waves; spall closure may also. The forward problem in seismology can, in principle, be solved by calculations beginning with explosive detonation and ending with the synthetic seismogram. The inverse problem can also, in principle, be solved by inverting observed seismic data to obtain an “equivalent elastic source,” but the solution cannot extend backward in space and time into the nonlinear inelastic processes of the explosion. The reduced-displacement potential is a common solution (the “equivalent elastic source”) of the forward and inverse problems, assuming a spherical source. Measured reduced-displacement potentials are compared with potentials calculated as solutions of the direct and inverse problems; there are significant differences between the results of the two types of calculations and between calculations and measurements. The simple spherical model of an explosion is not sufficient to account for observations of explosions over wide ranges of depth and yield. The explosions environment can have a large effect on explosion detection and yield estimation. The best sets of seismic observations for use in developing discrimination techniques are for high-magnitude high-yield explosions; the identification problem is most difficult for low-magnitude low-yield explosion. Most of the presently available explosion data (time, medium, depth, yield, etc.) are for explosions in a few media at the Nevada Test Site; some key questions concerning magnitude vs yield and mb vs Ms relation can be answered only by data for explosion in other media at other locations.},
}

@article{Schuett2023,
  title = {Three lines of defense against risks from AI},
  author = {Jonas Schuett},
  year = {2023},
  journal = {AI & SOCIETY},
  doi = {10.1007/s00146-023-01811-0},
  url = {https://doi.org/10.1007/s00146-023-01811-0},
  abstract = {Organizations that develop and deploy artificial intelligence (AI) systems need to manage the associated risks—for economic, legal, and ethical reasons. However, it is not always clear who is responsible for AI risk management. The three lines of defense (3LoD) model, which is considered best practice in many industries, might offer a solution. It is a risk management framework that helps organizations to assign and coordinate risk management roles and responsibilities. In this article, I suggest ways in which AI companies could implement the model. I also discuss how the model could help reduce risks from AI: it could identify and close gaps in risk coverage, increase the effectiveness of risk management practices, and enable the board of directors to oversee management more effectively. The article is intended to inform decision-makers at leading AI companies, regulators, and standard-setting bodies.},
}

@article{Côté2024,
  title = {Data cleaning and machine learning: a systematic literature review},
  author = {Pierre-Olivier Côté and Amin Nikanjam and Nafisa Ahmed and Dmytro Humeniuk and Foutse Khomh},
  year = {2024},
  journal = {Automated Software Engineering},
  volume = {31},
  pages = {54},
  doi = {10.1007/s10515-024-00453-w},
  url = {https://doi.org/10.1007/s10515-024-00453-w},
  abstract = {Machine Learning (ML) is integrated into a growing number of systems for various applications. Because the performance of an ML model is highly dependent on the quality of the data it has been trained on, there is a growing interest in approaches to detect and repair data errors (i.e., data cleaning). Researchers are also exploring how ML can be used for data cleaning; hence creating a dual relationship between ML and data cleaning. To the best of our knowledge, there is no study that comprehensively reviews this relationship. This paper’s objectives are twofold. First, it aims to summarize the latest approaches for data cleaning for ML and ML for data cleaning. Second, it provides future work recommendations. We conduct a systematic literature review of the papers published between 2016 and 2022 inclusively. We identify different types of data cleaning activities with and for ML: feature cleaning, label cleaning, entity matching, outlier detection, imputation, and holistic data cleaning. We summarize the content of 101 papers covering various data cleaning activities and provide 24 future work recommendations. Our review highlights many promising data cleaning techniques that can be further extended. We believe that our review of the literature will help the community develop better approaches to clean data.},
}

@article{Li2025_02,
  title = {CodeDoctor: multi-category code review comment generation},
  author = {Yingling Li and Yuhan Wu and Zi’ao Wang and Lei Huang and Junjie Wang and Jianping Li and Minying Huang},
  year = {2025},
  journal = {Automated Software Engineering},
  volume = {32},
  pages = {25},
  doi = {10.1007/s10515-025-00491-y},
  url = {https://doi.org/10.1007/s10515-025-00491-y},
  abstract = {Code review is an effective software quality assurance activity. However, this process is labor-intensive and time-consuming, requiring reviewers to carefully review under various categories (e.g., function, refactoring, documentation, etc) to generate review comments. Several approaches have been proposed for automatic review comment generation, although they can generate review comments, they hardly cover all manual review comments. Because most of these approaches simply utilize the information of submitted code and review comments, not fully modeling the features of code review (i.e., ignoring review category, the association of issue snippets and review comments). In this paper, we propose CodeDoctor, an automatic review comment generator with data augmentation and category-aware encoder-decoder to generate multi-category review comments. It consists of three main phases: (1) Data augmentation phase, which classifies review comments and builds review exemplars (i.e., the pairs of issue snippet and its comment) to augment review data by using a large language model (LLM) with prompt engineering and feedback loops; (2) Encoder phase, which encodes the inputs (i.e., review category, diff code and review exemplar) into semantic and token representations; (3) Decoder phase, which designs a category-focused decoder to capture the most relevant information of given category for multi-category review comment generation. Evaluations with five commonly-used and state-of-the-art baselines on two datasets show that CodeDoctor outperforms all baselines, with 1770% higher average BLEU-4, 111% higher average ROUGE-L and 49% higher average F1 than the best baseline. Furthermore, a human evaluation also confirms the significant potential of applying CodeDoctor in practical usage. Our approach can relieve the burden of reviewers by automatically generating multi-category review comments, and helps developers better detect code issues as early as possible, thereby facilitating software development.},
}

@article{Amigud2024,
  title = {The Age of the Intelligent Machine: Singularity, Efficiency, and Existential Peril},
  author = {Alexander Amigud},
  year = {2024},
  journal = {Philosophy & Technology},
  volume = {37},
  pages = {49},
  doi = {10.1007/s13347-024-00740-0},
  url = {https://doi.org/10.1007/s13347-024-00740-0},
  abstract = {Machine learning, and more broadly artificial intelligence (AI), is a fascinating technology and can be considered as the closest approximation to the Cartesian “thinking thing” that humans have ever created. Just as the industrial revolution required a new ethos, the age of intelligent machines will create its own, challenging the established moral, economic, and political presuppositions. This paper discusses the relationship between AI and society; it presents several thought experiments to explore the complexity of the relationship and highlights the insufficiency of the current normative paradigm in addressing technological expansion. I argue that many of the externalities, such as security risks, loss of privacy, and economic instability  will result from trying to fit the emerging technologies into the existing frame of efficiency and utility, by redefining the notions of human value, identity, autonomy, purity, and truth, among others. The age of the intelligent machine is elevating alienation to new levels, treating the individual as mere patterns in data—its primary commodity. I further argue that while the possibility of unintended consequences, due to the potential misuse of AI is ever present, the intelligent machine per se is unlikely to engage in a zero-sum game for power on its own initiative. I question whether singularity is at all attainable and argue that technology will forever remain a proxy for human interests. I conclude by posing questions for charting the path forward. Through this analysis, I aim to provide a more nuanced understanding of the complex relationship between the AI and humans.},
}

@article{Wu2025_01,
  title = {Leveraging FDA Labeling Documents and Large Language Model to Enhance Annotation, Profiling, and Classification of Drug Adverse Events with AskFDALabel},
  author = {Leihong Wu and Hong Fang and Yanyan Qu and Joshua Xu and Weida Tong},
  year = {2025},
  journal = {Drug Safety},
  doi = {10.1007/s40264-025-01520-1},
  url = {https://doi.org/10.1007/s40264-025-01520-1},
  abstract = {Drug adverse events (AEs) represent a significant public health concern. US Food and Drug Administration (FDA) drug labeling documents are an essential resource for studying drug safety such as assessing a drug’s likelihood to cause certain organ toxicities; however, the manual extraction of AEs is labor-intensive, requires specialized expertise, and is challenging to maintain, due to frequent updates of the labeling documents.},
}

@article{Cossatin2025,
  title = {Tell me more: integrating LLMs in a cultural heritage website for advanced information exploration support},
  author = {Angelo Geninatti Cossatin and Noemi Mauro and Fabio Ferrero and Liliana Ardissono},
  year = {2025},
  journal = {Information Technology & Tourism},
  doi = {10.1007/s40558-025-00312-8},
  url = {https://doi.org/10.1007/s40558-025-00312-8},
  abstract = {Cultural Heritage websites’ capability to satisfy diverse information needs is limited by their high-quality but constrained knowledge bases. Thus, we investigate their extension with external large language models (LLMs), enriching the provision of cultural content by leveraging LLMs’ continuous collection and integration of information from heterogeneous data sources. This extension raises important challenges in synchronizing the LLM’s behavior with the user’s browsing activity on the website to offer a unified interaction environment. To address these challenges, we propose a loosely coupled integration model that provides users with curated content and an assisted question-answering function to answer information needs that the system’s knowledge base fails to cover. Our model is agnostic to the LLM and synchronizes its behavior with the user’s browsing activity through implicit prompt engineering. We tested a baseline website without LLM integration, one with free-text interaction with the LLM, and another that combines free-text interaction with the suggestion of context-dependent questions. In a user study involving 44 participants, we found that the LLM-powered website has higher usability and that context-dependent question suggestions further enhance user experience, especially for people with low curiosity levels (according to Curiosity and Exploration Inventory-II - CEI-II) who are guided in formulating effective questions. This shows the potential of LLMs to enrich engagement with existing Cultural Heritage websites.},
}

@article{Yang2023_01,
  title = {The impact of ChatGPT and LLMs on medical imaging stakeholders: Perspectives and use cases},
  author = {Jiancheng Yang and Hongwei Bran Li and Donglai Wei},
  year = {2023},
  journal = {Meta-Radiology},
  volume = {1},
  pages = {100007},
  doi = {https://doi.org/10.1016/j.metrad.2023.100007},
  url = {https://www.sciencedirect.com/science/article/pii/S2950162823000073},
  abstract = {This study investigates the transformative potential of Large Language Models (LLMs), such as OpenAI ChatGPT, in medical imaging. With the aid of public data, these models, which possess remarkable language understanding and generation capabilities, are augmenting the interpretive skills of radiologists, enhancing patient-physician communication, and streamlining clinical workflows. The paper introduces an analytic framework for presenting the complex interactions between LLMs and the broader ecosystem of medical imaging stakeholders, including businesses, insurance entities, governments, research institutions, and hospitals (nicknamed BIGR-H). Through detailed analyses, illustrative use cases, and discussions on the broader implications and future directions, this perspective seeks to raise discussion in strategic planning and decision-making in the era of AI-enabled healthcare.},
}

@article{Ostojić2025,
  title = {Systematic Literature Review of Optimization Algorithms for P||Cmax Problem},
  author = {Ostojić, Dragutin and Ramljak, Dušan and Urošević, Andrija and Jolović, Marija and Drašković, Radovan and Kakka, Jainil and Jakšić Krüger, Tatjana and Davidović, Tatjana},
  year = {2025},
  journal = {Symmetry},
  volume = {17},
  doi = {10.3390/sym17020178},
  publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219005027&doi=10.3390%2fsym17020178&partnerID=40&md5=bb83949fc8e01b53a0e6859a83447873},
  abstract = {In the era of open data and open science, it is important that, before announcing their new results, authors consider all previous studies and ensure that they have competitive material worth publishing. To save time, it is popular to replace the exhaustive search of online databases with the utilization of generative Artificial Intelligence (AI). However, especially for problems in niche domains, generative AI results may not be precise enough and sometimes can even be misleading. A typical example is (Formula presented.), an important scheduling problem studied mainly in a wider context of parallel machine scheduling. As there is an uncovered symmetry between (Formula presented.) and other similar optimization problems, it is not easy for generative AI tools to include all relevant results into search. Therefore, to provide the necessary background data to support researchers and generative AI learning, we critically discuss comparisons between algorithms for (Formula presented.) that have been presented in the literature. Thus, we summarize and categorize the “state-of-the-art” methods, benchmark test instances, and compare methodologies, all over a long time period. We aim to establish a framework for fair performance evaluation of algorithms for (Formula presented.), and according to the presented systematic literature review, we uncovered that it does not exist. We believe that this framework could be of wider importance, as the identified principles apply to a plethora of combinatorial optimization problems. © 2025 by the authors.},
}

@article{Siebenlist2023,
  title = {Approaches towards using ChatGPT as an open data companion},
  author = {Siebenlist, Tobias},
  year = {2023},
  pages = {674–675},
  doi = {10.1145/3598469.3598554},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3598469.3598554},
  abstract = {Many challenges are associated with the public’s use of open data. Often, there needs to be more skills and knowledge to develop ideas from published raw data, implement specific projects, and generate added value. The emergence of generative AI tools can help remedy this. This work-in-progress article presents approaches towards using ChatGPT as a companion for interested users who want to deal with open data and points out exemplary applications.},
}

@article{Duke2023,
  title = {Data},
  author = {Toju Duke},
  year = {2023},
  pages = {37-58},
  doi = {10.1007/978-1-4842-9306-5_3},
  publisher = {Apress},
  url = {https://doi.org/10.1007/978-1-4842-9306-5_3},
  abstract = {After setting up and understanding the AI principles, the next foundational element and part of responsible AI development is looking at the data and its guiding principles.},
}

@article{Sehgal2024,
  title = {Navigating the Technology Deluge},
  author = {Attul Sehgal},
  year = {2024},
  pages = {259-290},
  doi = {10.1007/978-1-4842-9499-4_7},
  publisher = {Apress},
  url = {https://doi.org/10.1007/978-1-4842-9499-4_7},
  abstract = {Up to this point, through classic case examples, we have established the concept of DX, delved into the fundamentals of big data and analytics, explored various productivity tools, overviewed the basics of data mining and algorithms, and examined strategies for making more business-oriented decisions in our DX endeavors.},
}

@article{Fist1996,
  title = {P},
  author = {Stewart Fist},
  year = {1996},
  pages = {485-536},
  doi = {10.1007/978-1-4615-2093-1_17},
  publisher = {Springer US},
  url = {https://doi.org/10.1007/978-1-4615-2093-1_17},
}

@article{Küspert1990,
  title = {Trends in distributed and cooperative database management},
  author = {K Küspert and E Rahm},
  year = {1990},
  pages = {263-293},
  publisher = {Springer Berlin Heidelberg},
}

@article{Ishibashi1987,
  title = {C},
  author = {Seiichi Ishibashi},
  year = {1987},
  pages = {98-192},
  doi = {10.1007/978-1-4684-5433-8_3},
  publisher = {Springer US},
  url = {https://doi.org/10.1007/978-1-4684-5433-8_3},
}

@article{Weik1997,
  title = {L},
  author = {Martin H Weik},
  year = {1997},
  pages = {507-556},
  doi = {10.1007/978-1-4615-6023-4_12},
  publisher = {Springer US},
  url = {https://doi.org/10.1007/978-1-4615-6023-4_12},
}

@article{Ilčev2020,
  title = {Introduction},
  author = {Stojče Dimov Ilčev},
  year = {2020},
  pages = {1-105},
  doi = {10.1007/978-3-030-30632-8_1},
  publisher = {Springer International Publishing},
  url = {https://doi.org/10.1007/978-3-030-30632-8_1},
  abstract = {Introduction gives a short background to the development of radio and satellite communication, navigation, and surveillance (CNS) systems and overview, concepts, and applications of GNSS systems in the function of current GADSS network for tracking, determination, and positioning of missing and hijacked aircraft. This chapter also introduces a retrospective of old Future Air Navigation System (FANS), with its positive and negative elements, and it presents basic elements of new GADSS infrastructure.},
}

@article{Taulli2023,
  title = {Risks},
  author = {Tom Taulli},
  year = {2023},
  pages = {151-171},
  doi = {10.1007/978-1-4842-9852-7_9},
  publisher = {Apress},
  url = {https://doi.org/10.1007/978-1-4842-9852-7_9},
  abstract = {For decades, there have been many warnings about the dangers of AI. These have often been from science fiction novels and movies like 2001: A Space Odyssey and Terminator.},
}

@article{Ocampo2023,
  title = {Using GPT-3 to Achieve Semantically Relevant Data Sonificiation for an Art Installation},
  author = {Rodolfo Ocampo and Josh Andres and Adrian Schmidt and Caroline Pegram and Justin Shave and Charlton Hill and Brendan Wright and Oliver Bown},
  year = {2023},
  pages = {212-227},
  publisher = {Springer Nature Switzerland},
  abstract = {Large Language Models such as GPT-3 exhibit generative language capabilities with multiple potential applications in creative practice. In this paper, we present a method for data sonification that employs the GPT-3 model to create semantically relevant mappings between artificial intelligence-generated natural language descriptions of data, and human-generated descriptions of sounds. We implemented this method in a public art installation to generate a soundscape based on data from different systems. While common sonification approaches rely on arbitrary mappings between data values and sonic values, our approach explores the use of language models to achieve a mapping not via values but via meaning. We find our approach is a useful tool for musification practice and demonstrates a new application of generative language models in creative new media arts practice. We show how different prompts influence data to sound mappings, and highlight that matching the embeddings of texts of different lengths produces undesired behavior.},
}

@article{Nicoletti2023,
  title = {Industrial Revolutions and Supply Network 5.0},
  author = {Bernardo Nicoletti},
  year = {2023},
  pages = {43-101},
  doi = {10.1007/978-3-031-22032-6_3},
  publisher = {Springer International Publishing},
  url = {https://doi.org/10.1007/978-3-031-22032-6_3},
  abstract = {This chapter deals with industrial revolutions. It sets the context of industry 5.0 for the supply network 5.0. It also underlines the concept of digital transformation. It describes the leading technological solutions connected with industry 5.0. This chapter includes the emerging challenges these solutions and methods uncover and some advisable follow-up strategies to extract the most value from these transformational processes while minimizing their possible downturn.},
}

@article{Weiser2023,
  title = {Quality Control in L-PBF for Industrial Production by Means of Production-Integrated Measurement Technology},
  author = {Lukas Weiser and Marco Batschkowski and Niclas Eschner and T Landgräber and F Ohlsen and S Seiz and Gisela Lanza},
  year = {2023},
  pages = {475-514},
  doi = {10.1007/978-3-031-20752-5_29},
  publisher = {Springer International Publishing},
  url = {https://doi.org/10.1007/978-3-031-20752-5_29},
  abstract = {Production integrated quality control can provide direct feedback on quality deviations in production systems. Thus, it is a crucial enabler to guarantee high-quality standards, prevent defective parts, and prohibit waste for industrial production with laser powder bed fusion (L-PBF). Different in-process measurement technologies have already been implemented and applied in the L-PBF process. Based on a framework for structuring production-integrated measurement technologies, this chapter gives an overview of existing monitoring techniques and the used sensor types for L-PBF.},
}

@article{Wójcik2022,
  title = {Foundation Models in Healthcare: Opportunities, Biases and Regulatory Prospects in Europe},
  author = {Malwina Anna Wójcik},
  year = {2022},
  pages = {32-46},
  publisher = {Springer International Publishing},
  abstract = {This article concerns the rise of a new paradigm in AI - “foundation models,” which are pre-trained on broad data at scale and subsequently adapted to particular downstream tasks. In particular, it explores the issue from the perspective of healthcare and biomedicine, focusing on the benefits of foundation models, as well as their propensity to encode bias, which threatens to exacerbate discriminatory practices already experienced by patients in Europe. Section 1 offers a brief introduction concerning the use of AI in healthcare and biomedicine and the problem of divergencies in access to and quality of healthcare across Europe. Section 2 familiarises the reader with the technical qualities of foundation models and recent developments in the field. Section 3 explains how the new health data strategy proposed by the EU could foster the development of foundation models in healthcare. Section 4 elaborates on their benefits in healthcare and biomedicine, while Sect. 5 explores the risk of bias exhibited by foundation models. Section 6 comments on the uncertain status of foundation models under the proposed Artificial Intelligence Act and offers brief recommendations concerning future regulation. Section 7 concludes.},
}

@article{Ahmed2023,
  title = {Explainable Integration of Knowledge Graphs Using Large Language Models},
  author = {Abdullah Fathi Ahmed and Asep Fajar Firmansyah and Mohamed Ahmed Sherif and Diego Moussallem and Axel-Cyrille Ngonga Ngomo},
  year = {2023},
  pages = {124-139},
  publisher = {Springer Nature Switzerland},
  abstract = {Linked knowledge graphs build the backbone of many data-driven applications such as search engines, conversational agents and e-commerce solutions. Declarative link discovery frameworks use complex link specifications to express the conditions under which a link between two resources can be deemed to exist. However, understanding such complex link specifications is a challenging task for non-expert users of link discovery frameworks. In this paper, we address this drawback by devising NMV-LS, a language model-based verbalization approach for translating complex link specifications into natural language. NMV-LS relies on the results of rule-based link specification verbalization to apply continuous training on T5, a large language model based on the Transformer architecture. We evaluated NMV-LS on English and German datasets using well-known machine translation metrics such as BLUE, METEOR, ChrF++ and TER. Our results suggest that our approach achieves a verbalization performance close to that of humans and outperforms state of the art approaches. Our source code and datasets are publicly available at https://github.com/dice-group/NMV-LS.},
}

@article{Georgievskaya2023,
  title = {Artificial Intelligence Approaches for Skin Anti-aging and Skin Resilience Research},
  author = {Anastasia Georgievskaya and Daniil Danko and Richard A Baxter and Hugo Corstjens and Timur Tlyachev},
  year = {2023},
  pages = {189-214},
  doi = {10.1007/978-3-031-35176-1_10},
  publisher = {Springer International Publishing},
  url = {https://doi.org/10.1007/978-3-031-35176-1_10},
  abstract = {The skin is a complex organ whose functioning is affected by both environmental and intrinsic factors, making it a perfect model for studying the agingAging process at many different levels of analysis. Multi-dimensional dataMulti-dimensional data obtained in the course of agingAging-related research are difficult to analyze. However, with the use of artificial intelligenceArtificial Intelligence (AI), datasets at the molecular, geneticGenetics and biophysical information levels become more insightful and help strengthen skin resilienceSkin resilience. AI also plays a major role in the visualization and simulation of skin and its derivatives (hair and nails). AI-driven technologies thus contribute to advances in skin agingSkin aging research, including method development and data acquisition, evaluation and interpretation. It supports the development of new drugs, optimizes treatment recommendations and aids in substantiating the effectiveness of personalized approaches. This chapter outlines some future prospects of the application of AI in the areas of personalizationPersonalization and inclusiveness for both skin researchSkin research and clinical practiceClinical practice.},
}

@article{Greco2023,
  title = {The Future of STM Journals},
  author = {Albert N Greco},
  year = {2023},
  pages = {127-151},
  doi = {10.1007/978-3-031-31964-8_7},
  publisher = {Springer International Publishing},
  url = {https://doi.org/10.1007/978-3-031-31964-8_7},
  abstract = {The transformation of science, technology, and medical (STM) journals from a print orientation to a hybrid to a digital only format, along with increased open access (OA) regulations, and criticism about journal prices, taxed the resilience of many STM publishers. However, the purpose of a business has not changed since the dawn of time. It is to understand and satisfy the wants and needs of consumers. Most STM publishers have been doing this successfully since 1665, and it is likely that they will continue to understand and respond effectively to the constantly changing STM marketplace in the years to come.},
}

@article{Vidgof2023,
  title = {Large Language Models for Business Process Management: Opportunities and Challenges},
  author = {Maxim Vidgof and Stefan Bachhofner and Jan Mendling},
  year = {2023},
  pages = {107-123},
  publisher = {Springer Nature Switzerland},
  abstract = {Large language models are deep learning models with a large number of parameters. The models made noticeable progress on a large number of tasks, and as a consequence allowing them to serve as valuable and versatile tools for a diverse range of applications. Their capabilities also offer opportunities for business process management, however, these opportunities have not yet been systematically investigated. In this paper, we address this research problem by foregrounding various management tasks of the BPM lifecycle. We investigate six research directions highlighting problems that need to be addressed when using large language models, including usage guidelines for practitioners.},
}

@article{Hanke2024,
  title = {The Quayside Project: Some Reassembly Required},
  author = {Bob Hanke},
  year = {2024},
  pages = {167-214},
  doi = {10.1007/978-3-031-41546-3_4},
  publisher = {Springer International Publishing},
  url = {https://doi.org/10.1007/978-3-031-41546-3_4},
  abstract = {In this book, I have developed an approach the Quayside project that attends to journalism discourse and the smart city. In this chapter, I review and further contextualize the Quayside project story to explain why this smart city assemblage did not work out. Through retrospective assimilation, we can better understand how mediatized controversy unsettled the public image of a smart city project. In the narrative progression, a smart city frame was counterposed with two other frames that associated the project with three major problems: privacy, governance, and intellectual property. An oppositional dimension with persuasive force led to major modifications of the project. Outside of Sidewalk Toronto, government organizations had slower time scales for policy making. I conclude that mediatized controversy conditioned this particular smart city project’s alteration and dissolution. In closing, I return to journalism as curriculum to assess dialogical urban learning.},
}

@article{Peña2023,
  title = {Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs},
  author = {Alejandro Peña and Aythami Morales and Julian Fierrez and Ignacio Serna and Javier Ortega-Garcia and Íñigo Puente and Jorge Córdova and Gonzalo Córdova},
  year = {2023},
  pages = {20-33},
  publisher = {Springer Nature Switzerland},
  abstract = {The analysis of public affairs documents is crucial for citizens as it promotes transparency, accountability, and informed decision-making. It allows citizens to understand government policies, participate in public discourse, and hold representatives accountable. This is crucial, and sometimes a matter of life or death, for companies whose operation depend on certain regulations. Large Language Models (LLMs) have the potential to greatly enhance the analysis of public affairs documents by effectively processing and understanding the complex language used in such documents. In this work, we analyze the performance of LLMs in classifying public affairs documents. As a natural multi-label task, the classification of these documents presents important challenges. In this work, we use a regex-powered tool to collect a database of public affairs documents with more than 33K samples and 22.5M tokens. Our experiments assess the performance of 4 different Spanish LLMs to classify up to 30 different topics in the data in different configurations. The results shows that LLMs can be of great use to process domain-specific documents, such as those in the domain of public affairs.},
}

@article{Schacht2023,
  title = {PromptIE - Information Extraction with Prompt-Engineering and Large Language Models},
  author = {Sigurd Schacht and Sudarshan Kamath Barkur and Carsten Lanquillon},
  year = {2023},
  pages = {507-514},
  publisher = {Springer Nature Switzerland},
  abstract = {Extracting triples of subjects, objects, and predicates from text to populate knowledge bases traditionally involves several intermediate steps such as co-reference resolution, named entity recognition, and relationship extraction. Treating triple extraction as translation task from source sentences to sets of triples, we present an end-to-end solution for information extraction that uses task prefixes to prompts a fine-tuned large language model to extract triples from text. Thus, the need for data labeling and training multiple models is reduced.},
}

@article{Colucci2023,
  title = {On the Relevance of Explanation for RDF Resources Similarity},
  author = {Simona Colucci and Francesco M Donini and Eugenio Di Sciascio},
  year = {2023},
  pages = {96-107},
  publisher = {Springer Nature Switzerland},
  abstract = {Artificial Intelligence (AI) has been shown to productively affect organizational decision making, in terms of returned economic value. In particular, agile business may significantly benefit from the ability of AI systems to constantly pursue contextual knowledge awareness. Undoubtedly, a key added value of such systems is the ability to explain results. In fact, users are more inclined to trust and feel the accountability of systems, when the output is returned together with a human-readable explanation. Nevertheless, some of the information in an explanation might be irrelevant to users—despite its truthfulness. This paper discusses the relevance of explanation for resources similarity, provided by AI systems. In particular, the analysis focuses on one system based on Large Language Models (LLMs)—namely ChatGPT— and on one logic-based tool relying on the computation of the Least Common Subsumer in the Resource Description Framework (RDF). This discussion reveals the need for a formal distinction between relevant and irrelevant information, that we try to answer with a definition of relevance amenable to implementation.},
}

@article{May2023,
  title = {Publishers to Platforms: Social Media as Data},
  author = {Layla May},
  year = {2023},
  pages = {169-175},
  doi = {10.1007/978-3-031-44861-4_8},
  publisher = {Springer International Publishing},
  url = {https://doi.org/10.1007/978-3-031-44861-4_8},
  abstract = {The impactful role of social media for transnational feminist connections demonstrates how activism can be curtailed or facilitated by developing technology. Online activism is not necessarily the direct cause of policy change, but it can encourage discussion and collaboration and provide a voice to many who would not be able to reach such an international audience without the internet.},
}

@article{Peña2023_01,
  title = {Data Has the Power to Transform Society},
  author = {Carlos Alonso Peña and Alberto Palomo Lozano and Javier Esteve Pradera},
  year = {2023},
  pages = {179-198},
  doi = {10.1007/978-3-031-43773-1_9},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-43773-1_9},
  abstract = {Data serves as society’s transformative catalyst, fostering knowledge, propelling innovation, and empowering communities. The administration’s pivotal role in collectively distributing values and effectively governing citizen services cannot be overstated.},
}

@article{Wang2023_01,
  title = {Starting from “Two Suitcases”: My Journey Leading Academic and Research Libraries in America},
  author = {Xuemao Wang},
  year = {2023},
  pages = {245-257},
  doi = {10.1007/978-3-031-42379-6_22},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-42379-6_22},
  abstract = {I was born and raised in China and grew up during the tumultuous period of the Cultural Revolution. After working for an academic library during my undergraduate education in China, I came to the United States with an eagerness and curiosity to pursue the American Dream. Driven by the belief that one’s ultimate life goal is to reach one’s greatest potential, and influenced by the hardship of my early life in China, I have enjoyed pursuing my American Dream. Since coming to the United States more than 30 years ago, I earned three graduate degrees (library and information science, information technology, and business administration) and held six library positions, four of which were at R1 higher education institutions. I climbed the leadership ladder from a ranking staff position to such positions as dean, vice provost, and university librarian at large academic and research libraries. I am extremely grateful for the opportunities that this country and its people have given me.},
}

@article{Bautista2023,
  title = {Health Disparities Through Generative AI Models: A Comparison Study Using a Domain Specific Large Language Model},
  author = {Yohn Jairo Parra Bautista and Carlos Theran and Richard Aló and Vinicious Lima},
  year = {2023},
  pages = {220-232},
  publisher = {Springer Nature Switzerland},
  abstract = {Health disparities are differences in health outcomes and access to healthcare between different groups, including racial and ethnic minorities, low-income people, and rural residents. An artificial intelligence (AI) program called large language models (LLMs) can understand and generate human language, improving health communication and reducing health disparities. There are many challenges in using LLMs in human-doctor interaction, including the need for diverse and representative data, privacy concerns, and collaboration between healthcare providers and technology experts. We introduce the comparative investigation of domain-specific large language models such as SciBERT with a multi-purpose LLMs BERT. We used cosine similarity to analyze text queries about health disparities in exam rooms when factors such as race are used alone. Using text queries, SciBERT fails when it doesn’t differentiate between queries text: “race” alone and “perpetuates health disparities.” We believe clinicians can use generative AI to create a draft response when communicating asynchronously with patients. However, careful attention must be paid to ensure they are developed and implemented ethically and equitably.},
}

@article{Huang2023,
  title = {ChatGPT in Nutrition Science},
  author = {Ken Huang and Yuyan (Lynn) Duan},
  year = {2023},
  pages = {159-185},
  doi = {10.1007/978-3-031-45282-6_6},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-45282-6_6},
  abstract = {Nutrition science studies how nutrients affect health. Generative AI (GenAI) applications, like ChatGPT, can personalize nutrition recommendations considering individual data. It can also help create new nutrition products and identify dietary patterns related to health outcomes. When combined with Web3 technologies, GenAI offers fresh solutions to ongoing challenges in nutrition science.},
}

@article{Monteiro2023,
  title = {Optimization Strategies for BERT-Based Named Entity Recognition},
  author = {Monique Monteiro and Cleber Zanchettin},
  year = {2023},
  pages = {80-94},
  publisher = {Springer Nature Switzerland},
  abstract = {Transfer learning through language modeling achieved state-of-the-art results for several natural language processing tasks such as named entity recognition, question answering, and sentiment analysis. However, despite these advancements, some tasks still need more specific solutions. This paper explores different approaches to enhance the performance of Named Entity Recognition (NER) in transformer-based models that have been pre-trained for language modeling. We investigate model soups and domain adaptation methods for Portuguese language entity recognition, providing valuable insights into the effectiveness of these methods in NER performance and contributing to the development of more accurate models. We also evaluate NER performance in few/zero-shot learning settings with a causal language model. In particular, we evaluate diverse BERT-based models trained on different datasets considering general and specific domains. Our results show significant improvements when considering model soup techniques and in-domain pretraining compared to within-task pretraining.},
}

@article{Gordon2024_02,
  title = {The Challenge of the Built Environment},
  author = {Ian Gordon and Neil Thompson},
  year = {2024},
  pages = {33-106},
  doi = {10.1007/978-3-031-51008-3_2},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-51008-3_2},
  abstract = {In this chapter we begin to establish some of the unique aspects of working with data in the built environment sector. In doing so we describe the problem statement that we are seeking to answer through this book: can data improve the performance and outcomes of the built environment? We look at how the use of data can tie into the aspirations of the sector to create social value and drive sustainability using the Infrastructure Project Authority’s (IPA) Transforming Infrastructure Performance (TIP) Roadmap to 2030 as our rubric. We cover the profound role that better use of data must play in helping the built environment realise net zero, as well as covering pertinent topics for application of data to the built environment, namely interoperability, data sharing, safety, data ownership, and data valuation.},
}

@article{Gordon2024_03,
  title = {Data and Organisational Culture},
  author = {Ian Gordon and Neil Thompson},
  year = {2024},
  pages = {141-197},
  doi = {10.1007/978-3-031-51008-3_4},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-51008-3_4},
  abstract = {In this chapter we begin to address how to make the case for data within your own organisation. This means wrestling with what it means to have a positive/receptive data culture in your organisation, as well as the psychology of individual decision-making. We then touch upon working across a complex organisation, and how to foster collaboration with stakeholders and communities of practice. We describe the purpose and formulation of a data-enabled organisational strategy, data vision statement, data principles, and identify the data capabilities required to deliver these. We then dive into example use cases and benefits statements that one might encounter in a built environment organisation, before finishing with a description of the data roles and skills that you are likely to require to deliver and embed your data capabilities.},
}

@article{Ciesla2024,
  title = {The Current Era of Chatbots},
  author = {Robert Ciesla},
  year = {2024},
  pages = {53-89},
  doi = {10.1007/978-3-031-51004-5_4},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-51004-5_4},
  abstract = {In this chapter we’re moving on from the fundamentals of chatbots right to the cutting edge. Chatbots are no longer a mere scientific curiosity or for pure entertainment. Chatbot-technology is increasingly integrated with productivity applications, too (e.g. Brainasoft’s products and other virtual assistants). Compared to the trio of historical implementations featured in the previous chapter, the modern era fully leverages technologies like advanced NLP and artificial neural networks as discussed in Chap. 2.},
}

@article{Benedetti2024,
  title = {Using Fintech in Sovereign Wealth Fund Operations},
  author = {Hugo Benedetti and Francisco Pavlic},
  year = {2024},
  pages = {301-316},
  doi = {10.1007/978-3-031-50821-9_18},
  publisher = {Springer International Publishing},
  url = {https://doi.org/10.1007/978-3-031-50821-9_18},
  abstract = {The Palgrave Handbook of Sovereign Wealth Funds aims to provide a comprehensive analysis of SWFs from a multidimensional perspective. It spans the gamut from theoretical to practical while offering the right balance of detailed and user-friendly coverage. Discussion of relevant research permeates the handbook. This volume helps fill the gap by showing how SWFs are a growing and dynamic force in international finance. This chapter discusses the book’s distinguishing features, intended audience, and structure. It provides an overview of each section and chapter.},
}

@article{Burgess2024,
  title = {Living with AI},
  author = {Andrew Burgess},
  year = {2024},
  pages = {151-161},
  doi = {10.1007/978-3-031-50722-9_10},
  publisher = {Springer International Publishing},
  url = {https://doi.org/10.1007/978-3-031-50722-9_10},
  abstract = {The chapter discusses the impact of Artificial Intelligence (AI) on society, individuals, and businesses. It highlights the story of AI pioneers Marvin Minsky and Doug Engelbart, emphasising the need for AI to benefit people. The text explores the democratisation of AI, arguing that AI should be open, fair, and used for social benefit. It also discusses the potential risks of AI, such as misuse and societal impact. The author suggests implementing a Data Ethics Framework to manage these risks, which includes strategic, operational, and technical perspectives. He also recommends establishing a Data Ethics Forum for ethical decision-making. The chapter further discusses the impact of AI on jobs, suggesting that while some jobs may be lost, others could be created or enhanced. It emphasises the importance of senior management's commitment to an AI program and the need for businesses to understand and responsibly use AI. The chapter concludes by suggesting that with the right knowledge and tools, businesses can confidently use AI for transformational change.},
}

@article{Li2024_02,
  title = {Harnessing AI for Project Risk Management: A Paradigm Shift},
  author = {He Li and Mohammad Yazdi and Arman Nedjati and Rosita Moradi and Sidum Adumene and Uyen Dao and Amirhossein Moradi and Aida Haghighi and Francis Eemmanuel Obeng and Cheng-Geng Huang and Hooi Siang Kang and Reza Ghasemi Pirbalouti and Esmaeil Zarei and Majeed Dehghan and Mahlagha Darvishmotevali and Peiman Ghasemi and Payam Shayan Fard and Harish Garg},
  year = {2024},
  pages = {253-272},
  doi = {10.1007/978-3-031-51719-8_16},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-51719-8_16},
  abstract = {Project risk management is pivotal in ensuring the success of endeavours within contemporary organizations. As businesses navigate an intricate and competitive landscape, integrating artificial intelligence (AI) introduces a paradigm shift in how project risks are recognized, evaluated, and addressed. This chapter go through into the multifaceted impact of AI on project risk management, reshaping strategies and enhancing decision-making processes to elevate project outcomes. By leveraging predictive analytics and machine learning, AI uncovers hidden patterns in historical project data, enabling proactive risk anticipation and simulation-based mitigation strategies. AI's cognitive capabilities expedite risk assessment by aggregating diverse data sources, fostering agility in response strategies through real-time communication and collaboration among project teams. Beyond operational facets, AI-driven algorithms continuously adapt to evolving risk landscapes, refining risk management strategies and resource allocation. Ethical considerations arise with the expanding role of AI, emphasizing the importance of transparency and accountability in algorithmic decision-making. This chapter envisions a dynamic convergence of AI and project risk management, fostering a future where innovation harmonizes with human understanding, propelling projects towards excellence, resilience, and growth.},
}

@article{Bezuidenhout2024,
  title = {Character Comes from Practice: Longitudinal Practice-Based Ethics Training in Data Science},
  author = {Louise Bezuidenhout and Emanuele Ratti},
  year = {2024},
  pages = {181-201},
  doi = {10.1007/978-3-031-51560-6_11},
  publisher = {Springer International Publishing},
  url = {https://doi.org/10.1007/978-3-031-51560-6_11},
  abstract = {In this chapter, we propose a non-traditional RCR training in data science that is grounded in a virtue theory framework. First, we delineate the approach in more theoretical detail by discussing how the goal of RCR training is to foster the cultivation of certain moral abilities. We specify the nature of these ‘abilities’: while the ideal is the cultivation of virtues, the limited space allowed by RCR modules can only facilitate the cultivation of superficial abilities or proto-virtues, which help students to familiarize themselves with moral and political issues in the data science environment. Third, we operationalize our approach by stressing that (proto-)virtue acquisition (like skill acquisition) occurs through the technical and social tasks of daily data science activities, where these repetitive tasks provide the opportunities to develop (proto-)virtue capacity and to support the development of ethically robust data systems. Finally, we discuss a concrete example of implementing this approach. In particular, we describe how this method is applied to teach data ethics to students participating in the CODATA-RDA Data Science Summer Schools.},
}

@article{Moro-Visconti2024,
  title = {The Valuation of Software as a Prerequisite for Artificial Intelligence},
  author = {Roberto Moro-Visconti},
  year = {2024},
  pages = {345-404},
  doi = {10.1007/978-3-031-53622-9_6},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-53622-9_6},
  abstract = {Software, generally understood as a sequence of computer instructions for performing functions on devices such as hardware, falls within the category of intangible assets and, due to its pervasiveness in people’s daily lives, represents an essential tool for any user. As a result, some complex legal issues, as well as sophisticated economic evaluation approaches come out, to estimate the damage caused by counterfeiting, in an evolutionary scenario characterized by sharp technological discontinuities. The database indicates a set of data, homogeneous in content and format; these data are stored in a computer and interrogated via a computer-implemented system by using the provided access keys. The estimation follows the general evaluation criteria of the other intangible assets—the income, cost, or market approach—to be adapted to the specific characteristics of the software or database.},
}

@article{Flierl2024,
  title = {Artificial Intelligence and Information Literacy: Hazards and Opportunities},
  author = {Michael Flierl},
  year = {2024},
  pages = {52-63},
  publisher = {Springer Nature Switzerland},
  abstract = {While the technical definition of “artificial intelligence” is contested and generally refers to groups of different technologies, what is clear is that as of 2023 a new era of generative AI technologies has begun. AI systems have passed medical licensing exams, won art competitions, and are used in commercial applications to treat mental illness. This conceptual paper will explore the costs and possible benefits to such advanced technology with an emphasis on finding the most useful questions for information literacy (IL) professionals to devote precious time, resources, and energy on tackling. The time to develop the most pressing questions is now given the speed of AI development.},
}

@article{Vidal-Gil2024,
  title = {Data Platform for a Data-Driven Tourism Organization. A Conceptual Architecture},
  author = {Juan Vidal-Gil and Ramón Alberto Carrasco-González and María Francisca Blasco-López},
  year = {2024},
  pages = {103-112},
  publisher = {Springer Nature Switzerland},
  abstract = {The tourism sector is one of the sectors that has undergone most changes in recent years due to digital transformation. One of the pillars of this transformation is the management of organizations based on data-driven decision making. The raw material for these data-driven strategies is, of course, the sources of information used, which have changed and grown significantly in recent years. This article attempts to provide a conceptual architecture for a modern data platform that effectively manages and analyses these information sources and facilitates data-driven decision-making in tourism organizations.},
}

@article{Flierl2024_01,
  title = {Artificial Intelligence and Information Literacy: Hazards and Opportunities},
  author = {Michael Flierl},
  year = {2024},
  pages = {52-63},
  publisher = {Springer Nature Switzerland},
  abstract = {While the technical definition of “artificial intelligence” is contested and generally refers to groups of different technologies, what is clear is that as of 2023 a new era of generative AI technologies has begun. AI systems have passed medical licensing exams, won art competitions, and are used in commercial applications to treat mental illness. This conceptual paper will explore the costs and possible benefits to such advanced technology with an emphasis on finding the most useful questions for information literacy (IL) professionals to devote precious time, resources, and energy on tackling. The time to develop the most pressing questions is now given the speed of AI development.},
}

@article{Brunet-Gouet2024,
  title = {Can a Conversational Agent Pass Theory-of-Mind Tasks? A Case Study of ChatGPT with the Hinting, False Beliefs, and Strange Stories Paradigms},
  author = {Eric Brunet-Gouet and Nathan Vidal and Paul Roux},
  year = {2024},
  pages = {107-126},
  publisher = {Springer Nature Switzerland},
  abstract = {We investigate the possibility that the recently proposed OpenAI’s ChatGPT conversational agent could be examined with classical theory-of-mind paradigms. We used an indirect speech understanding task, the hinting task, a new text version of a False Belief/False Photographs paradigm, and the Strange Stories paradigm. The hinting task is usually used to assess individuals with autism or schizophrenia by requesting them to infer hidden intentions from short conversations involving two characters. In a first experiment, ChatGPT 3.5 exhibits quite limited performances on the Hinting task when either original scoring or revised rating scales are used. We introduced slightly modified versions of the hinting task in which either cues about the presence of a communicative intention were added or a specific question about the character’s intentions were asked. Only the latter demonstrated enhanced performances. No dissociation between the conditions was found. The Strange Stories were associated with correct performances but we could not be sure that the algorithm had no prior knowledge of the test. In the second experiment, the most recent version of ChatGPT (4-0314) exhibited better performances in the Hinting task, although they did not match the average scores of healthy subjects. In addition, the model could solve first and second order False Beliefs tests but failed on items with reference to a physical property like object visibility or more complex inferences. This work offers an illustration of the possible application of psychological constructs and paradigms to a conversational agent of a radically new nature.},
}

@article{Hanna2024,
  title = {The Global, Technological, and Institutional Contexts},
  author = {Nagy K Hanna},
  year = {2024},
  pages = {1-18},
  doi = {10.1007/978-3-031-54569-6_1},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-54569-6_1},
  abstract = {This chapter covers the broad global economic and technological contexts for development practice and the evolving role of aid agencies, particularly the World Bank. Countries and aid agencies have a chance to “do development right” and leapfrog old technologies and entrenched practices. Can digital solutions enable a cleaner and leaner lifestyle? Can smart cities and smart infrastructures lead to better capacity for adaptation and resilience, and at the same time, higher productivity?},
}

@article{Banipal2024,
  title = {Intelligent Code Comments Morphing and Generation},
  author = {Indervir Singh Banipal and Shubhi Asthana and Sourav Mazumder and Nadiya Kochura},
  year = {2024},
  pages = {654-660},
  publisher = {Springer Nature Switzerland},
  abstract = {Technologists with various personas such as data scientists, software architects, software engineers, data engineers and research scientists spend hours writing piece of code, using different algorithms and commenting their code snippets or scripts. However, it takes considerable amount of time to comment the code and customize these comments based on the knowledge level of audience. In an ideal world, if everyone belongs to the same persona which means similar background and skill set, having the same set of comments on source code repositories should be perfect. But with the advent of multiple personas in the recent years, there needs to be a mechanism wherein the comments are synthesized and custom tailored dynamically based on the persona viewing the code. The comments should be added at appropriate instances of the code to make it more readable, contextual, meaningful depending on the persona of the reader. In our paper, we propose a novel method to consider persona into account to generate comments for the source code using Natural Language Processing (NLP) and generative AI. The generated comments will takes into account the context and intention while writing the code. It also includes a real time feedback loop which helps enhance the persona understanding and model improvement.},
}

@article{Marabelli2024,
  title = {Discipline, Punish … and Workarounds},
  author = {Marco Marabelli},
  year = {2024},
  pages = {115-145},
  doi = {10.1007/978-3-031-53919-0_4},
  publisher = {Springer International Publishing},
  url = {https://doi.org/10.1007/978-3-031-53919-0_4},
  abstract = {AI’s ability to extract, classify, and process data on individuals and groups, as I explain in the previous chapters, necessarily leads to various forms of control. For instance, in Chapter 2, I discuss how Uber drivers can feel uncomfortable because the app manages their rides and monitors the minutiae of their work-life. In this chapter, I further expand on the Uber driver example (among many others) and rely on two studies that I recently conducted with data collected in the US and the UK.},
}

@article{Wang2024_04,
  title = {Risk Management},
  author = {Liang Wang and Jianxin Zhao},
  year = {2024},
  pages = {179-201},
  doi = {10.1007/978-3-031-55885-6_9},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-55885-6_9},
  abstract = {This chapter explores the risks involved in developing enterprise analytics and outlines strategies to mitigate these risks effectively. It covers topics such as technology obsolescence, cybersecurity threats, and the critical need for compliance with relevant regulations. Moreover, this chapter thoroughly examines the ethical considerations and responsibilities inherent in the use if data and AI.},
}

@article{Fushman2024,
  title = {Question Answering},
  author = {Dina Demner Fushman},
  year = {2024},
  pages = {231-263},
  doi = {10.1007/978-3-031-55865-8_9},
  publisher = {Springer International Publishing},
  url = {https://doi.org/10.1007/978-3-031-55865-8_9},
  abstract = {Question answering refers to the process of providing direct and precise answers to natural language questions. Biomedical question answering is a task directed towards aiding researchers, healthcare professionals and the public in managing the continuous growth of information in the biomedical domain. Question answering requires the use of complex natural language processing techniques in order to produce accurate responses. Question answering implies that the information about the topics of interest and the users’ preferences are extracted or inferred from the free-form questions posed in natural language. Questions and the need for personalized answers or summaries arise in all biomedical sub-domains: clinicians have questions about their patients that can be answered by the documents in electronic health records and the biomedical literature; patients have questions about their health that can be answered by consumer-friendly sources; biologists need informative summaries of the recent publications in their areas of interest; administrators have questions about healthcare policies; quality of healthcare, and disease outbreaks. These and many other areas of question answering and summarization are a fertile ground for research and development of applications. Note that many of the traditional knowledge-based approaches might seem irrelevant at the time of this writing as the Neural QA approaches show promise and potential to replace the traditional approaches. Knowing what was successful in the past and which elements are essential to getting the right answers, however, is needed to inform the development of the neural approaches. This chapter, therefore, provides an overview of the approaches to biomedical question answering as they were evolving.},
}

@article{Puschmann2024,
  title = {Information Technology},
  author = {Thomas Puschmann and H S H Prince Michael of Liechtenstein},
  year = {2024},
  pages = {5-37},
  doi = {10.1007/978-3-031-55700-2_2},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-55700-2_2},
  abstract = {This chapter describes the concept of digitalization and outlines converging and exponential technology as the main drivers of digitalization. Further, the chapter introduces the evolution and current status of financial technology as a major accelerator of change for the financial system. For this, blockchain, artificial intelligence, quantum computing, and web3 are described in more detail as major drivers of financial technology, which shape the future financial system.},
}

@article{Mills2024,
  title = {The Future of Fintech and the American Dream},
  author = {Karen G Mills},
  year = {2024},
  pages = {223-235},
  doi = {10.1007/978-3-031-55612-8_15},
  publisher = {Springer International Publishing},
  url = {https://doi.org/10.1007/978-3-031-55612-8_15},
  abstract = {Small business lending was first recorded over 3000 years ago when merchants in Mesopotamia borrowed silver and paid interest to fund their businesses. In many ways, the truths of small business lending have been constant over time. Small business lending is risky and filled with frictions. All this may be changing with the innovations that technology has brought to the sector. The author concludes with three predictions for the new world of small business lending and posits positive outcomes from transformations that fintech innovations will bring.},
}

@article{Mills2024_01,
  title = {Small Business Utopia},
  author = {Karen G Mills},
  year = {2024},
  pages = {139-151},
  doi = {10.1007/978-3-031-55612-8_10},
  publisher = {Springer International Publishing},
  url = {https://doi.org/10.1007/978-3-031-55612-8_10},
  abstract = {This chapter focuses on the future of small business lending and the transformations in the small business financial ecosystem. It envisions an ideal state, “Small Business Utopia,” where a small business’s entire financial life would be accessible through one application, allowing payments to be made or delayed, loans to be acquired, and financial advice to be obtained, at the push of a button. These innovations would enhance small businesses’ understanding of their cash flows and provide them with timely access to capital in ways that fit their needs. This chapter discusses the impact of big data and artificial intelligence on creating Small Business Utopia, and the challenges of integrating disparate information streams given the complexity of small business data. It also explores the benefits and risks of “black box” algorithms and machine learning including issues of bias in legacy data.},
}

@article{Ridley2024,
  title = {Enhancing Code Security Through Open-Source Large Language Models: A Comparative Study},
  author = {Norah Ridley and Enrico Branca and Jadyn Kimber and Natalia Stakhanova},
  year = {2024},
  pages = {233-249},
  publisher = {Springer Nature Switzerland},
  abstract = {Significant advances in the language processing field are providing new innovations, including the ability to analyze code for weaknesses. Typically, analyzing code security is performed by tools that use known vulnerable patterns, which may not adequately represent the intricacies of vulnerabilities in real-world projects. Such tools can fail to detect non-standard weaknesses in code samples, potentially leading to a loss of personal and financial information for end users of the code. Using language-based models to detect weaknesses that would have otherwise been missed by the currently available analysis tools is a promising new avenue of vulnerability detection. In this research, we employ 25 different models to evaluate the security of code samples. Using an existing dataset of insecure code, we prompt each model to detect weaknesses in the vulnerable code. Our findings indicate that most models are ill-equipped to deal with insecure code. Through our analysis, we identify strategies for improving weakness detection using language models.},
}

@article{Bialkova2024,
  title = {Data Management},
  author = {Svetlana Bialkova},
  year = {2024},
  pages = {167-186},
  doi = {10.1007/978-3-031-56471-0_10},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-56471-0_10},
  abstract = {The speed and accuracyAccuracy of data management are essential advantages offered by AIArtificial Intelligence (AI) systems. A further advantage could be if the data are transformed to insightful solutions facilitating business performance and end-userUser applications. The current chapter addresses how to possibly generate such solutions, translating userUser needs into explainable data architectures. Understanding how to generate, train, test, and optimise AI-generated behaviour is also in the focus hereby. Machine behaviour could be navigated by exposing AIArtificial Intelligence (AI) systems to specific training data. While substantial human effort was needed to annotate, characterise and interpret information, the enhancement of autonomous capabilities could mark a new era in data management. In this respect, classification algorithms for text, voice, and images are trained to optimise accuracyAccuracy on a specific set of human-labelled datasets. Most importantly, the selection, labelling, and management of a particular dataset and the chosen features can reshape not only the behaviour of an AI system. Rather, the userUser behaviour could be modified by the way the system is trained. However, data management may experienceExperience some biasBias, but this is a call to rethink AIArtificial Intelligence (AI) systems, in order to preclude biased responses. We further recommend remediation of the AI systems currently available on the market. As seen from the outcomes of the field studies reported hereby, informativenessInformativeness, accuracyAccuracy, and competenceCompetence are crucial parameters determining proper system functioning, and thus its adoption by usersUser. Therefore, by fine-tuning algorithms and data architectures, an effective approach for data management is expected to be created to appropriately meet the userUser expectations and business demands for transformational AIArtificial Intelligence (AI) solutions.},
}

@article{Sewall2024,
  title = {Dumb Devices/Smart Adversaries: Real Threats in Critical Infrastructure},
  author = {Adam Sewall},
  year = {2024},
  pages = {85-111},
  doi = {10.1007/978-3-031-61117-9_5},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-61117-9_5},
  abstract = {The rapid advancement of technology has brought about numerous benefits and conveniences to our lives. However, it has also opened up new avenues for malicious actors to target critical infrastructure systems. In recent years, the number and complexity of cybersecurity threats targeting critical infrastructure have risen at an alarming pace. Hacktivists, insider threats, and foreign adversaries pose significant risks to our critical systems. The consequences of a successful cyberattack on critical infrastructure could be catastrophic, leading to widespread disruption and potentially endangering human lives. In fact, there are actual cyber kinetic events where a cyberattack has caused a kinetic or physical event to occur. Therefore, it is crucial for cybersecurity professionals to stay ahead of these threats and develop effective strategies to mitigate them.},
}

@article{Stork2024,
  title = {Enabling Social Demography Research Using Semantic Technologies},
  author = {Lise Stork and Richard L Zijdeman and Ilaria Tiddi and Annette ten Teije},
  year = {2024},
  pages = {199-216},
  publisher = {Springer Nature Switzerland},
  abstract = {A shift in scientific publishing from paper-based to knowledge-based practices promotes reproducibility, machine actionability and knowledge discovery. This is important for disciplines like social demography, where study indicators are often social constructs such as race or education, hypothesis tests are challenging to compare due to their limited temporal and spatial coverage, and research output is presented in natural language, which can be ambiguous and imprecise. In this work, we present the MIRA resource, to aid researchers in their research workflow, and publish FAIR findings. MIRA consists of: (1) an ontology for social demography research, (2) a method for automated ontology population by prompting Large Language Models, and (3) a knowledge graph populated in terms of the ontology by annotating a set of research papers on health inequality. The resource allows researchers to formally represent their social demography research hypotheses, discovering research biases and novel research questions.},
}

@article{Hoda2024,
  title = {Research Design Canvas},
  author = {Rashina Hoda},
  year = {2024},
  pages = {61-92},
  doi = {10.1007/978-3-031-60533-8_4},
  publisher = {Springer International Publishing},
  url = {https://doi.org/10.1007/978-3-031-60533-8_4},
  abstract = {This chapter focuses on describing how to go about designing a research project. First, we will learn about framing the research study as a project. Then, we will be introduced to the research design canvas. While the details are specific to socio-technical grounded theory (STGT) studies, the research design canvas or template can be used for research project design in general. Next, we will learn about the 10 elements of the research design canvas, including the forming of the research team, identifying the domain and actors, selecting the phenomenon and topic to investigate, carefully assessing research ethics and considering the research values, formulating the guiding research questions, acknowledging the team’s research philosophy, deciding on the initial research protocols including data, techniques, and tools, and listing the desirable research impact. The chapter concludes by describing a pilot study to apply and refine the above elements of the research project design.},
}

@article{Tang2024,
  title = {A Comprehensive Analysis of Public Sentiment Towards ChatGPT’s Privacy Implications},
  author = {Liang Tang and Masooda Bashir},
  year = {2024},
  pages = {276-284},
  publisher = {Springer Nature Switzerland},
  abstract = {In this research, we examine the rapid proliferation of ChatGPT, a leading-edge chatbot powered by sophisticated large language model (LLM) technology, and its privacy implications on societal perspectives. While it demonstrates state-of-the-art capabilities in a variety of language-generating tasks, it also raises widespread public concerns regarding its societal impact. By employing advanced natural language processing (NLP) techniques, such as sentiment analysis and topic modeling, our study analyzes public attitudes towards ChatGPT using a dataset derived from Twitter. Our result shows that the overall sentiment is largely neutral and the public’s heightened sensitivity to privacy and security breaches. Among a wide range of topics mentioned in tweets, the most popular topics are malicious phishing, data privacy, international policy and Employee data concern in workplace.},
}

@article{Petrick2024,
  title = {Art in the Age of Algorithms: A Creator’s Perspective on the Artistry of AI Image Generation},
  author = {Kerstin Petrick},
  year = {2024},
  pages = {195-214},
  doi = {10.1007/978-3-031-61187-2_12},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-61187-2_12},
  abstract = {How can AI assist in exploring a broader range of artistic expressions? How is art generated using AI? What is AI’s role in igniting discussions about the very nature of creativity? Can machines truly replace artists? The article, in the form of an interview, addresses these and other questions, offering insights from an art creator and curator’s perspective on how AI is expanding creative boundaries and redefining artistic production and identity in the digital age. Prinz Gallery, an innovative online platform, stands at the forefront of AI-generated art, serving as a testimony to this AI-induced transformative paradigm shift. To underpin the significance of algorithm and model selection, the dialogue provides a chronological review of AI’s advancements in image generation with examples from the art world; from the introduction of Convolutional Neural Networks to the latest transformer and diffusion models. Upon evaluating the advantages and possible threads, the discourse recognizes AI’s role as a collaborative tool rather than a competitor to human creativity, underscoring its ability, through combinational and exploratory creative capabilities, to augment and enhance the artistic process. The article concludes by outlining the selection and creation process at Prinz Gallery, which integrates AI suggestions with human artistic choices, focusing on the emotional resonance and impact of the artwork, emphasizing the importance of a judicious approach to AI in art, and advocating for a balance that enhances rather than replaces human creativity. The conversation ultimately conveys a complex appreciation for the influence of AI in contemporary art, projecting a vision where AI serves as a revolutionary yet cooperative ally within the artistic landscape.},
}

@article{Schelhorn2024,
  title = {Designing a Large Language Model Based Open Data Assistant for Effective Use},
  author = {Till Carlo Schelhorn and Ulrich Gnewuch and Alexander Maedche},
  year = {2024},
  pages = {398-411},
  publisher = {Springer Nature Switzerland},
  abstract = {Open data is widely recognized for its potential positive impact on society and economy. However, many open data sets remain underutilized because users, such as civil servants and citizens, lack the necessary technical and analytical skills. Additionally, existing open data portals often fall short of providing user-friendly access to data. Although conversational agents equipped with Large Language Models have emerged as a promising solution to address these challenges, it is unclear how to design Large Language Model based open data assistants that allow users to formulate their information needs in natural language and ultimately use open data effectively. To address this gap, we undertake a Design Science Research project guided by the theory of effective use. In this first cycle of the project, we present meta-requirements and propose initial design principles on how to design a Large Language Model based open data assistant for effective use. Subsequently, we instantiate our principles in a prototype and evaluate it in a focus group with experts from a medium-sized German city. Our results contribute design knowledge in the form of design principles for open data assistants and inform future design cycles of our Design Science Research project.},
}

@article{Pezzica2024,
  title = {Exploring Alternative Urban and Architectural Virtual Realities Through Multidomain Digital Twins},
  author = {Camilla Pezzica and Chiara Chioni and Nick M L Mols},
  year = {2024},
  pages = {675-690},
  doi = {10.1007/978-3-031-62963-1_41},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-62963-1_41},
  abstract = {This chapter tackles the use of multidomain Digital Twins (DTs) in a Virtual Reality (VR) environment to analyze their potential for design analysis and decision-making. It presents the initial results of an ongoing interdisciplinary research, which draws parallels between the work of Renaissance architects and contemporary practices to conceptualize and explore the application of these technologies in architectural and urban regeneration projects within historic environments. The chapter also presents reproducible workflows that leverage the use of multidomain DTs for navigating and assessing alternative scenarios for urban and landscape design and management, and digital and design history, using the Unity3D game engine. The applicability of the proposal and its cultural relevance are illustrated using the case of Palazzo Rucellai in Florence, to visualize different configurations of the façade project, considering its immediate urban context. This case also opens a discussion on the potential for the integration of Artificial Intelligence (AI) techniques in the pro-posed pipeline.},
}

@article{Zimina2024,
  title = {TraQuLA: Transparent Question Answering Over RDF Through Linguistic Analysis},
  author = {Elizaveta Zimina and Kalervo Järvelin and Jaakko Peltonen and Aarne Ranta and Jyrki Nummenmaa},
  year = {2024},
  pages = {19-33},
  publisher = {Springer Nature Switzerland},
  abstract = {Answering complex questions over knowledge graphs has gained popularity recently. Systems based on large language models seem to achieve top performance. However, these models may generate content that looks reasonable but is incorrect. They also lack transparency, making it impossible to exactly explain why a particular answer was generated. To tackle these problems we present the TraQuLA (Transparent QUestion-answering through Linguistic Analysis) system – a rule-based system developed through linguistic analysis of datasets of complex questions over DBpedia and Wikidata. TraQuLA defines a question’s type and extracts its semantic component candidates (named entities, properties and class names). For the extraction of properties, whose natural language verbalisations are most diverse, we built an extensive database which matches DBpedia/Wikidata properties to natural language expressions, allowing linguistic variation. TraQuLA generates semantic parses for the components and ranks them by each question’s structure and morphological features. The ranked parses are then analysed top down according to their patterns, also noting linguistic aspects, until a solution is found and a SPARQL query is produced. TraQuLA outperforms the existing baseline systems on the LC-QuAD 1.0 and competes with ChatGPT-based systems on LC-QuAD 2.0. For the LC-QuAD 1.0 test set, we developed an evaluation approach that accepts multiple ways to answer the questions (some ignored by the dataset) and curated some errors. TraQuLa contains no “black boxes” of neural networks or machine learning and makes its answer construction traceable. Users can therefore better rely on them and assess their correctness.},
}

@article{Cabot2024,
  title = {Low-Modeling of Software Systems},
  author = {Jordi Cabot},
  year = {2024},
  pages = {19-28},
  publisher = {Springer Nature Switzerland},
  abstract = {There is a growing need for better development methods and tools to keep up with the increasing complexity of new software systems. New types of user interfaces, the need for intelligent components, sustainability concerns, ... bring new challenges that we need to handle. In the last years, model-driven engineering has been key to improving the quality and productivity of software development, but models themselves are becoming increasingly complex to specify and manage. In this paper, we present the concept of low-modeling as a solution to enhance current model-driven engineering techniques and get them ready for this new generation of software systems.},
}

@article{AlKatheeri2024,
  title = {A Graph Analytics Methodology for Analyzing Startup Ecosystems},
  author = {Ali AlKatheeri and Saif Abdulmajeed and Abdulla Albedwawi and Abdulalrheem AlSheebany and Samir Safi and Gurdal Ertek},
  year = {2024},
  pages = {63-71},
  publisher = {Springer Nature Switzerland},
  abstract = {Startup companies are engines of economic growth and diversity. A unicorn company is a privately held start-up company that has grown to a valuation of over $1 billion. As of January 2023, there were over 1,200 unicorn companies worldwide. In this study, we present a custom-developed graph analytics methodology for the systematic analysis of unicorn and startup ecosystems. We show the applicability of and insights obtained by the methodology through a case study using a public dataset on unicorn companies. Graph analytics through algorithmically generated visualizations can enable the derivation and understanding of meaningful insights, even by non-technical analysts and decision-makers in entrepreneurship ecosystems and government organizations.},
}

@article{Dunlap2024,
  title = {Pairing Security Advisories with Vulnerable Functions Using Open-Source LLMs},
  author = {Trevor Dunlap and John Speed Meyers and Bradley Reaves and William Enck},
  year = {2024},
  pages = {350-369},
  publisher = {Springer Nature Switzerland},
  abstract = {As the reliance on open-source software dependencies increases, managing the security vulnerabilities in these dependencies becomes complex. State-of-the-art industry tools use reachability analysis of code to alert developers when security vulnerabilities in dependencies are likely to impact their projects. These tools heavily rely on precisely identifying the location of the vulnerability within the dependency, specifically vulnerable functions. However, the process of identifying vulnerable functions is currently either manual or uses a naive automated approach that falsely assumes all changed functions in a security patch link are vulnerable. In this paper, we explore using open-source large language models (LLMs) to improve pairing security advisories with vulnerable functions. We explore various prompting strategies, learning paradigms (i.e., zero-shot vs. few-shot), and show our approach generalizes to other open-source LLMs. Compared to the naive automated approach, we show a 173% increase in precision while only having an 18% decrease in recall. The significant increase in precision to enhance vulnerable function identification lays the groundwork for downstream techniques that depend on this critical information for security analysis and threat mitigation.},
}

@article{Chan2024,
  title = {Towards a New Method for Designing Manufacturing Capabilities},
  author = {Anouck Chan and Janis Stirna and Jelena Zdravkovic and Thomas Polacsek and Simon Hacks and Janis Grabis and Claudio Favi},
  year = {2024},
  pages = {34-49},
  publisher = {Springer Nature Switzerland},
  abstract = {Many industries are experiencing the challenges and opportunities associated with the rapid pace of technological change. In manufacturing, the adoption of new materials and manufacturing processes is a common concern, for example, the use of new composite materials offer significant advantages in terms of energy efficiency and performance. However, their use poses challenges in terms of manufacturing and assembly. On the side of technological advances, the exploration of digitalisation, automation and robotics to improve efficiency is a strong trend. However, implementing these technologies requires investment and significant changes to existing assembly processes. To meet these challenges, fostering collaboration between design and manufacturing, as well as between manufacturer and its suppliers is often seen as the key solution for various industries. In this work, we present how capability modelling can help both parties to describe their demands and offers with respect to needed and provided quality properties such as set-up times or ecological footprint. These serve as input for a digital business ecosystem in which the pareto-optimal factory design can be chosen supported by a digital platform integrating the manufacturer and its suppliers.},
}

@article{Kaplan2024,
  title = {Unifying Economic and Language Models for Enhanced Sentiment Analysis of the Oil Market},
  author = {Himmet Kaplan and Ralf-Peter Mundani and Heiko Rölke and Albert Weichselbraun and Martin Tschudy},
  year = {2024},
  pages = {119-143},
  publisher = {Springer Nature Switzerland},
  abstract = {Crude oil, a critical component of the global economy, has its prices influenced by various factors such as economic trends, political events, and natural disasters. Traditional prediction methods based on historical data have their limits in forecasting, but recent advancements in natural language processing bring new possibilities for event-based analysis. In particular, Language Models (LM) and their advancement, the Generative Pre-trained Transformer (GPT), have shown potential in classifying vast amounts of natural language. However, these LMs often have difficulty with domain-specific terminology, limiting their effectiveness in the crude oil sector. Addressing this gap, we introduce CrudeBERT, a fine-tuned LM specifically for the crude oil market. The results indicate that CrudeBERT’s sentiment scores align more closely with the WTI Futures curve and significantly enhance price predictions, underscoring the crucial role of integrating economic principles into LMs.},
}

@article{Morgner2024_01,
  title = {Siemens: Acting Resiliently Through Hybrid Process Intelligence in the Supply Chain Metaverse},
  author = {Robert Morgner and Markus Burger},
  year = {2024},
  pages = {175-191},
  doi = {10.1007/978-3-031-61343-2_19},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-61343-2_19},
  abstract = {In the face of disruptions like COVID-19, the Ukraine conflict, or the Suez Canal blockage, Siemens Supply Chain Management is encountering various challenges. The ability to withstand such supply chain disturbances, has evolved into a competitive edge, highlighting the growing importance of supply chain resilience. The strategic component of Supply Chain Management changes. The strategic ability to successfully manage supply chains, therefore needs to be rethought. Shorter product life-cycles and times between disruptions, require a high-level of proactivity. Integrating process intelligence and artificial intelligence into supply chain operations, holds promise that both can play a crucial role in improving resilience. This chapter presents the Siemens cycle of resilience concept, that is based on both human and artificial intelligence. We show four process intelligence approaches in terms of (1) incident management, (2) crisis control, (3) agile recovery, and (4) supply chain design, where the combination of human and artificial intelligences helps us to improve supply chain resilience. We show technology- and people-related success factors, e.g., data connectivity or change management, which turned out to be crucial when implementing such approaches. This cycle of resilience concept marks a decisive step towards our mission of a supply chain metaverse.},
}

@article{Santos2024_01,
  title = {Kicking Prejudice: Large Language Models for Racism Classification in Soccer Discourse on Social Media},
  author = {Guto Leoni Santos and Vitor Gaboardi dos Santos and Colm Kearns and Gary Sinclair and Jack Black and Mark Doidge and Thomas Fletcher and Dan Kilvington and Patricia Takako Endo and Katie Liston and Theo Lynn},
  year = {2024},
  pages = {547-562},
  publisher = {Springer Nature Switzerland},
  abstract = {In the dynamic space of Twitter, now called X, interpersonal racism surfaces when individuals from dominant racial groups engage in behaviours that diminish and harm individuals from other racial groups. It can be manifested in various forms, including pejorative name-calling, racial slurs, stereotyping, and microaggressions. The consequences of racist speech on social media are profound, perpetuating social division, reinforcing systemic inequalities, and undermining community cohesion. In the specific context of football discourse, instances of racism and hate crimes are well-documented. Regrettably, this issue has seamlessly migrated to the football discourse on social media platforms, especially Twitter. The debate on Internet freedom and social media moderation intensifies, balancing the right to freedom of expression against the imperative to protect individuals and groups from harm. In this paper, we address the challenge of detecting racism on Twitter in the context of football by using Large Language Models (LLMs). We fine-tuned different BERT-based model architectures to classify racist content in the Twitter discourse surrounding the UEFA European Football Championships. The study aims to contribute insights into the nuanced language of hate speech in soccer discussions on Twitter while underscoring the necessity for context-sensitive model training and evaluation. Additionally, Explainable Artificial Intelligence (XAI) techniques, specifically the Integrated Gradient method, are used to enhance transparency and interpretability in the decision-making processes of the LLMs, offering a comprehensive approach to mitigating racism and offensive language in online sports discourses.},
}

@article{Winters2025,
  title = {Outlook},
  author = {Jane Winters},
  year = {2025},
  pages = {255-257},
  doi = {10.1007/978-3-031-64451-1_13},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-64451-1_13},
  abstract = {This short chapter reflects on the trajectory of the CLEOPATRA project, drawing out some of the themes that connect the work presented elsewhere in the book. It also considers how rapidly the field has changed during the lifetime of the project, and particularly the growing interest in and use of generative AI.},
}

@article{Alivanistos2024,
  title = {The Effect of Knowledge Graph Schema on Classifying Future Research Suggestions},
  author = {Dimitrios Alivanistos and Seth van der Bijl and Michael Cochez and Frank van Harmelen},
  year = {2024},
  pages = {149-170},
  publisher = {Springer Nature Switzerland},
  abstract = {The output of research doubles at least every 20 years and in most research fields the number of research papers has become overwhelming. A critical task for researchers is to find promising future directions and interesting scientific challenges in the literature. To tackle this problem, we hypothesize that structured representations of information in the literature can be used to identify these elements. Specifically, we look at structured representations in the form of Knowledge Graphs (KGs) and we investigate how using different input schemas for extraction impacts the performance on the tasks of classifying sentences as future directions. Our results show that the MECHANIC-Granular schema yields the best performance across different settings and achieves state of the art performance when combined with pretrained embeddings. Overall, we observe that schemas with limited variation in the resulting node degrees and significant interconnectedness lead to the best downstream classification performance.},
}

@article{Krüger2024,
  title = {SOMD@NSLP2024: Overview and Insights from the Software Mention Detection Shared Task},
  author = {Frank Krüger and Saurav Karmakar and Stefan Dietze},
  year = {2024},
  pages = {247-256},
  publisher = {Springer Nature Switzerland},
  abstract = {Software is a central part of the scientific process and involved in obtaining, analysing, visualising and processing research data. Understanding the provenance of research requires an understanding of the involved software. However, software citations in scientific publications often are informal, what creates challenges when aiming at understanding software adoption. This paper provides an overview of the Software Mention Detection (SOMD) shared task conducted as part of the 2024 Natural Scientific Language Processing Workshop, aiming at advancing the state-of-the-art with respect to NLP methods for detecting software mentions and additional information in scholarly publications. The SOMD shared task encompasses three subtasks, concerned with software mention recognition (subtask I), recognition of additional information (subtask II) and classification of involved relations (subtask III). We present an overview of the tasks, received submissions and used techniques. The best submissions achieved F1 scores of 0.74 (subtask I), 0.838 (subtask II) and 0.911 (subtask III) indicating both task feasibility but also potential for further performance gains.},
}

@article{Koohborfardhaghighi2024,
  title = {Unlocking the Power of LLM-Based Question Answering Systems: Enhancing Reasoning, Insight, and Automation with Knowledge Graphs},
  author = {Somayeh Koohborfardhaghighi and Gert De Geyter and Evan Kaliner},
  year = {2024},
  pages = {156-171},
  publisher = {Springer Nature Switzerland},
  abstract = {In today’s data-driven business landscape, Knowledge Graphs can be effectively layered on top of relational databases and ontologies, a powerful combination for transforming how businesses tackle complex queries and decision-making processes. In this paper, we present a series of experiments that demonstrate the opportunities and advantages of blending knowledge graphs with Large Language Models (LLMs) through a practical use case. Our experimental results provide insights into the reasoning capabilities of LLMs when utilizing Knowledge Graph-Prompting. Furthermore, we observed the significance of maintaining uniformity in the language employed during knowledge graph construction to ensure precise responses from LLMs when querying the knowledge graph. This consistency also resonates in the embedding space of the model, where elements like relationship types are reflected in the resulting embeddings.},
}

@article{Hodak2024,
  title = {Benchmarking Large Language Models: Opportunities and Challenges},
  author = {Miro Hodak and David Ellison and Chris Van Buren and Xiaotong Jiang and Ajay Dholakia},
  year = {2024},
  pages = {77-89},
  publisher = {Springer Nature Switzerland},
  abstract = {With exponentially growing popularity of Large Language Models (LLMs) and LLM-based applications like ChatGPT and Bard, the Artificial Intelligence (AI) community of developers and users are in need of representative benchmarks to enable careful comparison across a variety of use cases. The set of metrics has grown beyond accuracy and throughput to include energy efficiency, bias, trust and sustainability. This paper aims to provide an overview of popular LLMs from a benchmarking perspective. Key LLMs are described, and the associated datasets are characterized. A detailed discussion of benchmarking metrics covering training and inference stages is provided and challenges in evaluating these metrics are highlighted. A review of recent performance and benchmark submissions is included, and emerging trends are summarized. The paper lays the foundation for developing new benchmarks to allow informed comparison of different AI systems based on combinations of models, datasets, and metrics.},
}

@article{Walther2024,
  title = {WHERE: Humans, Technology, and Humane Technology},
  author = {Cornelia C Walther},
  year = {2024},
  pages = {103-194},
  doi = {10.1007/978-3-031-67823-3_2},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-67823-3_2},
  abstract = {Diving deeper, this chapter looks into the mutual influence of humans and AI. Grounded in the multidimensional framework introduced in Chapter 1, and the A-Frame that applies it in the context of human Agency Amid AI, we will examine human consciousness and artificial consciousness. From here we look at inferred intelligence, and the potential of tandems that leverage the complementarity of natural and artificial perception. The technological transition that humanity is traversing from one industrial revolution to the next is examined, with the proposition to expand the traditional triple bottom line to include purpose; the Quadruple bottom line recognizes the responsibility of the private sector towards the Common Good. This sets the stage to explore the place of human trust and artificial integrity, and the type of regulation needed to establish guardrails that grow with the challenge of an evolving subject. Data is looked at as the central bottleneck of an inclusive society, as biased inputs result in biased outcomes, and the amplification of exclusion in a hybrid society. Ironically the stronger the technology in our life grows, the weaker our cognitive defense mechanisms become, and the more at risk we are to fall prey to the all pervasive nature of artificial persuasion. Unless we are aware of the causes and consequences of our own perception, the potential of artificial influence, combined with the temptation of delegating mental effort to artificial assistance jeopardizes our autonomy, and the formation of free will. An exit from this dangerous dynamic starts by identifying what we want, and why; to clarify who we are and what we stand for. The alignment of humans and technology starts with personal and interpersonal harmonization. That alignment of individual aspirations and actions is the precursor of algorithms that are aligned with aspirations. This dynamic cannot be reverse engineered. The hybrid alignment conundrum cannot be solved online.},
}

@article{Kneusel2025,
  title = {AI-Designed Number Formats},
  author = {Ronald T Kneusel},
  year = {2025},
  pages = {323-354},
  doi = {10.1007/978-3-031-67482-2_12},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-67482-2_12},
  abstract = {Large language models are generative neural networks that predict a sequence of tokens (words or parts of words) when given an initial text prompt. Trained on vast amounts of text, such models have surprised researchers with emergent abilities enabling the models to achieve human-level performance in various subjects, including those where creativity plays a role. In this chapter, we ask a large language model (GPT-4) to design novel computer number systems.},
}

@article{O’Brien2024,
  title = {Dignity},
  author = {Wendy O’Brien},
  year = {2024},
  pages = {171-200},
  doi = {10.1007/978-3-031-68930-7_7},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-68930-7_7},
  abstract = {Through an interrogation of the false narratives of the digital capitalist age, this chapter refutes technocratic efforts to relegate privacy to the pre-digital era. Exploring the interconnections between user-generated content, denigrating videos about children, facial recognition technologies, and “synthetic media” such as voice cloning and deepfake CSAM, the chapter identifies that digital intrusions on children’s right to privacy are also intrusions on their right to human dignity. Assessing the depth and scope of the digital intrusion on children’s rights to freedom of thought, expression, and movement as an assault on children’s decisional autonomy, this chapter calls for a renewed commitment to equal human dignity to forestall new frontiers in the technology-enabled assault on children’s rights.},
}

@article{Arfat2024,
  title = {Legal Text Analysis Using Large Language Models},
  author = {Yasir Arfat and Marco Colella and Enrico Marello},
  year = {2024},
  pages = {258-268},
  publisher = {Springer Nature Switzerland},
  abstract = {Large Language Models are revolutionizing legal work by automating tasks like legal research, contract review, document drafting, and generating summaries from complex legal documents, making these processes more efficient. They’re trained in various legal texts, enabling them to provide relevant insights and suggestions. In this research paper, we created summaries using the Italian legal court decisions using the large language models ChatGPT3.5 and Gemini. We evaluated summaries created by ChatGPT3.5 and Gemini using natural language processing metrics such as Rouge, Meteor, and BertScore. We also checked the readability of generated summaries by applying the Gulpease score metrics. We found that the summaries produced by Gemini outperformed those from ChatGPT3.5, particularly with respect to the BertScore metrics.},
}

@article{Menzner2024,
  title = {Improved Models for Media Bias Detection and Subcategorization},
  author = {Tim Menzner and Jochen L Leidner},
  year = {2024},
  pages = {181-196},
  publisher = {Springer Nature Switzerland},
  abstract = {We present improved models for the granular detection and sub-classification news media bias in English news articles. We compare the performance of zero-shot versus fine-tuned large pre-trained neural transformer language models, explore how the level of detail of the classes affects performance on a novel taxonomy of 27 news bias-types, and demonstrate how using synthetically generated example data can be used to improve quality.},
}

@article{Nai2024,
  title = {Large Language Models and Recommendation Systems: A Proof-of-Concept Study on Public Procurements},
  author = {Roberto Nai and Emilio Sulis and Ishrat Fatima and Rosa Meo},
  year = {2024},
  pages = {280-290},
  publisher = {Springer Nature Switzerland},
  abstract = {In legal informatics research, decision support systems can be a valuable tool for practitioners facing a growing volume of data. An expert system based on information retrieval and a recommender system can benefit from the application of Large Language Models to improve the quality of results. This paper proposes a general framework based on Retrieval-Augmented Generation for addressing integrated recommendation systems with generative models in public procurement. Moreover, we addressed a practical application by adopting real datasets in the legal domain. To illustrate the feasibility of the approach, a proof-of-concept has been presented in the context of public procurement management within an Italian case study. The study and evaluation phases have been supervised by domain experts in the legal field to ensure robust analysis and relevance.},
}

@article{Azam2024,
  title = {Exploring Technologies for Semantic Metadata Enhancement},
  author = {Sadia Azam and Martina De Sanctis and Amleto Di Salle and Ludovico Iovino},
  year = {2024},
  pages = {459-469},
  publisher = {Springer Nature Switzerland},
  abstract = {A seamless integration between Content Management Systems (CMS) and Semantic Metadata Repositories (SMR) could potentially trigger huge improvements in content personalization and recommendation systems via automatic enrichment of content by using external resources. In the past, tools such as Apache Stanbol has been used to provide the semantic capabilities to CMS. These semantic capabilities include extraction of text, content enhancement, linked data integration and content improvement. Despite its promising features, Apache Stanbol faced limitations such as complexity, integration challenges, and a dwindling support community, leading to its discontinuation in 2020. This paper discusses the shift towards leveraging Large Language Models (LLMs) for semantic enrichment of CMS. LLMs, with their advanced natural language understanding and generation capabilities, represent a dynamic, robust, and scalable alternative for semantic processing. This transition aims to overcome the challenges associated with previous technologies, harnessing the state-of-the-art advancements in LLMs to achieve improved content personalization, context-aware recommendations, and an enriched user experience. This evolution underscores the potential of LLMs to revolutionize content management, offering a forward-looking perspective on the application of semantic technologies.},
}

@article{Kraus2025,
  title = {Trust in Automated Driving and the Three Stages of Trust Framework: Takeaways for Designing Human-Centered AI Applications},
  author = {Johannes Kraus and Martin Baumann},
  year = {2025},
  pages = {119-132},
  doi = {10.1007/978-3-031-70355-3_9},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-70355-3_9},
  abstract = {This chapter focuses on a transfer from research on trust processes in the interaction with automated vehicles to the interaction with current AI systems like Large Language Models (LLMs). After discussing parallels between the domains and transferable research insights, the psychological processes in trust calibration are discussed along the central propositions of the Three Stages of Trust framework (Kraus in Psychological processes in the formation and calibration of trust in automation, 2020). The framework emphasises the dynamic role of trust and the iterative learning about a system’s trustworthiness with presented information prior and during system interaction. Furthermore, it underlines the role of situational context and individual user differences. Designing AI systems that support calibrated trust involves transparent communication of capabilities and limitations, integrating human-centered design principles, and offering adaptive information to aid decision-making. These strategies aim to foster a balanced and informed use of AI, eventually fostering an efficient and safe interaction.},
}

@article{Mamalis2024,
  title = {A Large Language Model Agent Based Legal Assistant for Governance Applications},
  author = {Marios Evangelos Mamalis and Evangelos Kalampokis and Fotios Fitsilis and Georgios Theodorakopoulos and Konstantinos Tarabanis},
  year = {2024},
  pages = {286-301},
  publisher = {Springer Nature Switzerland},
  abstract = {Large Language Models (LLMs) have gained significant traction, primarily due to their potential disruptive influence across industries reliant on natural language processing. Governance stands out as one such sector. Notably, there has been a surge in research activity surrounding the implications of LLMs in deciphering complex legal corpora. This research offers substantial assistance to various stakeholders, including decision-makers, administrators, and citizens. This article focuses on the design and implementation of an LLM-based legal assistant tailored for interacting with legal resources. To achieve this, a real-world scenario has been chosen, incorporating models GPT3.5 and GPT4 as the LLMs, a well-defined legal corpus comprising European Union (EU) legislation and case law concerning the General Data Protection Regulation (GDPR), alongside a series of reference legal queries of varying complexity. Retrieval Augmented Generation (RAG) as well as agent methodologies are employed to seamlessly integrate the LLMs’ functionalities with the customized dataset. The results appear to be promising, as the system managed to correctly address the majority of the legal queries, though with variable precision. Expectantly, the complexity of the queries severely impacted the quality of the outcome.},
}

@article{Russodivito2024,
  title = {Black-Box Reconstruction Attacks on LLMs: A Preliminary Study in Code Summarization},
  author = {Marco Russodivito and Angelica Spina and Simone Scalabrino and Rocco Oliveto},
  year = {2024},
  pages = {391-398},
  publisher = {Springer Nature Switzerland},
  abstract = {Large Language Models (LLMs) have demonstrated effectiveness in tackling coding tasks, leading to their growing popularity in commercial solutions like GitHub Copilot and ChatGPT. These models, however, may be trained on proprietary code, raising concerns about potential leaks of intellectual property. A recent study indicates that LLMs can memorize parts of the source code, rendering them vulnerable to extraction attacks. However, it used white-box attacks which assume that adversaries have partial knowledge of the training set.},
}

@article{Ameri2024,
  title = {Open Manufacturing Capability Network Supported by Formal Ontologies},
  author = {Farhad Ameri and Mukund Shenoy and Ali Hasanzadeh and Sambhav Kapoor},
  year = {2024},
  pages = {310-324},
  publisher = {Springer Nature Switzerland},
  abstract = {Access to accurate manufacturing capability information is necessary for efficient supplier discovery and agile supply chain formation. However, manufacturing capability data, particularly for small and medium-sized manufacturers, is often unavailable or, if accessible, lacks essential qualities such as correctness, completeness, interoperability, and openness. The objective of the research presented in this paper is to develop an open Manufacturing Capability Network (MCN) that represents various manufacturers’ capabilities as an interconnected and formal knowledge graph. This capability graph is part of a larger graph referred to as the Supply and Demand Open Knowledge Network (SUDOKN). The ontologies that provide the semantics of the knowledge graph comply with the Basic Formal Ontology (BFO). A proof-of-concept knowledge graph, based on 1700 manufacturers, is presented in this work. The graph’s validity was assessed by submitting queries related to supplier discovery use cases. SUDOKN, once fully deployed, serves as a shared, canonical, and consensus-driven knowledge backbone, that supports supply chain analytics solutions with AI-ready data.},
}

@article{Yuan2024,
  title = {Navigating AI in Education—Towards a System Approach for Design of Educational Changes},
  author = {Li Yuan and Tore Hoel and Stephen Powell},
  year = {2024},
  pages = {75-92},
  doi = {10.1007/978-3-031-71232-6_5},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-71232-6_5},
  abstract = {As artificial intelligenceArtificial intelligence (AI) (AI) continues to advance, recent developments of Generative AIGenerative AI (GenAI) (GenAI) have sparked great interest, posing questions for policymakers, technology innovators, educators, and EdTech researchers about possible paradigm changes for educationEducation. This chapter critically examines the development of Artificial Intelligence in Education, which promised to revolutionise educational practices by providing effective, personalised, learning at scale—first through rudimentary teaching machinesTeaching machine and subsequently via advanced adaptive learningAdaptive learning systems. We argue that Adaptive Intelligent Tutoring Systems (ITS) reinforce the content delivery model and restrict pedagogic opportunities in teaching and learning when adapted to existing educational models. The chapter examines the relevance and value of existing theories of learning in the development of educational technologyEducational technology integration and a need for new theories when an AI agentAI agent becomes an active partner in teaching and learning process and discusses the complexity of educational innovation from interdisciplinary perspectives. Finally, we offer an analytical assessmentAssessment of the opportunities and limitations of GenAI in education using the cybernetic principle of varietyCybernetic principle of variety, and propose a framework to address organisational, pedagogical, and technological challenges for using GenAI to support new formal learning and pedagogical practices.},
}

@article{Poth2024,
  title = {Considerations About Integration of GenAI into Products and Services from an Ethical and Legal Perspective},
  author = {Alexander Poth and Anna Wildegger and Dan-Alexander Levien},
  year = {2024},
  pages = {155-171},
  publisher = {Springer Nature Switzerland},
  abstract = {Generative Artificial Intelligence (GenAI) has gained significant attention since the end of 2022. In many existing products and services chat-bots are integrated and completely new services based on GenAI, particularly those based on Large Language Models (LLMs) are under development or have been recently released. However, many aspects regarding the use of LLMs remain unclear, particularly in relation to the training and the inference phases. Training-related questions are about the selection of the data and their legal constraints and ethical considerations. Trained models are offered under new specific “AI licenses” with implications on the context of usage. Anyway, there are open aspects about inference outcomes respective with respect to the usage of LLMs. This work examines considerations in the context of the product and service development for the generation of test-cases facilitated by a LLM. Test-case generation is used in Information Technology (IT) to make derivations from specifications for test cases (respectively test-scripts) to validate the implementation. Test-case design and implementation can potentially be facilitated by LLMs – based on a hypothetical case, relevant aspects and considerations are discussed also including legal aspects based on the perspective of German law.},
}

@article{Yu2024,
  title = {Language-Enhanced Local-Global Aggregation Network for Multi-organ Trauma Detection},
  author = {Jianxun Yu and Qixin Hu and Meirui Jiang and Yaning Wang and Chin Ting Wong and Jing Wang and Huimao Zhang and Qi Dou},
  year = {2024},
  pages = {393-403},
  publisher = {Springer Nature Switzerland},
  abstract = {Abdominal trauma is one of the leading causes of death in the elderly population and increasingly poses a global challenge. However, interpreting CT scans for abdominal trauma is considerably challenging for deep learning models. Trauma may exist in various organs presenting different shapes and morphologies. In addition, a thorough comprehension of visual cues and various types of trauma is essential, demanding a high level of domain expertise. To address these issues, this paper introduces a language-enhanced local-global aggregation network that aims to fully utilize both global contextual information and local organ-specific information inherent in images for accurate trauma detection. Furthermore, the network is enhanced by text embedding from Large Language Models (LLM). This LLM-based text embedding possesses substantial medical knowledge, enabling the model to capture anatomical relationships of intra-organ and intra-trauma connections. We have conducted experiments on one public dataset of RSNA Abdominal Trauma Detection (ATD) and one in-house dataset. Compared with existing state-of-the-art methods, the F1-score of organ-level trauma detection improves from 51.4% to 62.5% when evaluated on the public dataset and from 61.9% to 65.2% on the private cohort, demonstrating the efficacy of our proposed approach for multi-organ trauma detection. Code is available at: https://github.com/med-air/TraumaDet},
}

@article{Khyzhniak2025,
  title = {Potential of Artificial Intelligence in the Assessment of System of Social Integration of Veterans of the Russian-Ukrainian War},
  author = {Oleksandr Khyzhniak and Jesús M Siqueiros García},
  year = {2025},
  pages = {213-235},
  doi = {10.1007/978-3-031-71678-2_10},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-71678-2_10},
  abstract = {War veterans in modern Ukraine are a particularly socially vulnerable group whose social integration requires new approaches, including the involvement of contemporary information and communication technologies and artificial intelligence (AI). Based on the use of “Diia”, an AI online application that helps Ukrainians receive social services, the study explores the potential of AI in social work with war veterans and the conditions for its effective application. The author identifies three areas of social work with war veterans: social rehabilitation, social integration, and social assistance. Indicators to measure this potential have been developed, which include the ability of the social work system to remove outdated approaches, forms, and types that cannot be digitized; social workers’ proficiency in digital technologies; the ability to implement these technologies in service delivery; and maintaining constructive social work through controlled AI application by professional staff (primarily state and municipal services). Our methods included Participatory Systems Mapping and gamification while working with Ukrainian war refugees. Ukraine is becoming a country of veterans due to prolonged war. It has been proved that the digitalization of veteran services and the use of AI in social work with veterans should be controlled processes with a transparent algorithm and control system.},
}

@article{Ciutat2025,
  title = {The Impact of the EU’s AI Act and Data Act on Digital Farming Technologies},
  author = {Lucas Ramon Ciutat},
  year = {2025},
  pages = {218-229},
  publisher = {Springer Nature Switzerland},
  abstract = {This paper examines the impact of the European Union”s Artificial Intelligence Act (AIA) and Data Act (DA) on digital farming technologies, focusing on their legal and regulatory implications. First, the paper delves into the AIA risk categorisation, highlighting how various digital farming AI systems may be categorised based on factors such as their type, function, capability and size, as well as the potential risks they pose to health, safety, and fundamental rights. It identifies two legal shortcomings concerning manipulation and interactive AI systems. Second, the paper assesses the provisions of the DA, focusing on the potential of the legislation to support farmers in accessing their data, breaking platform lock-ins, and ensuring transparency on how their data is used.},
}

@article{Liu2025_01,
  title = {Can OOD Object Detectors Learn from Foundation Models?},
  author = {Jiahui Liu and Xin Wen and Shizhen Zhao and Yingxian Chen and Xiaojuan Qi},
  year = {2025},
  pages = {213-231},
  publisher = {Springer Nature Switzerland},
  abstract = {Out-of-distribution (OOD) object detection is a challenging task due to the absence of open-set OOD data. Inspired by recent advancements in text-to-image generative models, such as Stable Diffusion, we study the potential of generative models trained on large-scale open-set data to synthesize OOD samples, thereby enhancing OOD object detection. We introduce SyncOOD, a simple data curation method that capitalizes on the capabilities of large foundation models to automatically extract meaningful OOD data from text-to-image generative models. This offers the model access to open-world knowledge encapsulated within off-the-shelf foundation models. The synthetic OOD samples are then employed to augment the training of a lightweight, plug-and-play OOD detector, thus effectively optimizing the in-distribution (ID)/OOD decision boundaries. Extensive experiments across multiple benchmarks demonstrate that SyncOOD significantly outperforms existing methods, establishing new state-of-the-art performance with minimal synthetic data usage. The project is available at https://github.com/CVMI-Lab/SyncOOD.},
}

@article{Abualhaija2025,
  title = {Legal Requirements Analysis: A Regulatory Compliance Perspective},
  author = {Sallam Abualhaija and Marcello Ceci and Lionel Briand},
  year = {2025},
  pages = {209-242},
  doi = {10.1007/978-3-031-73143-3_8},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-73143-3_8},
  abstract = {Modern software has been an integral part of everyday activities in many disciplines and application contexts. Introducing intelligent automation by leveraging artificial intelligence (AI) led to breakthroughs in many fields. The effectiveness of AI can be attributed to several factors, among which is the increasing availability of data. Regulations such as the General Data Protection Regulation (GDPR) in the European Union (EU) are introduced to ensure the protection of personal data. Software systems that collect, process or share personal data are subject to compliance with such regulations. Developing compliant software depends heavily on addressing legal requirements stipulated in applicable regulations, a central activity in the requirements engineering (RE) phase of the software development process. RE is concerned with specifying and maintaining requirements of a system-to-be, including legal requirements. Legal agreements which describe the policies organizations implement for processing personal data can provide an additional source to regulations for eliciting legal requirements. In this chapter, we explore a variety of methods for analysing legal requirements and exemplify them on GDPR. Specifically, we describe possible alternatives for creating machine-analysable representations from regulations, survey the existing automated means for enabling compliance verification against regulations and further reflect on the current challenges of legal requirements analysis. Analysing legal requirements is a core RE activity that relies to a large extent on natural language processing technologies. This chapter contributes with the necessary knowledge required for eliciting, representing and verifying legal requirements.},
}

@article{Fu2025,
  title = {Lite-SAM Is Actually What You Need for Segment Everything},
  author = {Jianhai Fu and Yuanjie Yu and Ningchuan Li and Yi Zhang and Qichao Chen and Jianping Xiong and Jun Yin and Zhiyu Xiang},
  year = {2025},
  pages = {456-471},
  publisher = {Springer Nature Switzerland},
  abstract = {The Segment Anything model (SAM) has brought significant changes to the segmentation field with its superior performance, but its extensive computational resource requirements remain a limiting factor. Many works, such as MobileSAM, Edge-SAM, and MobileSAM-v2, have explored lightweight solutions. However, their use of traditional Grid Search sampling strategies or two-stage concatenation methods, which do not allow for end-to-end training, severely limit the performance of segment everything (SegEvery).},
}

@article{Coulon2024,
  title = {Introduction to Behavioral Finance},
  author = {Yannick Coulon},
  year = {2024},
  pages = {1-24},
  doi = {10.1007/978-3-031-72553-1_1},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-72553-1_1},
  abstract = {This chapter establishes the differences between classical and behavioral finance. The lack of rationality and market efficiency are powerful factors justifying behavioral finance, as are financial bubbles.},
}

@article{Mikroyannidis2024,
  title = {Smart Assessment and Guided Education with Responsible AI},
  author = {Alexander Mikroyannidis and John Domingue and Aisling Third and David Tarrant and Tom Pieroni},
  year = {2024},
  pages = {17-26},
  publisher = {Springer Nature Switzerland},
  abstract = {This paper presents the vision of the SAGE-RAI (Smart Assessment and Guided Education with Responsible AI) project. Inspired by Bloom’s seminal work on the efficacy of personalised learning, the SAGE-RAI project aims to leverage responsible Generative AI towards transforming teaching and learning practices for improved student outcomes. By exploring the integration of Generative AI into tutoring processes, we seek to provide scalable, personalised learning experiences for large cohorts of students. Our research focuses on harnessing Generative AI to offer tailored educational content and generate constructive feedback for students. By applying responsible AI practices, we aim to mitigate issues such as misinformation, copyright infringement, and bias.},
}

@article{Busacca2025,
  title = {Who’s Afraid of “Big Bad” Generative AI? Brief Notes on the IDPA Provision Against OpenAI ChatGPT},
  author = {Angela Busacca and Melchiorre Alberto Monaca},
  year = {2025},
  pages = {117-144},
  doi = {10.1007/978-3-031-73880-7_9},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-73880-7_9},
  abstract = {The rise of ChatGPT on the market and the rapid appearance of other similar tools proposed by companiescompeting with OpenAI and, more generally, by the Big Players of the web, have renewed the centrality of thedebates on the use of generative AI systems, especially in consideration of the notable spread of some versionswhich are accessible, free-of-charge and extremely easy to use. Although progress in the technological revolutionis one of the hallmarks of the current age and its positive impact is undeniable, the rise of generative AI systemsappears to be characterized by an unprecedented speed, which risks multiplying socially relevant applications andoverwhelming lifestyles and professional fields all too quickly without the necessary adaptation and reaction timesto innovation. As is easy to see, there are multiple and complex issues connected to the use and diffusion ofgenerative AI systems and involve not only strictly legal profiles, but also ethical and social ones, especially inrelation to the possibility of massive application, with relevant consequences in different fields: in the labour market,in production processes, but also in education and communication systems and in human relations themselves.Within this complex framework of reference, the essay will focus on the dispute between Italian DPA and Open AI,with reference both to the first disputes which arose in the spring of 2023 and to the more recent disputes fromJanuary 2024, in which different processing data violations and specially GDPR rule violations are hypothesized.Apart from the Italian case and the question of the safeguard of data subjects’ rights, in the last paragraph someconsiderations of a more general nature will be presented on the AI Act (latest) text, regarding regulation ofgenerative AI systems that use GPT and LLM algorithms.},
}

@article{Tang2025,
  title = {Patent, Still a Leading Indicator in AI Technology Innovation?},
  author = {Shirley Tang},
  year = {2025},
  pages = {87-100},
  doi = {10.1007/978-3-031-73639-1_5},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-73639-1_5},
  abstract = {With significant investments and exponential growth in the global AI market, the importance of patents as an indicator of innovation outputs has become more pronounced. However, the rise of AI-generated inventions poses new challenges to the existing patent system. The research delves into the patentability of AI inventions, with a focus on the Dr. Thaler and DABUS case, which questioned whether AI can be recognized as an inventor. This case has ignited global discussions on the ownership of AI-generated inventions and other disclosure requirements in IP systems. Additionally, we explore alternative appropriability strategies rather than patenting. The research highlights the need for a balanced and efficient patent system that fosters continuous innovation in the era of autonomous AI rather than incites a patent racing between humans and machines, which seems to open Pandora’s box.},
}

@article{Machado2025,
  title = {Chatbot Decision Support with Intent-Rich Structures in the Hospitality Industry},
  author = {Eduardo Machado and Rui Henriques},
  year = {2025},
  pages = {333-346},
  publisher = {Springer Nature Switzerland},
  abstract = {The digital transformation of the hospitality sector has been driven by recent advances in the capabilities of conversational agents to effectively deliver services to users. Conversational performance of chatbots is nevertheless challenged by the high cardinality of request intents. This study aims at exploring the rich structure behind request intents to support their detection, therefore guiding conversations. To this end, we propose a novel approach that learns a hierarchical structure of intents based on their semantic similarities and predictive dependencies (shared errors), and subsequently incorporates these structures into the- learning of Large Language Models for intent detection. Gathered results from Hijiffy and Clinc150 case studies highlight statistically significant improvements across performance metrics against state-of-the-art approaches, highlighting the relevance of modeling structural relationships between intents. The particular ability of the proposed models to handle the high imbalance and cardinality of intents opens new opportunities for user adherence towards agent-mediated services in the hospitality domain.},
}

@article{Hernández-Gómez2024,
  title = {Neuropathic Pain Detection Through Embedding Synergies of Deep Language and Image Models},
  author = {Kevin A Hernández-Gómez and Julian Gil-Gonzalez and David A Cárdenas-Peña and Álvaro A Orozco-Gutiérrez},
  year = {2024},
  pages = {3-12},
  publisher = {Springer Nature Switzerland},
  abstract = {The Global Burden of Disease states that neuropathic pain is suffered by 7–8% of adults worldwide, with severe repercussions on daily life, like drug abuse and psychological disorders. This work introduces a methodology for neuropathic pain classification through embedding synergies of the large language model BERT and image model ResNet50, handling clinical questionaries and EEG records, respectively. The classification task is a three-class problem with low, moderate, and severe pain categories. The embeddings of clinical data learned by BERT and the ResNet50-encoded topo-plots from EEG data feed an SVM classifier, further trained in a GroupKFold scheme from a thirty-six patients dataset. The accuracy obtained of 60%, outperforming single modality approaches, demonstrates the potential of multimodal approaches for enhanced pain diagnosis and treatment.},
}

@article{Dueñas2025,
  title = {Utility Tokens and Their Regulation Under MiCA},
  author = {Alfonso Martínez-Echevarría y García de Dueñas and Rafael del Castillo Ionov},
  year = {2025},
  pages = {233-250},
  doi = {10.1007/978-3-031-74889-9_10},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-74889-9_10},
  abstract = {The MiCA Regulation regulates crypto assets which are not financial instruments, providing them with a legal regimen through a specific regulation. The MiCA regulation covers the majority of crypto asset classes currently being traded. Developing a single digital market requires a solid legal base that offers its participants security in developing distribution ledger technology projects for issuing and trading crypto assets. Utility tokens may be issued without obtaining preliminary authorisation if the projects comply with the requirements regarding the preparation, notification and publication of the crypto-asset white paper.},
}

@article{Kareem2025,
  title = {Using Learning from Answer Sets for Robust Question Answering with LLM},
  author = {Irfan Kareem and Katie Gallagher and Manuel Borroto and Francesco Ricca and Alessandra Russo},
  year = {2025},
  pages = {112-125},
  publisher = {Springer Nature Switzerland},
  abstract = {Large Language Models (LLMs) lack the ability for commonsense reasoning and learning from text. In this work, we present a system, called LLM2LAS, for learning commonsense knowledge from story-based question and answering expressed in natural language. LLM2LAS combines the semantic parsing capability of LLMs with ILASP for learning commonsense knowledge expressed as answer set programs. LLM2LAS requires only few examples of questions and answers to learn general commonsense knowledge and correctly answer unseen questions. An empirical evaluation demonstrates the viability of our approach.},
}

@article{Ndiaye2025,
  title = {Building Trustworthiness as a Requirement for AI in Africa: Challenges, Stakeholders and Perspectives},
  author = {Seydina Moussa Ndiaye},
  year = {2025},
  pages = {41-67},
  doi = {10.1007/978-3-031-75674-0_3},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-75674-0_3},
  abstract = {As Africa embraces the potential of Artificial Intelligence (AI) for socio-economic development, the continent faces unique challenges and opportunities in building a trustworthy and sovereign AI ecosystem. While several African nations, led by Mauritius in 2018, have launched national AI strategies, the continent must navigate complex dynamics, including the digital divide and the risk of digital colonialism. The reliance on foreign AI solutions can undermine Africa's autonomy and perpetuate dependency, making it crucial to prioritise locally developed technologies that align with the continent's cultural and socio-economic realities. The African Union and other international initiatives have laid the groundwork for responsible AI deployment, emphasising ethics, inclusivity and local sovereignty. However, the success of AI in Africa hinges on the active engagement of diverse stakeholders, including governments, educational institutions, private sector entities, communities and multilateral organisations. These stakeholders must collaborate to create an ecosystem that fosters innovation, upholds ethical standards and mitigates the risks of external dependency by investing in homegrown AI solutions. Governments play a crucial role in establishing regulatory frameworks, promoting public-sector AI applications and forming strategic partnerships. Simultaneously, educational institutions are essential in cultivating AI talent and driving research, while the private sector and communities contribute to ecosystem vitality. Ensuring an inclusive, adaptive and resilient AI ecosystem will require ongoing collaboration and trust-building among all parties. Ultimately, a vibrant, self-regulated AI ecosystem can position Africa as a leader in global AI governance, harnessing technology for sustainable development while safeguarding its sovereignty.},
}

@article{Vollmers2025,
  title = {UniQ-Gen: Unified Query Generation Across Multiple Knowledge Graphs},
  author = {Daniel Vollmers and Nikit Srivastava and Hamada M Zahera and Diego Moussallem and Axel-Cyrille Ngonga Ngomo},
  year = {2025},
  pages = {174-189},
  publisher = {Springer Nature Switzerland},
  abstract = {Generating SPARQL queries is crucial for extracting relevant information from diverse knowledge graphs. However, the structural and semantic differences among these graphs necessitate training or fine-tuning a tailored model for each one. In this paper, we propose UniQ-Gen, a unified query generation approach to generate SPARQL queries across various knowledge graphs. UniQ-Gen integrates entity recognition, disambiguation, and linking through a BERT-NER model and employs cross-encoder ranking to align questions with the Freebase ontology. We conducted several experiments on different benchmark datasets such as LC-QuAD 2.0, GrailQA, and QALD-10. The evaluation results demonstrate that our approach achieves performance equivalent to or better than models fine-tuned for individual knowledge graphs. This finding suggests that fine-tuning a unified model on a heterogeneous dataset of SPARQL queries across different knowledge graphs eliminates the need for separate models for each graph, thereby reducing resource requirements.},
}

@article{Hewage2024,
  title = {Data Protection Challenges and Opportunities Due to Emerging AI and ML Technologies},
  author = {Chaminda Hewage and Lasith Yasakethu and Dushantha Nalin K Jayakody},
  year = {2024},
  pages = {1-27},
  doi = {10.1007/978-3-031-76473-8_1},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-76473-8_1},
  abstract = {Artificial intelligence (AI) and machine learning (ML) research and innovation have created significant advances in how technology is used in diverse sectors. These technologies can create significant impact outside academia, for example, on our economy, society, and industry landscape. However, most of these AI technologies could be biased or inaccurate if they are not developed and validated well. Irresponsible AI applications can pose significant risks to individuals’ rights. People have started to resist “black box-” type AI models. They want more transparent, equitable, and responsible AI models. To this end, technological paradigm changes are taking place to make emerging AI models more responsible and useful for individuals and the society. To reinforce the technological efforts, policies and regulations are being enacted across the world to regulate “out-of-control” AI and ML applications. This chapter discusses emerging AI and ML applications, data protection challenges, and privacy-preserving AI and ML algorithms.},
}

@article{Filippone2025,
  title = {SDF-FuzzIA: A Fuzzy-Ontology Based Plug-in for the Intelligent Analysis of Geo-Thematic Data},
  author = {Giuseppe Filippone and Gianmarco La Rosa and Marco Elio Tabacchi},
  year = {2025},
  pages = {163-169},
  publisher = {Springer Nature Switzerland},
  abstract = {This short paper presents a description of SDF-FuzzIA, a Fuzzy-Ontology LLM-based system for the intelligent analysis of geo-thematic data that serves as a plug-in to the Sustainability Decision Framework (SDF) Decision Support System (DSS). A description of the components implemented in the system is given, followed by an explanation of the interaction between the components and the main system. As this still is a work in progress, future directions and possible hurdles are explored.},
}

@article{Kumari2025,
  title = {Guardians of Accountability: The Role of Media in Oversight and Governance of Generative AI Applications in Fintech},
  author = {Priyanka Kumari and Shishir Kr. Singh and Vinit Kumar Jha Utpal},
  year = {2025},
  pages = {345-359},
  doi = {10.1007/978-3-031-76957-3_18},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-76957-3_18},
  abstract = {The integration of Generative Artificial Intelligence (AI) applications presents new opportunities and challenges in the field of financial technology (Fintech), especially with regard to governance and oversight. This study looks into how important it is for the media to improve transparency in the Fintech industry, with a particular emphasis on applications of generative AI. This research aims to clarify the complex relationships between media, oversight mechanisms, and governance structures in ensuring the responsible deployment of generative AI in Fintech through an analysis of existing literature, regulatory frameworks, and case studies. Fintech companies now have access to cutting-edge tools for risk assessment, fraud detection, customer support, and investment strategies thanks to the spread of generative AI. These innovations have the potential to truly transform the industry. On the other hand, questions about ethics, accountability, and transparency are brought up by the opacity and inherent complexity of generative AI algorithms. The media plays a vital role as a watchdog in this regard, promoting public debate, awareness, and examination of the moral and societal implications of Fintech's use of generative artificial intelligence. This paper investigates how media platforms impact public perceptions, regulatory responses, and industry practices related to Generative AI in Fintech. It does this by drawing upon theoretical frameworks from media studies, governance theory, and AI ethics. It also looks at how expert analysis, investigative journalism, and stakeholder involvement can help identify potential risks, biases, and malpractices related to AI-driven financial technologies. This research sheds light on the dynamics of media influence on regulatory enforcement, corporate accountability, and consumer trust in the Fintech ecosystem through in-depth case studies. This paper also looks at the efficiency of industry self-regulation programs and current regulatory frameworks in tackling the problems brought on by generative AI in Fintech. By demonstrating the complementary roles of public advocacy, regulatory interventions, and journalistic investigations in promoting responsible innovation and risk mitigation in the Fintech sector, it explores the symbiotic relationship between media scrutiny and regulatory oversight. In addition, it examines the governance gaps and moral conundrums associated with the application of generative AI in Fintech, including concerns about algorithmic bias, data privacy, and systemic vulnerabilities. The study's findings emphasize the critical role that the media plays in advancing the ethical governance, accountability, and transparency of generative AI applications in the Fintech industry. Media organizations have the power to stimulate positive public discourse, industry best practices, and regulatory changes that will guarantee AI-driven financial innovations balance potential risks with societal benefits. In the end, this paper advances our understanding of the intricate interactions that will shape Fintech's future in the AI era between media, oversight mechanisms, and governance frameworks.},
}

@article{Moroz2024,
  title = {Employing Generative Artificial Intelligence in Replacement of Traditional Backend Systems},
  author = {Artur Moroz and Illia Solohubov and Andrii Oliinyk and Sergey Subbotin and Stepan Skrupsky},
  year = {2024},
  pages = {49-58},
  publisher = {Springer Nature Switzerland},
  abstract = {This paper investigates the application and benefits of using Generative Artificial Intelligence (GenAI) techniques, with an emphasis on large language models (LLMs) as an innovative approach in the development of information systems. The focus lies primarily on systems employing open datasets where data confidentiality is not a focal concern. By replacing the traditional programmed logic with these advanced AI models, this paper presents the measurable improvements in the efficiency and adaptability of system design, development, and maintenance. The key advantages discussed include an enhanced rate and accuracy in query processing and response provision. Despite accentuating these substantial benefits, the paper acknowledges potential limitations with the alignment of these models to certain user-specific needs.},
}

@article{Agyemang2025,
  title = {AI in Education: An Analysis of Large Language Models for Twi Automatic Short Answer Grading},
  author = {Alex Agyemang and Tim Schlippe},
  year = {2025},
  pages = {107-123},
  publisher = {Springer Nature Switzerland},
  abstract = {Automatic short answer grading can significantly enhance the speed and fairness of grading, making it particularly valuable in areas with a shortage of teachers, such as Africa [1]. However, for most African languages it is very challenging to build automatic short answer grading systems due to the limited availability of natural language processing corpora. Furthermore, only experts can deal with the complex algorithms, required for training and fine-tuning traditional automatic short answer grading systems. Given that state-of-the-art large language models have the potential to address these problems through their growing capabilities and ease of use through prompting, particularly in zero-shot and few-shot learning, we investigated their performance for grading student answers in the African language Twi. To address the absence of a Twi corpus, we translated and validated the University of North Texas benchmark corpus [2], creating the first Twi automatic short answer grading corpus. On this corpus, we evaluated the performances of the large language models GPT-4o [3], Claude 3 Sonnet [4], and LLaMA 3 [5] as well as for comparison two more traditional approaches: a fine-tuned AfroLM and a cross-lingual M-BERT approach. Among individual models, the cross-lingual M-BERT had the best performance with a mean absolute error of 0.79 points out of 5 points, followed by fine-tuned AfroLM at 0.73 points and Claude 3 Sonnet at 1.00 points. However, combining AfroLM and M-BERT outputs achieved the lowest mean absolute error of 0.64 points, which is less than the human grader variance of 0.75 points in the original corpus [6]. Combining the outputs of the large language models GPT-4o, Claude 3 Sonnet, and LLaMA 3, obtained through few-shot learning, yielded a mean absolute error of 1.10 points.},
}

@article{Makse2024,
  title = {Social Media Influencers and Politics},
  author = {Hernán A Makse and Marta Zava},
  year = {2024},
  pages = {145-240},
  doi = {10.1007/978-3-031-78058-5_2},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-78058-5_2},
  abstract = {For ages, influencing the masses has been the privilege of the elites. New decentralized and interactive social media have changed all that by empowering people to produce their own content of information to spread influence. In this chapter we apply the theories of influencer identification of Chap. 1to understand the spread of influence in social media. We focus on the arrival of new influencers in the political arena and study how these new influencers are driving opinion trends, how they contribute to polarization of the masses, and how they spread fake news and misinformation to influence society at large. We also discuss how they can determine nationwide elections. The theories of influencers described in Chapt. 1have to be fed with enormous amounts of data to be effective. These data are generated continuously in the digital era. We first review the process of data generation and collection from social media. To measure opinion trends, we augment the theory of influencers with artificial intelligence, through machine learning and natural language processing, to learn and predict opinion trends including predicting nationwide elections. We process large amount of data with AI and classify people and influencers according to different topics, trends and political tendencies. At the same, AI can differentiate fake news and misinformation from genuine news. Combining the theory of influence with AI, we will understand the rise of political movements that can bring new political ideas to power, and the influence of misinformation in political campaigns and opinion trends.},
}

@article{Siragusa2025,
  title = {DR-Minerva: A Multimodal Language Model Based on Minerva for Diagnostic Information Retrieval},
  author = {Irene Siragusa and Salvatore Contino and Roberto Pirrone},
  year = {2025},
  pages = {288-300},
  publisher = {Springer Nature Switzerland},
  abstract = {This paper illustrates the development of Minerva Diagnostic Retriever (DR-Minerva), a Visual Language Model specialized in the medical domain. Prompted using a textual input with the patient’s information along with a CT or MR scan, the model provides information about the body part and the scanning modality of the given image. The model relies on the Flamingo architecture, which is well known for its good in-context and few-shot learning capabilities, and it encodes textual data using Minerva, a novel Large Language Model trained on English and Italian data. Model performances are improved via fine-tuning the aforementioned model, and using external knowledge by means of a Retrieval Augmented Generation approach. At inference time, the model is injected with the retrieved examples in form of in-context learning. The authors developed a rearranged version of the MedPix® multi-modal medical dataset, that was used for both the development and the test of the model as long as for retrieval. A detailed description of the system is reported along with the experimental results that are discussed in thoroughly. Dataset and models used are available on GitHub (https://github.com/CHILab1/MedPix-2.0.).},
}

@article{Polo-Bautista2025,
  title = {Semi-automatic Construction of Knowledge Graphs on Natural Disasters in Mexico Using Large Language Models},
  author = {Luis Roberto Polo-Bautista and Sandra Dinora Orantes-Jiménez and Francisco Carrillo-Brenes and Luis M Vilches-Blázquez},
  year = {2025},
  pages = {148-167},
  publisher = {Springer Nature Switzerland},
  abstract = {The growing trend of developing Large Language Models (LLMs) has gained popularity due to their ability to process and generate natural language, which has applications in various industries. However, the use of LLMs to build domain-specific knowledge graphs remains challenging due to the reliance on human experts to define entities and relationships and to address problems such as information granularity, lack of timeliness, etc. This work presents a workflow that integrates three different LLMs (Llama 3, GPT-4o, and Claude 3 Sonnet) to perform a semiautomated construction of knowledge graphs from news related to natural disasters in Mexico. This ongoing work provides a preliminary assessment of the ability of LLMs to represent specific knowledge domains, with the potential to improve accessibility and retrieval of relevant data, thus facilitating future identification of associations and patterns related to natural disasters. The experiments carried out have shown that our workflow enriches the identification and relationships of entities from the news corpus. However, our current evaluation shows that the LLMs used are far from replacing human intervention.},
}

@article{Ratta2025,
  title = {AI Supported Knowledge Graph Design and Generation},
  author = {Marco Ratta},
  year = {2025},
  pages = {125-134},
  publisher = {Springer Nature Switzerland},
  abstract = {Knowledge Graphs (KG) have risen to be a powerful mechanism to represent data. Despite this most data sources are generally still represented via heterogeneous non-graph data structures. Converting these into KGs necessitates considerable effort from experts, proving this to be a time consuming process. While tools have been developed to aid KG builders, a gap still exists in terms of technologies that support the automation of designing KG building pipelines. Addressing this gap motivates this research. The aim is to first understand the problem at the knowledge level and, inspired by the recent release of generative tools such as GPT-Engineer, to put forward a conversational agent aimed at assisting the user in building their pipelines. We report on the preliminary findings that we have so far reached during the first year of research in deriving the requirements for building KG generating pipelines from the literature.},
}

@article{Sabané2025,
  title = {Leveraging Conversational AI for Accelerating User-Driven Software Testing},
  author = {Aminata Sabané and Laura Plein and Tegawendé F Bissyandé},
  year = {2025},
  pages = {81-88},
  publisher = {Springer Nature Switzerland},
  abstract = {This work addresses a research challenge in automating the translation of natural language inputs into programming language specifications. We consider the case of bug reports, which are informally written by users, and that must be specifying into executable test cases for reproducing the bug on the target software. Software bugs are indeed largely reported in natural language by users. Yet, we lack reliable tools to automatically address reported bugs (i.e., enabling their analysis, reproduction, and bug fixing). We therefore build on the recent promises brought by ChatGPT for various tasks, including in software engineering, and establish the following research question: What if Conversational Artificial Intelligence (AI) models could be used to explore the semantics of bug reports as well as to automate their reproduction? We evaluate the capabilities of ChatGPT, a state-of-the-art conversational AI, i.e., chatbot, using the popular Defects4J benchmark with its associated bug reports. The results reveal that ChatGPT can generate executable test cases that could trigger 50% of the bugs reported in natural language. These results are promising not only for the research community, but also for practitioners.},
}

@article{Kranenburg2024,
  title = {The First Domain Lies in What Is Currently Called Pathology},
  author = {Rob van Kranenburg},
  year = {2024},
  pages = {17-45},
  doi = {10.1007/978-3-031-80645-2_2},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-80645-2_2},
  abstract = {Although the term the Internet of Things was coined by Kevin Ashton in 1999, the systemic thinking to install miniature sensors and computers into the real world started in the 70s, 80s, and 90s coined as pervasive computing, ubicomp, and ambient intelligence. In this chapter about 20 IoT experts talk about the beginning in their various roles and fields. Showing how difficult it is to spot emergent trends when you are in the middle of it, it explains that IoT is a horizontal operation. You see it as IoT when you see the connectivity that is driving it as a separate driver. The critique that IoT received from surveillance and privacy activists that was strong in the first decade of the twentieth century was compartmentalized into the discussion on the GDPR and the issue of fake news, fueled by the latest developments in AI. This is why there has been up till now no holistic understanding of what kind of hybridity has formed—or been organized—in under 25 years. The argument is made that we lack a pedagogy for this merging of analogue and digital. One of the most damaging aspects is that the endless scrolling and lack of beginnings and ends in the web and apps deprive us of moments of “closure,” moments that are essential to what it means and should feel to be human. Instead of sets of properties building a meta-connected infrastructure, a meta-connected object was built as the mobile phone. It has created a vast and ubiquitous “extra.” This “extra” has not yet been recognized as something that should be brought under multi-governance public control. Currently it is seen as a mobile phone. In fact, it has way more potentiality as a “kenner.”},
}

@article{Lau2025,
  title = {Introduction},
  author = {Theodora Lau},
  year = {2025},
  pages = {1-20},
  doi = {10.1007/978-3-031-81647-5_1},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-81647-5_1},
}

@article{Cigliano2025,
  title = {The Convergence of Open Data, Linked Data, Ontologies, and Large Language Models: Enabling Next-Generation Knowledge Systems},
  author = {Andrea Cigliano and Francesca Fallucchi},
  year = {2025},
  pages = {197-213},
  publisher = {Springer Nature Switzerland},
  abstract = {This paper explores the convergence of Open Data initiatives, Linked Data technologies, ontological knowledge representation, and Large Language Models (LLMs) in generative Artificial Intelligence (AI). It examines how these complementary approaches can be integrated to create more powerful, flexible, and context-aware knowledge systems. The paper provides an overview of the open data landscape, the Semantic Web and Linked Data vision, ontologies and knowledge organization systems, and recent advances in LLMs. It then discusses how these technologies can be synergistically combined to enable next-generation knowledge systems that leverage both structured knowledge and natural language understanding. Potential applications in areas such as scientific research, government transparency, and intelligent information retrieval are discussed. The paper also addresses key challenges including scalability, data quality, ethical considerations, and the need for explainable AI. A strategic roadmap for realizing this integration is proposed, emphasizing collaboration between academia, industry, and government. While significant technical and ethical challenges remain, the convergence of these technologies has the potential to fundamentally transform how we interact with and derive insights from information, enabling more intelligent and context-aware knowledge systems to address complex real-world problems.},
}

@article{Dupuy2025,
  title = {DATA-FW: An Ontology Network for Annotating Open Datasets},
  author = {Antoine Dupuy and Nathalie Aussenac-Gilles and Christophe Baehr and Cassia Trojahn},
  year = {2025},
  pages = {15-27},
  publisher = {Springer Nature Switzerland},
  abstract = {Open datasets are often exposed with insufficient metadata, making difficult to end users the task of identifying those that better fit their needs. One way to overcome this weaknesses is to guarantee compliance of data to the FAIR principles, in particular where the use of ontologies is a key aspect for proving richer metadata schemes. This paper proposes an ontology network, DATA-FW, that aims at representing rich metadata to assist in dataset usage. It exploits different features that are required to meet the user’s needs, which have been divided into four distinct components: Core Metadata Component, Structure Component, Usage Component, and Quality Component. Each Component reuses existing and known vocabularies and serves a specific purpose.},
}

@article{Lau2025_01,
  title = {Ownership, Rights, and Governance},
  author = {Theodora Lau},
  year = {2025},
  pages = {155-169},
  doi = {10.1007/978-3-031-81647-5_9},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-81647-5_9},
}

@article{Lau2025_02,
  title = {Representation Matters},
  author = {Theodora Lau},
  year = {2025},
  pages = {109-121},
  doi = {10.1007/978-3-031-81647-5_6},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-81647-5_6},
}

@article{Štefčík2025_01,
  title = {Research in Translation Studies},
  author = {Jozef Štefčík},
  year = {2025},
  pages = {231-278},
  doi = {10.1007/978-3-031-87205-1_5},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-87205-1_5},
  abstract = {Research in Translation Studies explores diverse and innovative areas, reflecting the field's adaptation to technological, cultural, and global changes. It integrates artificial intelligence and machine learning to improve translation quality and efficiency. The issues addressed in the research are related to the market demands of the time, i.e. from exploring equivalence from different aspects to simultaneously exploring the integration of AI into translation from both a process and product perspective. Technological revolution has brought new elements to TS that need to be first researched and understood and then integrated into education.},
}

@article{Troussas2025_01,
  title = {A Novel Framework of Human–Computer Interaction and Human-Centered Artificial Intelligence in Learning Technology},
  author = {Christos Troussas and Akrivi Krouska and Cleo Sgouropoulou},
  year = {2025},
  pages = {387-431},
  doi = {10.1007/978-3-031-84453-9_9},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-84453-9_9},
  abstract = {This chapter presents a novel framework that combines principles of Human–Computer Interaction (HCI) and Human-Centered Artificial Intelligence (AI) to create adaptive learning technology systems tailored to modern educational needs. The framework is designed around five core pillars: personalization, collaboration, adaptability, accessibility, and creativity. These pillars guide the development of educational platforms that dynamically adjust to individual learner preferences, foster interactive learning environments, and enhance overall engagement and inclusivity. The framework serves as a blueprint for integrating advanced AI technologies with proven pedagogical practices, offering educators and developers a structured approach to design adaptive systems. The chapter elaborates on how this framework can be applied in diverse educational contexts, such as personalized tutoring systems, virtual collaborative platforms, and immersive learning simulations. Practical examples and case studies demonstrate its real-world applicability and transformative potential. Furthermore, ethical considerations, including data privacy, algorithmic fairness, and inclusivity, are addressed to ensure responsible implementation. By synthesizing insights from psychology, cognitive science, and education, this framework provides a robust foundation for advancing educational technology. It equips educators and system designers with the tools to create innovative, scalable, and sustainable learning environments that adapt to the diverse and evolving needs of learners.},
}

@article{Štefčík2025_02,
  title = {Development of Translation Studies in the New Millenium},
  author = {Jozef Štefčík},
  year = {2025},
  pages = {29-118},
  doi = {10.1007/978-3-031-87205-1_2},
  publisher = {Springer Nature Switzerland},
  url = {https://doi.org/10.1007/978-3-031-87205-1_2},
  abstract = {Translation studies have expanded significantly in the new millennium, embracing technological advancements and interdisciplinary approaches. Research has increasingly focused on cultural localization, audiovisual translation (e.g., subtitling and dubbing), and multimodal translation to meet the demands of a globalized digital landscape. Theoretical models have also evolved, emphasizing inclusivity and diversity. These developments underscore the field's dynamic nature, adapting to the challenges and opportunities of an interconnected world.},
}

@article{Weber2013,
  title = {ICT services classification},
  author = {Rolf H Weber and Mira Burri},
  year = {2013},
  pages = {51-124},
  doi = {10.1007/978-3-642-31635-7_3},
  publisher = {Springer Berlin Heidelberg},
  url = {https://doi.org/10.1007/978-3-642-31635-7_3},
  abstract = {The preceding chapter has discussed the overall classification framework, the available classification regimes of different organizations and the general problems with the adaptation of traditional classifications to the new technological environment. This chapter III now looks – after a short general introduction into computer and communications services – more closely into those categories of services being of major interest in the digital economy, namely IT (computer-related) services, telecommunications services and audiovisual services.},
}

@article{Ferretti2004,
  title = {L l},
  author = {Vittorio Ferretti},
  year = {2004},
  pages = {477-512},
  doi = {10.1007/978-3-642-18573-1_16},
  publisher = {Springer Berlin Heidelberg},
  url = {https://doi.org/10.1007/978-3-642-18573-1_16},
}

@article{Ilčev2017,
  title = {Introduction},
  author = {Stojče Dimov Ilčev},
  year = {2017},
  pages = {1-83},
  doi = {10.1007/978-3-319-39171-7_1},
  publisher = {Springer International Publishing},
  url = {https://doi.org/10.1007/978-3-319-39171-7_1},
  abstract = {The safety of navigation through all past ages has been a primary preoccupation for all seamen and shipping owners.},
}

@article{Schnitzhofer2024,
  title = {Die DNA des selbstfahrenden Staates},
  author = {Florian Schnitzhofer and Patrick Pils and Philipp Seper-Ambros},
  year = {2024},
  pages = {75-127},
  doi = {10.1007/978-3-658-45660-3_4},
  publisher = {Springer Fachmedien Wiesbaden},
  url = {https://doi.org/10.1007/978-3-658-45660-3_4},
  abstract = {Das Kapitel beschreibt den selbstfahrenden Staat, der auf einer technischen Basis für die Verwaltung in demokratischen Systemen funktioniert, unabhängig von der gewählten Politik. Voraussetzung ist das Wohl der Menschen sowie die rechtsstaatliche Gewaltentrennung. Verantwortung bleibt bei Menschen, obwohl Algorithmen und Software viele Verwaltungsprozesse automatisieren. Die Genauigkeit und Transparenz dieser Prozesse sind essenziell, und alle Daten und Entscheidungen müssen revisionssicher dokumentiert werden. Der selbstfahrende Staat soll Bürokratie reduzieren, Effizienz steigern und die Verwaltung transparent gestalten, während er gleichzeitig den Datenschutz und die Bürgerrechte wahrt. Die Legislative, Exekutive und Judikative müssen dabei mit intuitiven, benutzerfreundlichen Softwarelösungen arbeiten, die ihnen ermöglichen, ihre Aufgaben effektiv zu erfüllen.},
}

@article{Kreutzer2024,
  title = {Developing an AI Journey in your Own Company},
  author = {Ralf T Kreutzer},
  year = {2024},
  pages = {417-451},
  doi = {10.1007/978-3-658-46131-7_12},
  publisher = {Springer Fachmedien Wiesbaden},
  url = {https://doi.org/10.1007/978-3-658-46131-7_12},
  abstract = {Artificial Intelligence has the potential to fundamentally change politics, economy and society. Despite this, there is still a great deal of uncertainty about how and especially how quickly this technology will continue to develop. Therefore, every company is called upon to start an individual AI journey—now!},
}

@article{Scheer2024,
  title = {Digital Industry Concepts for the Composable Enterprise},
  author = {August-Wilhelm Scheer},
  year = {2024},
  pages = {163-224},
  doi = {10.1007/978-3-658-43089-4_10},
  publisher = {Springer Fachmedien Wiesbaden},
  url = {https://doi.org/10.1007/978-3-658-43089-4_10},
  abstract = {Various digital drivers are creating a large number of new products and processes across all industries. Both industries that produce ″information-related″ products or services (e.g. the media) and industries that produce physical products are exposed to these disruptive changes. The following shows how this creates holistic, disruptive business models for companies.},
}

@article{Malhotra2024,
  title = {Digital India: Past, Present and Future},
  author = {Charru Malhotra},
  year = {2024},
  pages = {325-345},
  doi = {10.1007/978-3-658-43014-6_19},
  publisher = {Springer Fachmedien Wiesbaden},
  url = {https://doi.org/10.1007/978-3-658-43014-6_19},
  abstract = {India is going through the most unprecedented transformation in human history of the free world. The decade of 2020–2030 in India will witness major, non-linear transformations, manifesting as global megatrends that will generate high-impact opportunities, at unseen speed and scale, necessitating just-in-time response agility from demand-supply-trust ecosystems built on a strong core of digital technologies and connectedness. This beseeches unprecedented, coordinated action from governments, market-players and citizens, for technology will emerge as the foundation for success. Although the Digital India initiatives are focused on reducing digital divide, a multi-pronged strategy is required for India to emerge as a leading digital economy. The objectives of any national level digital initiative can be successfully achieved if such a strategy is implemented through which the framework, policies, guidelines, reforms, and business process re-engineering are constantly evolved to address the needs and aspirations of citizens. With such national strategic catalysts, an overarching objective of “Minimum Government and Maximum Governance” can be achieved only when governments imbue the “Whole-of-the-Government Approach”. Several global Indices too must be leveraged further to benchmark the governance performance such as e-Governance Development Index (eGDI), Good Governance Index, e-Participation Index (ePI) and so on.},
}

@article{Ferretti2000,
  title = {L},
  author = {Vittorio Ferretti},
  year = {2000},
  pages = {481-518},
  doi = {10.1007/978-3-642-98088-6_12},
  publisher = {Springer Berlin Heidelberg},
  url = {https://doi.org/10.1007/978-3-642-98088-6_12},
}

@article{Ferretti1996,
  title = {Dictionary German — English},
  author = {Vittorio Ferretti},
  year = {1996},
  pages = {1-654},
  doi = {10.1007/978-3-642-80131-0_1},
  publisher = {Springer Berlin Heidelberg},
  url = {https://doi.org/10.1007/978-3-642-80131-0_1},
}

@article{Ferretti1996_01,
  title = {Dictionary German — English},
  author = {Vittorio Ferretti},
  year = {1996},
  pages = {655-1370},
  doi = {10.1007/978-3-642-80131-0_2},
  publisher = {Springer Berlin Heidelberg},
  url = {https://doi.org/10.1007/978-3-642-80131-0_2},
}

@article{Nottingham2020,
  title = {The Role of International Consumer Policy in Fostering Innovation and Empowering Consumers to Make Informed Choices},
  author = {Kara D Nottingham and Izabel Cardozo},
  year = {2020},
  pages = {413-441},
  doi = {10.1007/978-981-15-8948-5_28},
  publisher = {Springer Singapore},
  url = {https://doi.org/10.1007/978-981-15-8948-5_28},
  abstract = {This study addresses the topic of consumer policy from an international law perspective, exploring the content and evolving meaning of the right to information in the age of fast-paced scientific and technological advancement, according to the United Nations Guidelines for Consumer Protection (“the Guidelines”), which were adopted in 1985 and last revised in 20151.},
}

@article{Sarschar2025,
  title = {Discussion},
  author = {Mahja Sarschar},
  year = {2025},
  pages = {77-101},
  doi = {10.1007/978-3-658-47208-5_7},
  publisher = {Springer Fachmedien Wiesbaden},
  url = {https://doi.org/10.1007/978-3-658-47208-5_7},
  abstract = {In this work, a mixed-method approach was used to answer the research question.},
}

@article{Xiaoyi2025,
  title = {The Use of Artificial Intelligence in Chinese Humanities and Social Sciences Research},
  author = {Li Xiaoyi and Cao Jingwen},
  year = {2025},
  pages = {277-300},
  doi = {10.1007/978-3-658-46344-1_20},
  publisher = {Springer Fachmedien Wiesbaden},
  url = {https://doi.org/10.1007/978-3-658-46344-1_20},
  abstract = {This paper investigates AI’s current integration in Chinese humanities and social sciences research, aiming to offer insights for its future role. Employing literature review and case studies, it surveys disciplinary involvement, highlights evolving trends, and outlines diverse application approaches. It elucidates AI’s multifaceted roles in research facilitation, analyzes associated advantages and challenges, and proposes forward-looking recommendations for enhanced collaboration.},
}

@article{Tomasko2003,
  title = {The Descent Imager/Spectral Radiometer (DISR) Experiment on the Huygens Entry Probe of Titan},
  author = {M G Tomasko and D Buchhauser and M Bushroe and L E Dafoe and L R Doose and A Eibl and C Fellows and E McFarlane and G M Prout and M J Pringle and B Rizk and C See and P H Smith and K Tsetsenekos},
  year = {2003},
  pages = {469-551},
  doi = {10.1007/978-94-017-3251-2_13},
  publisher = {Springer Netherlands},
  url = {https://doi.org/10.1007/978-94-017-3251-2_13},
  abstract = {The payload of the Huygens Probe into the atmosphere of Titan includes the Descent Imager/Spectral Radiometer (DISR). This instrument includes an integrated package of several optical instruments built around a silicon charge coupled device (CCD) detector, a pair of linear InGaAs array detectors, and several individual silicon detectors. Fiber optics are used extensively to feed these detectors with light collected from three frame imagers, an upward and downward-looking visible spectrometer, an upward and downward looking near-infrared spectrometer, upward and downward looking violet phtotometers, a four-channel solar aerole camera, and a sun sensor that determines the azimuth and zenith angle of the sun and measures the flux in the direct solar beam at 940 nm. An onboard optical calibration system uses a small lamp and fiber optics to track the relative sensitivity of the different optical instruments relative to each other during the seven year cruise to Titan. A 20 watt lamp and collimator are used to provide spectrally continuous illumination of the surface during the last 100 m of the descent for measurements of the reflection spectrum of the surface. The instrument contains software and hardware data compressors to permit measurements of upward and downward direct and diffuse solar flux between 350 and 1700 nm in some 330 spectral bands at approximately 2 km vertical resolution from an alititude of 160 km to the surface. The solar aureole camera measures the brightness of a 6° wide strip of the sky from 25 to 75° zenith angle near and opposite the azimuth of the sun in two passbands near 500 and 935 nm using vertical and horizontal polarizers in each spectral channel at a similar vertical resolution. The downward-looking spectrometers provide the reflection spectrum of the surface at a total of some 600 locations between 850 and 1700 nm and at more than 3000 locations between 480 and 960 nm. Some 500 individual images of the surface are expected which can be assembled into about a dozen panoramic mosaics covering nadir angles from 6° to 96° at all azimuths. The spatial resolution of the images varies from 300 m at 160 km altitude to some 20 cm in the last frames. The scientific objectives of the experiment fall into four areas including (1) measurement of the solar heating profile for studies of the thermal balance of Titan; (2) imaging and spectral reflection measurements of the surface for studies of the composition, topography, and physical processes which form the surface as well as for direct measurements of the wind profile during the descent; (3) measurements of the brightness and degree of linear polarization of scattered sunlight including the solar aureole together with measurements of the extinction optical depth of the aerosols as a function of wavelength and altitude to study the size, shape, vertical distribution, optical properties, sources and sinks of aerosols in Titan’s atmosphere; and (4) measurements of the spectrum of downward solar flux to study the composition of the atmosphere, especially the mixing ratio profile of methane throughout the descent. We briefly outline the methods by which the flight instrument was calibrated for absolute response, relative spectral response, and field of view over a very wide temperature range. We also give several examples of data collected in the Earth’s atmosphere using a spare instrument including images obtained from a helicopter flight program, reflection spectra of various types of terrain, solar aureole measurements including the determination of aerosol size, and measurements of the downward flux of violet, visible, and near infrared sunlight. The extinction optical depths measured as a function of wavelength are compared to models of the Earth’s atmosphere and are divided into contributions from molecular scattering, aerosol extinction, and molecular absorption. The test observations during simulated descents with mountain and rooftop venues in the Earth’},
}

@article{Rodean1981_01,
  title = {Inelastic Processes in Seismic Wave Generation by Underground Explosions},
  author = {Howard C Rodean},
  year = {1981},
  pages = {97-189},
  publisher = {Springer Netherlands},
  abstract = {There are similarities and differences between chemical and nuclear explosions underground. Most of the differences are in the early stages of the explosions. The later stages are similar with respect to seismic wave generation. Three sources of seismic waves from explosions are coincident in space and time, or nearly so: the explosion itself, explosion-induced tectonic strain release, and (probably) spall-closure following explosion-produced spall. Cavity collapse and explosion-induced aftershocks are two sources of delayed seismic signals. Theories, computer calculations, and measurements of spherical stress waves from explosions are described and compared, with emphasis on the transition from inelastic to almost-elastic relations between stress and strain. Two aspects of nonspherical explosion geometry are considered: tectonic strain release and surface spall. Tectonic strain release affects the generation of surface waves; spall closure may also. The forward problem in seismology can, in principle, be solved by calculations beginning with explosive detonation and ending with the synthetic seismogram. The inverse problem can also, in principle, be solved by inverting observed seismic data to obtain an “equivalent elastic source,” but the solution cannot extend backward in space and time into the nonlinear inelastic processes of the explosion. The reduced-displacement potential is a common solution (the “equivalent elastic source”) of the forward and inverse problems, assuming a spherical source. Measured reduced-displacement potentials are compared with potentials calculated as solutions of the direct and inverse problems; there are significant differences between the results of the two types of calculations and between calculations and measurements. The simple spherical model of an explosion is not sufficient to account for observations of explosions over wide ranges of depth and yield. The explosions environment can have a large effect on explosion detection and yield estimation. The best sets of seismic observations for use in developing discrimination techniques are for high-magnitude high-yield explosions; the identification problem is most difficult for low-magnitude low-yield explosion. Most of the presently available explosion data (time, medium, depth, yield, etc.) are for explosions in a few media at the Nevada Test Site; some key questions concerning magnitude vs yield and mb vs Ms relation can be answered only by data for explosion in other media at other locations.},
}

@article{McHenry1987,
  title = {Computer Networks in the Soviet Scientific Community},
  author = {William K McHenry},
  year = {1987},
  pages = {151-175},
  doi = {10.1007/978-94-009-3647-8_10},
  publisher = {Springer Netherlands},
  url = {https://doi.org/10.1007/978-94-009-3647-8_10},
  abstract = {Computer networks are offering scientists a host a new capabilities ranging from access to remote resources and databases to enhanced communications through electronic mail. Although networking has traditionally been one of the weakest areas of Soviet computer science [1], a number of local area networks at research institutes and the first part of a nationwide packet switching network for the Academy of Sciences have now been established. Along with surveying the achievements in these areas, this paper examines the influence of two base networking technologies, data transmission media and computers, on the development of networks. Only the non-military sphere is considered.},
}

@article{Sutherland2025,
  title = {Evaluation of AI for Strategic Communication Practices and Continuous Improvement},
  author = {Karen E Sutherland},
  year = {2025},
  pages = {335-374},
  doi = {10.1007/978-981-96-2575-8_11},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-96-2575-8_11},
  abstract = {EvaluationEvaluation is a fundamental part of the strategic communication process to measure campaign performance, to better understand target audiences and to support continuous improvementContinuous improvement (Buhmann and Likely, The International Encyclopaedia of Strategic Communication 1:625–640, 2018; Macnamara, International Journal of Strategic Communication 12:180–195, 2018; Mahoney, J. (2023). Strategic Communication: Campaign Planning. Routledge.).},
}

@article{Brindha2025,
  title = {Introduction to Multimodal Generative AI},
  author = {R Brindha and R K Pongiannan and A Bharath and V K S M Sanjeevi},
  year = {2025},
  pages = {1-36},
  doi = {10.1007/978-981-96-2355-6_1},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-96-2355-6_1},
  abstract = {A state-of-the-art method in machine learning is called Multimodal Generative Artificial Intelligence (AI), which aims to produce a variety of outputs in many modalities, including text, audio, and images. This survey explores multimodal generative AI’s developments, techniques, and uses. First, we clarify the main concepts behind such models, such as adaptive autoencoders (VAEs), generative adversarial networks (GANs), and their various extensions. The methods for fusing and aligning different modalities—such as conditional generation, cross-modal embeddings, and attention mechanisms—are then covered. A cutting-edge machine learning method called multi-modal generative artificial intelligence (AI) generates multiple methods of outputs such as text, audio, and visuals. This survey examines the progress, methods, and applications of generative AI. The article also examines various applications of multimodal generative AI, including image captioning, music composition, and text-to-picture synthesis. We also discussed challenges and ethical considerations related to multimodal content, such as privacy issues and prejudice reduction. Finally, we focus on a research roadmap that emphasizes the need for a multimodal, socially responsible, controlled, and comprehensible AI system.},
}

@article{Kumar2025,
  title = {Design of Indo-Pacific Core and Peripheral Digital Economic Communities},
  author = {T M Vinod Kumar},
  year = {2025},
  pages = {3-163},
  doi = {10.1007/978-981-96-1793-7_1},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-96-1793-7_1},
  abstract = {This brief aims to help design the Indo-Pacific Core and Peripheral Digital Economic Communities, focussing on emerging and ever-changing geopolitics and geoeconomics. The brief discusses the background of the Indo-Pacific Digital Economic Community, the Indo-Pacific International Collaborative Research, the Digital Economic Development, the Core and Peripheral Digital Economic Community, the Megacity System, and the Digital Knowledge-based Economy for enhanced household income generation. It also discusses the approach to transforming an existing community into a digital economic community, the feasibility of changing it within a regional framework of megacity systems and industrial corridors with large-scale industrialisation and heavy investment in infrastructure, the impact of the digital economy on households with the lowest income, roadblocks, essential nature of Indo-Pacific countries, and the differences between Core and Peripheral Smart Economic Communities. The design brief also discusses the vision, goals, and objectives ideal for transforming a community into a digital economic community and the tax and anti-corruption initiatives suitable for digital economic communities. Digital public goods (DPG) are the foundation of any nation’s digital economy, and the digital integration framework made in India, ASEAN, and the Indo-Pacific Economic Framework is essential. The digital economic community is designed to provide enhanced living and a digital economic community with higher income commensurate with a higher standard of living. The training of digital economic communities for developing the Indo-Pacific region, ASEAN Digital Integration Framework, and Indo-Pacific Economic Framework for Prosperity are also discussed.},
}

@article{Li2025_03,
  title = {Automatic Post-editing of Speech Recognition System Output Using Large Language Models},
  author = {Sheng Li and Jiyi Li and Yang Cao},
  year = {2025},
  pages = {178-186},
  publisher = {Springer Nature Singapore},
  abstract = {This paper explores the integration of automatic speech recognition (ASR) with large language models (LLMs), aiming to validate the effectiveness of this combination, particularly for automatic post-editing (PE) tasks. Initially, we investigate the use of LLMs for ASR PE error correction, performing second-pass rescoring on the output transcriptions generated by the ASR system, using both N-best decoding hypotheses and lattices. Subsequently, we examine the combination of ASR outputs from various systems using LLMs, addressing a classic system combination task. Experimental results demonstrate that LLMs can offer substantial assistance in automatic PE.},
}

@article{Rose2025,
  title = {Vaccine Literacy},
  author = {Chelsea Rose and Ruth Parker and Scott C Ratzan},
  year = {2025},
  pages = {1-18},
  doi = {10.1007/978-981-97-0821-5_119-1},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-97-0821-5_119-1},
  abstract = {Vaccination is one of public health’s most significant achievements due to its success in saving millions of lives and eradicating diseases. Vaccine hesitancy, which is fueled by misinformation and ineffective communication strategies, is an obstacle to vaccination and threatens global health. To mitigate vaccine hesitancy and reduce the prevalence and mortality from vaccine-preventable diseases, it is key to invest in vaccine literacy and effective communication. This chapter underscores the roles of vaccine literacy and effective communication in vaccine acceptance. Ineffective communication strategies that inhibit vaccine literacy, as well as effective communication strategies that improve vaccine literacy are explored in relation to eight principles of vaccine literacy. This chapter also highlights reasons to invest in vaccine literacy and effective communication and offers future recommendations for improving vaccine literacy.},
}

@article{Al-kfairy2025,
  title = {ChatGPT Through the Users’ Eyes: Sentiment Analysis of Privacy and Security Issues},
  author = {Mousa Al-kfairy and Ahmed Al-Adaileh and Obsa Sendaba},
  year = {2025},
  pages = {41-67},
  publisher = {Springer Nature Singapore},
  abstract = {This research investigates user perceptions of security and privacy in the context of AI technologies, focusing specifically on ChatGPT. We conducted a sentiment analysis of approximately 11,000 tweets collected from X Platform (formerly Twitter) between November 2022 and January 2024. Advanced natural language processing techniques were employed to preprocess the tweets, eliminating irrelevant data and refining the text for analysis. Sentiment analysis tools were then used to categorize user sentiments as positive, negative, or neutral. The results reveal a complex duality in user attitudes towards ChatGPT. While users generally appreciate the functional benefits and innovative capabilities of ChatGPT, there is substantial concern regarding data privacy and the potential misuse of AI technologies. Positive sentiments often highlighted the efficiency and utility of ChatGPT in various applications, whereas negative sentiments were predominantly focused on privacy risks and ethical considerations. These findings underscore the need for developers to prioritize transparent data handling practices and incorporate robust security features to address user concerns. Additionally, the study highlights the importance of updated regulations that protect user data while fostering innovation. Policymakers are urged to develop comprehensive policies that balance the need for security with the benefits of technological advancement. This study contributes to a deeper understanding of user perceptions of AI technologies, emphasizing the importance of a balanced approach that considers both benefits and risks. The insights gained provide a foundation for future research and inform strategies to enhance user trust and acceptance of AI technologies like ChatGPT.},
}

@article{Chakraborty2024,
  title = {Technologies Used in AI-Empowered Knowledge Management},
  author = {Sayan Chakraborty and Bitan Misra and Nilanjan Dey},
  year = {2024},
  pages = {13-21},
  doi = {10.1007/978-981-97-2574-8_3},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-97-2574-8_3},
  abstract = {The use of technologies that allow educational institutions to handle enormous amounts of data effectively and obtain insights to improve student outcomes is known as AI-empowered knowledge management. These technologies, which enable institutions to customize learning experiences, offer focused support, and automate administrative activities, include big data analytics, machine learning, explainable AI, knowledge representation with reasoning, natural language processing, and large language modeling.},
}

@article{Hirakawa2024,
  title = {Another Way of Development Through ICT: Possibility and Vulnerability},
  author = {Hitoshi Hirakawa},
  year = {2024},
  pages = {149-201},
  doi = {10.1007/978-981-97-3106-0_5},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-97-3106-0_5},
  abstract = {Since the 1990s, the importance of Information and Communications Technology (ICT)-enabled services/digitally deliverable services has steadily increased along with economic globalization. Until now, the driving force of the world economy has been world trade, mainly in goods. The digital economy has now arrived, with finance, telecommunications, software development, Business Process Outsourcing (BPO), and other service transactions becoming increasingly important, and Artificial Intelligence (AI) and big data becoming the greatest source of competitiveness. On the one hand, this economy has opened the way for some emerging and developing economies to develop through ICT and computer-based digital-based services trade, bringing great expectations to some emerging and developing economies. On the other hand, it has created increasingly difficult catch-up barriers for many developing countries. This chapter identifies the ICT-based services trade that has been the focus of much attention at the turn of the century, and refers to some of the key issues related to the development of emerging and developing economies that have been the subject of much discussion. At the same time, it examines the possibilities and challenges for the development of emerging and developing economies opened up by the development of ICTs.},
}

@article{Kavyashree2025,
  title = {Generative Adversarial Networks (GANs) for Education: State-Of-Art and Applications},
  author = {K N Kavyashree and Ganeshayya Shidaganti},
  year = {2025},
  pages = {355-370},
  publisher = {Springer Nature Singapore},
  abstract = {In the ever-changing educational landscape, technology integration has become essential to improving students’ understanding of basic topics. The COVID-19 pandemic hastened the transition to online education while posing new difficulties for teachers and students alike. The disruption of traditional classroom dynamics was exacerbated by the lack of digital innovation and interactive instructional tools, which made student participation less effective. Traditional teaching paradigms must make way for ones that encourage self-directed learning and fortify technical infrastructure in order to handle these difficulties. Generative Adversarial Networks (GANs), a cutting-edge use of deep learning, are starting to revolutionize educational technology. With the use of GANs, a range of features that are tailored to the individual learning preferences of each student may be created, thereby bridging knowledge gaps and providing a more dynamic and interesting learning environment for students. This research investigates several state-of-the-art features that leverage technology linked to GANs to enhance students’ virtual learning environments. The novel applications of generative adversarial networks are explored, with a focus on applications for student observation, academic performance, questioning and answering, and personalized learning.},
}

@article{Ferenczy2024,
  title = {Democratic Resilience. Lessons to Tell, Lessons to Learn},
  author = {Zsuzsa Anna Ferenczy},
  year = {2024},
  pages = {83-110},
  doi = {10.1007/978-981-97-6481-5_4},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-97-6481-5_4},
  abstract = {A shared belief in democracy is the core of EU-Taiwan cooperation, which both sides have capitalized on  in their common fight against foreign information manipulation and interference (FIMI). Awareness that Beijing and Moscow support one another to undermine democracy has brought Europe and Taiwan closer and created an opportunity for Taiwan to contribute to Europe’s counter-disinformation efforts. Europeans opened up to learning from Taiwan, and FIMI cooperation has become central to shaping the new reality in EU-Taiwan cooperation. The Russia-China strategic convergence in the information space explains why the future of Taiwan matters beyond China, and why the future of Ukraine matters beyond Europe. As a response to Chinese interference attempts and decades of intimidation, Taiwan has embraced democracy as a defence mechanism. Taiwan’s handling of Chinese interference can inspire Europe, both in better understanding CCP tactics and in its response to the tactics. In 2024, Taiwan successfully elected Lai Ching-te as their next president. Yet, Taiwan is not immune to hybrid threats either. Russia and China have learnt from each other to leverage disinformation. Europe and Taiwan must learn more from one another on countering disinformation in ways most suitable to their needs.},
}

@article{Shaban2024_02,
  title = {Digital Rights, Digital Representation, and Digital Justice—Towards Digital Democracy and Freedom of Expression},
  author = {Abdul Shaban},
  year = {2024},
  pages = {765-899},
  doi = {10.1007/978-981-97-4734-4_7},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-97-4734-4_7},
  abstract = {Access to the internet has been advocated as human right or basic right as it mediates and leads to not only participation in the democratic process but also access to the basic services which a society or state has to offer. The state and corporate governance is also being shaped from the usual bureaucratic ones to that which works on the network of flow of information. Digitalization is also making possible not only the interaction of citizens with the state machinery in real-time and also economical but also leading to ‘open governance’, where the rules, processes, and deliveries or actions have become more transparent. ‘Digital governance’, and/or ‘e-governance’ is the term used to define the internet-mediated governance process.},
}

@article{Shaban2024_03,
  title = {Digital Turn and Theorizing the Digital Geographies},
  author = {Abdul Shaban},
  year = {2024},
  pages = {17-151},
  doi = {10.1007/978-981-97-4734-4_2},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-97-4734-4_2},
  abstract = {Progress of civilization is characterized by a horizontal and vertical expansion of techno-sphere, which currently has digital technologies and AI at the core. Although the evolution of machines and its impact on human behaviour, and socio-economic, political, and cultural activities have been a subject of debate for the last two centuries (Corpo 2017; Ash et al. 2016), the rise of digital technologies, especially since the early 1990s with commercialization of internet and availability of cheap computing machines, have had massive disruptive and transformative impacts on human to human, human and environment, human and machine and body and mind relationships.},
}

@article{Wong2024,
  title = {Blended Learning and AI: Enhancing Teaching and Learning in Higher Education},
  author = {Katrine K Wong},
  year = {2024},
  pages = {39-61},
  publisher = {Springer Nature Singapore},
  abstract = {Learner engagement is essential for positive student learning experiences and outcomes. While the higher education sector has accumulated years of experience with learner engagement with conventional face-to-face instruction, it is also placing increasingly more emphasis on the combination of in-person and online teaching and learning (T&L) activities, which enhances T&L in multiple ways. Promoting and supporting learner engagement in blended learning environment is receiving greater attention and scrutiny in recent years. This keynote presentation will be situated in an AI-infused T&L landscape: the speaker will survey recent scholarship on generative artificial intelligence and blended learning and share her insights into strategies for delivering blended learning to maximize the benefit for instructors and learners in the emerging digital learning environments of higher education.},
}

@article{Dey2025,
  title = {An Analysis of Automatic Question Generation Research Progress and Challenges},
  author = {Debopam Dey and Dwijen Rudrapal},
  year = {2025},
  pages = {247-258},
  publisher = {Springer Nature Singapore},
  abstract = {Automatic question generation (AQG) is a compelling and challenging area of research within Natural Language Processing (NLP). This field focuses on the automatic creation of questions from a given text, enhancing applications such as reading comprehension exercises, reducing the time teachers spend preparing questions and aiding second-language learners. The primary motivation for AQG research is the need for scalable, effective solutions to content production, evaluation and knowledge sharing. Traditional question-creation methods are labour-intensive and time-consuming, necessitating human annotators. With the rapid growth of digital data, automated systems capable of extracting relevant information and generating questions are increasingly essential. AQG aims to develop computational systems that can understand text and produce meaningful questions that test comprehension and problem-solving skills. This paper classifies various AQG approaches, analyses their results using automatic evaluation scores, reviews different datasets and their availability, and discusses current and potential evaluation techniques.},
}

@article{Uddagiri2024,
  title = {Ethical and Privacy Challenges of Generative AI},
  author = {Chandrasekhar Uddagiri and Bala Venkateswarlu Isunuri},
  year = {2024},
  pages = {219-244},
  doi = {10.1007/978-981-97-8460-8_11},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-97-8460-8_11},
  abstract = {The development of Generative Artificial Intelligence (Generative AI) presents significant ethical and privacy issues that need to be carefully considered as it opens up previously unheard-of opportunities for creativity, automation, and problem-solving. This chapter explores the convergence of technology, society, and privacy as it digs into the complex world of ethical issues surrounding the use of generative AI. A wide range of concerns falls under the category of ethical considerations, such as the potential for social injustices to be amplified, biases present in training data, and the moral implications of content produced by artificial intelligence. This chapter explores how biases in training datasets might unintentionally reinforce societal preconceptions and provide discriminating results in AI-generated applications. Additionally, it looks at how organizations and developers can help reduce prejudice and guarantee fairness in generative AI systems. The chapter also explores the implications AI-generated content has for society, from text and images to full stories. The ethical implications of producing and sharing content that conflates real and artificial intelligence bring up issues with inaccuracy, manipulation, and potentially harmful applications. With a focus on protecting individual privacy, the chapter examines the fine line that must be drawn between fostering innovation and ensuring the welfare of society. Regarding privacy, the chapter discusses the risks posed by generative artificial intelligence, especially about safeguarding personal information. The chapter examines the possibility of privacy being exploited by creating synthetic content and highlights the necessity of strong security measures to prevent nefarious actors from compromising personal data. The chapter also encourages a proactive approach to addressing these issues by outlining developing best practices and current frameworks for ethical AI development. To create a responsible AI ecosystem that puts individual privacy first, it offers methods for incorporating explainability, accountability, and transparency into Generative AI systems. This chapter seeks to contribute to the ongoing discussion on the direction of AI development by critically analyzing the ethical and privacy issues surrounding generative AI. To fully realize the transformational potential of generative AI while avoiding unforeseen repercussions, it emphasizes the significance of an ethical foundation and strong privacy safeguards.},
}

@article{Wan2024,
  title = {Aspect-Based Sentiment Classification Model Based on Multi-view Information Fusion},
  author = {Yujie Wan and Tianyu Cai and Yilin Li and Shenggen Ju},
  year = {2024},
  pages = {16-28},
  publisher = {Springer Nature Singapore},
  abstract = {Aspect-based sentiment classification is one of the hot tasks in the field of natural language processing. The task aims to judge the sentiment polarity of the target word, also known as the aspect term, specified in the sentence. The current mainstream models aggregate the information of the aspect term neighbor nodes through the graph neural network model to judge the sentiment polarity. Compared with the previous research, this method has achieved obvious results, but it still faces some problems. First of all, the limited scale of the existing public data set constrains the training of the model, and the general knowledge representation ability has certain deficiencies. Secondly, existing methods use single-view information to judge sentiment polarity, but lack multi-view information and corresponding information fusion methods, the complementarity of sentiment feature information from different perspectives has not been studied. To solve the above problems, an aspect-based sentiment classification model based on multi-view information fusion is proposed. By constructing an inference result set from the large language model (LLM), the LLM’s results are used to enhance the model’s knowledge representation ability. A multi-view information fusion module is proposed to integrate information from two aspects: local fusion and global fusion, and make full use of information from different angles. The experimental results show that the model has higher classification ability than the current mainstream models, and the effectiveness of each module of the model is verified by a variety of experiments.},
}

@article{Khine2024,
  title = {Educational Data Mining and Learning Analytics},
  author = {Myint Swe Khine},
  year = {2024},
  pages = {1-159},
  doi = {10.1007/978-981-97-9350-1_1},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-97-9350-1_1},
  abstract = {Since the advent of the internet, online and distance education has become the predominant mode of instructional delivery in education and training settings. Effective online learning is not solely dependent on instructional design. Factors such as student engagement, learning styles, and personal characteristics also play a significant role in determining the success of online learning. Educational data mining and learning analytics provide distance educators with insights into the students, learning patterns, and methods to support the learners. Educational Data Mining (EDM) and Learning Analytics (LA) are two closely related fields that both deal with the analysis of data in order to improve learning experiences. Romero and Ventura (2020) define EDM as the development of methods for analysing the unique types of data that are collected from learning environments. EDM is also the application of Data Mining (DM) techniques to this specific type of dataset that originates from educational environments in order to address important educational questions. EDM is concerned with the identification of patterns within educational data that may otherwise remain hidden. This is achieved through the application of statistical and machine learning techniques, which enable the identification of relationships between variables.},
}

@article{Shaban2024_04,
  title = {History, Space, and Digital Modelling},
  author = {Abdul Shaban},
  year = {2024},
  pages = {69-130},
  doi = {10.1007/978-981-97-9278-8_2},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-97-9278-8_2},
  abstract = {Analysis of the location of events in time and space and their (co)evolutions have been major interests of science. No event escapes time and space and their relative gridded locations have been an area of increased interest. Before the Industrial Revolution and the development of sophisticated instruments, the events which occupied relatively large locations both in time and space were more possible to be analysed and understood.},
}

@article{Ferraris2025,
  title = {Enhancing TrUStAPIS Methodology in the Web of Things with LLM-Generated IoT Trust Semantics},
  author = {Davide Ferraris and Konstantinos Kotis and Christos Kalloniatis},
  year = {2025},
  pages = {125-144},
  publisher = {Springer Nature Singapore},
  abstract = {In the Internet of Things (IoT) there are ecosystems where their physical ’smart’ entities virtually interact with each other. Often, this interaction occurs among unknown entities, making trust an essential requirement to overcome uncertainty in several aspects of this interaction. However, trust is a complex concept, and incorporating it in IoT is still a challenging topic. For this reason, it is highly significant to specify and model trust in early stages of the System Development Life Cycle (SDLC) of IoT-integrated systems, thus enhancing the aforementioned task. TrUStAPIS is a requirements engineering methodology recently introduced for incorporating trust requirements during IoT-based system design. The scope of this paper is to provide an extension of TrUStAPIS by introducing IoT trust semantics compatible with the W3C Web of Things (WoT) recommendations generated with the assistance of Large Language Models (LLMs). Taking advantage of LLMs as a tool for integrating and refining existing methodologies, in this paper we present our work towards a revision of the TrUStAPIS methodology. In this work, we contribute a new conceptual model and a refined JSON-LD ontology that takes into account IoT trust semantics, providing eventually a valuable tool for software engineers to design and model IoT-based systems and services.},
}

@article{Shaban2024_05,
  title = {History, Space, and Digital Modelling},
  author = {Abdul Shaban},
  year = {2024},
  pages = {69-130},
  doi = {10.1007/978-981-97-9278-8_2},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-97-9278-8_2},
  abstract = {Analysis of the location of events in time and space and their (co)evolutions have been major interests of science. No event escapes time and space and their relative gridded locations have been an area of increased interest. Before the Industrial Revolution and the development of sophisticated instruments, the events which occupied relatively large locations both in time and space were more possible to be analysed and understood.},
}

@article{Liu2023,
  title = {Opportunities and Challenges of Digital Financial Development},
  author = {Zhiyi Liu and Wenxuan Hou},
  year = {2023},
  pages = {1-15},
  doi = {10.1007/978-981-99-7305-7_1},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-99-7305-7_1},
  abstract = {The historical process of financial technology is complex and diverse, tracing back to the origins of early computerized trading and evolving with the rise of the internet, mobile payments, and blockchain technology.},
}

@article{Nulkar2024_01,
  title = {Natural Capital},
  author = {Gurudas Nulkar},
  year = {2024},
  pages = {1-81},
  doi = {10.1007/978-981-99-6893-0_1},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-99-6893-0_1},
  abstract = {Natural resources are the foundation of the human economy. Every product or service produced in the economy and sold in the marketplace consumes natural resources. Land, fossil fuel, minerals, water, gases, timber, plant extracts and resins, and organisms are converted into goods and services and in this process financial capital is created in the marketplace. Resources provided by nature thus form the natural capital that the economy feeds upon.},
}

@article{Gupta2024_01,
  title = {Importance of Identifying Consent},
  author = {Indranath Gupta},
  year = {2024},
  pages = {397-551},
  doi = {10.1007/978-981-99-6778-0_5},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-99-6778-0_5},
  abstract = {Certain key phrases and terms are primarily associated with the idea of consent used in the parlance of data protection and privacy. For instance, free/freely, specific, informed, unconditional, unambiguous and a clear affirmative action/act.},
}

@article{Gupta2024_02,
  title = {Data Protection—Issues and Perspectives},
  author = {Indranath Gupta},
  year = {2024},
  pages = {193-395},
  doi = {10.1007/978-981-99-6778-0_4},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-99-6778-0_4},
  abstract = {The objective is to place the data subject in focus while deciding the course of actions that protect and prevent the misuse of personal information. Often, the data controllers or those processing personal data work towards compliance within prescribed norms. These norms are set according to the legislative interventions and guidelines that jurisdictions provide. While there are common grounds in different jurisdictions, they have added their understanding in framing data protection norms.},
}

@article{Gupta2024_03,
  title = {Capturing Different Discourse to Privacy},
  author = {Indranath Gupta},
  year = {2024},
  pages = {1-64},
  doi = {10.1007/978-981-99-6778-0_1},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-99-6778-0_1},
}

@article{Song2025,
  title = {Large Language Model and Application for Railway Track Management Based on Domain Specialization},
  author = {Yu Song and Yuanjie Tang and Rengkui Liu},
  year = {2025},
  pages = {194-204},
  publisher = {Springer Nature Singapore},
  abstract = {This paper explores the method of domain specialization for Large Language Models in the field of railway track management, as well as their potential and practical effectiveness in knowledge-based question answering scenarios. Given the complexity of the railway track management work scenarios and the challenges associated with existing data acquisition and processing, this paper has proposed a domain-specialized large language model tailored for railway track management. Additionally, by integrating layout analysis, OCR image recognition technologies, and the self-instruct method, the efficiency and accuracy of data processing have been enhanced, and the training dataset has been expanded. Moreover, the paper employs knowledge retrieval augmentation techniques to mitigate the issue of hallucinations produced by the model, thereby improving the model's accuracy, relevance, and completeness in knowledge question answering. Furthermore, the paper has established a dataset and evaluation metrics specifically for question answering in the railway track management field and assessed them using a general Large Language Model. This research not only aids relevant personnel in their daily operations and enhances work efficiency but also provides valuable references for future applications of Large Language Models in similar domains.},
}

@article{Gupta2024_04,
  title = {Capturing Different Discourse to Privacy},
  author = {Indranath Gupta},
  year = {2024},
  pages = {1-64},
  doi = {10.1007/978-981-99-6778-0_1},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-99-6778-0_1},
}

@article{Cronin2024,
  title = {Text-Based Generative Intelligent Agents: Beyond Traditional Chatbots and Virtual Assistants},
  author = {Irena Cronin},
  year = {2024},
  pages = {181-193},
  doi = {10.1007/979-8-8688-0282-9_12},
  publisher = {Apress},
  url = {https://doi.org/10.1007/979-8-8688-0282-9_12},
  abstract = {Generative Intelligent Agents (GIAs) and Generative AI are deeply intertwined concepts within the realm of artificial intelligence. Generative AI refers to the subset of AI technologies that can autonomously generate complex content, such as text, images, audio, and other media that resemble human-like creations. GIAs are a specific application of Generative AI, embodying systems or agents that utilize these generative capabilities to perform tasks, solve problems, and interact with their environment in an intelligent manner.},
}

@article{Haber2024,
  title = {Evolving Identity Security Threats},
  author = {Morey J Haber and Darran Rolls},
  year = {2024},
  pages = {153-184},
  doi = {10.1007/979-8-8688-0233-1_14},
  publisher = {Apress},
  url = {https://doi.org/10.1007/979-8-8688-0233-1_14},
  abstract = {By executing well on the preceding steps, you will address most of your identity security needs, eliminate or mitigate many identity attack vectors, and vastly reduce your threat surface. This is something you can tangibly measure when you see the number of alerts, investigations, and incidents decrease in your Security Operations Center (SOC).},
}

@article{Olynick2024,
  title = {Ethical Design in an AI-Driven World},
  author = {Diana Olynick},
  year = {2024},
  pages = {235-265},
  doi = {10.1007/979-8-8688-0083-2_8},
  publisher = {Apress},
  url = {https://doi.org/10.1007/979-8-8688-0083-2_8},
  abstract = {In the fields of quantum mechanics and general relativity, there is a term called “singularity.” It refers to the theoretical phenomenon where gravitational forces cause matter to present infinite density, like the example of black holes, which are presumed to contain a singularity area that appears to break all the laws of physics. This is called quantum singularity. In the same way that we are uncertain about the actual happenings inside a black hole, it’s also uncertain how AI surpassing human intelligence will behave and use its autonomous capabilities.},
}

@article{De2023,
  title = {Introduction to APIs},
  author = {Brajesh De},
  year = {2023},
  pages = {1-26},
  doi = {10.1007/979-8-8688-0054-2_1},
  publisher = {Apress},
  url = {https://doi.org/10.1007/979-8-8688-0054-2_1},
  abstract = {API stands for application programming interface. An API exposes a business service or an enterprise asset to the application developers. Applications can be installed and accessed from a variety of devices, such as smartphones, tablets, kiosks, gaming consoles, connected cars, and so forth. Google Maps APIs for locating a place on a map, Facebook APIs for gaming or sharing content, and Amazon APIs for product information are some examples of APIs. Developers use these APIs to build cool and innovative apps that enrich the user experience. For example, developers can use APIs from different travel companies to build an app that compares and displays each travel company’s price for the same hotel. A user can then make an informed decision and book the hotel through the company providing the best offer. This saves users from comparing on their own—thus improving the overall experience. APIs thus provide an improved user experience.},
}

@article{Kieser2024,
  title = {Using Large Language Models to Probe Cognitive Constructs, Augment Data, and Design Instructional Materials},
  author = {Fabian Kieser and Peter Wulff},
  year = {2024},
  pages = {293-313},
  doi = {10.1007/978-981-99-9379-6_14},
  publisher = {Springer Nature Singapore},
  url = {https://doi.org/10.1007/978-981-99-9379-6_14},
  abstract = {Significant advances in AI research, particularly in the field of machine learning, have opened up the possibility of solving problems in various specialist areas with the help of AI systems. In particular, advances with large language models have implications for education. This paper uses the currently most prominent large language model, GPT-4, to investigate the extent to which the process for solving physics problems can be modelled according to established problem-solving models. Furthermore, we evaluate in which ways specific prompting can be used to apply different problem solving strategies and which types of physical problems this large language model is able to generate when specifically prompted. Our investigations show that while GPT-4 is capable of solving physics problems, it occasionally uses common misconceptions and incomplete problem solving approaches. Specific problem solving strategies can be applied by giving specific instructions (i.e., prompts). Our findings have implications for utilizing GPT-4 for fostering problem solving strategies for students in physics.},
}

@article{Baruah2024,
  title = {Smart Factory},
  author = {Bidwan Baruah and Krishnakumar Ramadoss and Abarajith Vivekanandha},
  year = {2024},
  pages = {209-269},
  doi = {10.1007/979-8-8688-0890-6_5},
  publisher = {Apress},
  url = {https://doi.org/10.1007/979-8-8688-0890-6_5},
  abstract = {In recent times, there has been a growing acknowledgement across all industries of the imperative need for digital transformation within manufacturing operations to uphold competitiveness and resilience. The COVID-19 pandemic highlighted weaknesses in global supply chains and vulnerabilities within various sectors. The pandemic has emphasized the necessity for more adaptable, agile solutions that are fully integrated digitally. Additionally, evolving consumer expectations are driving the advancement of smart factory technologies and reshaping the concept of future manufacturing facilities. Commonly called the “Amazon effect,” the demand for next-day delivery has been steadily increasing, significantly boosting the requirement for smart factory technology as outdated systems struggle to meet the logistical demands of this trend. Moreover, manufacturers are tired of facing increased risks and operational disruptions. Therefore, it is essential to implement digital factory technologies to improve efficiency and visibility, especially in today’s dynamic business environment.},
}

@article{Sharma2024_01,
  title = {AI Privacy},
  author = {Rohan Sharma},
  year = {2024},
  pages = {37-45},
  doi = {10.1007/979-8-8688-0796-1_4},
  publisher = {Apress},
  url = {https://doi.org/10.1007/979-8-8688-0796-1_4},
  abstract = {As we increasingly rely on AI systems to drive innovation and efficiency, the risk of privacy breaches has become a ticking time bomb. With the ability to collect, analyze, and predict human behavior, AI systems pose a significant threat to individual privacy. The question is no longer if, but when the next major data breach will occur. As Marlon Brando aptly put it, "Privacy is not something we are merely entitled to, it's an absolute prerequisite." This chapter explores the critical need for robust AI privacy governance, providing a framework for organizations to safeguard sensitive information while leveraging AI technologies.},
}

@article{Saxena2024,
  title = {Challenges in Mainstream Adoption},
  author = {Anshul Saxena and Shalaka Verma and Jayant Mahajan},
  year = {2024},
  pages = {267-293},
  doi = {10.1007/979-8-8688-0559-2_9},
  publisher = {Apress},
  url = {https://doi.org/10.1007/979-8-8688-0559-2_9},
  abstract = {To construct an informative and comprehensive Chapter 9on the integration of artificial intelligence (AI) into the Banking, Financial Services, and Insurance (BFSI) sector, it's crucial to delve into the multifaceted challenges and opportunities this technology presents. The chapter aims to navigate through the technical, organizational, and regulatory hurdles and the nuanced market dynamics that shape the landscape of AI adoption within the sector. This exploration is not just about highlighting the hurdles but also about showcasing the innovative strides being made in overcoming these challenges, ensuring a balanced narrative that reflects both the potential and the pitfalls of AI in BFSI.},
}

@article{Bjerg2024,
  title = {Understanding Language Models},
  author = {Jonas Bjerg},
  year = {2024},
  pages = {21-55},
  doi = {10.1007/979-8-8688-0456-4_3},
  publisher = {Apress},
  url = {https://doi.org/10.1007/979-8-8688-0456-4_3},
  abstract = {Language models have a role in natural language processing (NLP), a branch of artificial intelligence that focuses on the interaction between computers and human language. In essence, language models are structures created to predict the likelihood of word sequences occurring in a given language. This predictive ability forms the basis for applications ranging from spell-check and autocomplete features to complex tasks like machine translation and text generation.},
}

@article{Gupta2024_05,
  title = {Databricks Platform: From Lakehouse to Data Intelligence Platform},
  author = {Nikhil Gupta and Jason Yip},
  year = {2024},
  pages = {1-13},
  doi = {10.1007/979-8-8688-0444-1_1},
  publisher = {Apress},
  url = {https://doi.org/10.1007/979-8-8688-0444-1_1},
  abstract = {The intensifying pace of digital transformation has led companies to amass increasing volumes of diverse data from various sources. This data explosion carries enormous potential for organizations to uncover transformative insights to guide innovation and decision-making through advanced analytics.},
}

@article{Gupta2024_06,
  title = {External Communities},
  author = {Arun Gupta},
  year = {2024},
  pages = {217-281},
  doi = {10.1007/979-8-8688-0977-4_6},
  publisher = {Apress},
  url = {https://doi.org/10.1007/979-8-8688-0977-4_6},
  abstract = {The globally diverse nature of open source makes engaging with external communities increasingly vital. These communities are wide-ranging covering code, industry events, and open source foundations. Typically, it starts with getting involved in an open source project either as a consumer, contributor, or maintainer. The discussion starts online through using the code, filing issues, and contributing code and docs. The desire to meet other maintainers and users of the project grows stronger as you start working with them closely. If the project belongs to an open source foundation, then you may consider joining the foundation to participate in the strategic direction of the foundation. Engaging with different channels of vibrant and active communities within these projects ensures sustainability and maximizes the impact of an organization’s efforts.},
}

@article{Hawkins2024,
  title = {Navigating the Shadows: The Ethical Maze of AI},
  author = {William Hawkins},
  year = {2024},
  pages = {85-109},
  doi = {10.1007/979-8-8688-0911-8_6},
  publisher = {Apress},
  url = {https://doi.org/10.1007/979-8-8688-0911-8_6},
  abstract = {It’s time to identify where the bright promise of this technology meets the intricate challenges of moral choices. This chapter reveals the delicate balance between innovation and responsibility, guiding you through the nuanced paths where AI's potential is weighed against its pitfalls.},
}

@article{Cagle2024,
  title = {Information Curation},
  author = {Anton Cagle and Ahmed Ceifelnasr Ahmed},
  year = {2024},
  pages = {213-223},
  doi = {10.1007/979-8-8688-0902-6_11},
  publisher = {Apress},
  url = {https://doi.org/10.1007/979-8-8688-0902-6_11},
  abstract = {Enterprise AI applications are driven by a dynamic flow of information. Information, as described in this book, is the combination of data and context. The data comes in many different formats and temporal patterns. The context for an information set is defined by the business purpose and incentives that are defined around a specified combination of data and related processes. Quality information is the lifeblood of successful AI applications, and in this chapter, we will discuss the different types of information available, the nature of that information through its life cycle, and strategies for acquiring and maintaining quality information.},
}

@article{Schuett2023_01,
  title = {Three lines of defense against risks from AI},
  author = {Jonas Schuett},
  year = {2023},
  journal = {AI & SOCIETY},
  doi = {10.1007/s00146-023-01811-0},
  url = {https://doi.org/10.1007/s00146-023-01811-0},
  abstract = {Organizations that develop and deploy artificial intelligence (AI) systems need to manage the associated risks—for economic, legal, and ethical reasons. However, it is not always clear who is responsible for AI risk management. The three lines of defense (3LoD) model, which is considered best practice in many industries, might offer a solution. It is a risk management framework that helps organizations to assign and coordinate risk management roles and responsibilities. In this article, I suggest ways in which AI companies could implement the model. I also discuss how the model could help reduce risks from AI: it could identify and close gaps in risk coverage, increase the effectiveness of risk management practices, and enable the board of directors to oversee management more effectively. The article is intended to inform decision-makers at leading AI companies, regulators, and standard-setting bodies.},
}

@article{Areyuna2024,
  title = {Probing left-handed heavy neutral leptons in the Vector Scotogenic Model},
  author = {C Paulo Areyuna and Jilberto Zamora-Saa and Alfonso R Zerwekh},
  year = {2024},
  journal = {Journal of High Energy Physics},
  volume = {2024},
  pages = {153},
  doi = {10.1007/JHEP02(2024)153},
  url = {https://doi.org/10.1007/JHEP02(2024)153},
  abstract = {In this work, we consider an extension to the Standard Model composed by a Massive Vector Doublet under SU(2)L and a Left-handed Heavy Neutral Lepton. We study the production of these exotic leptons with the Same Flavor Opposite Sign standard lepton pair, and jets, considering Drell-Yan and Vector Boson Fusion as independent cases. We find that for the latter, the dilepton angular distribution is different enough from the background to use it as a smoking-gun for our model. Based on this fact, we establish limits on the parameter space considering previous experimental searches in this final state.},
}

@article{Duke2025,
  title = {Responsible AI and AI Governance},
  author = {Toju Duke and Paolo Giudici},
  year = {2025},
  pages = {3-24},
  doi = {10.1007/979-8-8688-1166-1_1},
  publisher = {Apress},
  url = {https://doi.org/10.1007/979-8-8688-1166-1_1},
  abstract = {Responsible AI is a new and nascent field, and the term “responsible AI” has been used interchangeably with the term “ethical AI” in recent years. In this chapter, we’ll look at a brief history of responsible AI and the factors influencing its emergence as a new and incredibly valuable part of AI development. We will also review new global AI regulation and policies from UNESCO to the EU AI Act and a few other global AI policies, highlighting the different principles in each policy and the importance of adopting the recommendations to build AI governance within your organization/project.},
}

@article{Gu2024,
  title = {A-ESA Modeling Styles},
  author = {Sean (Chunhong) Gu},
  year = {2024},
  pages = {245-351},
  doi = {10.1007/979-8-8688-0992-7_5},
  publisher = {Apress},
  url = {https://doi.org/10.1007/979-8-8688-0992-7_5},
  abstract = {There are many architectural styles to choose from. Choosing the right one is critical for enterprise solution architecture and enterprise software systems. In fact, an architectural style can have a significant impact on solution quality attributes. Reference architectures, patterns, and best practices highlight some of the architectural style considerations. Table 5-1 provides a list of IT architectural styles.},
}

@article{Bughin2024_01,
  title = {Doing versus saying: responsible AI among large firms},
  author = {Jacques Bughin},
  year = {2024},
  journal = {AI & SOCIETY},
  doi = {10.1007/s00146-024-02014-x},
  url = {https://doi.org/10.1007/s00146-024-02014-x},
  abstract = {Responsible Artificial Intelligence (RAI) is a subset of the ethics associated with the use of artificial intelligence, which will only increase with the recent advent of new regulatory frameworks. However, if many firms have announced the establishment of AI governance rules, there is currently an important gap in understanding whether and why these announcements are being implemented or remain “decoupled” from operations. We assess how large global firms have so far implemented RAI, and the antecedents to RAI implementation across a wide range of RAI initiatives. We find that the operationalization of RAI practices is scattered across firms, with only a fringe of companies extensively industrializing RAI. Social pressure pushes RAI design (”saying”) rather than implementation but the reverse is true for competitive pressure. AI capabilities as a bundle of data quality AI architecture, and talents are strongly associated with RAI design to scaling.},
}

@article{Kouloukoui2025,
  title = {Balancing risks and benefits: public perceptions of AI through traditional surveys and social media analysis},
  author = {Daniel Kouloukoui and Nathalie de Marcellis-Warin and Thierry Warin},
  year = {2025},
  journal = {AI & SOCIETY},
  doi = {10.1007/s00146-025-02232-x},
  url = {https://doi.org/10.1007/s00146-025-02232-x},
  abstract = {The rapid evolution of Artificial Intelligence (AI) has broadened discussions about its social and technological implications. The purpose of this study is to examine the perception of risks and benefits associated with AI, focusing on privacy, cybersecurity, trust in government management, and online device acceptance over time. We used an integrative approach that combined questionnaires and data analysis from X via Natural Language Processing (NLP). The traditional survey had 1013 participants in 2018 and 1000 in 2021, while X’s analysis examined approximately 7.5 million tweets in the same period. The results indicate a constant concern with the risks of AI, such as the dehumanization of services (86%), job loss (82%), changes in labor functions (89%), privacy (87%), and cyber-attacks (87%). X also highlighted ethical concerns and the use of algorithms, perceived as a “threat cloud.” Acceptance of connected devices was low (23%), reflecting security and privacy concerns. Confidence in government management was remarkably low in relation to cybersecurity (12%) and privacy (13%). Nevertheless, there is a modest increase in the perception of the benefits of AI, such as technological advances and efficiency. This study underscores the need for collaborative and effective policies and the promotion of a culture of responsibility to maximize the benefits of AI while mitigating risks. The practical implications of these findings are discussed, emphasizing the importance of policies and regulations to address emerging digital threats.},
}

@article{Bozdağ2024,
  title = {The AI-mediated intimacy economy: a paradigm shift in digital interactions},
  author = {Ayşe Aslı Bozdağ},
  year = {2024},
  journal = {AI & SOCIETY},
  doi = {10.1007/s00146-024-02132-6},
  url = {https://doi.org/10.1007/s00146-024-02132-6},
  abstract = {This article critically examines the paradigm shift from the attention economy to the intimacy economy—a market system where personal and emotional data are exchanged for customized experiences that cater to individual emotional and psychological needs. It explores how AI transforms these personal and emotional inputs into services, thereby raising essential questions about the authenticity of digital interactions and the potential commodification of intimate experiences. The study delineates the roles of human–computer interaction and AI in deepening personal connections, significantly impacting emotional dynamics, and underscores AI’s role in various applications, from healthcare to grief tech, highlighting both enhancements in emotional connections and potential disruptions to genuine human interactions. An AI-mediated framework (AMIE) is introduced to assess how AI reshapes these connections and the overall digital society through personalized interactions. This framework explores the interplay between human emotions and AI-generated responses within the new Avatar Sphere, emphasizing the necessity for regulatory measures to safeguard digital identities, recognize emotional data as intellectual property, and maintain system transparency. It highlights the critical need for maintaining genuine human interactions and advocates for context-aware consent, continuous monitoring, and cross-cultural considerations to foster ethical AI practices. Leveraging blockchain technology and decentralized autonomous organizations, the framework proposes methods to enhance individual control over emotional data, mitigating the commodification risks. The findings contribute to ongoing discussions on AI ethics, digital privacy, and the future of human-AI interactions, providing valuable insights for cultivating a responsible intimacy economy.},
}

@article{Algherairy2024,
  title = {A review of dialogue systems: current trends and future directions},
  author = {Atheer Algherairy and Moataz Ahmed},
  year = {2024},
  journal = {Neural Computing and Applications},
  volume = {36},
  pages = {6325-6351},
  doi = {10.1007/s00521-023-09322-1},
  url = {https://doi.org/10.1007/s00521-023-09322-1},
  abstract = {Advances in dialogue systems have recently been made in various fields as an easy to use and inexpensive option to support or replace workers. However, developing dialogue systems that produce satisfactory responses to user queries on par with human workers still presents significant challenges. The primary purpose of this review is to analyse prominent studies on dialogue systems in the literature. Comparison frameworks were developed to perform an in-depth analysis in terms of approaches, data sets and evaluation metrics. Unlike previous reviews, we thoroughly examined how reinforcement learning is applied to dialogue systems. We also analysed studies attempting to interleave the two main types of dialogue systems (i.e. open-domain dialogue and task-oriented dialogue). We present some open-source platforms for developing dialogue systems. Finally, we identified research gaps and discussed potential research directions.},
}

@article{Liu2025_02,
  title = {Toward the unification of generative and discriminative visual foundation model: a survey},
  author = {Xu Liu and Tong Zhou and Chong Wang and Yuping Wang and Yuanxin Wang and Qinjingwen Cao and Weizhi Du and Yonghuan Yang and Junjun He and Yu Qiao and Yiqing Shen},
  year = {2025},
  journal = {The Visual Computer},
  volume = {41},
  pages = {3371-3412},
  doi = {10.1007/s00371-024-03608-8},
  url = {https://doi.org/10.1007/s00371-024-03608-8},
  abstract = {The advent of foundation models, which are pre-trained on vast datasets, has ushered in a new era of computer vision, characterized by their robustness and remarkable zero-shot generalization capabilities. Mirroring the transformative impact of foundation models like large language models in natural language processing, visual foundation models (VFMs) have become a catalyst for groundbreaking developments in computer vision. This review paper delineates the pivotal trajectories of VFMs, emphasizing their scalability and proficiency in generative tasks such as text-to-image synthesis, as well as their adeptness in discriminative tasks including image segmentation. While generative and discriminative models have historically charted distinct paths, we undertake a comprehensive examination of the recent strides made by VFMs in both domains, elucidating their origins, seminal breakthroughs, and pivotal methodologies. Additionally, we collate and discuss the extensive resources that facilitate the development of VFMs and address the challenges that pave the way for future research endeavors. A crucial direction for forthcoming innovation is the amalgamation of generative and discriminative paradigms. The nascent application of generative models within discriminative contexts signifies the early stages of this confluence. This survey aspires to be a contemporary compendium for scholars and practitioners alike, charting the course of VFMs and illuminating their multifaceted landscape.},
}

@article{Tan2025,
  title = {Data visualization in healthcare and medicine: a survey},
  author = {Xunan Tan and Xiang Suo and Wenjun Li and Lei Bi and Fangshu Yao},
  year = {2025},
  journal = {The Visual Computer},
  volume = {41},
  pages = {3037-3058},
  doi = {10.1007/s00371-024-03586-x},
  url = {https://doi.org/10.1007/s00371-024-03586-x},
  abstract = {Visualization analysis is crucial in healthcare as it provides insights into complex data and aids healthcare professionals in efficiency. Information visualization leverages algorithms to reduce the complexity of high-dimensional heterogeneous data, thereby enhancing healthcare professionals’ understanding of the hidden associations among data structures. In the field of healthcare visualization, efforts have been made to refine and enhance the utility of data through diverse algorithms and visualization techniques. This review aims to summarize the existing research in this domain and identify future research directions. We searched Web of Science, Google Scholar and IEEE Xplore databases, and ultimately, 76 articles were included in our analysis. We collected and synthesized the research findings from these articles, with a focus on visualization, artificial intelligence and supporting tasks in healthcare. Our study revealed that researchers from diverse fields have employed a wide range of visualization techniques to visualize various types of data. We summarized these visualization methods and proposed recommendations for future research. We anticipate that our findings will promote further development and application of visualization techniques in healthcare.},
}

@article{Fink2025,
  title = {Retrieval-augmented generation improves precision and trust of a GPT-4 model for emergency radiology diagnosis and classification: a proof-of-concept study},
  author = {Anna Fink and Johanna Nattenmüller and Stephan Rau and Alexander Rau and Hien Tran and Fabian Bamberg and Marco Reisert and Elmar Kotter and Thierno Diallo and Maximilian F Russe},
  year = {2025},
  journal = {European Radiology},
  doi = {10.1007/s00330-025-11445-z},
  url = {https://doi.org/10.1007/s00330-025-11445-z},
  abstract = {This study evaluated the effect of enhancing a GPT-4 model with retrieval-augmented generation on its ability to diagnose and classify traumatic injuries based on radiology reports.},
}

@article{Rashid2025,
  title = {The paradigm of digital health: AI applications and transformative trends},
  author = {Zubia Rashid and Hania Ahmed and Neha Nadeem and Syeda Bushra Zafar and Muhammad Zubair Yousaf},
  year = {2025},
  journal = {Neural Computing and Applications},
  doi = {10.1007/s00521-025-11081-0},
  url = {https://doi.org/10.1007/s00521-025-11081-0},
  abstract = {Artificial intelligence is transforming healthcare by enhancing diagnostic accuracy with AI-driven imaging and personalized treatment plans. This review focuses on various applications of AI across diverse healthcare domains, from diagnostics and imaging to surgical and procedural care, patient monitoring, pharmaceuticals, rehabilitation, healthcare operations, and public health, examining factors influencing its adoption like technological readiness and regulatory support. Real-time intraoperative guidance, drug discovery, integration of telehealth, and the creation of personalized care plans are some examples where AI-driven improvements can significantly influence practice and will shape the future of traditional healthcare delivery. These drives with machine learning and deep learning AI technologies have resulted in genomics, vaccine development, and public health surveillance innovations. Based on an extensive literature review of articles from Scopus, PubMed, and Google Scholar, AI is perceived to have a major impact on reshaping the face of healthcare delivery. There has been tremendous progress by AI in early detection of diseases, sophisticated robotic surgeries, and personalized medicine, but its successful integration comes with significant challenges, ensuring data security, safeguarding against security breaches, and algorithmic bias and transparency. Achieving the full potential of AI in health requires the joint effort of technology developers, healthcare professionals, and policymakers to challenge and overcome the challenges with safe, effective, and ethical implementation of AI. With investment in AI research, development, and training, healthcare systems can now create that power to benefit patients, increase efficiency, and spur innovation.},
}

@article{Gheewala2025,
  title = {In-depth survey: deep learning in recommender systems—exploring prediction and ranking models, datasets, feature analysis, and emerging trends},
  author = {Shivangi Gheewala and Shuxiang Xu and Soonja Yeom},
  year = {2025},
  journal = {Neural Computing and Applications},
  doi = {10.1007/s00521-024-10866-z},
  url = {https://doi.org/10.1007/s00521-024-10866-z},
  abstract = {Due to the exponential growth of online information, users are often welcomed with a huge range of products and services along with descriptions, reviews, and comments. Although this information available to users is valuable, at the same time, massive data sources confuse them to retrieve desired content, which is known as information overload. Recommender systems are examined as effective tools that play a vital role in filtering information and ultimately addressing the information overload problem. Various online platforms use recommendation systems to provide users with more relevant and personalized content. With the remarkable success of deep learning in the field of artificial intelligence, it procures much attention in the recommendation research area in recent years. The exiting literature on recommender system research commonly distinguishes between two main directions: rating prediction and top-N ranking. In this survey paper, we examine deep learning methodologies in the context of both rating prediction and top-N ranking recommendation approaches. Additionally, we investigate pre- and post-modeling critiques of recommender systems and provide insights into exiting benchmark datasets, feature learning analysis, and evaluation measuring techniques. In the end, we highlight the new generation recommender system trend with respective future research directions.},
}

@article{Khan2024,
  title = {Exploring contactless techniques in multimodal emotion recognition: insights into diverse applications, challenges, solutions, and prospects},
  author = {Umair Ali Khan and Qianru Xu and Yang Liu and Altti Lagstedt and Ari Alamäki and Janne Kauttonen},
  year = {2024},
  journal = {Multimedia Systems},
  volume = {30},
  pages = {115},
  doi = {10.1007/s00530-024-01302-2},
  url = {https://doi.org/10.1007/s00530-024-01302-2},
  abstract = {In recent years, emotion recognition has received significant attention, presenting a plethora of opportunities for application in diverse fields such as human–computer interaction, psychology, and neuroscience, to name a few. Although unimodal emotion recognition methods offer certain benefits, they have limited ability to encompass the full spectrum of human emotional expression. In contrast, Multimodal Emotion Recognition (MER) delivers a more holistic and detailed insight into an individual's emotional state. However, existing multimodal data collection approaches utilizing contact-based devices hinder the effective deployment of this technology. We address this issue by examining the potential of contactless data collection techniques for MER. In our tertiary review study, we highlight the unaddressed gaps in the existing body of literature on MER. Through our rigorous analysis of MER studies, we identify the modalities, specific cues, open datasets with contactless cues, and unique modality combinations. This further leads us to the formulation of a comparative schema for mapping the MER requirements of a given scenario to a specific modality combination. Subsequently, we discuss the implementation of Contactless Multimodal Emotion Recognition (CMER) systems in diverse use cases with the help of the comparative schema which serves as an evaluation blueprint. Furthermore, this paper also explores ethical and privacy considerations concerning the employment of contactless MER and proposes the key principles for addressing ethical and privacy concerns. The paper further investigates the current challenges and future prospects in the field, offering recommendations for future research and development in CMER. Our study serves as a resource for researchers and practitioners in the field of emotion recognition, as well as those intrigued by the broader outcomes of this rapidly progressing technology.},
}

@article{Franciscatto2024,
  title = {Situational Data Integration in Question Answering systems: a survey over two decades},
  author = {Maria Helena Franciscatto and Luis Carlos Erpen de Bona and Celio Trois and Marcos Didonet Del FabroFabro and João Carlos Damasceno Lima},
  year = {2024},
  journal = {Knowledge and Information Systems},
  volume = {66},
  pages = {5875-5918},
  doi = {10.1007/s10115-024-02136-0},
  url = {https://doi.org/10.1007/s10115-024-02136-0},
  abstract = {Question Answering (QA) systems provide accurate answers to questions; however, they lack the ability to consolidate data from multiple sources, making it difficult to manage complex questions that could be answered with additional data retrieved and integrated on the fly. This integration is inherent to Situational Data Integration (SDI) approaches that deal with dynamic requirements of ad hoc queries that neither traditional database management systems, nor search engines are effective in providing an answer. Thus, if QA systems include SDI characteristics, they could be able to return validated and immediate information for supporting users decisions. For this reason, we surveyed QA-based systems, assessing their capabilities to support SDI features, i.e., Ad hoc Data Retrieval, Data Management, and Timely Decision Support. We also identified patterns concerning these features in the surveyed studies, highlighting them in a timeline that shows the SDI evolution in the QA domain. To the best of your knowledge, this study is precursor in the joint analysis of SDI and QA, showing a combination that can favor the way systems support users. Our analyses show that most of SDI features are rarely addressed in QA systems, and based on that, we discuss directions for further research.},
}

@article{Maisto2025,
  title = {Domain embeddings for generating complex descriptions of concepts in Italian language},
  author = {Alessandro Maisto},
  year = {2025},
  journal = {Cognitive Processing},
  volume = {26},
  pages = {91-120},
  doi = {10.1007/s10339-024-01234-9},
  url = {https://doi.org/10.1007/s10339-024-01234-9},
  abstract = {In this work, we propose a Distributional Semantic resource enriched with linguistic and lexical information extracted from electronic dictionaries. This resource is designed to bridge the gap between the continuous semantic values represented by distributional vectors and the discrete descriptions provided by general semantics theory. Recently, many researchers have focused on the connection between embeddings and a comprehensive theory of semantics and meaning. This often involves translating the representation of word meanings in Distributional Models into a set of discrete, manually constructed properties, such as semantic primitives or features, using neural decoding techniques. Our approach introduces an alternative strategy based on linguistic data. We have developed a collection of domain-specific co-occurrence matrices derived from two sources: a list of Italian nouns classified into four semantic traits and 20 concrete noun sub-categories and Italian verbs classified by their semantic classes. In these matrices, the co-occurrence values for each word are calculated exclusively with a defined set of words relevant to a particular lexical domain. The resource includes 21 domain-specific matrices, one comprehensive matrix, and a Graphical User Interface. Our model facilitates the generation of reasoned semantic descriptions of concepts by selecting matrices directly associated with concrete conceptual knowledge, such as a matrix based on location nouns and the concept of animal habitats. We assessed the utility of the resource through two experiments, achieving promising outcomes in both the automatic classification of animal nouns and the extraction of animal features.},
}

@article{Clunie2025,
  title = {Summary of the National Cancer Institute 2023 Virtual Workshop on Medical Image De-identification—Part 1: Report of the MIDI Task Group - Best Practices and Recommendations, Tools for Conventional Approaches to De-identification, International Approaches to De-identification, and Industry Panel on Image De-identification},
  author = {David Clunie and Fred Prior and Michael Rutherford and Stephen Moore and William Parker and Haridimos Kondylakis and Christian Ludwigs and Juergen Klenk and Bob Lou and Lawrence (Tony) O’Sullivan and Dan Marcus and Jiri Dobes and Abraham Gutman and Keyvan Farahani},
  year = {2025},
  journal = {Journal of Imaging Informatics in Medicine},
  volume = {38},
  pages = {1-15},
  doi = {10.1007/s10278-024-01182-y},
  url = {https://doi.org/10.1007/s10278-024-01182-y},
  abstract = {De-identification of medical images intended for research is a core requirement for data-sharing initiatives, particularly as the demand for data for artificial intelligence (AI) applications grows. The Center for Biomedical Informatics and Information Technology (CBIIT) of the US National Cancer Institute (NCI) convened a virtual workshop with the intent of summarizing the state of the art in de-identification technology and processes and exploring interesting aspects of the subject. This paper summarizes the highlights of the first day of the workshop, the recordings, and presentations of which are publicly available for review. The topics covered included the report of the Medical Image De-Identification Initiative (MIDI) Task Group on best practices and recommendations, tools for conventional approaches to de-identification, international approaches to de-identification, and an industry panel.},
}

@article{Congès2024,
  title = {R-IO SUITE: integration of LLM-based AI into a knowledge management and model-driven based platform dedicated to crisis management},
  author = {Aurélie Congès and Audrey Fertier and Nicolas Salatgé and Sébastien Rebière and Frederick Benaben},
  year = {2024},
  journal = {Software and Systems Modeling},
  doi = {10.1007/s10270-024-01237-2},
  url = {https://doi.org/10.1007/s10270-024-01237-2},
  abstract = {This article presents how the R-IO SUITE software platform, a decision support system for crisis management entirely based on model-driven engineering principles, considerably benefits from large language model (LLM)-based artificial intelligence (AI). The different components of the R-IO SUITE platform are used to climb the four abstraction layers: data, information, decision and action through interpretation (from data to information), exploitation (from information to decision) and implementation (from decision to action). These transitions between layers are supported by a knowledge base embedding knowledge instances structured according to a crisis management metamodel. From a functional perspective, this platform is fully operational, however, to be able to cover any type of crisis situation, the knowledge base should be enriched, first, from a “resource perspective” (to embed the various available means to deal with any faced situation), and second, from an “issue perspective” (to understand all risks and damage that can appear on a crisis situation). It is not reasonable to consider creating and maintaining such an exhaustive knowledge base. However, the connection of the R-IO SUITE platform with LLM software such as ChatGPT© makes it possible, by generating appropriate prompts, to update on-the-fly the knowledge base according to the faced context. This article shows how the LLM AI can provide complementary knowledge to formally fulfil the knowledge base to make it relevant to the faced crisis situation. This article presents the R-IO SUITE as a LLM-empowered model-driven platform to become an extended crisis management supporting system.},
}

@article{Bolaños2024,
  title = {Artificial intelligence for literature reviews: opportunities and challenges},
  author = {Francisco Bolaños and Angelo Salatino and Francesco Osborne and Enrico Motta},
  year = {2024},
  journal = {Artificial Intelligence Review},
  volume = {57},
  pages = {259},
  doi = {10.1007/s10462-024-10902-3},
  url = {https://doi.org/10.1007/s10462-024-10902-3},
  abstract = {This paper presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates prior research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions for future research. We highlight three primary research challenges: integrating advanced AI solutions, such as large language models and knowledge graphs, improving usability, and developing a standardised evaluation framework. We also propose best practices to ensure more robust evaluations in terms of performance, usability, and transparency. Overall, this review offers a detailed overview of AI-enhanced SLR tools for researchers and practitioners, providing a foundation for the development of next-generation AI solutions in this field.},
}

@article{Schmitt2024,
  title = {Digital deception: generative artificial intelligence in social engineering and phishing},
  author = {Marc Schmitt and Ivan Flechais},
  year = {2024},
  journal = {Artificial Intelligence Review},
  volume = {57},
  pages = {324},
  doi = {10.1007/s10462-024-10973-2},
  url = {https://doi.org/10.1007/s10462-024-10973-2},
  abstract = {The advancement of Artificial Intelligence (AI) and Machine Learning (ML) has profound implications for both the utility and security of our digital interactions. This paper investigates the transformative role of Generative AI in Social Engineering (SE) attacks. We conduct a systematic review of social engineering and AI capabilities and use a theory of social engineering to identify three pillars where Generative AI amplifies the impact of SE attacks: Realistic Content Creation, Advanced Targeting and Personalization, and Automated Attack Infrastructure. We integrate these elements into a conceptual model designed to investigate the complex nature of AI-driven SE attacks—the Generative AI Social Engineering Framework. We further explore human implications and potential countermeasures to mitigate these risks. Our study aims to foster a deeper understanding of the risks, human implications, and countermeasures associated with this emerging paradigm, thereby contributing to a more secure and trustworthy human-computer interaction.},
}

@article{Mateos2024,
  title = {A systematic literature review of recent advances on context-aware recommender systems},
  author = {Pablo Mateos and Alejandro Bellogín},
  year = {2024},
  journal = {Artificial Intelligence Review},
  volume = {58},
  pages = {20},
  doi = {10.1007/s10462-024-10939-4},
  url = {https://doi.org/10.1007/s10462-024-10939-4},
  abstract = {Recommender systems are software mechanisms whose usage is to offer suggestions for different types of entities like products, services, or contacts that could be useful or interesting for a specific user. Other ways have been explored in the field to enhance the power of these systems by integrating the context as an additional attribute. This inclusion tries to extract the user preferences more accurately taking into account multiple components such as temporal, spatial, or social ones. Notwithstanding the magnitude of context-awareness in this area, the research community is in agreement with the lack of framework for context information and how to integrate it into recommender systems. Under this premise, this paper focuses on a comprehensive systematic literature review of the state-of-the-art recommendation techniques and their characteristics to benefit from contextual information. The following survey presents the following contributions as outcomes of our study: (i) determine a framework where multiple aspects are taken into account to have a clear definition of context representation, (ii) the techniques used to incorporate context, and (iii) the evaluation of these methods in terms of reproducibility and effectiveness. Our review also covers some crucial topics about context integration, classification of the contexts, application domains, and evaluation of the used datasets, metrics, and code implementations, where we observed clear shiftings in algorithmic and evaluation trends towards Neural Network approaches and ranking metrics, respectively. Just as importantly, future research opportunities and directions are exposed as final closure, standing out the exploitation of various data sources and the scalability and customization of existing solutions.},
}

@article{LIU2025,
  title = {Demystifying the black box: AI-enhanced logistic regression for lead scoring},
  author = {Bingran LIU},
  year = {2025},
  journal = {Applied Intelligence},
  volume = {55},
  pages = {574},
  doi = {10.1007/s10489-025-06430-4},
  url = {https://doi.org/10.1007/s10489-025-06430-4},
  abstract = {To mitigate interpretability challenges in business decision-making due to the black-box nature of generative Artificial Intelligence(AI), and to address high information processing costs and inconsistent feature collection standards, a novel marketing lead evaluation framework integrating large language models (LLMs) and classical machine learning algorithms was developed. The framework encompasses three modules: (1) a multi-agent question-answering system leveraging Retrieval-Augmented Generation(RAG) and LLMs; (2) a feature extraction and memory module for precise natural language and public data processing; and (3) a logistic regression (LR) model, trained on 540,000 automotive lead records, with associated calculation logic for decision support. Results indicate that the multi-agent system accurately identifies intentions and routes modules, the feature extraction module reduces manual follow-up costs, and the LR-guided LLM output enhances interpretability. These findings highlight the framework’s potential for auditing abnormal events and advancing marketing intelligence and business systematization.},
}

@article{Ahmed2025,
  title = {Unveiling the frontiers of deep learning: Innovations shaping diverse domains},
  author = {Shams Forruque Ahmed and Md. Sakib Bin Alam and Maliha Kabir and Shaila Afrin and Sabiha Jannat Rafa and Aanushka Mehjabin and Amir H Gandomi},
  year = {2025},
  journal = {Applied Intelligence},
  volume = {55},
  pages = {573},
  doi = {10.1007/s10489-025-06259-x},
  url = {https://doi.org/10.1007/s10489-025-06259-x},
  abstract = {Deep learning (DL) allows computer models to learn, visualize, optimize, refine, and predict data. To understand its present state, examining the most recent advancements and applications of deep learning across various domains is essential. However, prior reviews focused on DL applications in only one or two domains. The current review thoroughly investigates the use of DL in four different broad fields due to the plenty of relevant research literature in these domains. This wide range of coverage provides a comprehensive and interconnected understanding of DL’s influence and opportunities, which is lacking in other reviews. The study also discusses DL frameworks and addresses the benefits and challenges of utilizing DL in each field, which is only occasionally available in other reviews. DL frameworks like TensorFlow and PyTorch make it easy to develop innovative DL applications across diverse domains by providing model development and deployment platforms. This helps bridge theoretical progress and practical implementation. Deep learning solves complex problems and advances technology in many fields, demonstrating its revolutionary potential and adaptability. CNN-LSTM models with attention mechanisms can forecast traffic with 99% accuracy. Fungal-diseased mango leaves can be classified with 97.13% accuracy by the multi-layer CNN model. However, deep learning requires rigorous data collection to analyze and process large amounts of data because it is independent of training data. Thus, large-scale medical, research, healthcare, and environmental data compilation are challenging, reducing deep learning effectiveness. Future research should address data volume, privacy, domain complexity, and data quality issues in DL datasets.},
}

@article{Rodgers2024,
  title = {Solving business problems: the business-driven data-supported process},
  author = {Mark Rodgers and Sayan Mukherjee and Benjamin Melamed and Alok Baveja and Ajai Kapoor},
  year = {2024},
  journal = {Annals of Operations Research},
  volume = {332},
  pages = {705-741},
  doi = {10.1007/s10479-023-05770-z},
  url = {https://doi.org/10.1007/s10479-023-05770-z},
  abstract = {Businesses nowadays often feel impelled to rush and implement Big Data and Artificial Intelligence initiatives in their organizations without clarity on their business problems, nor the appropriate methodologies for extracting actionable insights from the data. In contrast, this paper presents a process that starts with an articulated business problem instead of a “data rush”. The presented process, dubbed the Business-Driven Data-Supported (BDDS) process, is rigorously anchored in concepts from Theory of Constraints and Information Theory. BDDS guides businesses in solving their problems by stating observed performance gaps, uncovering their underlying root cause, formulating the “right question”, utilizing only the “right data”, and choosing the “right methodology” to extract the “right information” from the data, leading to the “right actionable insights.” Also provided is a prescriptive framework, dubbed the Data-to-Information-Extraction-Methodology (DIEM), for selecting the “right methodology”, based on the understanding level of relevant system dependencies and the availability of relevant data. The BDDS process is illustrated by an example from the healthcare industry, and the efficacy and applicability of the DIEM framework are supported by eleven case studies from a broad range of industries.},
}

@article{Poth2024_01,
  title = {Technology adoption performance evaluation applied to testing industrial REST APIs},
  author = {Alexander Poth and Olsi Rrjolli and Andrea Arcuri},
  year = {2024},
  journal = {Automated Software Engineering},
  volume = {32},
  pages = {5},
  doi = {10.1007/s10515-024-00477-2},
  url = {https://doi.org/10.1007/s10515-024-00477-2},
  abstract = {Testing is an important task within software development. To write test cases and integrate them into an automated test suite requires a significant amount of work. Given a set of requirements and specifications of a software, testing is needed to verify its correctness. When done manually, it is an expensive and error prone task. To facilitate such work, automated test-case generation via tools could be useful. Test-case generation can be facilitated by deterministic algorithm-driven approaches or non-deterministic approaches such as with AI (e.g., evolutionary and LLM). The different approaches come with their strengths and weaknesses, which must be considered when integrating these approaches into a product test procedure in industry. Several novel testing techniques and tools have been developed in academia and industry, but how effective they are and how to integrate them in real-world large industrial scenarios is still unclear. In this paper, a systematic approach is presented to evaluate test-case generation methodologies and integrate them into a scalable enterprise setup. The specific context is black-box testing of REST APIs, based on their OpenAPI schemas. The aim is to facilitate IT product development and service delivery. The proposed Technology Adoption Performance Evaluation (TAPE) approach is evaluated by a case study within the Group IT of Volkswagen AG. We evaluated existing tools such as OpenAPI Generator, EvoMaster and StarCoder which are built on different technologies. Our results show that these tools are of benefit for test engineers to facilitate test-case specification and design within the Group IT of Volkswagen AG.},
}

@article{Côté2024_01,
  title = {Data cleaning and machine learning: a systematic literature review},
  author = {Pierre-Olivier Côté and Amin Nikanjam and Nafisa Ahmed and Dmytro Humeniuk and Foutse Khomh},
  year = {2024},
  journal = {Automated Software Engineering},
  volume = {31},
  pages = {54},
  doi = {10.1007/s10515-024-00453-w},
  url = {https://doi.org/10.1007/s10515-024-00453-w},
  abstract = {Machine Learning (ML) is integrated into a growing number of systems for various applications. Because the performance of an ML model is highly dependent on the quality of the data it has been trained on, there is a growing interest in approaches to detect and repair data errors (i.e., data cleaning). Researchers are also exploring how ML can be used for data cleaning; hence creating a dual relationship between ML and data cleaning. To the best of our knowledge, there is no study that comprehensively reviews this relationship. This paper’s objectives are twofold. First, it aims to summarize the latest approaches for data cleaning for ML and ML for data cleaning. Second, it provides future work recommendations. We conduct a systematic literature review of the papers published between 2016 and 2022 inclusively. We identify different types of data cleaning activities with and for ML: feature cleaning, label cleaning, entity matching, outlier detection, imputation, and holistic data cleaning. We summarize the content of 101 papers covering various data cleaning activities and provide 24 future work recommendations. Our review highlights many promising data cleaning techniques that can be further extended. We believe that our review of the literature will help the community develop better approaches to clean data.},
}

@article{Ragazzi2024,
  title = {LAWSUIT: a LArge expert-Written SUmmarization dataset of ITalian constitutional court verdicts},
  author = {Luca Ragazzi and Gianluca Moro and Stefano Guidi and Giacomo Frisoni},
  year = {2024},
  journal = {Artificial Intelligence and Law},
  doi = {10.1007/s10506-024-09414-w},
  url = {https://doi.org/10.1007/s10506-024-09414-w},
  abstract = {Large-scale public datasets are vital for driving the progress of abstractive summarization, especially in law, where documents have highly specialized jargon. However, the available resources are English-centered, limiting research advancements in other languages. This paper introduces LAWSUIT, a collection of 14K Italian legal verdicts with expert-authored abstractive maxims drawn from the Constitutional Court of the Italian Republic. LAWSUIT presents an arduous task with lengthy source texts and evenly distributed salient content. We offer extensive experiments with sequence-to-sequence and segmentation-based approaches, revealing that the latter achieve better results in full and few-shot settings. We openly release LAWSUIT to foster the development and automation of real-world legal applications.},
}

@article{Zhang2025_02,
  title = {Unpacking perceived risks and AI trust influences pre-service teachers’ AI acceptance: A structural equation modeling-based multi-group analysis},
  author = {Chengming Zhang and Min Hu and Weidong Wu and Farrukh Kamran and Xining Wang},
  year = {2025},
  journal = {Education and Information Technologies},
  volume = {30},
  pages = {2645-2672},
  doi = {10.1007/s10639-024-12905-7},
  url = {https://doi.org/10.1007/s10639-024-12905-7},
  abstract = {Artificial intelligence (AI) integration in education has grown significantly recently. However, the potential risks of AI have led to educators being wary of implementing AI systems. To discover whether AI systems can be effective in the classroom in the future, it is critical to understand how risk factors (e.g., perceived safety risks, perceived privacy risks, and urban/rural differences) affect pre-service teachers’ AI acceptance. Therefore, the study aimed to (1) explore the influence of perceived risks and AI trust on pre-service teachers’ intentions to use AI-based educational applications, and (2) investigate possible variations in potential determinants of their intentions to use AI based on urban–rural differences. In this study, data from 483 pre-service teachers in China (262 from rural areas) were obtained by survey and analyzed using confirmatory factor analysis (CFA) and structural equation modeling-based multi-group analysis. The study’s findings demonstrated that while AI trust influenced pre-service teachers’ AI acceptance, the effect was less pronounced than perceived ease of use and perceived usefulness. Most notably, findings showed that perceived privacy and safety risks negatively influence AI trust among pre-service teachers from rural areas, which was a trend not observed in pre-service teachers from urban areas. As a result, to integrate AI-based applications into educational settings, pre-service teachers believed that the AI system must be functionally robust, user-friendly, and transparent. In addition, urban–rural differences considerably affect pre-service teachers’ AI acceptance. This study provides further relevant recommendations for educators and policymakers.},
}

@article{Li2025_04,
  title = {CodeDoctor: multi-category code review comment generation},
  author = {Yingling Li and Yuhan Wu and Zi’ao Wang and Lei Huang and Junjie Wang and Jianping Li and Minying Huang},
  year = {2025},
  journal = {Automated Software Engineering},
  volume = {32},
  pages = {25},
  doi = {10.1007/s10515-025-00491-y},
  url = {https://doi.org/10.1007/s10515-025-00491-y},
  abstract = {Code review is an effective software quality assurance activity. However, this process is labor-intensive and time-consuming, requiring reviewers to carefully review under various categories (e.g., function, refactoring, documentation, etc) to generate review comments. Several approaches have been proposed for automatic review comment generation, although they can generate review comments, they hardly cover all manual review comments. Because most of these approaches simply utilize the information of submitted code and review comments, not fully modeling the features of code review (i.e., ignoring review category, the association of issue snippets and review comments). In this paper, we propose CodeDoctor, an automatic review comment generator with data augmentation and category-aware encoder-decoder to generate multi-category review comments. It consists of three main phases: (1) Data augmentation phase, which classifies review comments and builds review exemplars (i.e., the pairs of issue snippet and its comment) to augment review data by using a large language model (LLM) with prompt engineering and feedback loops; (2) Encoder phase, which encodes the inputs (i.e., review category, diff code and review exemplar) into semantic and token representations; (3) Decoder phase, which designs a category-focused decoder to capture the most relevant information of given category for multi-category review comment generation. Evaluations with five commonly-used and state-of-the-art baselines on two datasets show that CodeDoctor outperforms all baselines, with 1770% higher average BLEU-4, 111% higher average ROUGE-L and 49% higher average F1 than the best baseline. Furthermore, a human evaluation also confirms the significant potential of applying CodeDoctor in practical usage. Our approach can relieve the burden of reviewers by automatically generating multi-category review comments, and helps developers better detect code issues as early as possible, thereby facilitating software development.},
}

@article{Ait2025,
  title = {On the suitability of hugging face hub for empirical studies},
  author = {Adem Ait and Javier Luis Cánovas Izquierdo and Jordi Cabot},
  year = {2025},
  journal = {Empirical Software Engineering},
  volume = {30},
  pages = {57},
  doi = {10.1007/s10664-024-10608-8},
  url = {https://doi.org/10.1007/s10664-024-10608-8},
  abstract = {Empirical studies in software engineering mainly rely on the data available on code-hosting platforms, being GitHub the most representative. Nevertheless, in the last years, the emergence of Machine Learning (ML) has led to the development of platforms specifically designed for hosting ML-based projects, with Hugging Face Hub (HFH) as the most popular one. So far, there have been no studies evaluating the potential of HFH for such studies.},
}

@article{Hu2025_02,
  title = {Factors influencing Chinese pre-service teachers’ adoption of generative AI in teaching: an empirical study based on UTAUT2 and PLS-SEM},
  author = {Linlin Hu and Hao Wang and Yunfei Xin},
  year = {2025},
  journal = {Education and Information Technologies},
  doi = {10.1007/s10639-025-13353-7},
  url = {https://doi.org/10.1007/s10639-025-13353-7},
  abstract = {Although Generative Artificial Intelligence (GAI) has demonstrated significant potential in education, there is a lack of research on pre-service teachers’ behavioral intentions toward GAI. This study is based on the UTAUT2 model and, for the first time, introduces perceived risk as a key variable to systematically investigate the factors influencing Chinese pre-service teachers’ behavioral intentions and future usage of GAI in teaching. The innovation of this study lies in its analysis of the unique needs of pre-service teachers across disciplines, which reveals critical factors in technology acceptance. These findings offer new theoretical insights and practical guidance for the effective application of GAI in K-12 education. The study involved 563 pre-service teachers from three renowned teacher training universities in China. Data were collected using an online questionnaire platform and analysed using Partial Least Squares Structural Equation Modelling (PLS-SEM). The results indicated that effort expectancy, social influence, hedonic motivation, and habit significantly enhanced pre-service teachers’ behavioural intention to use GAI in future teaching. At the same time, perceived risk had a negative effect. In addition, performance expectaancy, facilitating conditions, and price value did not significantly influence behavioural intentions, but behavioural intentions and facilitating conditions had significant predictive effects on future use. Based on the findings, this study provides practical recommendations for optimising the user experience of GAI tools, enhancing technical support, and reducing perceived risk.},
}

@article{Sánchez-Vaquerizo2024,
  title = {Urban Digital Twins and metaverses towards city multiplicities: uniting or dividing urban experiences?},
  author = {Javier Argota Sánchez-Vaquerizo},
  year = {2024},
  journal = {Ethics and Information Technology},
  volume = {27},
  pages = {4},
  doi = {10.1007/s10676-024-09812-3},
  url = {https://doi.org/10.1007/s10676-024-09812-3},
  abstract = {Urban Digital Twins (UDTs) have become the new buzzword for researchers, planners, policymakers, and industry experts when it comes to designing, planning, and managing sustainable and efficient cities. It encapsulates the last iteration of the technocratic and ultra-efficient, post-modernist vision of smart cities. However, while more applications branded as UDTs appear around the world, its conceptualization remains ambiguous. Beyond being technically prescriptive about what UDTs are, this article focuses on their aspects of interaction and operationalization in connection to people in cities, and how enhanced by metaverse ideas they can deepen societal divides by offering divergent urban experiences based on different stakeholder preferences. Therefore, firstly this article repositions the term UDTs by comparing existing concrete and located applications that have a focus on interaction and participation, including some that may be closer to the concept of UDT than is commonly assumed. Based on the components found separately in the different studied cases, it is possible to hypothesize about possible future, more advanced realizations of UDTs. This enables us to contrast their positive and negative societal impacts. While the development of new immersive interactive digital worlds can improve planning using collective knowledge for more inclusive and diverse cities, they pose significant risks not only the common ones regarding privacy, transparency, or fairness, but also social fragmentation based on urban digital multiplicities. The potential benefits and challenges of integrating this multiplicity of UDTs into participatory urban governance emphasize the need for human-centric approaches to promote socio-technical frameworks able to mitigate risks as social division.},
}

@article{Asatryan2025,
  title = {Evidence-based policy or beauty contest? An LLM-based meta-analysis of EU cohesion policy evaluations},
  author = {Zareh Asatryan and Carlo Birkholz and Friedrich Heinemann},
  year = {2025},
  journal = {International Tax and Public Finance},
  volume = {32},
  pages = {625-655},
  doi = {10.1007/s10797-024-09875-4},
  url = {https://doi.org/10.1007/s10797-024-09875-4},
  abstract = {Independent and high-quality evaluations of government policies are an important input for designing evidence-based policy. Institutional frictions and lack of incentives to write such evaluations, on the other hand, carry the risk of turning the system into a costly beauty contest. We study one of the most advanced markets of policy evaluations in the world, the evaluations of EU Cohesion Policy interventions by the EU Member States. We use a large language model to quantify the findings of about 2,300 evaluations, and complement this data with our own survey of the evaluation authors. We show that the findings of evaluations are inconsistent with those of the academic literature on the output impacts of Cohesion Policy. Using further variation across Member States, our analysis suggests that the market of evaluations is rather oligopolistic within Member States, that it is very fragmented across the EU, and that there is often a strong involvement of managing authorities in the work of formally independent evaluators. These factors contribute to making the findings of the evaluations overly optimistic (beautiful) risking their relevance for (evidence-based) policy. We conclude by discussing reform options to make the evaluations of EU Cohesion Policy more unbiased and effective.},
}

@article{Li2024_03,
  title = {Making It Possible for the Auditing of AI: A Systematic Review of AI Audits and AI Auditability},
  author = {Yueqi Li and Sanjay Goel},
  year = {2024},
  journal = {Information Systems Frontiers},
  doi = {10.1007/s10796-024-10508-8},
  url = {https://doi.org/10.1007/s10796-024-10508-8},
  abstract = {Artificial intelligence (AI) technologies have become the key driver of innovation in society. However, numerous vulnerabilities of AI systems can lead to negative consequences for society, such as biases encoded in the training data and algorithms and lack of transparency. This calls for AI systems to be audited to ensure that the impact on society is understood and mitigated. To enable AI audits, auditability measures need to be implemented. This study provides a systematic review of academic work and regulatory work on AI audits and AI auditability. Results reveal the current understanding of the AI audit scope, audit challenges, and auditability measures. We identify and categorize AI auditability measures for each audit area and specific process to be audited and the party responsible for each process to be audited. Our findings will guide existing efforts to make AI systems auditable across the lifecycle of AI systems.},
}

@article{Matteis2024,
  title = {Managerial implications of blockchains in the public sector: elements for the development of a conceptual framework for innovation},
  author = {Fabio De Matteis and Mario Angelelli and Fabrizio Striani and Angelo Corallo},
  year = {2024},
  journal = {Journal of Management and Governance},
  doi = {10.1007/s10997-024-09724-w},
  url = {https://doi.org/10.1007/s10997-024-09724-w},
  abstract = {This paper aims to provide the basis for a conceptual framework on the linkage that integrates the application areas of blockchains in the public sector and their impact dimensions. Applying the literature review methodology, 33 selected articles were explored in-depth to identify their thematic key points, which led to their attribution to two categories consistent with the RQs of this work: application areas and impact dimensions of the blockchain technology in the public sector organizations. The results enable the identification of a cross-functionality of the impact dimensions (trust, governance, and sustainability) that cut across the main application areas (public health, transactions and fiscal processes, and policy-making), from which derives the basis for a conceptual framework. The conceptual framework links the application areas and impacts of blockchain in the public sector and leads to some concluding remarks on managerial/policy implications and future research opportunities.},
}

@article{Paoli2025,
  title = {Reflections on inductive thematic saturation as a potential metric for measuring the validity of an inductive thematic analysis with LLMs},
  author = {Stefano De Paoli and Walter S Mathis},
  year = {2025},
  journal = {Quality & Quantity},
  volume = {59},
  pages = {683-709},
  doi = {10.1007/s11135-024-01950-6},
  url = {https://doi.org/10.1007/s11135-024-01950-6},
  abstract = {This paper presents a set of reflections on saturation and the use of Large Language Models (LLMs) for performing Thematic Analysis (TA). The paper suggests that initial thematic saturation (ITS) could be used as a metric to assess part of the transactional validity of TA with LLM, focusing on the initial coding. The paper presents the initial coding of two datasets of different sizes, and it reflects on how the LLM reaches some form of analytical saturation during the coding. The procedure proposed in this work leads to the creation of two codebooks, one comprising the total cumulative initial codes and the other the total unique codes. The paper proposes a metric to synthetically measure ITS using a simple mathematical calculation employing the ratio between slopes of the unique and total codes. The paper contributes to the initial body of work exploring how to perform qualitative analysis with LLMs.},
}

@article{Trinidad2025,
  title = {From Accountability to Algorithms: Interorganizational Learning and the Transformation of Quantification in Education},
  author = {Jose Eos Trinidad},
  year = {2025},
  journal = {Qualitative Sociology},
  doi = {10.1007/s11133-024-09590-w},
  url = {https://doi.org/10.1007/s11133-024-09590-w},
  abstract = {While studies often explore the intended and unintended consequences of technologies, few have theorized how and why they change. One crucial transformation in quantitative technologies is the shift from evaluative accountability to predictive algorithms, such as in schools that use dropout prediction systems. Using the case of ninth-grade early warning indicators, I argue that the transformation of quantification resulted from interorganizational learning, or the acquisition of new knowledge through the interaction of different organizations. In particular, I show how technology changes gradually from organization-level evaluation to individual-based prediction to systems-focused improvement. Pivotal to such changes were new forms of knowledge that emerged (1) as “instructing” organizations directed changes and “receiving” organizations resisted them; (2) as organizations in various fields reciprocally collaborated; and (3) as similar organizations practiced networked learning. Although studies have traditionally highlighted the “discipline” of technologies, I illustrate the power of organizational agents to resist, adapt, and change them—with implications for the study of quantification, work, institutional change, and education.},
}

@article{Patil2024,
  title = {Role of artificial intelligence in cancer detection using protein p53: A Review},
  author = {Manisha R Patil and Anand Bihari},
  year = {2024},
  journal = {Molecular Biology Reports},
  volume = {52},
  pages = {46},
  doi = {10.1007/s11033-024-10051-4},
  url = {https://doi.org/10.1007/s11033-024-10051-4},
  abstract = {Normal cell development and prevention of tumor formation rely on the tumor-suppressor protein p53. This crucial protein is produced from the Tp53 gene, which encodes the p53 protein. The p53 protein plays a vital role in regulating cell growth, DNA repair, and apoptosis (programmed cell death), thereby maintaining the integrity of the genome and preventing the formation of tumors. Since p53 was discovered 43 years ago, many researchers have clarified its functions in the development of tumors. With the support of the protein p53 and targeted artificial intelligence modeling, it will be possible to detect cancer and tumor activity at an early stage. This will open up new research opportunities. In this review article, a comprehensive analysis was conducted on different machine learning techniques utilized in conjunction with the protein p53 to predict and speculate cancer. The study examined the types of data incorporated and evaluated the performance of these techniques. The aim was to provide a thorough understanding of the effectiveness of machine learning in predicting and speculating cancer using the protein p53.},
}

@article{Billingsley2025,
  title = {The Practical Epistemologies of Design and Artificial Intelligence},
  author = {William Billingsley},
  year = {2025},
  journal = {Science & Education},
  volume = {34},
  pages = {807-824},
  doi = {10.1007/s11191-024-00517-z},
  url = {https://doi.org/10.1007/s11191-024-00517-z},
  abstract = {This article explores the epistemological trade-offs that practical and technology design fields make by exploring past philosophical discussions of design, practitioner research, and pragmatism. It argues that as technologists apply Artificial Intelligence (AI) and machine learning (ML) to more domains, the technology brings this same set of epistemological trade-offs with it. The basis of the technology becomes the basis of what it finds. There are correlations between questions that designers face in sampling and gathering data that is rich with context, and those that large-scale machine learning faces in how it approaches the rich context and subjectivity within its training data. AI, however, processes enormous amounts of data and produces models that can be explored. This makes its form of pragmatic inquiry that is amenable to optimisation. Finally, the paper explores implications for education that stem from how we apply AI to pedagogy and explanation, suggesting that the availability of AI-generated explanations and materials may also push pedagogy in directions of pragmatism: the evidence that explanations are effective may precede explorations of why they should be.},
}

@article{Pham2023,
  title = {Credit risk assessment and executives’ legal expertise},
  author = {Mia Hang Pham and Yulia Merkoulova and Chris Veld},
  year = {2023},
  journal = {Review of Accounting Studies},
  volume = {28},
  pages = {2361-2400},
  doi = {10.1007/s11142-022-09699-9},
  url = {https://doi.org/10.1007/s11142-022-09699-9},
  abstract = {We study whether firms that are led by chief executive officers (CEOs) with law degrees (lawyer CEOs) have different credit ratings and costs of debt from other firms. Our sample consists of Standard & Poor’s 1500 firms from 1992 to 2020, 9.2% of which have lawyer CEOs. We find that these firms have better credit ratings, compared to other firms. On average, their cost of debt is 10% lower than that of firms led by CEOs without legal backgrounds. Our results are robust to different specifications, sampling methods, and controls, such as firm and CEO characteristics. We identify two ways that CEO expertise translates into higher credit ratings: lawyer CEOs are associated with a lower future volatility of stock returns and a reduction in information risk. The decreased business risk and better financial reporting are associated with 5% lower auditing fees for firms with lawyer CEOs.},
}

@article{Deja2025,
  title = {The causal effect of the global crisis on open science research impact: a bibliometric causal analysis},
  author = {Marek Deja},
  year = {2025},
  journal = {Scientometrics},
  volume = {130},
  pages = {1303-1325},
  doi = {10.1007/s11192-025-05241-1},
  url = {https://doi.org/10.1007/s11192-025-05241-1},
  abstract = {This research aims to assess the causal effects of the global crisis, namely the COVID-19 pandemic, on the dissemination of research in open science by examining changes in research impact using field-normalized measures. This is done by conceptualizing the pandemic-related changes as a coercive force that accelerated the adoption of open science practices, thus improving their recognition in different scientific disciplines. Using a Causal Autoregressive Integrated Moving Average (C-ARIMA) model, the study contrasts counterfactual scenarios to evaluate the potential impact of research in the absence of the pandemic against what was empirically observed. The pandemic considerably increased the FWCI of open science research, reaching a peak in February 2022. The overall effect suggests continued recognition of open science after the pandemic. The average monthly increase in FWCI was 0.335 above the expected growth, with a relative effect ranging from 15 to 23%. The pandemic hastened the influence of open science, with fields like social science, computer science, medicine, and engineering experiencing significant unexpected changes, while business management and accounting, or arts and humanities do not noted a significant effect. Pandemic restrictions and an increased need for transparency were likely major catalysts to improve the visibility and impact of open science in academia.},
}

@article{Prodanovic2024,
  title = {Urban nature-based solutions planning for biodiversity outcomes: human, ecological, and artificial intelligence perspectives},
  author = {Veljko Prodanovic and Peter M Bach and Milan Stojkovic},
  year = {2024},
  journal = {Urban Ecosystems},
  volume = {27},
  pages = {1795-1806},
  doi = {10.1007/s11252-024-01558-6},
  url = {https://doi.org/10.1007/s11252-024-01558-6},
  abstract = {Nature-based solutions (NBS) harness ecosystem services for urban enhancement, promoting biodiversity, habitat creation, and water management while improving human well-being. However, decision-making often favours specific NBS designs, leading to uneven benefits distribution. Whereas human-centric NBS design relies on convenience, financial sustainability, historical aspects, and amenity increase through NBS technical solutions, flora- and fauna-centric (or eco-centric) design targets spatial connectedness of blue-green spaces, increase in species richness, and habitat within urban centres. Both approaches can shape the urban biodiversity landscape, yet; they often clash around planning priorities. Recent advances in AI offer potential for AI-centric urban planning, though its role remains unclear. This study examines the interplay between biodiversity and NBS planning across human-, eco-, and AI-centric domains, aiming for balanced urban outcomes. We blended narrative, integrative, and systematic literature review and propose future steps for more balanced NBS development. The findings of this work suggest that AI presents an opportunity for a more balanced NBS design through its applications in climate change prediction, water management, and project visualisation. Incorporating AI into urban planning tools can expedite modelling process, improve stakeholder communication, and enhance project outcomes visualisation. By integrating human, eco, and AI-centric approaches, urban planners can foster resilience and sustainability in NBS implementation, ensuring equitable distribution of benefits across urban landscapes.},
}

@article{Pandey2025_01,
  title = {SDC-estimator: an effectual software defect count estimation technique for the upcoming version of software project},
  author = {Sushant Kumar Pandey and Anil Kumar Tripathi},
  year = {2025},
  journal = {Innovations in Systems and Software Engineering},
  doi = {10.1007/s11334-025-00601-9},
  url = {https://doi.org/10.1007/s11334-025-00601-9},
  abstract = {Estimating the number of defects in a software project before the testing phase can reduce the enormous amount of development cost and help in allocating testing resources optimally and efficiently. However, estimating the number of defects in each module of a successive version of the software system will be more efficient in minimizing the development cost. The article proposed a method for estimating the number of defects in the successive version of a software system. The Cross Version Defect Count Estimation (CVDCE) technique estimates the number of defects in the next version of a software system. We proposed a new CVDCE model named Software Defect Count Estimator (SDC-Estimator). We utilized Long and Short Term Memory along with Attention layer architecture in our proposed model. We used seven software projects and their existing versions from the PROMISE repository. First, we have created a meta-content dataset from the different versions of the same software and used it for the training of the proposed model. We compared the performance of the SDC-Estimator with fourteen baseline and three state-of-the-art deep learning methods. We found, that in six out of seven and four out of seven projects, MAE is lowermost, and accuracy is higher respectively, whereas, in two out of seven software, MSE is minimum. We also found the mean of performance measures produced by the proposed model across all seven projects is most optimal compared to baseline methods. The proposed model tackles class imbalance and overfitting problems using random oversampling and dropout regularization, respectively, to produce fair and unbiased results. We observed the improvement of the proposed model on benchmark techniques by inspecting 20% of lines of code. In most situations, the improvements are significant, and it has a large effect size across all seven projects.},
}

@article{Carrera-Rivera2024,
  title = {AdaptUI: A Framework for the development of Adaptive User Interfaces in Smart Product-Service Systems},
  author = {Angela Carrera-Rivera and Felix Larrinaga and Ganix Lasa and Giovanna Martinez-Arellano and Gorka Unamuno},
  year = {2024},
  journal = {User Modeling and User-Adapted Interaction},
  volume = {34},
  pages = {1929-1980},
  doi = {10.1007/s11257-024-09414-0},
  url = {https://doi.org/10.1007/s11257-024-09414-0},
  abstract = {Smart Product–Service Systems (S-PSS) represent an innovative business model that integrates intelligent products with advanced digital capabilities and corresponding e-services. The user experience (UX) within an S-PSS is heavily influenced by the customization of services and customer empowerment. However, conventional UX analysis primarily focuses on the design stage and may not adequately respond to the evolving user needs during the usage stage and how to exploit the data surrounding the use of S-PSS. To overcome these limitations, this article introduces a practical framework for developing Adaptive User Interfaces within S-PSS. This framework integrates ontologies and Context-aware recommendation systems, with user interactions serving as the primary data source, facilitating the development of adaptive user interfaces. One of the main contributions of this work lies on the integration of various components to achieve the creation of Adaptive User Interfaces for digital services. A case study of a smart device app is presented, to demonstrate the practical implementation of the framework, with a hands-on development approach, considering technological aspects and utilizing appropriate tools. The results of the evaluation of the recommendation engine show that using a context-aware approach improves the precision of recommendations. Furthermore, pragmatic aspects of UX, such as usefulness and system efficiency, are evaluated with participants with an overall positive impact on the use of the smart device.},
}

@article{Orwat2024,
  title = {Normative Challenges of Risk Regulation of Artificial Intelligence},
  author = {Carsten Orwat and Jascha Bareis and Anja Folberth and Jutta Jahnel and Christian Wadephul},
  year = {2024},
  journal = {NanoEthics},
  volume = {18},
  pages = {11},
  doi = {10.1007/s11569-024-00454-9},
  url = {https://doi.org/10.1007/s11569-024-00454-9},
  abstract = {Approaches aimed at regulating artificial intelligence (AI) include a particular form of risk regulation, i.e. a risk-based approach. The most prominent example is the European Union’s Artificial Intelligence Act (AI Act). This article addresses the challenges for adequate risk regulation that arise primarily from the specific type of risks involved, i.e. risks to the protection of fundamental rights and fundamental societal values. This is mainly due to the normative ambiguity of such rights and societal values when attempts are made to select, interpret, specify or operationalise them for the purposes of risk assessments and risk mitigation. This is exemplified by (1) human dignity, (2) informational self-determination, data protection and privacy, (3) anti-discrimination, fairness and justice, and (4) the common good. Normative ambiguities require normative choices, which are assigned to different actors under the regime of the AI Act. Particularly critical normative choices include selecting normative concepts by which to operationalise and specify risks, aggregating and quantifying risks (including the use of metrics), balancing value conflicts, setting levels of acceptable risks, and standardisation. To ensure that these normative choices do not lack democratic legitimacy and to avoid legal uncertainty, further political processes and scientific debates are suggested.},
}

@article{Allman2024,
  title = {Trends and Topics in Educational Technology, 2024 Edition},
  author = {Bohdana Allman and Royce Kimmons and Wei Wang and Hanhui Bao and Joshua M Rosenberg and Matthew J Koehler},
  year = {2024},
  journal = {TechTrends},
  volume = {68},
  pages = {402-410},
  doi = {10.1007/s11528-024-00950-5},
  url = {https://doi.org/10.1007/s11528-024-00950-5},
}

@article{Nguyen2024,
  title = {Privacy-preserving explainable AI: a survey},
  author = {Thanh Tam Nguyen and Thanh Trung Huynh and Zhao Ren and Thanh Toan Nguyen and Phi Le Nguyen and Hongzhi Yin and Quoc Viet Hung Nguyen},
  year = {2024},
  journal = {Science China Information Sciences},
  volume = {68},
  pages = {111101},
  doi = {10.1007/s11432-024-4123-4},
  url = {https://doi.org/10.1007/s11432-024-4123-4},
  abstract = {As the adoption of explainable AI (XAI) continues to expand, the urgency to address its privacy implications intensifies. Despite a growing corpus of research in AI privacy and explainability, there is little attention on privacy-preserving model explanations. This article presents the first thorough survey about privacy attacks on model explanations and their countermeasures. Our contribution to this field comprises a thorough analysis of research papers with a connected taxonomy that facilitates the categorization of privacy attacks and countermeasures based on the targeted explanations. This work also includes an initial investigation into the causes of privacy leaks. Finally, we discuss unresolved issues and prospective research directions uncovered in our analysis. This survey aims to be a valuable resource for the research community and offers clear insights for those new to this domain. To support ongoing research, we have established an online resource repository, which will be continuously updated with new and relevant findings.},
}

@article{Inam2024,
  title = {Health Data Sciences and Cardiovascular Disease in Africa: Needs and the Way Forward},
  author = {Maha Inam and Sana Sheikh and Adeel Khoja and Amina Abubakar and Reena Shah and Zainab Samad and Anthony Ngugi and Farhana Alarakhiya and Akbar Waljee and Salim S Virani},
  year = {2024},
  journal = {Current Atherosclerosis Reports},
  volume = {26},
  pages = {659-671},
  doi = {10.1007/s11883-024-01235-1},
  url = {https://doi.org/10.1007/s11883-024-01235-1},
  abstract = {The rising burden of cardiovascular disease (CVD) in Africa is of great concern. Health data sciences is a rapidly developing field which has the potential to improve health outcomes, especially in low-middle income countries with burdened healthcare systems. We aim to explore the current CVD landscape in Africa, highlighting the importance of health data sciences in the region and identifying potential opportunities for application and growth by leveraging health data sciences to improve CVD outcomes.},
}

@article{Wang2024_05,
  title = {ROS package search for robot software development: a knowledge graph-based approach},
  author = {Shuo Wang and Xinjun Mao and Shuo Yang and Menghan Wu and Zhang Zhang},
  year = {2024},
  journal = {Frontiers of Computer Science},
  volume = {19},
  pages = {196320},
  doi = {10.1007/s11704-024-3660-9},
  url = {https://doi.org/10.1007/s11704-024-3660-9},
  abstract = {In recent years, ROS (Robot Operating System) packages have become increasingly popular as a type of software artifact that can be effectively reused in robotic software development. Indeed, finding suitable ROS packages that closely match the software’s functional requirements from the vast number of available packages is a nontrivial task using current search methods. The traditional search methods for ROS packages often involve inputting keywords related to robotic tasks into general-purpose search engines (e.g., Google) or code hosting platforms (e.g., GitHub) to obtain approximate results of all potentially suitable ROS packages. However, the accuracy of these search methods remains relatively low because the task-related keywords may not precisely match the functionalities offered by the ROS packages. To improve the search accuracy of ROS packages, this paper presents a novel semantic-based search approach that relies on the semantic-level ROS Package Knowledge Graph (RPKG) to automatically retrieve the most suitable ROS packages. Firstly, to construct the RPKG, we employ multi-dimensional feature extraction techniques to extract semantic concepts, including code file name, category, hardware device, characteristics, and function, from the dataset of ROS package text descriptions. The semantic features extracted from this process result in a substantial number of entities (32,294) and relationships (54,698). Subsequently, we create a robot domain-specific small corpus and further fine-tune a pre-trained language model, BERT-ROS, to generate embeddings that effectively represent the semantics of the extracted features. These embeddings play a crucial role in facilitating semantic-level understanding and comparisons during the ROS package search process within the RPKG. Secondly, we introduce a novel semantic matching-based search algorithm that incorporates the weighted similarities of multiple features from user search queries, which searches out more accurate ROS packages than the traditional keyword search method. To validate the enhanced accuracy of ROS package searching, we conduct comparative case studies between our semantic-based search approach and four baseline search approaches: ROS Index, GitHub, Google, and ChatGPT. The experiment results demonstrate that our approach achieves higher accuracy in terms of ROS package searching, outperforming the other approaches by at least 21% from 5 levels, including top1, top5, top10, top15, and top20.},
}

@article{Restrepo2024,
  title = {Spaces of mathematical chemistry},
  author = {Guillermo Restrepo},
  year = {2024},
  journal = {Theory in Biosciences},
  volume = {143},
  pages = {237-251},
  doi = {10.1007/s12064-024-00425-4},
  url = {https://doi.org/10.1007/s12064-024-00425-4},
  abstract = {In an effort to expand the domain of mathematical chemistry and inspire research beyond the realms of graph theory and quantum chemistry, we explore five mathematical chemistry spaces and their interconnectedness. These spaces comprise the chemical space, which encompasses substances and reactions; the space of reaction conditions, spanning the physical and chemical aspects involved in chemical reactions; the space of reaction grammars, which encapsulates the rules for creating and breaking chemical bonds; the space of substance properties, covering all documented measurements regarding substances; and the space of substance representations, composed of the various ontologies for characterising substances.},
}

@article{Oulefki2025,
  title = {Digital twins and AI transforming healthcare systems through innovation and data-driven decision making},
  author = {Adel Oulefki and Abbes Amira and Sebti Foufou},
  year = {2025},
  journal = {Health and Technology},
  doi = {10.1007/s12553-025-00947-x},
  url = {https://doi.org/10.1007/s12553-025-00947-x},
  abstract = {The advent of Digital Twins DTs in healthcare signifies a paradigm shift toward precision medicine, driven by the escalating demand for bespoke healthcare solutions. DTs, serving as virtual counterparts of tangible entities, have carved a niche in healthcare, offering a platform to simulate and scrutinize individual health dynamics. This paper aims to explore how Artificial Intelligence AI-enabled DTs can enhance healthcare outcomes by providing a AI and data-driven approach to patient management.},
}

@article{Fassnacht2024,
  title = {Data sharing practices: The interplay of data, organizational structures, and network dynamics},
  author = {Marcel Fassnacht and Jannis Leimstoll and Carina Benz and Daniel Heinz and Gerhard Satzger},
  year = {2024},
  journal = {Electronic Markets},
  volume = {34},
  pages = {47},
  doi = {10.1007/s12525-024-00732-0},
  url = {https://doi.org/10.1007/s12525-024-00732-0},
  abstract = {With the proliferation of data and advanced analytics, organizations are increasingly recognizing the potential value of sharing data across organizational boundaries. However, there is a lack of empirical evidence and systematic frameworks to guide the design of effective data sharing practices. Realizing the full potential of data sharing requires the effective design and implementation of data sharing practices by considering the interplay of data, organizational structures, and network dynamics. This study presents an empirically and theoretically grounded taxonomy of data sharing practices drawing on existing literature and real-world data sharing cases. The subsequent cluster analysis identifies four generic archetypes of data sharing practices, differing in their primary orientation toward compliance, efficiency, revenue, or society. From a theoretical perspective, our work conceptualizes data sharing practices as a foundation for a more systematic and detailed exploration in future research. At the practitioner level, we enable organizations to strategically develop and scale data sharing practices to effectively leverage data as a strategic asset.},
}

@article{Yuan2024_01,
  title = {A review of current research on occupant-centric control for improving comfort and energy efficiency},
  author = {Yue Yuan and Chengcheng Song and Liying Gao and Kejun Zeng and Yixing Chen},
  year = {2024},
  journal = {Building Simulation},
  volume = {17},
  pages = {1675-1692},
  doi = {10.1007/s12273-024-1170-1},
  url = {https://doi.org/10.1007/s12273-024-1170-1},
  abstract = {Occupant-centric control (OCC) is intelligent control of building systems based on the real comfort needs of occupants. This paper provides a comprehensive review of how real-world data on energy-related occupant behavior (OB) can be integrated and applied in OCC systems. The aim is to accurately portray the real occupant needs and improve energy efficiency without sacrificing occupant comfort. This paper first introduces two types of OB: detailed occupancy states and energy-interaction behaviors, including methods to monitor, establish, and predict these OB. Then, OCC is divided into real-time control and model-based predictive control, and each of these four scenarios is discussed. It extensively reviews OCC methods for different equipment in four cases, covering control strategies, control scales, comfort enhancement scenarios, and energy-saving potential for each category. It is summarized that despite extensive research on OB, there are still significant challenges in integrating this research into OCC. A major issue is the lack of a bridge connecting monitoring acquired information and controls. In addition, the article reviews the current state of OCC platform development. The future direction should be combined with advanced Internet of Things (IoT) technologies, WiFi, and other communication technologies to obtain information about people’s behavior and real needs in order to create truly energy efficient and comfortable smart environments. The article also discusses how enhancing the real-time feedback capability of the OCC system can help improve the overall control system capability and the importance of testing through experimentation.},
}

@article{Zhou2024,
  title = {Recommendations for Inactive Users: a Cross Domain Approach with Graph Neural Networks},
  author = {Jun Zhou and Ziqi Liu and Meijuan Tan and Xiangyu Meng and Xiaocheng Cheng and Jianping Wei and Zhiqiang Zhang and Fengyuan Yu and Chaochao Chen and Jianwei Yin},
  year = {2024},
  journal = {International Journal of Machine Learning and Cybernetics},
  doi = {10.1007/s13042-024-02423-w},
  url = {https://doi.org/10.1007/s13042-024-02423-w},
  abstract = {Understanding inactive users and satisfying inactive users via recommendation are the keys to user growth and engagement for many Internet companies. However, learning inactive users’ representations and their preferences is still challenging because the features available are missing and the positive responses or labels are insufficient. In this paper, we propose a cross domain learning approach to exclusively recommend customized items to inactive users by leveraging the knowledge of active users. Particularly, based on the observations that users’ browsing behaviors, i.e. page browsing in an app, are correlated with their social networks’ browsing behaviors, we represent users, no matter active or inactive users, by their friends’ browsing behaviors based on a graph neural network (GNN) layer atop of a heterogeneous graph defined on social networks (user-user friendships) and browsing behaviors (user-page clicks). We jointly optimize the learning tasks of active users in source domain and inactive users in target domain based on the domain invariant features extracted from the embedding of our GNN layer, where the domain invariant features that are learned to benefit both tasks on active/inactive users, and are indiscriminate with respect to the shift between the domains. We describe our cross domain graph neural networks (CD-GNN) together with our learning and serving system, and show that our approach significantly improves the click-through rate of inactive users in real-world environment at Alipay by 28.55%. We also conduct extensive experiments to show that our approach can well capture the preference of inactive users, and significantly outperforms state-of-the-art approaches in terms of RMSE by 5% especially when we decrease the available features and samples of inactive users in target domain using public data.},
}

@article{Pfeiffer2024,
  title = {New Laws and Regulation},
  author = {Jella Pfeiffer and Jens F Lachenmaier and Oliver Hinz and Wil van der Aalst},
  year = {2024},
  journal = {Business & Information Systems Engineering},
  volume = {66},
  pages = {653-666},
  doi = {10.1007/s12599-024-00902-6},
  url = {https://doi.org/10.1007/s12599-024-00902-6},
}

@article{Blohm2024,
  title = {Data products, data mesh, and data fabric},
  author = {Ivo Blohm and Felix Wortmann and Christine Legner and Felix Köbler},
  year = {2024},
  journal = {Business & Information Systems Engineering},
  volume = {66},
  pages = {643-652},
  doi = {10.1007/s12599-024-00876-5},
  url = {https://doi.org/10.1007/s12599-024-00876-5},
}

@article{Sai2025,
  title = {Unleashing the Power of Generative AI in Agriculture 4.0 for Smart and Sustainable Farming},
  author = {Siva Sai and Sanjeev Kumar and Aanchal Gaur and Shivam Goyal and Vinay Chamola and Amir Hussain},
  year = {2025},
  journal = {Cognitive Computation},
  volume = {17},
  pages = {63},
  doi = {10.1007/s12559-025-10420-6},
  url = {https://doi.org/10.1007/s12559-025-10420-6},
  abstract = {Generative artificial intelligence (GAI) represents a pioneering class of artificial intelligence systems renowned for producing diverse media, such as text and images. Agriculture 4.0 (AG-4.0) is a concept that integrates advanced technologies such as the Internet of Things (IoT), data analytics, artificial intelligence, and precision agriculture into the agricultural sector. The integration of GAI and AG-4.0 can generate new and valuable agricultural insights and solutions through pattern recognition and data analysis. This integration enhances farming practices by generating predictive models, simulating optimal growth conditions, diagnosing plant diseases, and optimizing genetic traits. In spite of the tremendous scope of GAI in agriculture, there has been no detailed study concerning the applications and scope of GAI in AG-4.0. Addressing this research gap, we explore various applications, real-world products, and limitations of GAI in agriculture. We explore how GAI models such as ChatGPT and Dall-E can be personalized advisors for farmers, help increase awareness about farmer relief programs, design farm layouts, and many other such applications. Additionally, we cover four real-world GAI products deployed to assist farmers. Since GAI is a growing technology, it poses challenges such as scarcity of data, data privacy, and interpretability. We elaborately discuss these limitations and suggest multiple directions for future research in GAI for agriculture.},
}

@article{Amigud2024_01,
  title = {The Age of the Intelligent Machine: Singularity, Efficiency, and Existential Peril},
  author = {Alexander Amigud},
  year = {2024},
  journal = {Philosophy & Technology},
  volume = {37},
  pages = {49},
  doi = {10.1007/s13347-024-00740-0},
  url = {https://doi.org/10.1007/s13347-024-00740-0},
  abstract = {Machine learning, and more broadly artificial intelligence (AI), is a fascinating technology and can be considered as the closest approximation to the Cartesian “thinking thing” that humans have ever created. Just as the industrial revolution required a new ethos, the age of intelligent machines will create its own, challenging the established moral, economic, and political presuppositions. This paper discusses the relationship between AI and society; it presents several thought experiments to explore the complexity of the relationship and highlights the insufficiency of the current normative paradigm in addressing technological expansion. I argue that many of the externalities, such as security risks, loss of privacy, and economic instability  will result from trying to fit the emerging technologies into the existing frame of efficiency and utility, by redefining the notions of human value, identity, autonomy, purity, and truth, among others. The age of the intelligent machine is elevating alienation to new levels, treating the individual as mere patterns in data—its primary commodity. I further argue that while the possibility of unintended consequences, due to the potential misuse of AI is ever present, the intelligent machine per se is unlikely to engage in a zero-sum game for power on its own initiative. I question whether singularity is at all attainable and argue that technology will forever remain a proxy for human interests. I conclude by posing questions for charting the path forward. Through this analysis, I aim to provide a more nuanced understanding of the complex relationship between the AI and humans.},
}

@article{Freiman2025,
  title = {‘Opacity’ and ‘Trust’: From Concepts and Measurements to Public Policy},
  author = {Ori Freiman and John McAndrews and Jordan Mansell and Clifton van der Linden},
  year = {2025},
  journal = {Philosophy & Technology},
  volume = {38},
  pages = {29},
  doi = {10.1007/s13347-025-00862-z},
  url = {https://doi.org/10.1007/s13347-025-00862-z},
  abstract = {This paper provides four insights relating to policy-making, focusing on the complex relationship between the abstract concept of trust—with its numerous empirical expressions—and the concept of opacity in AI technologies. First, we set the ground by discussing the nature of trust as it evolves from interpersonal to technological realms (§ 2), examining the plethora of measurement methods and objects that reflect the concept’s rich diversity. We then investigate the concept of opacity in AI systems (§ 3), challenging the conventional wisdom that less opacity invariably leads to greater trust, and illustrate this relationship’s nuanced and context-dependent reality. The conclusion (§ 4) bridges these theoretical discussions with pragmatic policy-making considerations, crystallizing abstract ideas into actionable directives. We present four key insights for policy-makers: First, policies must mediate between the abstract concept of trust and the concrete outcomes of AI interactions. Second, they should mirror societal values. Third, policy-making must be adaptive, evolving alongside technological advancements to effectively address both technical and policy issues relating to trust and opacity. Lastly, we anticipate that academic discourse will inform future policies, suggesting the creation of a standardized trust index for AI systems, transparency guidelines based on opacity levels, and explainability protocols for high-stakes applications. These insights aim to ensure that AI-based technologies are governed in a manner that upholds societal values and fosters the common good.},
}

@article{García2024,
  title = {Datafeudalism: The Domination of Modern Societies by Big Tech Companies},
  author = {Carlos Saura García},
  year = {2024},
  journal = {Philosophy & Technology},
  volume = {37},
  pages = {90},
  doi = {10.1007/s13347-024-00777-1},
  url = {https://doi.org/10.1007/s13347-024-00777-1},
  abstract = {This article critically examines the domination exerted by big digital companies on the current social, economic, and political context of modern societies, with a particular focus on the implications for the proper functioning of democracy. The objective of this article is to introduce and develop the concept of datafeudalism, expose its emergence for the proper functioning of modern societies and democracy, and to propose courses of action to reverse this situation. To achieve this purpose, firstly, the evolution from surveillance capitalism to datafeudalism will be discussed. Secondly, the structures and operating logic of data feudalism will be analyzed. Thirdly, the harmful impacts of datafeudalism on the proper functioning of the democratic systems of the European Union will be examined. Finally, an attempt will be made to outline courses of action that will make it possible to reverse the situation of economic, social and political tyranny exercised by big digital companies through datafeudalism.},
}

@article{Singh2025,
  title = {Banking transformation in PSU banks through the adoption of Metaverse: Indian context},
  author = {Anamica Singh and Anchal Luthra and Seema Garg and Namrata Pancholi and Vinita Sharma},
  year = {2025},
  journal = {International Journal of System Assurance Engineering and Management},
  doi = {10.1007/s13198-024-02611-5},
  url = {https://doi.org/10.1007/s13198-024-02611-5},
  abstract = {The banking sector is one of the most important parts of an economy; banking systems promote and adopt new technologies to provide customers with better services and enhance their effectiveness and efficiency. The success of these technological implementations depends on customers who are fully motivated to adopt them. In the Indian context, the adoption rate of new technology in banking is very low. Thus, this study examines the factors influencing Indian bank customers’ behavioral intention and adoption of Metaverse. Hence, the purpose of the paper is to understand how adopting the Metaverse will help create customer technology trust that further develops engagement among customers in the banking industry in India. The paper has considered both qualitative and quantitative methods. Firstly, the paper focuses on interviews with bankers and customers to understand their perception of the Metaverse. Secondly, data has been collected with the help of a questionnaire from customers who often use digital platforms for their financial transactions. Structural Equation Modelling (SEM) was used to analyze the data. The results demonstrate that customer value (β = 0.45, p < 0.001), enterprise value (β = 0.30, p < 0.025), and service value (β = 0.25, p < 0.012) all significantly enhance technology trust in the Metaverse. Additionally, technology trust positively influences customer engagement (β = 0.20, p < 0.005) and customer satisfaction (β = 0.28, p < 0.001), emphasizing its pivotal role in the customer experience. The finding of this paper would help banking organizations understand how the Metaverse can reduce the risk to customers and help banks prove their trust and engagement.},
}

@article{Tzoulia2025,
  title = {Harmonized Standards in the Public Domain? Better Not …},
  author = {Eleni Tzoulia},
  year = {2025},
  journal = {IIC - International Review of Intellectual Property and Competition Law},
  doi = {10.1007/s40319-025-01571-y},
  url = {https://doi.org/10.1007/s40319-025-01571-y},
  abstract = {This study critically examines the CJEU’s judgment in the case Public.Resource.Org, Inc., Right to Know CLG v European Commission on public access to harmonized standards. The study argues that these standards, as private documents held by the European Commission, are in principle subject to IP rights and should not be made unconditionally accessible for product safety or other sustainability-related purposes. The Court’s decision in this case reflects an overly permissive interpretation of the “overriding interest” concept under Art. 4(2) in fine of Regulation 1049/2001, deviating from established case law and raising competitive concerns. Determining whether an interest justifies the disclosure of an IP-protected EU institution document requires case-by-case assessments through a three-step test. This interest must be “public”, concrete, and “pressing”, without negating the underlying IP rights, while also adhering to the principle of proportionality.},
}

@article{Napolitano2024,
  title = {Potential of Artificial Intelligence to Accelerate Drug Development for Rare Diseases},
  author = {Giulio Napolitano and Canan Has and Anne Schwerk and Jui-Hung Yuan and Carsten Ullrich},
  year = {2024},
  journal = {Pharmaceutical Medicine},
  volume = {38},
  pages = {79-86},
  doi = {10.1007/s40290-023-00504-9},
  url = {https://doi.org/10.1007/s40290-023-00504-9},
  abstract = {The growth in breadth and depth of artificial intelligence (AI) applications has been fast, running hand in hand with the increasing amount of digital data available. Here, we comment on the application of AI in the field of drug development, with a strong focus on the specific achievements and challenges posed by rare diseases. Data paucity and high costs make drug development for rare diseases especially hard. AI can enable otherwise inaccessible approaches based on the large-scale integration of heterogeneous datasets and knowledge bases, guided by expert biological understanding. Obstacles still exist for the routine use of AI in the usually conservative pharmaceutical domain, which can easily become disillusioned. It is crucial to acknowledge that AI is a powerful, supportive tool that can assist but not replace human expertise in the various phases and aspects of drug discovery and development.},
}

@article{Wu2025_02,
  title = {Leveraging FDA Labeling Documents and Large Language Model to Enhance Annotation, Profiling, and Classification of Drug Adverse Events with AskFDALabel},
  author = {Leihong Wu and Hong Fang and Yanyan Qu and Joshua Xu and Weida Tong},
  year = {2025},
  journal = {Drug Safety},
  doi = {10.1007/s40264-025-01520-1},
  url = {https://doi.org/10.1007/s40264-025-01520-1},
  abstract = {Drug adverse events (AEs) represent a significant public health concern. US Food and Drug Administration (FDA) drug labeling documents are an essential resource for studying drug safety such as assessing a drug’s likelihood to cause certain organ toxicities; however, the manual extraction of AEs is labor-intensive, requires specialized expertise, and is challenging to maintain, due to frequent updates of the labeling documents.},
}

@article{Shaikh2024,
  title = {The role of large language models in agriculture: harvesting the future with LLM intelligence},
  author = {Tawseef Ayoub Shaikh and Tabasum Rasool and K Veningston and Syed Mufassir Yaseen},
  year = {2024},
  journal = {Progress in Artificial Intelligence},
  doi = {10.1007/s13748-024-00359-4},
  url = {https://doi.org/10.1007/s13748-024-00359-4},
  abstract = {Significant accomplishments in many agricultural applications during the past decade attest to the fast progress and use of deep learning and machine learning methods in agricultural systems. However, these conventional models have a few drawbacks: They are not generalizable since they are trained on large, costly labeled datasets, require expert expertise to create and maintain, and are often built for specific applications. Significant accomplishments in language, vision, and decision-making tasks across several domains have been shown recently by massive pre-trained models, also known as large models (LMs). Recent years have seen large language models (LLMs) demonstrate remarkable competence in a variety of fields, including natural language processing (NLP), by encompassing different advancements in terms of architecture, training methods, context duration, fine-tuning, multi-modality, datasets, efficiency, benchmarking, and many other. The massive amounts of data used to train these models span many domains and modalities. After training, they can handle a wide range of tasks with less tweaking and less task-specific labeled data. Despite its effectiveness and promising future, agricultural artificial intelligence (AAI) has received less attention than other applications of LLMs. To better understand the problem area and open up new research pathways in this sector, this work aims to examine the possibilities of LLMs in smart agriculture by offering conceptual tools and a technical base. Herein, we delve into the potential applications of large models in agriculture, primarily categorizing them into four categories: Agricultural applications of large language models (LLMs), large vision models (LVMs) for precise agricultural applications, multimodal large language models (MLLMs) and model assessment, and intelligent and precise agriculture using reinforcement learning large models (RLLMs). Further, we review some of the most prominent LLMs, including three famous LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions, and limitations. Next, we evaluate famous LLM evaluation metrics and look at datasets for training, fine-tuning, and evaluation. Finally, we focus our discussion on issues and possible future research directions of LLMs in the agricultural sector. This review article aims to provide academics and practitioners with a panoramic perspective of the field and a quick reference to help them draw out relevant ideas from the extensive summaries of prior publications to broaden their LLM research.},
}

@article{Cossatin2025_01,
  title = {Tell me more: integrating LLMs in a cultural heritage website for advanced information exploration support},
  author = {Angelo Geninatti Cossatin and Noemi Mauro and Fabio Ferrero and Liliana Ardissono},
  year = {2025},
  journal = {Information Technology & Tourism},
  doi = {10.1007/s40558-025-00312-8},
  url = {https://doi.org/10.1007/s40558-025-00312-8},
  abstract = {Cultural Heritage websites’ capability to satisfy diverse information needs is limited by their high-quality but constrained knowledge bases. Thus, we investigate their extension with external large language models (LLMs), enriching the provision of cultural content by leveraging LLMs’ continuous collection and integration of information from heterogeneous data sources. This extension raises important challenges in synchronizing the LLM’s behavior with the user’s browsing activity on the website to offer a unified interaction environment. To address these challenges, we propose a loosely coupled integration model that provides users with curated content and an assisted question-answering function to answer information needs that the system’s knowledge base fails to cover. Our model is agnostic to the LLM and synchronizes its behavior with the user’s browsing activity through implicit prompt engineering. We tested a baseline website without LLM integration, one with free-text interaction with the LLM, and another that combines free-text interaction with the suggestion of context-dependent questions. In a user study involving 44 participants, we found that the LLM-powered website has higher usability and that context-dependent question suggestions further enhance user experience, especially for people with low curiosity levels (according to Curiosity and Exploration Inventory-II - CEI-II) who are guided in formulating effective questions. This shows the potential of LLMs to enrich engagement with existing Cultural Heritage websites.},
}

@article{Jean-Quartier2024,
  title = {Sharing practices of software artefacts and source code for reproducible research},
  author = {Claire Jean-Quartier and Fleur Jeanquartier and Sarah Stryeck and Jörg Simon and Birgit Söser and Ilire Hasani-Mavriqi},
  year = {2024},
  journal = {International Journal of Data Science and Analytics},
  doi = {10.1007/s41060-024-00617-7},
  url = {https://doi.org/10.1007/s41060-024-00617-7},
  abstract = {While source code of software and algorithms depicts an essential component in all fields of modern research involving data analysis and processing steps, it is uncommonly shared upon publication of results throughout disciplines. Simple guidelines to generate reproducible source code have been published. Still, code optimization supporting its repurposing to different settings is often neglected and even less thought of to be registered in catalogues for a public reuse. Though all research output should be reasonably curated in terms of reproducibility, it has been shown that researchers are frequently non-compliant with availability statements in their publications. These do not even include the use of persistent unique identifiers that would allow referencing archives of code artefacts at certain versions and time for long-lasting links to research articles. In this work, we provide an analysis on current practices of authors in open scientific journals in regard to code availability indications, FAIR principles applied to code and algorithms. We present common repositories of choice among authors. Results further show disciplinary differences of code availability in scholarly publications over the past years. We advocate proper description, archiving and referencing of source code and methods as part of the scientific knowledge, also appealing to editorial boards and reviewers for supervision.},
}

@article{Ghosh2024,
  title = {The conundrum of erasing digital footprints: A regulatory challenge},
  author = {Diya Sarkar Ghosh and Prafulla Chandra Mishra and Tulishree Pradhan},
  year = {2024},
  journal = {Jindal Global Law Review},
  volume = {15},
  pages = {25-59},
  doi = {10.1007/s41020-024-00223-5},
  url = {https://doi.org/10.1007/s41020-024-00223-5},
  abstract = {As technology advances, the idea of deleting one’s digital traces has sparked discussions, and worries. Now, digital material can persist permanently and may be simply found through search engines. The mechanisms like auto suggest, and machine learning contribute to information leakage through search engines. This prompts consideration of how individuals can efficiently oversee their online presence, and maintain authority over their personal data. To solve this concern, Europe has remedied it by recognising the principle of the right to be forgotten (hereinafter mentioned as RTBF), and given primacy to individual privacy, or the public’s interest in information. Unlike Europe, the United States of America gives primacy to information access as well as freedom of speech and believes the RTBF conflicts with the First Amendment. The concept of the RTBF under different nomenclatures has evolved as a kind of possible approach to address the side effects of digital footprints across the globe. Meanwhile, India has recently introduced the Digital Personal Data Protection Act 2023, which gives a place to the right to erasure. The review of the literature reveals a dearth of research analysis on Indian jurisprudence around the new data protection law, and RTBF. The research explores whether the RTBF finds a place in Indian jurisprudence. The authors gain a better grasp of the study by contrasting the Indian regulatory framework with its Western counterparts and propose possible directions for future research.},
}

@article{Holmes2024,
  title = {AIED—Coming of Age?},
  author = {Wayne Holmes},
  year = {2024},
  journal = {International Journal of Artificial Intelligence in Education},
  volume = {34},
  pages = {1-11},
  doi = {10.1007/s40593-023-00352-3},
  url = {https://doi.org/10.1007/s40593-023-00352-3},
}

@article{Holmes2024_01,
  title = {AIED—Coming of Age?},
  author = {Wayne Holmes},
  year = {2024},
  journal = {International Journal of Artificial Intelligence in Education},
  volume = {34},
  pages = {1-11},
  doi = {10.1007/s40593-023-00352-3},
  url = {https://doi.org/10.1007/s40593-023-00352-3},
}

@article{Osborne2024,
  title = {The AI community building the future? A quantitative analysis of development activity on Hugging Face Hub},
  author = {Cailean Osborne and Jennifer Ding and Hannah Rose Kirk},
  year = {2024},
  journal = {Journal of Computational Social Science},
  volume = {7},
  pages = {2067-2105},
  doi = {10.1007/s42001-024-00300-8},
  url = {https://doi.org/10.1007/s42001-024-00300-8},
  abstract = {Open model developers have emerged as key actors in the political economy of artificial intelligence (AI), but we still have a limited understanding of collaborative practices in the open AI ecosystem. This paper responds to this gap with a three-part quantitative analysis of development activity on the Hugging Face (HF) Hub, a popular platform for building, sharing, and demonstrating models. First, various types of activity across 348,181 model, 65,761 dataset, and 156,642 space repositories exhibit right-skewed distributions. Activity is extremely imbalanced between repositories; for example, over 70% of models have 0 downloads, while 1% account for 99% of downloads. Furthermore, licenses matter: there are statistically significant differences in collaboration patterns in model repositories with permissive, restrictive, and no licenses. Second, we analyse a snapshot of the social network structure of collaboration in model repositories, finding that the community has a core-periphery structure, with a core of prolific developers and a majority of isolate developers (89%). Upon removing these isolates from the network, collaboration is characterised by high reciprocity regardless of developers’ network positions. Third, we examine model adoption through the lens of model usage in spaces, finding that a minority of models, developed by a handful of companies, are widely used on the HF Hub. Overall, the findings show that various types of activity across the HF Hub are characterised by Pareto distributions, congruent with open source software development patterns on platforms like GitHub. We conclude with recommendations for researchers, and practitioners to advance our understanding of open AI development.},
}

@article{Haque2024,
  title = {Utilizing structural metrics from knowledge graphs to enhance the robustness quantification of large language models},
  author = {Mohd Ariful Haque and Marufa Kamal and Roy George and Kishor Datta Gupta},
  year = {2024},
  journal = {International Journal of Data Science and Analytics},
  doi = {10.1007/s41060-024-00643-5},
  url = {https://doi.org/10.1007/s41060-024-00643-5},
  abstract = {Knowledge graphs (KGs) play a critical role in organizing large stores of unstructured information into structured formats. This structured information is then accessible through SPARQL queries or graph libraries based on their structure. KGs enhance search, power AI systems, and facilitate knowledge discovery across domains. In this research, we explore the capabilities of different large language models (LLMs) like CodeLlama, Mistral, and Vicuna, which are recognized for text generation, in handling textual information tasks for constructing knowledge graphs with structured data. Utilizing these LLMs, we generate class descriptions for all the classes of well-known KGs like DBpedia, YAGO, and Google Knowledge Graph. Using these class descriptions, we have extracted RDF triples and used different preprocessing techniques for better refinement and extraction of the graph triples from the generated result. These extracted triples are used for the graph ontology creation. Highlighting the contribution of LLMs to structured graph formation, our study includes a comparison of the constructed KGs using the three LLMs with the existing Knowledge Graphs. Later, these KGs are evaluated using six structural quality metrics encompassing both class and property-related information crucial for KG formation. Our insights prove valuable for researchers exploring these domains, offering guidance on overcoming challenges and maximizing the potential of large language models in knowledge graph construction, text generation, and text extraction.},
}

@article{Wirtz2023,
  title = {How intelligent automation, service robots, and AI will reshape service products and their delivery},
  author = {Jochen Wirtz and Valentina Pitardi},
  year = {2023},
  journal = {Italian Journal of Marketing},
  volume = {2023},
  pages = {289-300},
  doi = {10.1007/s43039-023-00076-1},
  url = {https://doi.org/10.1007/s43039-023-00076-1},
  abstract = {Intelligent Automation in form of robots, smart self-service technologies, wearable technologies, software and systems such as machine learning, generative artificial intelligence (AI) such as ChatGPT, and the metaverse are increasingly adopted in a wide range of customer-facing service settings. The shift toward robot- and AI-powered services will lead to improved customer experiences, service quality, and productivity all at the same time. However, these also carry ethical, fairness, and privacy risks for customers and society. In this opinion piece, we discuss the implications of the service revolution for service firms, their marketing, and their customers, and provide avenues for future research opportunities.},
}

@article{Bäumer2024,
  title = {Mirroring Privacy Risks with Digital Twins: When Pieces of Personal Data Suddenly Fit Together},
  author = {Frederik Simon Bäumer and Sergej Schultenkämper and Michaela Geierhos and Yeong Su Lee},
  year = {2024},
  journal = {SN Computer Science},
  volume = {5},
  pages = {1109},
  doi = {10.1007/s42979-024-03413-z},
  url = {https://doi.org/10.1007/s42979-024-03413-z},
  abstract = {With the proliferation of social media, more personal information is being shared online than ever before, raising significant privacy concerns. This paper presents a novel approach to identify and mitigate privacy risks by generating digital twins from social media data. We propose a comprehensive framework that includes data collection, processing, and analysis, with special attention to data standardization, pseudonymization, and the use of synthetic data to ensure privacy compliance. We apply and evaluate state-of-the-art techniques such as Large Language Models, Generative Adversarial Networks, and Vision-Language Models to generate synthetic but realistic social media data that support the construction of accurate and representative digital twins while ensuring strict privacy compliance. Our approach demonstrates the potential for digital twins to help identify and mitigate privacy risks associated with social media use. We discuss the value and feasibility of this concept and suggest that further refinement of the techniques and conditions involved is needed.},
}

@article{Krook2025,
  title = {A systematic literature review of artificial intelligence (AI) transparency laws in the European Union (EU) and United Kingdom (UK): a socio-legal approach to AI transparency governance},
  author = {Joshua Krook and Peter Winter and John Downer and Jan Blockx},
  year = {2025},
  journal = {AI and Ethics},
  doi = {10.1007/s43681-025-00674-z},
  url = {https://doi.org/10.1007/s43681-025-00674-z},
  abstract = {This systematic literature review examines AI transparency laws and governance in the European Union (EU) and the United Kingdom (UK) through a socio-legal lens. The study highlights the importance of transparency in AI systems as a key regulatory focus globally, driven by the need to address the risks posed by opaque, ‘black box’ algorithms that can lead to unfair outcomes, privacy violations, and a lack of accountability. It identifies significant differences between the EU and UK approaches to AI regulation post-Brexit, with the EU's tiered, risk-based framework and the UK's more flexible, sector-specific strategy. The review categorises the literature into five themes: the necessity of AI transparency, challenges in achieving transparency, techniques for governing transparency, laws governing AI transparency, and soft law governance toolkits. The findings suggest that while technical solutions like eXplainable AI (XAI) and counterfactual methodologies are widely discussed, there is a critical need for a comprehensive, whole-of-organisation approach to embedding AI transparency within the cultural and operational fabric of organisations. This approach is argued to be more effective than top-down mandates, fostering an internal culture where transparency is valued and sustained. The study concludes by advocating for the development of AI transparency toolkits, particularly for small and medium-sized enterprises (SMEs), to address sociotechnical barriers and ensure that transparency in AI systems is practically implemented across various organisational contexts. These toolkits would serve as practical guides for companies to adopt best practices in AI transparency, aligning with both legal requirements and broader sociocultural considerations.},
}

@article{Zhang2024_03,
  title = {Right to be forgotten in the Era of large language models: implications, challenges, and solutions},
  author = {Dawen Zhang and Pamela Finckenberg-Broman and Thong Hoang and Shidong Pan and Zhenchang Xing and Mark Staples and Xiwei Xu},
  year = {2024},
  journal = {AI and Ethics},
  doi = {10.1007/s43681-024-00573-9},
  url = {https://doi.org/10.1007/s43681-024-00573-9},
  abstract = {The Right to be Forgotten (RTBF) was first established as the result of the ruling of Google Spain SL, Google Inc. v AEPD, Mario Costeja González, and was later included as the Right to Erasure under the General Data Protection Regulation (GDPR) of European Union to allow individuals the right to request personal data be deleted by organizations. Specifically for search engines, individuals can send requests to organizations to exclude their information from the query results. It was a significant emergent right as the result of the evolution of technology. With the recent development of Large Language Models (LLMs) and their use in chatbots, LLM-enabled software systems have become popular. But they are not excluded from the RTBF. Compared with the indexing approach used by search engines, LLMs store, and process information in a completely different way. This poses new challenges for compliance with the RTBF. In this paper, we explore these challenges and provide our insights on how to implement technical solutions for the RTBF, including the use of differential privacy, machine unlearning, model editing, and guardrails. With the rapid advancement of AI and the increasing need of regulating this powerful technology, learning from the case of RTBF can provide valuable lessons for technical practitioners, legal experts, organizations, and authorities.},
}

@article{Dua2025,
  title = {The ethics of national artificial intelligence plans: an empirical lens},
  author = {Manpriya Dua and J P Singh and Amarda Shehu},
  year = {2025},
  journal = {AI and Ethics},
  doi = {10.1007/s43681-025-00663-2},
  url = {https://doi.org/10.1007/s43681-025-00663-2},
  abstract = {Over fifty countries have published national infrastructure and strategy plans on Artificial Intelligence (AI), outlining their values and priorities regarding AI research, development, and deployment. This paper utilizes a deliberation and capabilities-based ethics framework rooted in providing freedom of agency and choice to human beings– to investigate how different countries approach AI ethics within their national plans. We explore the commonalities and variations in national priorities and their implications for a deliberation and capabilities-based ethics approach. Combining established and novel methodologies such as content analysis, graph structuring, and generative AI, we uncover a complex landscape where traditional geostrategic formations intersect with new alliances, thereby revealing how various groups and associated values are prioritized. For instance, the Ibero-American AI strategy highlights strong connections among Latin American nations, particularly with Spain, emphasizing gender diversity but pragmatically and predominantly as a workforce issue. In contrast, a US-led coalition of “science and tech first movers" is more focused on advancing foundational AI and diverse applications. The European Union AI strategy showcases leading states like France and Germany while addressing regional divides, with more focus and detail on social mobility, sustainability, standardization, and democratic governance of AI. These findings offer an empirical lens into the current global landscape of AI development and ethics, revealing distinct national trajectories in the pursuit of ethical AI.},
}

@article{Ibrahim2024,
  title = {A survey on augmenting knowledge graphs (KGs) with large language models (LLMs): models, evaluation metrics, benchmarks, and challenges},
  author = {Nourhan Ibrahim and Samar Aboulela and Ahmed Ibrahim and Rasha Kashef},
  year = {2024},
  journal = {Discover Artificial Intelligence},
  volume = {4},
  pages = {76},
  doi = {10.1007/s44163-024-00175-8},
  url = {https://doi.org/10.1007/s44163-024-00175-8},
  abstract = {Integrating Large Language Models (LLMs) with Knowledge Graphs (KGs) enhances the interpretability and performance of AI systems. This research comprehensively analyzes this integration, classifying approaches into three fundamental paradigms: KG-augmented LLMs, LLM-augmented KGs, and synergized frameworks. The evaluation examines each paradigm’s methodology, strengths, drawbacks, and practical applications in real-life scenarios. The findings highlight the substantial impact of these integrations in fundamentally improving real-time data analysis, efficient decision-making, and promoting innovation across various domains. In this paper, we also describe essential evaluation metrics and benchmarks for assessing the performance of these integrations, addressing challenges like scalability and computational overhead, and providing potential solutions. This comprehensive analysis underscores the profound impact of these integrations on improving real-time data analysis, enhancing decision-making efficiency, and fostering innovation across various domains.},
}

@article{Zali2024,
  title = {Digital Twins for Smarter Iranian Cities: A Future Studies Perspective},
  author = {Nader Zali and Ali Soltani and Peyman Najafi and Salima Ebadi Qajari and Mehrdad Mehrju},
  year = {2024},
  journal = {Computational Urban Science},
  volume = {4},
  pages = {43},
  doi = {10.1007/s43762-024-00155-9},
  url = {https://doi.org/10.1007/s43762-024-00155-9},
  abstract = {This study explores the future of Urban Digital Twin (UDT) in urban planning systems of developing countries, with a focus on Iran. Despite UDT's growing popularity, its implementation in developing countries is limited. The research identifies critical factors influencing UDT development, including organisational acceptance, urban infrastructure, policy and legislation, and technology and innovation. Using a futures studies approach, the study employs the Delphi method, MICMAC (Matrix Impact Cross-Reference Multiplication Applied to a Classification) technique, and SISMW (Strategic Uncertainties and Strengths Weaknesses Opportunities and Threats Matrix) methodologies to analyse these factors. The study reveals that international sanctions, organisational factors, technological factors, and infrastructure limitations hinder UDT development in Iran. However, UDT technology has the potential to transform urban planning in developing countries. The study provides a roadmap for collaboration between public and private sectors and research institutes to facilitate UDT implementation, highlighting the importance of legislative frameworks, digital infrastructure, innovation, and stakeholder engagement. Policy implications suggest that governments should prioritise supportive policies, investments in digital infrastructure, and collaborative efforts to address data privacy, security, and ownership issues. By addressing these challenges, developing countries can leverage UDT technology to improve urban planning, resource management, and quality of life.},
}

@article{Jiang2025,
  title = {Advancing translational human dynamics research: bridging space, mind, and computational urban science in the era of GeoAI},
  author = {Bin Jiang and Tao Cheng and Ming-Hsiang Tsou and Di Zhu and Xinyue Ye},
  year = {2025},
  journal = {Computational Urban Science},
  volume = {5},
  pages = {12},
  doi = {10.1007/s43762-025-00171-3},
  url = {https://doi.org/10.1007/s43762-025-00171-3},
  abstract = {Human dynamics research has undergone a significant transformation over the past decade, driven by interdisciplinary collaboration and technological innovation. This opinion paper examines the evolution of the field in the past ten years, focusing on its integration of GIScience (Geographic Information Science), social science, and public health to tackle spatial and societal challenges such as urban sustainability, disaster response, and epidemics. Key advancements include the adoption of living structure theory, which redefines space as a dynamic and interconnected entity linked to human well-being and ecological sustainability, and the application of cutting-edge technologies like GeoAI (Geospatial Artificial Intelligence) and digital twins for adaptive modeling and informed decision-making. Despite these advancements, challenges persist, including incomplete data, mismatched scales, and barriers to equitable access to geospatial information. Addressing these issues necessitates innovative approaches such as multiscale modeling, open data platforms, and inclusive methodologies. Increased funding opportunities offer pathways for accelerating translational research. By integrating advanced theories, user-centered technologies, and collaborative frameworks, human dynamics research is poised to transform urban systems into sustainable, resilient, and equitable environments. This paradigm shift underscores the importance of ethical considerations and inclusivity, offering a holistic approach that aligns with human and ecological needs.},
}

@article{Ali2024,
  title = {Towards more effective summative assessment in OBE: a new framework integrating direct measurements and technology},
  author = {Qutaiba I Ali},
  year = {2024},
  journal = {Discover Education},
  volume = {3},
  pages = {107},
  doi = {10.1007/s44217-024-00208-5},
  url = {https://doi.org/10.1007/s44217-024-00208-5},
  abstract = {This paper contributes to the ongoing efforts aimed at enhancing Outcome-Based Education (OBE) assessment methodologies by addressing some critical gaps and exploring new solutions. Our work focuses on two main areas: firstly, this study proposes an improved assessment method for OBE. It refines traditional approaches by classifying course materials according to their relevance to learning outcomes, weighting them by importance, connecting these outcomes to student goals and assigning difficulty levels to modules. All modules are directly assessed through a final exam with a consistent rubric, and student success is measured by a holistic score that considers the weighted attainment levels across all learning outcomes and modules. Secondly, this paper provides theoretical guidance for integrating Generative Artificial Intelligence (AI) and blockchain technologies into OBE assessment. It examines the potential impact of these technologies at various assessment stages, laying the groundwork for practical implementation.},
}

@article{McClymont2024,
  title = {Internet-based Surveillance Systems and Infectious Diseases Prediction: An Updated Review of the Last 10 Years and Lessons from the COVID-19 Pandemic},
  author = {Hannah McClymont and Stephen B Lambert and Ian Barr and Sotiris Vardoulakis and Hilary Bambrick and Wenbiao Hu},
  year = {2024},
  journal = {Journal of Epidemiology and Global Health},
  volume = {14},
  pages = {645-657},
  doi = {10.1007/s44197-024-00272-y},
  url = {https://doi.org/10.1007/s44197-024-00272-y},
  abstract = {The last decade has seen major advances and growth in internet-based surveillance for infectious diseases through advanced computational capacity, growing adoption of smart devices, increased availability of Artificial Intelligence (AI), alongside environmental pressures including climate and land use change contributing to increased threat and spread of pandemics and emerging infectious diseases. With the increasing burden of infectious diseases and the COVID-19 pandemic, the need for developing novel technologies and integrating internet-based data approaches to improving infectious disease surveillance is greater than ever. In this systematic review, we searched the scientific literature for research on internet-based or digital surveillance for influenza, dengue fever and COVID-19 from 2013 to 2023. We have provided an overview of recent internet-based surveillance research for emerging infectious diseases (EID), describing changes in the digital landscape, with recommendations for future research directed at public health policymakers, healthcare providers, and government health departments to enhance traditional surveillance for detecting, monitoring, reporting, and responding to influenza, dengue, and COVID-19.},
}

@article{Adebamowo2023,
  title = {The promise of data science for health research in Africa},
  author = {Clement A Adebamowo and Shawneequa Callier and Simisola Akintola and Oluchi Maduka and Ayodele Jegede and Christopher Arima and Temidayo Ogundiran and Sally N Adebamowo and BridgELSI Project as part of the DS-I Africa Consortium},
  year = {2023},
  journal = {Nature Communications},
  volume = {14},
  pages = {6084},
  doi = {10.1038/s41467-023-41809-2},
  url = {https://doi.org/10.1038/s41467-023-41809-2},
  abstract = {Data science health research promises tremendous benefits for African populations, but its implementation is fraught with substantial ethical governance risks that could thwart the delivery of these anticipated benefits. We discuss emerging efforts to build ethical governance frameworks for data science health research in Africa and the opportunities to advance these through investments by African governments and institutions, international funding organizations and collaborations for research and capacity development.},
}

@article{Nani2025,
  title = {From Code to Candidacy: Albania’s Accelerated European Union Application with ChatGPT},
  author = {Albi Nani},
  year = {2025},
  journal = {Digital Society},
  volume = {4},
  pages = {3},
  doi = {10.1007/s44206-024-00158-3},
  url = {https://doi.org/10.1007/s44206-024-00158-3},
  abstract = {In December 2023, Albania announced that its public administration would be using ChatGPT to accelerate its application to join the European Union. While digital governance scholars have frequently assessed cases involving the use of AI in the public sector, the Albanian case represents a unique study as it is among the first governments to explicitly use Generative AI in a key role within public administration. However, the proposed use-cases of translation and document summary do not directly address the real concerns withholding Albania from the EU, namely the lack of a healthy democracy typical of a post-communist state, demonstrating that the government may be succumbing to technological solutionism. This brief communication will provide background on the Albanian case, describing Albania’s historic progress with digital governance and assessing how effective ChatGPT may be in enhancing the country’s public administration. This offers digital governance researchers a base to pursue further research on the use of ChatGPT in Albania’s journey to join the EU, and the broader use of Generative AI in the public sector.},
}

@article{Reichstein2025,
  title = {Early warning of complex climate risk with integrated artificial intelligence},
  author = {Markus Reichstein and Vitus Benson and Jan Blunk and Gustau Camps-Valls and Felix Creutzig and Carina J Fearnley and Boran Han and Kai Kornhuber and Nasim Rahaman and Bernhard Schölkopf and José María Tárraga and Ricardo Vinuesa and Karen Dall and Joachim Denzler and Dorothea Frank and Giulia Martini and Naomi Nganga and Danielle C Maddix and Kommy Weldemariam},
  year = {2025},
  journal = {Nature Communications},
  volume = {16},
  pages = {2564},
  doi = {10.1038/s41467-025-57640-w},
  url = {https://doi.org/10.1038/s41467-025-57640-w},
  abstract = {As climate change accelerates, human societies face growing exposure to disasters and stress, highlighting the urgent need for effective early warning systems (EWS). These systems monitor, assess, and communicate risks to support resilience and sustainable development, but challenges remain in hazard forecasting, risk communication, and decision-making. This perspective explores the transformative potential of integrated Artificial Intelligence (AI) modeling. We highlight the role of AI in developing multi-hazard EWSs that integrate Meteorological and Geospatial foundation models (FMs) for impact prediction. A user-centric approach with intuitive interfaces and community feedback is emphasized to improve crisis management. To address climate risk complexity, we advocate for causal AI models to avoid spurious predictions and stress the need for responsible AI practices. We highlight the FATES (Fairness, Accountability, Transparency, Ethics, and Sustainability) principles as essential for equitable and trustworthy AI-based Early Warning Systems for all. We further advocate for decadal EWSs, leveraging climate ensembles and generative methods to enable long-term, spatially resolved forecasts for proactive climate adaptation.},
}

@article{Lintner2024,
  title = {A systematic review of AI literacy scales},
  author = {Tomáš Lintner},
  year = {2024},
  journal = {npj Science of Learning},
  volume = {9},
  pages = {50},
  doi = {10.1038/s41539-024-00264-4},
  url = {https://doi.org/10.1038/s41539-024-00264-4},
  abstract = {With the opportunities and challenges stemming from the artificial intelligence developments and its integration into society, AI literacy becomes a key concern. Utilizing quality AI literacy instruments is crucial for understanding and promoting AI literacy development. This systematic review assessed the quality of AI literacy scales using the COSMIN tool aiming to aid researchers in choosing instruments for AI literacy assessment. This review identified 22 studies validating 16 scales targeting various populations including general population, higher education students, secondary education students, and teachers. Overall, the scales demonstrated good structural validity and internal consistency. On the other hand, only a few have been tested for content validity, reliability, construct validity, and responsiveness. None of the scales have been tested for cross-cultural validity and measurement error. Most studies did not report any interpretability indicators and almost none had raw data available. There are 3 performance-based scale available, compared to 13 self-report scales.},
}

@article{MacNish2025,
  title = {Application of machine learning and genomics for orphan crop improvement},
  author = {Tessa R MacNish and Monica F Danilevicz and Philipp E Bayer and Mitchell S Bestry and David Edwards},
  year = {2025},
  journal = {Nature Communications},
  volume = {16},
  pages = {982},
  doi = {10.1038/s41467-025-56330-x},
  url = {https://doi.org/10.1038/s41467-025-56330-x},
  abstract = {Orphan crops are important sources of nutrition in developing regions and many are tolerant to biotic and abiotic stressors; however, modern crop improvement technologies have not been widely applied to orphan crops due to the lack of resources available. There are orphan crop representatives across major crop types and the conservation of genes between these related species can be used in crop improvement. Machine learning (ML) has emerged as a promising tool for crop improvement. Transferring knowledge from major crops to orphan crops and using machine learning to improve accuracy and efficiency can be used to improve orphan crops.},
}

@article{Ott2023,
  title = {ThoughtSource: A central hub for large language model reasoning data},
  author = {Simon Ott and Konstantin Hebenstreit and Valentin Liévin and Christoffer Egeberg Hother and Milad Moradi and Maximilian Mayrhauser and Robert Praas and Ole Winther and Matthias Samwald},
  year = {2023},
  journal = {Scientific Data},
  volume = {10},
  pages = {528},
  doi = {10.1038/s41597-023-02433-3},
  url = {https://doi.org/10.1038/s41597-023-02433-3},
  abstract = {Large language models (LLMs) such as GPT-4 have recently demonstrated impressive results across a wide range of tasks. LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to ‘hallucinate’ facts, and there are concerns about their underlying biases. Letting models verbalize reasoning steps as natural language, a technique known as chain-of-thought prompting, has recently been proposed as a way to address some of these issues. Here we present ThoughtSource, a meta-dataset and software library for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to improve future artificial intelligence systems by facilitating qualitative understanding of CoTs, enabling empirical evaluations, and providing training data. This first release of ThoughtSource integrates seven scientific/medical, three general-domain and five math word question answering datasets.},
}

@article{Kraemer2025,
  title = {Artificial intelligence for modelling infectious disease epidemics},
  author = {Moritz U G Kraemer and Joseph L.-H. Tsui and Serina Y Chang and Spyros Lytras and Mark P Khurana and Samantha Vanderslott and Sumali Bajaj and Neil Scheidwasser and Jacob Liam Curran-Sebastian and Elizaveta Semenova and Mengyan Zhang and H Juliette T Unwin and Oliver J Watson and Cathal Mills and Abhishek Dasgupta and Luca Ferretti and Samuel V Scarpino and Etien Koua and Oliver Morgan and Houriiyah Tegally and Ulrich Paquet and Loukas Moutsianas and Christophe Fraser and Neil M Ferguson and Eric J Topol and David A Duchêne and Tanja Stadler and Patricia Kingori and Michael J Parker and Francesca Dominici and Nigel Shadbolt and Marc A Suchard and Oliver Ratmann and Seth Flaxman and Edward C Holmes and Manuel Gomez-Rodriguez and Bernhard Schölkopf and Christl A Donnelly and Oliver G Pybus and Simon Cauchemez and Samir Bhatt},
  year = {2025},
  journal = {Nature},
  volume = {638},
  pages = {623-635},
  doi = {10.1038/s41586-024-08564-w},
  url = {https://doi.org/10.1038/s41586-024-08564-w},
  abstract = {Infectious disease threats to individual and public health are numerous, varied and frequently unexpected. Artificial intelligence (AI) and related technologies, which are already supporting human decision making in economics, medicine and social science, have the potential to transform the scope and power of infectious disease epidemiology. Here we consider the application to infectious disease modelling of AI systems that combine machine learning, computational statistics, information retrieval and data science. We first outline how recent advances in AI can accelerate breakthroughs in answering key epidemiological questions and we discuss specific AI methods that can be applied to routinely collected infectious disease surveillance data. Second, we elaborate on the social context of AI for infectious disease epidemiology, including issues such as explainability, safety, accountability and ethics. Finally, we summarize some limitations of AI applications in this field and provide recommendations for how infectious disease epidemiology can harness most effectively current and future developments in AI.},
}

@article{Szolnoky2024,
  title = {Tomorrow’s patient management: LLMs empowered by external tools},
  author = {Kelvin Szolnoky and Tobias Nordström and Martin Eklund},
  year = {2024},
  journal = {Nature Reviews Urology},
  doi = {10.1038/s41585-024-00965-w},
  url = {https://doi.org/10.1038/s41585-024-00965-w},
  abstract = {Large language models are gaining increasing interest in the medical community; however, an important but overlooked aspect of their capacity is their ability to integrate with tools. This integration greatly extends their potential application in health care.},
}

@article{Acion2023,
  title = {Generative AI poses ethical challenges for open science},
  author = {Laura Acion and Mariela Rajngewerc and Gregory Randall and Lorena Etcheverry},
  year = {2023},
  journal = {Nature Human Behaviour},
  volume = {7},
  pages = {1800-1801},
  doi = {10.1038/s41562-023-01740-4},
  url = {https://doi.org/10.1038/s41562-023-01740-4},
}

@article{Zhang2025_03,
  title = {An LLM driven dataset on the spatiotemporal distributions of street and neighborhood crime in China},
  author = {Yan Zhang and Mei-Po Kwan and Libo Fang},
  year = {2025},
  journal = {Scientific Data},
  volume = {12},
  pages = {467},
  doi = {10.1038/s41597-025-04757-8},
  url = {https://doi.org/10.1038/s41597-025-04757-8},
  abstract = {Crime is a significant social, economic, and legal issue. This research presents an open-access spatiotemporal repository of street and neighborhood crime data, comprising approximately one million records of crimes in China, with specific geographic coordinates (latitude and longitude) and timestamps for each incident. The dataset is based on publicly available law court judgment documents. Artificial intelligence (AI) technologies are employed to extract crime events at the neighborhood or even building level from vast amounts of unstructured judicial text. This dataset enables more precise spatial analysis of crime incidents, offering valuable insights across interdisciplinary fields such as economics, sociology, and geography. It contributes significantly to the achievement of the United Nations Sustainable Development Goals (SDGs), particularly in fostering sustainable cities and communities, and plays a crucial role in advancing efforts to reduce all forms of violence and related mortality rates.},
}

@article{Niu2024,
  title = {PharmaBench: Enhancing ADMET benchmarks with large language models},
  author = {Zhangming Niu and Xianglu Xiao and Wenfan Wu and Qiwei Cai and Yinghui Jiang and Wangzhen Jin and Minhao Wang and Guojian Yang and Lingkang Kong and Xurui Jin and Guang Yang and Hongming Chen},
  year = {2024},
  journal = {Scientific Data},
  volume = {11},
  pages = {985},
  doi = {10.1038/s41597-024-03793-0},
  url = {https://doi.org/10.1038/s41597-024-03793-0},
  abstract = {Accurately predicting ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) properties early in drug development is essential for selecting compounds with optimal pharmacokinetics and minimal toxicity. Existing ADMET-related benchmark sets are limited in utility due to their small dataset sizes and the lack of representation of compounds used in drug discovery projects. These shortcomings hinder their application in model building for drug discovery. To address this issue, we propose a multi-agent data mining system based on Large Language Models that effectively identifies experimental conditions within 14,401 bioassays. This approach facilitates merging entries from different sources, culminating in the creation of PharmaBench. Additionally, we have developed a data processing workflow to integrate data from various sources, resulting in 156,618 raw entries. Through this workflow, we constructed PharmaBench, a comprehensive benchmark set for ADMET properties, which comprises eleven ADMET datasets and 52,482 entries. This benchmark set is designed to serve as an open-source dataset for the development of AI models relevant to drug discovery projects.},
}

@article{Lu2024,
  title = {Unleashing the power of AI in science-key considerations for materials data preparation},
  author = {Yongchao Lu and Hong Wang and Lanting Zhang and Ning Yu and Siqi Shi and Hang Su},
  year = {2024},
  journal = {Scientific Data},
  volume = {11},
  pages = {1039},
  doi = {10.1038/s41597-024-03821-z},
  url = {https://doi.org/10.1038/s41597-024-03821-z},
  abstract = {The release of ChatGPT has triggered global attention on artificial intelligence (AI), and AI for science is thus becoming a hot topic in the scientific community. When we think about unleashing the power of AI to accelerate scientific research, the question coming to our mind first is whether there is a continuous supply of highly available data at a sufficiently large scale.},
}

@article{Dong2025,
  title = {Knowledge graph construction for intelligent cockpits based on large language models},
  author = {Haomin Dong and Wenbin Wang and Zhenjiang Sun and Ziyi Kang and Xiaojun Ge and Fei Gao and Jixin Wang},
  year = {2025},
  journal = {Scientific Reports},
  volume = {15},
  pages = {7635},
  doi = {10.1038/s41598-025-92002-y},
  url = {https://doi.org/10.1038/s41598-025-92002-y},
  abstract = {As intelligent cockpits rapidly evolve towards “proactive natural interaction,” traditional rule-based user behavior inference methods are facing scalability, generalization, and accuracy bottlenecks, leading to the development and deployment of functions oriented towards pseudo-demands. Effectively capturing and representing the hidden associative knowledge in intelligent cockpits can enhance the system’s understanding of user behavior and environmental contexts, thereby precisely discerning real user needs. In this context, knowledge graphs (KGs) have emerged as an effective tool, enabling the retrieval and organization of vast amounts of information within interconnected and interpretable structures. However, rapidly and flexibly generating domain-specific KGs still poses significant challenges. To address this, this paper introduces a novel knowledge graph construction (KGC) model, GLM-TripleGen, dedicated to analyzing the states and behaviors within intelligent cockpits. This model aims to precisely mine the latent relationships between cockpit state factors and behavioral sequences, effectively addressing key challenges such as the ambiguity in entity recognition and the complexity of relationship extraction within cockpit data. To enhance the adaptability of GLM-TripleGen to the intelligent cockpit domain, this paper constructs an instruction-following dataset based on vehicle states and in-cockpit interaction behaviors, containing a large number of prompt texts paired with corresponding triple labels, to support model fine-tuning. During the fine-tuning process, the Low-Rank Adaptation (LoRA) method is employed to effectively optimize model parameters, significantly reducing training costs. Extensive experiments demonstrate that GLM-TripleGen outperforms existing state-of-the-art KGC methods, accurately generating normalized cockpit triple units. Furthermore, GLM-TripleGen exhibits exceptional robustness and generalization ability, handling various unknown entities and relationships with minimal generalization processing.},
}

@article{Deebani2025,
  title = {Synergistic transfer learning and adversarial networks for breast cancer diagnosis: benign vs. invasive classification},
  author = {Wejdan Deebani and Lubna Aziz and Arshad Aziz and Wael Sh. Basri and Wedad M Alawad and Sara A Althubiti},
  year = {2025},
  journal = {Scientific Reports},
  volume = {15},
  pages = {7461},
  doi = {10.1038/s41598-025-90288-6},
  url = {https://doi.org/10.1038/s41598-025-90288-6},
  abstract = {Current breast cancer diagnosis methods often face limitations such as high cost, time consumption, and inter-observer variability. To address these challenges, this research proposes a novel deep learning framework that leverages generative adversarial networks (GANs) for data augmentation and transfer learning to enhance breast cancer classification using convolutional neural networks (CNNs). The framework uses a two-stage augmentation approach. First, a conditional Wasserstein GAN (cWGAN) generates synthetic breast cancer images based on clinical data, enhancing training stability and enabling targeted feature incorporation. Second, traditional augmentation techniques (e.g., rotation, flipping, cropping) are applied to both original and synthetic images. A multi-scale transfer learning technique is also employed, integrating three pre-trained CNNs (DenseNet-201, NasNetMobile, ResNet-101) with a multi-scale feature enrichment scheme, allowing the model to capture features at various scales. The framework was evaluated on the BreakHis dataset, achieving an accuracy of 99.2% for binary classification and 98.5% for multi-class classification, significantly outperforming existing methods. This framework offers a more efficient, cost-effective, and accurate approach for breast cancer diagnosis. Future work will focus on generalizing the framework to clinical datasets and integrating it into diagnostic workflows.},
}

@article{Egaña2024,
  title = {Stochastic image spectroscopy: a discriminative generative approach to hyperspectral image modelling and classification},
  author = {Alvaro F Egaña and Alejandro Ehrenfeld and Franco Curotto and Juan F Sánchez-Pérez and Jorge F Silva},
  year = {2024},
  journal = {Scientific Reports},
  volume = {14},
  pages = {19308},
  doi = {10.1038/s41598-024-69732-6},
  url = {https://doi.org/10.1038/s41598-024-69732-6},
  abstract = {This paper introduces a new latent variable probabilistic framework for representing spectral data of high spatial and spectral dimensionality, such as hyperspectral images. We use a generative Bayesian model to represent the image formation process and provide interpretable and efficient inference and learning methods. Surprisingly, our approach can be implemented with simple tools and does not require extensive training data, detailed pixel-by-pixel labeling, or significant computational resources. Numerous experiments with simulated data and real benchmark scenarios show encouraging image classification performance. These results validate the unique ability of our framework to discriminate complex hyperspectral images, irrespective of the presence of highly discriminative spectral signatures.},
}

@article{Liu2023_01,
  title = {A medical multimodal large language model for future pandemics},
  author = {Fenglin Liu and Tingting Zhu and Xian Wu and Bang Yang and Chenyu You and Chenyang Wang and Lei Lu and Zhangdaihong Liu and Yefeng Zheng and Xu Sun and Yang Yang and Lei Clifton and David A Clifton},
  year = {2023},
  journal = {npj Digital Medicine},
  volume = {6},
  pages = {226},
  doi = {10.1038/s41746-023-00952-2},
  url = {https://doi.org/10.1038/s41746-023-00952-2},
  abstract = {Deep neural networks have been integrated into the whole clinical decision procedure which can improve the efficiency of diagnosis and alleviate the heavy workload of physicians. Since most neural networks are supervised, their performance heavily depends on the volume and quality of available labels. However, few such labels exist for rare diseases (e.g., new pandemics). Here we report a medical multimodal large language model (Med-MLLM) for radiograph representation learning, which can learn broad medical knowledge (e.g., image understanding, text semantics, and clinical phenotypes) from unlabelled data. As a result, when encountering a rare disease, our Med-MLLM can be rapidly deployed and easily adapted to them with limited labels. Furthermore, our model supports medical data across visual modality (e.g., chest X-ray and CT) and textual modality (e.g., medical report and free-text clinical note); therefore, it can be used for clinical tasks that involve both visual and textual data. We demonstrate the effectiveness of our Med-MLLM by showing how it would perform using the COVID-19 pandemic “in replay”. In the retrospective setting, we test the model on the early COVID-19 datasets; and in the prospective setting, we test the model on the new variant COVID-19-Omicron. The experiments are conducted on 1) three kinds of input data; 2) three kinds of downstream tasks, including disease reporting, diagnosis, and prognosis; 3) five COVID-19 datasets; and 4) three different languages, including English, Chinese, and Spanish. All experiments show that our model can make accurate and robust COVID-19 decision-support with little labelled data.},
}

@article{Ebel2024,
  title = {2024 ESA-ECMWF workshop report: current status, progress and opportunities in machine learning for Earth system observation and prediction},
  author = {Patrick Ebel and Rochelle Schneider and Massimo Bonavita and Mariana Clare and Anna Jungbluth and Maryam Pourshamsi and Matthew Chantry and Mihai Alexe and Alessandro Sebastianelli and Marcin Chrust},
  year = {2024},
  journal = {npj Climate and Atmospheric Science},
  volume = {7},
  pages = {241},
  doi = {10.1038/s41612-024-00757-4},
  url = {https://doi.org/10.1038/s41612-024-00757-4},
  abstract = {This report summarises the main outcomes of the 4th edition of the workshop on Machine Learning (ML) for Earth System Observation and Prediction (ESOP / ML4ESOP) co-organised by the European Space Agency (ESA) and the European Centre for Medium-Range Weather Forecasts (ECMWF). The 4-day workshop was held on 7-10 May 2024 in a hybrid format at the ESA Frascati site with an interactive online component, featuring over 46 expert talks with a record number of submissions and about 800 registrations. The workshop offered leading experts a platform to exchange on the current opportunities, challenges and future directions for applying ML methodology to ESOP. To structure the presentations and discussions, the workshop featured five main thematic areas covering key topics and emerging trends. The most promising research directions and significant outcomes were identified by each thematic area’s Working Group and are the focus of this document.},
}

@article{Narayan2025,
  title = {Addressing contemporary threats in anonymised healthcare data using privacy engineering},
  author = {Sanjiv M Narayan and Nitin Kohli and Megan M Martin},
  year = {2025},
  journal = {npj Digital Medicine},
  volume = {8},
  pages = {145},
  doi = {10.1038/s41746-025-01520-6},
  url = {https://doi.org/10.1038/s41746-025-01520-6},
  abstract = {Cyber-attacks on healthcare entities and leaks of personal identifiable information (PII) are a growing threat. However, it is now possible to learn sensitive characteristics of an individual without PII, by combining advances in artificial intelligence, analytics, and online repositories. We discuss privacy threats and privacy engineering solutions, emphasizing the selection of privacy enhancing technologies for various healthcare cases. Future solutions must consider dynamic flows of data throughout their lifecycle.},
}

@article{Sella2025,
  title = {Preserving information while respecting privacy through an information theoretic framework for synthetic health data generation},
  author = {Nadir Sella and Florent Guinot and Nikita Lagrange and Laurent-Philippe Albou and Jonathan Desponds and Hervé Isambert},
  year = {2025},
  journal = {npj Digital Medicine},
  volume = {8},
  pages = {49},
  doi = {10.1038/s41746-025-01431-6},
  url = {https://doi.org/10.1038/s41746-025-01431-6},
  abstract = {Generating synthetic data from medical records is a complex task intensified by patient privacy concerns. In recent years, multiple approaches have been reported for the generation of synthetic data, however, limited attention was given to jointly evaluate the quality and the privacy of the generated data. The quality and privacy of synthetic data stem from multivariate associations across variables, which cannot be assessed by comparing univariate distributions with the original data. Here, we introduce a novel algorithm (MIIC-SDG) for generating synthetic data from electronic records based on a multivariate information framework and Bayesian network theory. We also propose a new metric to quantitatively assess the trade-off between the Quality and Privacy Scores (QPS) of synthetic data generation methods. The performance of MIIC-SDG is demonstrated on different clinical datasets and favorably compares with state-of-the-art synthetic data generation methods, based on the QPS trade-off between several quality and privacy metrics.},
}

@article{Genderen2024,
  title = {Charting a new course in healthcare: early-stage AI algorithm registration to enhance trust and transparency},
  author = {Michel E van Genderen and Davy van de Sande and Lotty Hooft and Andreas Alois Reis and Alexander D Cornet and Jacobien H F Oosterhoff and Björn J P van der Ster and Joost Huiskens and Reggie Townsend and Jasper van Bommel and Diederik Gommers and Jeroen van den Hoven},
  year = {2024},
  journal = {npj Digital Medicine},
  volume = {7},
  pages = {119},
  doi = {10.1038/s41746-024-01104-w},
  url = {https://doi.org/10.1038/s41746-024-01104-w},
  abstract = {AI holds the potential to transform healthcare, promising improvements in patient care. Yet, realizing this potential is hampered by over-reliance on limited datasets and a lack of transparency in validation processes. To overcome these obstacles, we advocate the creation of a detailed registry for AI algorithms. This registry would document the development, training, and validation of AI models, ensuring scientific integrity and transparency. Additionally, it would serve as a platform for peer review and ethical oversight. By bridging the gap between scientific validation and regulatory approval, such as by the FDA, we aim to enhance the integrity and trustworthiness of AI applications in healthcare.},
}

@article{Schmidt2024,
  title = {Mapping the regulatory landscape for artificial intelligence in health within the European Union},
  author = {Jelena Schmidt and Nienke M Schutte and Stefan Buttigieg and David Novillo-Ortiz and Eric Sutherland and Michael Anderson and Bart de Witte and Michael Peolsson and Brigid Unim and Milena Pavlova and Ariel Dora Stern and Elias Mossialos and Robin van Kessel},
  year = {2024},
  journal = {npj Digital Medicine},
  volume = {7},
  pages = {229},
  doi = {10.1038/s41746-024-01221-6},
  url = {https://doi.org/10.1038/s41746-024-01221-6},
  abstract = {Regulatory frameworks for artificial intelligence (AI) are needed to mitigate risks while ensuring the ethical, secure, and effective implementation of AI technology in healthcare and population health. In this article, we present a synthesis of 141 binding policies applicable to AI in healthcare and population health in the EU and 10 European countries. The EU AI Act sets the overall regulatory framework for AI, while other legislations set social, health, and human rights standards, address the safety of technologies and the implementation of innovation, and ensure the protection and safe use of data. Regulation specifically pertaining to AI is still nascent and scarce, though a combination of data, technology, innovation, and health and human rights policy has already formed a baseline regulatory framework for AI in health. Future work should explore specific regulatory challenges, especially with respect to AI medical devices, data protection, and data enablement.},
}

@article{Tzachor2023,
  title = {Large language models and agricultural extension services},
  author = {A Tzachor and M Devare and C Richards and P Pypers and A Ghosh and J Koo and S Johal and B King},
  year = {2023},
  journal = {Nature Food},
  volume = {4},
  pages = {941-948},
  doi = {10.1038/s43016-023-00867-x},
  url = {https://doi.org/10.1038/s43016-023-00867-x},
  abstract = {Several factors have traditionally hampered the effectiveness of agricultural extension services, including limited institutional capacity and reach. Here we assess the potential of large language models (LLMs), specifically Generative Pre-trained Transformer (GPT), to transform agricultural extension. We focus on the ability of LLMs to simplify scientific knowledge and provide personalized, location-specific and data-driven agricultural recommendations. We emphasize shortcomings of this technology, informed by real-life testing of GPT to generate technical advice for Nigerian cassava farmers. To ensure a safe and responsible dissemination of LLM functionality across farming worldwide, we propose an idealized LLM design process with human experts in the loop.},
}

@article{Longpre2024,
  title = {A large-scale audit of dataset licensing and attribution in AI},
  author = {Shayne Longpre and Robert Mahari and Anthony Chen and Naana Obeng-Marnu and Damien Sileo and William Brannon and Niklas Muennighoff and Nathan Khazam and Jad Kabbara and Kartik Perisetla and Xinyi (Alexis) Wu and Enrico Shippole and Kurt Bollacker and Tongshuang Wu and Luis Villa and Sandy Pentland and Sara Hooker},
  year = {2024},
  journal = {Nature Machine Intelligence},
  volume = {6},
  pages = {975-987},
  doi = {10.1038/s42256-024-00878-8},
  url = {https://doi.org/10.1038/s42256-024-00878-8},
  abstract = {The race to train language models on vast, diverse and inconsistently documented datasets raises pressing legal and ethical concerns. To improve data transparency and understanding, we convene a multi-disciplinary effort between legal and machine learning experts to systematically audit and trace more than 1,800 text datasets. We develop tools and standards to trace the lineage of these datasets, including their source, creators, licences and subsequent use. Our landscape analysis highlights sharp divides in the composition and focus of data licenced for commercial use. Important categories including low-resource languages, creative tasks and new synthetic data all tend to be restrictively licenced. We observe frequent miscategorization of licences on popular dataset hosting sites, with licence omission rates of more than 70% and error rates of more than 50%. This highlights a crisis in misattribution and informed use of popular datasets driving many recent breakthroughs. Our analysis of data sources also explains the application of copyright law and fair use to finetuning data. As a contribution to continuing improvements in dataset transparency and responsible use, we release our audit, with an interactive user interface, the Data Provenance Explorer, to enable practitioners to trace and filter on data provenance for the most popular finetuning data collections: www.dataprovenance.org.},
}

@article{Shrestha2023,
  title = {Building open-source AI},
  author = {Yash Raj Shrestha and Georg von Krogh and Stefan Feuerriegel},
  year = {2023},
  journal = {Nature Computational Science},
  volume = {3},
  pages = {908-911},
  doi = {10.1038/s43588-023-00540-0},
  url = {https://doi.org/10.1038/s43588-023-00540-0},
  abstract = {Artificial intelligence (AI) drives innovation across society, economies and science. We argue for the importance of building AI technology according to open-source principles to foster accessibility, collaboration, responsibility and interoperability.},
}

@article{Choi2024,
  title = {Accelerating materials language processing with large language models},
  author = {Jaewoong Choi and Byungju Lee},
  year = {2024},
  journal = {Communications Materials},
  volume = {5},
  pages = {13},
  doi = {10.1038/s43246-024-00449-9},
  url = {https://doi.org/10.1038/s43246-024-00449-9},
  abstract = {Materials language processing (MLP) can facilitate materials science research by automating the extraction of structured data from research papers. Despite the existence of deep learning models for MLP tasks, there are ongoing practical issues associated with complex model architectures, extensive fine-tuning, and substantial human-labelled datasets. Here, we introduce the use of large language models, such as generative pretrained transformer (GPT), to replace the complex architectures of prior MLP models with strategic designs of prompt engineering. We find that in-context learning of GPT models with few or zero-shots can provide high performance text classification, named entity recognition and extractive question answering with limited datasets, demonstrated for various classes of materials. These generative models can also help identify incorrect annotated data. Our GPT-based approach can assist material scientists in solving knowledge-intensive MLP tasks, even if they lack relevant expertise, by offering MLP guidelines applicable to any materials science domain. In addition, the outcomes of GPT models are expected to reduce the workload of researchers, such as manual labelling, by producing an initial labelling set and verifying human-annotations.},
}

@article{Berger2024,
  title = {Hybrid intelligence for reconciling biodiversity and productivity in agriculture},
  author = {T Berger and H Gimpel and A Stein and C Troost and S Asseng and M Bichler and C Bieling and R Birner and I Grass and J Kollmann and S D Leonhardt and F M Schurr and W Weisser},
  year = {2024},
  journal = {Nature Food},
  volume = {5},
  pages = {270-272},
  doi = {10.1038/s43016-024-00963-6},
  url = {https://doi.org/10.1038/s43016-024-00963-6},
  abstract = {Hybrid intelligence — arising from the sensible, targeted fusion of human minds and cutting-edge computational systems — holds great potential for enhancing the sustainability of agriculture. Leveraging the combined strengths of both collective human and artificial intelligence helps identify and stress-test pathways towards the reconciliation of biodiversity and productivity.},
}

@article{Tsybina2025,
  title = {Smart cities: the data to decisions process},
  author = {Eve Tsybina and Viswadeep Lebakula and Fengxiu Zhang and Qian Hu and Kathryn B Laskey},
  year = {2025},
  journal = {Nature Cities},
  volume = {2},
  pages = {135-143},
  doi = {10.1038/s44284-024-00194-7},
  url = {https://doi.org/10.1038/s44284-024-00194-7},
  abstract = {Smart cities improve citizen services by converting data into data-driven decisions. This conversion is not coincidental and depends on the underlying movement of information through four layers: devices, data communication and handling, operations, and planning and economics. Here we examine how this flow of information enables smartness in five major infrastructure sectors: transportation, energy, health, governance and municipal utilities. We show how success or failure within and between layers results in disparities in city smartness across different regions and sectors. Regions such as Europe and Asia exhibit higher levels of smartness compared to Africa and the USA. Furthermore, within one region, such as the USA or the Middle East, smarter cities manage the flow of information more efficiently. Sectors such as transportation and municipal utilities, characterized by extensive data, strong analytics and efficient information flow, tend to be smarter than healthcare and energy. The flow of information, however, generates risks associated with data collection and artificial intelligence deployment at each layer. We underscore the importance of seamless data transformation in achieving cost-effective and sustainable urban improvements and identify both supportive and impeding factors in the journey towards smarter cities.},
}

@article{Maurushat2015,
  title = {Hacktivism and Whistleblowing in the Era of Forced Transparency?},
  author = {Alana Maurushat},
  year = {2015},
  pages = {253-265},
  doi = {10.1057/9781137474162_17},
  publisher = {Palgrave Macmillan UK},
  url = {https://doi.org/10.1057/9781137474162_17},
  abstract = {Leni Riefenstahl is one of the most controversial women and artists of the last century. Painter, actress, dancer, director genius, personal friend of Hitler, and commissioned film director of the Propaganda Ministry in Nazi Germany, Riefenstahl is both reviled and revered. She is best known for her direction of two films, Triumph of the Will (Riefenstahl 1935) and Olympia (Riefenstahl 1938).},
}

@article{Zabezhailo2024,
  title = {On the Problem of Explaining the Results of Intelligent Data Analysis},
  author = {M I Zabezhailo},
  year = {2024},
  journal = {Pattern Recognition and Image Analysis},
  volume = {34},
  pages = {498-502},
  doi = {10.1134/S1054661824700263},
  url = {https://doi.org/10.1134/S1054661824700263},
  abstract = {The possibilities of evaluating the acceptability of results generated by artificial intelligence (AI) systems in the process of intelligent data analysis (IDA) are evaluated. The explanations generated by the IDA system are one of the convenient tools for such an evaluation. Some known approaches to the generation of explanations are considered. Particular attention is paid to causal explanations, which are based on empirical causal dependences extracted from data in the IDA process. Some algorithmic properties of causal explanations are given.},
}

@article{Yang2025,
  title = {Navigating the landscape of AI literacy education: insights from a decade of research (2014–2024)},
  author = {Yuqin Yang and Ying Zhang and Daner Sun and Wenmeng He and Yantao Wei},
  year = {2025},
  journal = {Humanities and Social Sciences Communications},
  volume = {12},
  pages = {374},
  doi = {10.1057/s41599-025-04583-8},
  url = {https://doi.org/10.1057/s41599-025-04583-8},
  abstract = {As artificial intelligence (AI) becomes increasingly integrated into various fields, the need to enhance learners’ AI literacy is more urgent than ever. Despite its growing importance, a comprehensive review of AI literacy education research has been lacking. This study addresses that gap by mapping the current landscape, tracing its evolution, and identifying key themes through a bibliometric analysis of research from 2014 to 2024, utilizing CiteSpace for data visualization and analysis. This study systematically selected 335 relevant articles from databases, including Web of Science Core Collection, Scopus, and Science Direct, following PRISMA guidelines. Our methodology involved keyword co-occurrence mapping to trace the development paths and thematic evolution within the field. By examining publication trends and thematic clusters, we provide insights into the progression and focal points of AI literacy education research over the past decade. The study reveals three key insights. First, AI literacy education research has shifted from an exploratory phase to rapid growth, with a marked increase in publications. Second, four distinct developmental trajectories have emerged, emphasizing the interdisciplinary nature of the field and its connections to information, digital, and algorithmic literacy. Third, nine prominent research themes have been identified, with data literacy, machine learning, AI literacy, the technology acceptance model, and computational thinking as focal points. These themes highlight AI’s evolving role, particularly in education, and shifts in research priorities. This review provides a comprehensive understanding of AI literacy education’s evolution and its implications for education, ethics, and society. As AI continues to influence modern education and industry, this analysis serves as a valuable resource for researchers, educators, and policymakers navigating the complex intersection of AI and literacy.},
}

@article{Tian2024,
  title = {Emerging landscapes of “alternative-academic” careers in library and information science: Evolutionary patterns and prospects in the Chinese context},
  author = {Ye Tian and Kuang-Hua Chen},
  year = {2024},
  journal = {Humanities and Social Sciences Communications},
  volume = {11},
  pages = {1331},
  doi = {10.1057/s41599-024-03821-9},
  url = {https://doi.org/10.1057/s41599-024-03821-9},
  abstract = {This study investigated the emerging landscape of alternative-academic careers in the field of library and information science in China using text mining techniques on a dataset of 7832 job postings from 334 institutions between 2016 and 2023. The findings revealed that alternative-academic positions are widely distributed across academic, cultural, governmental, and commercial institutions, with a growing presence in interdisciplinary and applied research domains. The results highlight the emergence of four distinct functional types: resource construction, academic services, intelligence research, and social functions. Alternative-academic roles have evolved towards greater professionalization, specialization, and collaboration, reflecting the shifting demands of the digital information landscape.},
}

@article{Radanliev2024,
  title = {Digital security by design},
  author = {Petar Radanliev},
  year = {2024},
  journal = {Security Journal},
  volume = {37},
  pages = {1640-1679},
  doi = {10.1057/s41284-024-00435-3},
  url = {https://doi.org/10.1057/s41284-024-00435-3},
  abstract = {This paper scrutinises the evolving digital security landscape, encompassing technological advancements, regulatory frameworks, and industry-specific challenges. It explores the influence of technologies like AI, quantum computing, and blockchain on security paradigms whilst identifying emergent threats. The study analyses the interplay between digital security and legislative policies, underlining their impact on industry practices and individual behaviours. Sector-specific examinations are conducted, pinpointing unique security concerns in sectors such as healthcare and finance and advocating bespoke solutions. The study highlights discrepancies between security intentions and actions, proposing strategies to bridge this divide. Projecting into the future, we anticipate shifts in technology and regulation, culminating in pragmatic recommendations for stakeholders. This article offers an informed perspective on digital security, laying the groundwork for proactive approaches in a dynamic digital environment.},
}

@article{Zhou2024_01,
  title = {Classification, detection, and segmentation performance of image-based AI in intracranial aneurysm: a systematic review},
  author = {Zhiyue Zhou and Yuxuan Jin and Haili Ye and Xiaoqing Zhang and Jiang Liu and Wenyong Zhang},
  year = {2024},
  journal = {BMC Medical Imaging},
  volume = {24},
  pages = {164},
  doi = {10.1186/s12880-024-01347-9},
  url = {https://doi.org/10.1186/s12880-024-01347-9},
  abstract = {The detection and management of intracranial aneurysms (IAs) are vital to prevent life-threatening complications like subarachnoid hemorrhage (SAH). Artificial Intelligence (AI) can analyze medical images, like CTA or MRA, spotting nuances possibly overlooked by humans. Early detection facilitates timely interventions and improved outcomes. Moreover, AI algorithms offer quantitative data on aneurysm attributes, aiding in long-term monitoring and assessing rupture risks.},
}

@article{Behdenna2023,
  title = {pyComBat, a Python tool for batch effects correction in high-throughput molecular data using empirical Bayes methods},
  author = {Abdelkader Behdenna and Maximilien Colange and Julien Haziza and Aryo Gema and Guillaume Appé and Chloé-Agathe Azencott and Akpéli Nordor},
  year = {2023},
  journal = {BMC Bioinformatics},
  volume = {24},
  pages = {459},
  doi = {10.1186/s12859-023-05578-5},
  url = {https://doi.org/10.1186/s12859-023-05578-5},
  abstract = {Variability in datasets is not only the product of biological processes: they are also the product of technical biases. ComBat and ComBat-Seq are among the most widely used tools for correcting those technical biases, called batch effects, in, respectively, microarray and RNA-Seq expression data.},
}

@article{Assmann2020,
  title = {EuPRAXIA Conceptual Design Report},
  author = {R W Assmann and M K Weikum and T Akhter and D Alesini and A S Alexandrova and M P Anania and N E Andreev and I Andriyash and M Artioli and A Aschikhin and T Audet and A Bacci and I F Barna and S Bartocci and A Bayramian and A Beaton and A Beck and M Bellaveglia and A Beluze and A Bernhard and A Biagioni and S Bielawski and F G Bisesto and A Bonatto and L Boulton and F Brandi and R Brinkmann and F Briquez and F Brottier and E Bründermann and M Büscher and B Buonomo and M H Bussmann and G Bussolino and P Campana and S Cantarella and K Cassou and A Chancé and M Chen and E Chiadroni and A Cianchi and F Cioeta and J A Clarke and J M Cole and G Costa and M -E. Couprie and J Cowley and M Croia and B Cros and P A Crump and R D’Arcy and G Dattoli and A Del Dotto and N Delerue and M Del Franco and P Delinikolas and S De Nicola and J M Dias and D Di Giovenale and M Diomede and E Di Pasquale and G Di Pirro and G Di Raddo and U Dorda and A C Erlandson and K Ertel and A Esposito and F Falcoz and A F},
  year = {2020},
  journal = {The European Physical Journal Special Topics},
  volume = {229},
  pages = {3675-4284},
  doi = {10.1140/epjst/e2020-000127-8},
  url = {https://doi.org/10.1140/epjst/e2020-000127-8},
  abstract = {This report presents the conceptual design of a new European research infrastructure EuPRAXIA. The concept has been established over the last four years in a unique collaboration of 41 laboratories within a Horizon 2020 design study funded by the European Union. EuPRAXIA is the first European project that develops a dedicated particle accelerator research infrastructure based on novel plasma acceleration concepts and laser technology. It focuses on the development of electron accelerators and underlying technologies, their user communities, and the exploitation of existing accelerator infrastructures in Europe. EuPRAXIA has involved, amongst others, the international laser community and industry to build links and bridges with accelerator science — through realising synergies, identifying disruptive ideas, innovating, and fostering knowledge exchange. The Eu-PRAXIA project aims at the construction of an innovative electron accelerator using laser- and electron-beam-driven plasma wakefield acceleration that offers a significant reduction in size and possible savings in cost over current state-of-the-art radiofrequency-based accelerators. The foreseen electron energy range of one to five gigaelectronvolts (GeV) and its performance goals will enable versatile applications in various domains, e.g. as a compact free-electron laser (FEL), compact sources for medical imaging and positron generation, table-top test beams for particle detectors, as well as deeply penetrating X-ray and gamma-ray sources for material testing. EuPRAXIA is designed to be the required stepping stone to possible future plasma-based facilities, such as linear colliders at the high-energy physics (HEP) energy frontier. Consistent with a high-confidence approach, the project includes measures to retire risk by establishing scaled technology demonstrators. This report includes preliminary models for project implementation, cost and schedule that would allow operation of the full Eu-PRAXIA facility within 8—10 years.},
}

@article{Silva2024,
  title = {Health equity innovation in precision medicine: data stewardship and agency to expand representation in clinicogenomics},
  author = {Patrick J Silva and Vasiliki Rahimzadeh and Reid Powell and Junaid Husain and Scott Grossman and Adam Hansen and Jennifer Hinkel and Rafael Rosengarten and Marcia G Ory and Kenneth S Ramos},
  year = {2024},
  journal = {Health Research Policy and Systems},
  volume = {22},
  pages = {170},
  doi = {10.1186/s12961-024-01258-9},
  url = {https://doi.org/10.1186/s12961-024-01258-9},
  abstract = {Most forms of clinical research examine a very minute cross section of the patient journey. Much of the knowledge and evidence base driving current genomic medicine practice entails blind spots arising from underrepresentation and lack of research participation in clinicogenomic databases. The flaws are perpetuated in AI models and clinical practice guidelines that reflect the lack of diversity in data being used. Participation in clinical research and biobanks is impeded in many populations due to a variety of factors that include knowledge, trust, healthcare access, administrative barriers, and technology gaps. A recent symposium brought industry, clinical, and research participants in clinicogenomics to discuss practical challenges and potential for new data sharing models that are patient centric and federated in nature and can address health disparities that might be perpetuated by lack of diversity in clinicogenomic research, biobanks, and datasets. Clinical data governance was recognized as a multiagent problem, and governance practices need to be more patient centric to address most barriers. Digital tools that preserve privacy, document provenance, and enable the management of data as intellectual property have great promise. Policy updates realigning and rationalizing clinical data governance practices are warranted.},
}

@article{Tornimbene2025,
  title = {Harnessing the power of artificial intelligence for disease-surveillance purposes},
  author = {Barbara Tornimbene and Zoila Beatriz Leiva Rioja and John Brownstein and Adam Dunn and Sylvain Faye and Jude Kong and Nada Malou and Clara Nordon and Benjamin Rader and Oliver Morgan},
  year = {2025},
  journal = {BMC Proceedings},
  volume = {19},
  pages = {7},
  doi = {10.1186/s12919-025-00320-w},
  url = {https://doi.org/10.1186/s12919-025-00320-w},
  abstract = {The COVID-19 pandemic accelerated the development of AI-driven tools to improve public health surveillance and outbreak management. While AI programs have shown promise in disease surveillance, they also present issues such as data privacy, prejudice, and human-AI interactions. This sixth session of the of the WHO Pandemic and Epidemic Intelligence Innovation Forum examines the use of Artificial Intelligence (AI) in public health by collecting the experience of key global health organizations, such the Boston Children's Hospital, the Global South AI for Pandemic & Epidemic Preparedness & Response (AI4PEP) network, Medicines Sans Frontières (MSF), and the University of Sydney. AI's utility in clinical care, particularly in diagnostics, medication discovery, and data processing, has resulted in improvements that may also benefit public health surveillance. However, the use of AI in global health necessitates careful consideration of ethical issues, particularly those involving data use and algorithmic bias. As AI advances, particularly with large language models, public health officials must develop governance frameworks that stress openness, accountability, and fairness. These systems should address worldwide differences in data access and ensure that AI technologies are tailored to specific local needs. Ultimately, AI's ability to improve healthcare efficiency and equity is dependent on multidisciplinary collaboration, community involvement, and inclusive AI designs in ensuring equitable healthcare outcomes to fit the unique demands of global communities.},
}

@article{Bronzini2024,
  title = {Glitter or gold? Deriving structured insights from sustainability reports via large language models},
  author = {Marco Bronzini and Carlo Nicolini and Bruno Lepri and Andrea Passerini and Jacopo Staiano},
  year = {2024},
  journal = {EPJ Data Science},
  volume = {13},
  pages = {41},
  doi = {10.1140/epjds/s13688-024-00481-2},
  url = {https://doi.org/10.1140/epjds/s13688-024-00481-2},
  abstract = {Over the last decade, several regulatory bodies have started requiring the disclosure of non-financial information from publicly listed companies, in light of the investors’ increasing attention to Environmental, Social, and Governance (ESG) issues. Publicly released information on sustainability practices is often disclosed in diverse, unstructured, and multi-modal documentation. This poses a challenge in efficiently gathering and aligning the data into a unified framework to derive insights related to Corporate Social Responsibility (CSR). Thus, using Information Extraction (IE) methods becomes an intuitive choice for delivering insightful and actionable data to stakeholders. In this study, we employ Large Language Models (LLMs), In-Context Learning, and the Retrieval-Augmented Generation (RAG) paradigm to extract structured insights related to ESG aspects from companies’ sustainability reports. We then leverage graph-based representations to conduct statistical analyses concerning the extracted insights. These analyses revealed that ESG criteria cover a wide range of topics, exceeding 500, often beyond those considered in existing categorizations, and are addressed by companies through a variety of initiatives. Moreover, disclosure similarities emerged among companies from the same region or sector, validating ongoing hypotheses in the ESG literature. Lastly, by incorporating additional company attributes into our analyses, we investigated which factors impact the most on companies’ ESG ratings, showing that ESG disclosure affects the obtained ratings more than other financial or company data.},
}

@article{Loeffler2024,
  title = {Reinvent 4: Modern AI–driven generative molecule design},
  author = {Hannes H Loeffler and Jiazhen He and Alessandro Tibo and Jon Paul Janet and Alexey Voronov and Lewis H Mervin and Ola Engkvist},
  year = {2024},
  journal = {Journal of Cheminformatics},
  volume = {16},
  pages = {20},
  doi = {10.1186/s13321-024-00812-5},
  url = {https://doi.org/10.1186/s13321-024-00812-5},
  abstract = {REINVENT 4 is a modern open-source generative AI framework for the design of small molecules. The software utilizes recurrent neural networks and transformer architectures to drive molecule generation. These generators are seamlessly embedded within the general machine learning optimization algorithms, transfer learning, reinforcement learning and curriculum learning. REINVENT 4 enables and facilitates de novo design, R-group replacement, library design, linker design, scaffold hopping and molecule optimization. This contribution gives an overview of the software and describes its design. Algorithms and their applications are discussed in detail. REINVENT 4 is a command line tool which reads a user configuration in either TOML or JSON format. The aim of this release is to provide reference implementations for some of the most common algorithms in AI based molecule generation. An additional goal with the release is to create a framework for education and future innovation in AI based molecular design. The software is available from https://github.com/MolecularAI/REINVENT4and released under the permissive Apache 2.0 license. Scientific contribution. The software provides an open–source reference implementation for generative molecular design where the software is also being used in production to support in–house drug discovery projects. The publication of the most common machine learning algorithms in one code and full documentation thereof will increase transparency of AI and foster innovation, collaboration and education.},
}

@article{Chen2024_03,
  title = {Constructing and exploring neuroimaging projects: a survey from clinical practice to scientific research},
  author = {Ziyan Chen and Abraham Ayodeji Adegboro and Lan Gu and Xuejun Li},
  year = {2024},
  journal = {Insights into Imaging},
  volume = {15},
  pages = {272},
  doi = {10.1186/s13244-024-01848-9},
  url = {https://doi.org/10.1186/s13244-024-01848-9},
  abstract = {Over the past decades, numerous large-scale neuroimaging projects that involved the collection and release of multimodal data have been conducted globally. Distinguished initiatives such as the Human Connectome Project, UK Biobank, and Alzheimer’s Disease Neuroimaging Initiative, among others, stand as remarkable international collaborations that have significantly advanced our understanding of the brain. With the advancement of big data technology, changes in healthcare models, and continuous development in biomedical research, various types of large-scale projects are being established and promoted worldwide. For project leaders, there is a need to refer to common principles in project construction and management. Users must also adhere strictly to rules and guidelines, ensuring data safety and privacy protection. Organizations must maintain data integrity, protect individual privacy, and foster stakeholders’ trust. Regular updates to legislation and policies are necessary to keep pace with evolving technologies and emerging data-related challenges.},
}

@article{Unger2024,
  title = {Deep learning in cancer genomics and histopathology},
  author = {Michaela Unger and Jakob Nikolas Kather},
  year = {2024},
  journal = {Genome Medicine},
  volume = {16},
  pages = {44},
  doi = {10.1186/s13073-024-01315-6},
  url = {https://doi.org/10.1186/s13073-024-01315-6},
  abstract = {Histopathology and genomic profiling are cornerstones of precision oncology and are routinely obtained for patients with cancer. Traditionally, histopathology slides are manually reviewed by highly trained pathologists. Genomic data, on the other hand, is evaluated by engineered computational pipelines. In both applications, the advent of modern artificial intelligence methods, specifically machine learning (ML) and deep learning (DL), have opened up a fundamentally new way of extracting actionable insights from raw data, which could augment and potentially replace some aspects of traditional evaluation workflows. In this review, we summarize current and emerging applications of DL in histopathology and genomics, including basic diagnostic as well as advanced prognostic tasks. Based on a growing body of evidence, we suggest that DL could be the groundwork for a new kind of workflow in oncology and cancer research. However, we also point out that DL models can have biases and other flaws that users in healthcare and research need to know about, and we propose ways to address them.},
}

@article{Yang2025_01,
  title = {Build the virtual cell with artificial intelligence: a perspective for cancer research},
  author = {Tao Yang and Yuan-Yi Wang and Fei Ma and Bing-He Xu and Hai-Li Qian},
  year = {2025},
  journal = {Military Medical Research},
  volume = {12},
  pages = {4},
  doi = {10.1186/s40779-025-00591-6},
  url = {https://doi.org/10.1186/s40779-025-00591-6},
}

@article{Smith-Mutegi2025,
  title = {Perceptions of STEM education and artificial intelligence: a Twitter (X) sentiment analysis},
  author = {Demetrice Smith-Mutegi and Yoseph Mamo and Jinhee Kim and Helen Crompton and Matthew McConnell},
  year = {2025},
  journal = {International Journal of STEM Education},
  volume = {12},
  pages = {9},
  doi = {10.1186/s40594-025-00527-5},
  url = {https://doi.org/10.1186/s40594-025-00527-5},
  abstract = {Artificial intelligence (AI) is becoming increasingly prevalent in science, technology, engineering, and mathematics (STEM) education, holding promising potential for supporting the design and implementation of quality STEM education. However, there is a lack of data-based research studying the diverse perceptions of AI in STEM education as conveyed on social media, the factors that influence those perceptions, or the change in those perceptions over time among public audiences.},
}

@article{Brown2025,
  title = {A scoping study: crime and connected and autonomous vehicles},
  author = {Ashley Brown and Shane D Johnson and Nilufer Tuptuk},
  year = {2025},
  journal = {Crime Science},
  volume = {14},
  pages = {2},
  doi = {10.1186/s40163-025-00245-x},
  url = {https://doi.org/10.1186/s40163-025-00245-x},
  abstract = {Connected and Autonomous Vehicles (CAVs) integrate advanced communication and autonomous driving technologies, enabling them to operate independently or with minimal human intervention. Despite the anticipated benefits for transportation, CAVs could be vulnerable to a wide variety of crimes unless security and crime prevention measures are proactively integrated into the technologies enabling their operation. To understand the potential crime threats, an extensive scoping review was conducted, covering incidents reported in the news and media, along with academic articles from crime and cybersecurity research. A total of 70 news articles related to crime incidents were identified, along with 12 academic articles on crimes. In addition, the findings from 35 survey papers addressing security attacks against CAVs, along with 29 additional papers covering security attacks not addressed in those surveys were synthesised. A total of 22 crime threats were identified. A 2-day workshop with experts was then held to present the findings from the review, conduct a crime scenario development exercise to identify any crime threats that were not identified in the review, and to assess the identified crimes. During the workshop, experts generated 6 new crime scenarios, bringing the total to 28 crime threats. To identify and prioritise high-risk crimes for future work, the experts were then asked to rate the threats based on their anticipated harm, achievability, frequency, and defeat-ability. The crime threats with the highest risk ratings included illegal transportation, vehicle theft, vehicle part theft, ransom for financial gain, and vandalism. The implications of our findings for research, policy and practice are discussed.},
}

@article{Gonzales2023,
  title = {Implications of AI innovation on economic growth: a panel data study},
  author = {Julius Tan Gonzales},
  year = {2023},
  journal = {Journal of Economic Structures},
  volume = {12},
  pages = {13},
  doi = {10.1186/s40008-023-00307-w},
  url = {https://doi.org/10.1186/s40008-023-00307-w},
  abstract = {The application of artificial intelligence (AI) across firms and industries warrants a line of research focused on determining its overall effect on economic variables. As a general-purpose technology (GPT), for example, AI helps in the production, marketing, and customer acquisition of firms, increasing their productivity and consumer reach. Aside from these, other effects of AI include enhanced quality of services, improved work accuracy and efficiency, and increased customer satisfaction. Hence, this study aims to gauge the impact of AI on the economy, specifically on long-run economic growth. This study conjectures a positive relationship between AI and economic growth. To test this hypothesis, this study makes use of a panel dataset of countries from 1970 to 2019, and the number of AI patents as a measure of AI. A text search query is performed to distinguish AI patents from other types of innovations in a public database. Employing fixed effects and generalized method of moments (GMM) estimation, this paper finds a positive relationship between AI and economic growth, which is higher than the effect of the total population of patents on growth. Furthermore, other results indicate that AI’s influence on growth is more robust among advanced economies, and more evident towards the latter periods of the dataset.},
}

@article{Tan2025_01,
  title = {Digital evolution: Novo Nordisk’s shift to ontology-based data management},
  author = {Shawn Zheng Kai Tan and Shounak Baksi and Thomas Gade Bjerregaard and Preethi Elangovan and Thrishna Kuttikattu Gopalakrishnan and Darko Hric and Joffrey Joumaa and Beidi Li and Kashif Rabbani and Santhosh Kannan Venkatesan and Joshua Daniel Valdez and Saritha Vettikunnel Kuriakose},
  year = {2025},
  journal = {Journal of Biomedical Semantics},
  volume = {16},
  pages = {6},
  doi = {10.1186/s13326-025-00327-4},
  url = {https://doi.org/10.1186/s13326-025-00327-4},
  abstract = {The amount of biomedical data is growing, and managing it is increasingly challenging. While Findable, Accessible, Interoperable and Reusable (FAIR) data principles provide guidance, their adoption has proven difficult, especially in larger enterprises like pharmaceutical companies. In this manuscript, we describe how we leverage an Ontology-Based Data Management (OBDM) strategy for digital transformation in Novo Nordisk Research & Early Development. Here, we include both our technical blueprint and our approach for organizational change management. We further discuss how such an OBDM ecosystem plays a pivotal role in the organization’s digital aspirations for data federation and discovery fuelled by artificial intelligence. Our aim for this paper is to share the lessons learned in order to foster dialogue with parties navigating similar waters while collectively advancing the efforts in the fields of data management, semantics and data driven drug discovery.},
}

@article{Howard2022,
  title = {Application of energy informatics in Danish research projects},
  author = {Daniel Anthony Howard and Zheng Ma and Bo Nørregaard Jørgensen},
  year = {2022},
  journal = {Energy Informatics},
  volume = {5},
  pages = {57},
  doi = {10.1186/s42162-022-00245-0},
  url = {https://doi.org/10.1186/s42162-022-00245-0},
  abstract = {To enable sound scientific research in future energy informatics projects, it is necessary to obtain an overview of the current state of the research field to identify unaddressed gaps and challenges. Therefore, this paper aims to investigate the research trends and achievements within energy informatics in a Danish context within the last three decades. This paper reviews 207 energy informatics projects collected until the second quarter of 2022. Quantitative analysis results show that most projects have focused on applying energy informatics through energy-aware control of end-user consumption. The qualitative review finds an emphasis on data usage and end-users which aligns with the quantitative review. Furthermore, it tends to focus on specific end-users, e.g., buildings and heat pumps. Four overall recommendations are established: (I) Increased emphasis on research for sector coupling to aid in unlocking energy system flexibility, (II) project data value chain output, focusing on structuring and managing the data to make it applicable for future re-use, (III) utilizing industrial loads and incorporating an end-user perspective, (IV) inclusion of research institutions for the improved overall output of the projects through interdisciplinary solutions.},
}

@article{Denkhaus2024,
  title = {Vom E-Government zur Digitalisierung},
  author = {Wolfgang Denkhaus and Benedikt Falkhofen},
  year = {2024},
  pages = {49-106},
  doi = {10.37307/b.978-3-503-23763-0.01},
  publisher = {Erich Schmidt Verlag GmbH & Co. KG},
  url = {https://doi.org/10.37307/b.978-3-503-23763-0.01},
  abstract = {Der digitale Wandel und seine Folgen zählen zu den aktuellen „Megathemen“ in Politik und Medien, Wirtschaft, Wissenschaft und Verwaltung.1 Mit dem Topos der „Digitalisierung“ werden Transformationsprozesse beschrieben, die Wirtschaft und Gesellschaft tiefgreifend verändern. Digitalisierung steht, so lauten gängige Befunde, für „den bedeutendsten technologischen Aufbruch seit dem Beginn der Industrialisierung Ende des 18. Jahrhunderts“.2},
}

@article{Zhao2024_02,
  title = {Artificial intelligence for geoscience: Progress, challenges, and perspectives},
  author = {Tianjie Zhao and Sheng Wang and Chaojun Ouyang and Min Chen and Chenying Liu and Jin Zhang and Long Yu and Fei Wang and Yong Xie and Jun Li and Fang Wang and Sabine Grunwald and Bryan M. Wong and Fan Zhang and Zhen Qian and Yongjun Xu and Chengqing Yu and Wei Han and Tao Sun and Zezhi Shao and Tangwen Qian and Zhao Chen and Jiangyuan Zeng and Huai Zhang and Husi Letu and Bing Zhang and Li Wang and Lei Luo and Chong Shi and Hongjun Su and Hongsheng Zhang and Shuai Yin and Ni Huang and Wei Zhao and Nan Li and Chaolei Zheng and Yang Zhou and Changping Huang and Defeng Feng and Qingsong Xu and Yan Wu and Danfeng Hong and Zhenyu Wang and Yinyi Lin and Tangtang Zhang and Prashant Kumar and Antonio Plaza and Jocelyn Chanussot and Jiabao Zhang and Jiancheng Shi and Lizhe Wang},
  year = {2024},
  journal = {The Innovation},
  volume = {5},
  pages = {100691},
  doi = {https://doi.org/10.1016/j.xinn.2024.100691},
  url = {https://www.sciencedirect.com/science/article/pii/S2666675824001292},
  abstract = {This paper explores the evolution of geoscientific inquiry, tracing the progression from traditional physics-based models to modern data-driven approaches facilitated by significant advancements in artificial intelligence (AI) and data collection techniques. Traditional models, which are grounded in physical and numerical frameworks, provide robust explanations by explicitly reconstructing underlying physical processes. However, their limitations in comprehensively capturing Earth’s complexities and uncertainties pose challenges in optimization and real-world applicability. In contrast, contemporary data-driven models, particularly those utilizing machine learning (ML) and deep learning (DL), leverage extensive geoscience data to glean insights without requiring exhaustive theoretical knowledge. ML techniques have shown promise in addressing Earth science-related questions. Nevertheless, challenges such as data scarcity, computational demands, data privacy concerns, and the “black-box” nature of AI models hinder their seamless integration into geoscience. The integration of physics-based and data-driven methodologies into hybrid models presents an alternative paradigm. These models, which incorporate domain knowledge to guide AI methodologies, demonstrate enhanced efficiency and performance with reduced training data requirements. This review provides a comprehensive overview of geoscientific research paradigms, emphasizing untapped opportunities at the intersection of advanced AI techniques and geoscience. It examines major methodologies, showcases advances in large-scale models, and discusses the challenges and prospects that will shape the future landscape of AI in geoscience. The paper outlines a dynamic field ripe with possibilities, poised to unlock new understandings of Earth’s complexities and further advance geoscience exploration.},
}

@article{Huang2025,
  title = {Patent litigation and narrative R&D disclosures: Evidence from the adoption of anti-troll legislation},
  author = {Rui Huang and Jeong-Bon Kim and Louise Yi Lu and Dongyue Wang and Yangxin Yu},
  year = {2025},
  journal = {Research Policy},
  volume = {54},
  pages = {105127},
  doi = {https://doi.org/10.1016/j.respol.2024.105127},
  url = {https://www.sciencedirect.com/science/article/pii/S0048733324001768},
  abstract = {The last two decades have witnessed a sharp increase in patent litigation in the United States (U.S.), mainly driven by patent trolls. By exploiting the staggered adoption of Anti-Troll laws across 34 states as a plausible exogenous shock that reduces the risk of patent litigation by these trolls, we show that firms significantly increase their narrative R&D disclosures following the enactment of Anti-Troll laws. This effect is less pronounced in firms facing higher competitive pressure, and more pronounced in firms that are more exposed to threats from patent trolls. Further analyses alleviate the concern that the impact of Anti-Troll laws on disclosures is attributable to state-level economic or policy changes. Our results highlight the significant role of patent troll litigation in influencing the dissemination of narrative R&D information.},
}

@article{Balboni2013,
  title = {Privacy by design and anonymisation techniques in action: Case study of Ma3tch technology},
  author = {Paolo Balboni and Milda Macenaite},
  year = {2013},
  journal = {Computer Law & Security Review},
  volume = {29},
  pages = {330-340},
  doi = {https://doi.org/10.1016/j.clsr.2013.05.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0267364913000964},
  abstract = {Privacy by Design is now enjoying widespread acceptance. The EU has recently expressly included it as one of the key principles in the revised data protection legal framework. But how does Privacy by design and data anonymisation work in practise? In this article the authors address this question from a practical point of view by analysing a case study on EU Financial Intelligence Units (“FIUs”) using the Ma3tch technology as additional feature to the existing exchange of information via FIU.NET decentralised computer network. They present, analyse, and evaluate Ma3tch technology from the perspective of personal data protection. The authors conclude that Ma3tch technology can be seen as a valuable example of Privacy by Design. It achieves data anonymisation and enhances data minimisation and data security, which are the fundamental elements of Privacy by Design. Therefore, it may not only improve the exchange of information among FIUs and allow for the data processing to be in line with applicable data protection requirements, but it may also substantially contribute to the protection of privacy of related data subjects. At the same time, the case study clearly shows that Privacy by Design needs to be supported and complemented by appropriate organisational and technical procedures to assure that the technology solutions devised to protect privacy would in fact do so.},
}

@article{Aboukadri2024,
  title = {Machine learning in identity and access management systems: Survey and deep dive},
  author = {Sara Aboukadri and Aafaf Ouaddah and Abdellatif Mezrioui},
  year = {2024},
  journal = {Computers & Security},
  volume = {139},
  pages = {103729},
  doi = {https://doi.org/10.1016/j.cose.2024.103729},
  url = {https://www.sciencedirect.com/science/article/pii/S0167404824000300},
  abstract = {The evolution of identity and access management (IAM) has been driven by the expansion of online services, cloud computing, and the Internet of Things (IoT). The proliferation of remote work, mobile applications, and interconnected devices has intensified the demand for robust identity protection and access control. As digital interactions and data sharing become more prevalent across industries, IAM has gained prominence, compelled by the need to safeguard sensitive information, prevent unauthorized access, and adhere to increasingly stringent regulatory frameworks. In parallel with IAM's evolution, the integration of artificial intelligence (AI) has emerged as a pivotal avenue for enhancing IAM effectiveness. This survey delves into the fusion of machine learning (ML) techniques to fortify IAM, with a specific focus on its core processes: authentication, authorization, and auditing. Addressing fundamental questions regarding ML's role in enhancing IAM processes, we begin by proposing a comprehensive definition of IAM within a unified layered-wise reference model, highlighting Authentication, Authorization, and Auditing functions (with focus on monitoring). Furthermore, our survey comprehensively explores ML-based solutions within IAM systems, presenting a taxonomy of state-of-the-art methodologies categorized by their application in IAM processes. Drawing from both qualitative and quantitative insights from cited references, we investigate how ML enhances the performance and security of IAM processes. Additionally, by investigating challenges in implementing ML in IAM systems, we shed light on issues such as data privacy concerns and the interpretability of ML-driven decisions. In conclusion, this paper makes a substantial contribution to the IAM landscape by providing comprehensive insights into the transformative role of ML. Addressing pivotal questions, our survey offers a roadmap to leverage ML's potential for enhancing the performance, security, and efficacy of IAM systems.},
}

@article{Betz2023,
  title = {Game changers in science and technology - now and beyond},
  author = {Ulrich A.K. Betz and Loukik Arora and Reem A. Assal and Hatylas Azevedo and Jeremy Baldwin and Michael S. Becker and Stefan Bostock and Vinton Cheng and Tobias Egle and Nicola Ferrari and Elena K. Schneider-Futschik and Stefan Gerhardy and Alexandra Hammes and Achim Harzheim and Thomas Herget and Cristina Jauset and Simon Kretschmer and Corey Lammie and Nina Kloss and Steve Marquis Fernandes and Claudia-Gabriela Mitrofan and Iuliia Myrgorodska and Daniela Nedbalek and Siegfried G. Neumann and Stella Paffenholz and Laia Pascual Ponce and Birgit Rogell and Dragana Savic and Gergana Velikova and Christian Schumacher and Nina Weisshaar and Mohammadzadeh Yahya and Joshua Y.C. Yang and Guoping Zhao},
  year = {2023},
  journal = {Technological Forecasting and Social Change},
  volume = {193},
  pages = {122588},
  doi = {https://doi.org/10.1016/j.techfore.2023.122588},
  url = {https://www.sciencedirect.com/science/article/pii/S0040162523002731},
  abstract = {The recent devastating pandemic has drastically reminded humanity of the importance of constant scientific and technological progress. A strong interdisciplinary dialogue between academic and industrial scientists of various specialties, entrepreneurs, managers and the public is paramount in triggering new breakthrough ideas which often emerge at the interface of disciplines. The following sections, compiled by a highly diverse group of authors, are summarizing recently achieved game-changing leaps in science and technology. The game-changers range from paradigm shifts in scientific theories to make impact over several decades to game-changers that have the potential to change our everyday lives tomorrow. The paper is an interdisciplinary dialogue of relevance for academic interdisciplinary thinkers, large corporations' strategic planners, and top executives alike; it provides a glimpse into what further breakthroughs the future may hold and thereby intends to spark new ideas with its readers.},
}

@article{Ouhssini2024,
  title = {Advancements in detecting, preventing, and mitigating DDoS attacks in cloud environments: A comprehensive systematic review of state-of-the-art approaches},
  author = {Mohamed Ouhssini and Karim Afdel and Mohamed Akouhar and Elhafed Agherrabi and Abdallah Abarda},
  year = {2024},
  journal = {Egyptian Informatics Journal},
  volume = {27},
  pages = {100517},
  doi = {https://doi.org/10.1016/j.eij.2024.100517},
  url = {https://www.sciencedirect.com/science/article/pii/S111086652400080X},
  abstract = {This comprehensive study examines cutting-edge strategies for combating Distributed Denial of Service (DDoS) attacks in cloud environments, addressing a critical gap in recent literature. Through a systematic review of the latest advancements, we propose a framework for identifying, preventing, and mitigating DDoS threats specifically tailored to cloud infrastructures. Our research highlights the urgent need for robust defense mechanisms to enhance cloud security, minimize service disruptions, and safeguard against data breaches. By analyzing the strengths and limitations of current models, we underscore the importance of continued innovation in this rapidly evolving field. This study provides essential insights for academics and industry professionals aiming to enhance the resilience of cloud infrastructure against the ongoing and adaptive menace of DDoS attacks.},
}

@article{Sahih2024,
  title = {Blockchain-enabled solutions for fair and efficient peer-to-peer renewable energy trading: An experimental comparison},
  author = {Amin Zakhirehkar Sahih and Alireza Abbasi and Milad Ghasri},
  year = {2024},
  journal = {Journal of Cleaner Production},
  volume = {455},
  pages = {142301},
  doi = {https://doi.org/10.1016/j.jclepro.2024.142301},
  url = {https://www.sciencedirect.com/science/article/pii/S0959652624017499},
  abstract = {Peer-to-peer (P2P) renewable energy trading, facilitated by designing P2P market smart contracts on blockchain servers, is a promising approach to increase investments in cleaner energy generation. To enhance trading efficiency and pricing fairness, as the major challenges of P2P market designs, this study introduces two new market mechanisms, Hybrid Auction Coalition (HAC), and innovative coalition business model (ICBM), respectively. The performance of these market mechanisms is contrasted with existing mechanisms from the literature with respect to electricity bills, market efficiency, fairness, and blockchain feasibility using a comprehensive list of indicators including costs and profits, fairness, market efficiency, and technical viability. Compared to traditional billing, ICBM and HAC increase sellers’ profit by 88% and 66% respectively, while both impose 13% more costs on buyers. ICBM and HAC also set the fairest prices compared to the existing markets. ICBM is shown to improve blockchain feasibility and market efficiency due to lighter on-chain calculations, and absolute clearing mechanisms. The results also demonstrate that HAC outperforms standalone auctions in every aspect which endorses the hybridization benefits.},
}

@article{Jiao2025,
  title = {Developing a healthy food access index (HFAI): Web-based mapping and future directions for AI integrations},
  author = {Junfeng Jiao and Kijin Seong and Marcus Sammer and Ryan Hardesty Lewis and Alison Reese and Norma Olvera and Susie L. Gronseth and Elizabeth Anderson-Fletcher and Ioannis Kakadiaris},
  year = {2025},
  journal = {Cities},
  volume = {161},
  pages = {105908},
  doi = {https://doi.org/10.1016/j.cities.2025.105908},
  url = {https://www.sciencedirect.com/science/article/pii/S0264275125002082},
  abstract = {Food insecurity remains a pressing issue in urban communities, particularly in underserved areas with limited access to healthy and affordable food. This study introduces a comprehensive approach to assessing and addressing food access challenges in the Greater Houston region by developing a Healthy Food Access Index (HFAI). The HFAI incorporates a wide range of data sources, including economic and sociocultural deprivation indices, retail food environment measures, physical access indicators, and accommodation access data, that offer a multidimensional understanding of food access. Using GIS-based mapping and hot spot analysis, we identify areas of critical need, highlighting significant disparities in food access and health across the region. We also present the web-based platform, “AI-FEED,” designed to support community leaders and urban planners by visualizing food access challenges and providing AI-driven suggestions for optimizing food pantry placements, incentivizing healthy options at dollar stores, and improving public transit routes. This platform integrates real-time data, offering dynamic and proactive solutions to enhance food security and improve public health outcomes. Our study introduces the importance of targeted, data-driven interventions in addressing food insecurity and demonstrates the potential of AI and digital platforms in supporting sustainable urban food systems.},
}

@article{Bachmann2024,
  title = {Climate-resilient strategy planning using the SWOT methodology: A case study of the Japanese wind energy sector},
  author = {Lisa Bachmann and Ricarda Lex and Florian Regli and Saira Vögeli and Evelyn Mühlhofer and Jamie W. McCaughey and Susanne Hanger-Kopp and David N. Bresch and Chahan M. Kropf},
  year = {2024},
  journal = {Climate Risk Management},
  volume = {46},
  pages = {100665},
  doi = {https://doi.org/10.1016/j.crm.2024.100665},
  url = {https://www.sciencedirect.com/science/article/pii/S2212096324000822},
  abstract = {As climate change leads to more frequent and intense extreme weather events, industry stakeholders and policymakers must assess their business strategies, practices, and entire sector policies under these uncertain conditions. Much recent research has integrated quantitative climate risk modeling into frameworks to engage policymakers and inform adaptation decisions in a general way, but relatively little attention has been devoted to extending this to strategic business and investment decisions. This falls short of identifying economic opportunities and threats in a wider socio-economic context, such as the development of new technologies or evolving political and regulatory environments. Here, a methodology is developed to integrate quantitative climate risk modeling with SWOT analysis (strengths, weaknesses, opportunities, and threats) which is commonly used in business and investment strategic planning. This moves the focus from avoidance of negative outcomes to prospective planning in an evolving environment. This methodology is illustrated with a case study of the Japanese wind energy sector, using open-access data and the open-source climate risk-assessment platform CLIMADA. This Climate risk assessment indicates threats from increasing damages to the wind energy infrastructure, as well as the profitability of typhoon-resistant wind turbines under present and future climate. Expert interviews and extensive literature research on opportunities and threats, however, also show that the transition towards renewable energies faces restrictive market dynamics, political and social hurdles, which set external conditions surpassing physically-informed dimensions. Beyond this illustrative case study, the methodology developed here bridges established concepts in climate risk modeling and strategic management and thus can be used to identify industry-centric ways forward for climate-resilient planning across a wide range of economic sectors.},
}

@article{Sanchez}2024,
  title = {Shared autonomous micro-mobility for walkable cities},
  author = {Naroa {Coretti Sanchez} and Kent Larson},
  year = {2024},
  journal = {Transportation Research Interdisciplinary Perspectives},
  volume = {27},
  pages = {101236},
  doi = {https://doi.org/10.1016/j.trip.2024.101236},
  url = {https://www.sciencedirect.com/science/article/pii/S2590198224002227},
  abstract = {The integration of autonomy into micro-mobility systems has given rise to a new solution known as shared autonomous micro-mobility (SAmM)11SAmM = Shared autonomous micro-mobility.. This paper delves into the opportunities that emerge from this concept, which is driven by the resurgence of walkable, human-centric cities and the advancements in autonomous vehicles. While most autonomous vehicle research has focused on cars and taxis, SAmM proposes more lightweight and environmentally friendly alternatives that align with the vision of more human-centered, walkable cities. At the same time, the integration of autonomy into micro-mobility systems holds the potential to enhance efficiency by reducing fleet sizes and increase convenience by transforming them into on-demand mobility services. This study presents a roadmap for the successful implementation of SAmM, covering essential aspects such as vehicle design, infrastructure requirements, and operational strategies. This research sheds light on the transformative potential of SAmM and provides insights for mobility operators, urban planners, policymakers, and researchers.},
}

@article{Kopiika2025,
  title = {Rapid post-disaster infrastructure damage characterisation using remote sensing and deep learning technologies: A tiered approach},
  author = {Nadiia Kopiika and Andreas Karavias and Pavlos Krassakis and Zehao Ye and Jelena Ninic and Nataliya Shakhovska and Sotirios Argyroudis and Stergios-Aristoteles Mitoulis},
  year = {2025},
  journal = {Automation in Construction},
  volume = {170},
  pages = {105955},
  doi = {https://doi.org/10.1016/j.autcon.2024.105955},
  url = {https://www.sciencedirect.com/science/article/pii/S0926580524006915},
  abstract = {Critical infrastructure is vital for connectivity and economic growth but faces systemic threats from human-induced damage, climate change and natural disasters. Rapid, multi-scale damage assessments are essential, yet integrated, automated methodologies remain underdeveloped. This paper presents a multi-scale tiered approach, which addresses this gap, by demonstrating how automated damage characterisation can be achieved using digital technologies. The methodology is then applied and validated through a case study in Ukraine involving 17 bridges damaged by targeted human interventions. Technology is deployed across regional to component scales, integrating assessments using Sentinel-1 SAR images, crowdsourced data, and high-resolution images for deep learning to enable automatic damage detection and characterisation. The interferometric coherence difference and semantic segmentation of images are utilised in a tiered multi-scale approach to enhance the reliability of damage characterisation at various scales. This integrated methodology automates and accelerates decision-making, facilitating more efficient restoration and adaptation efforts and ultimately enhancing infrastructure resilience.},
}

@article{Parambil2024,
  title = {Integrating AI-based and conventional cybersecurity measures into online higher education settings: Challenges, opportunities, and prospects},
  author = {Medha Mohan Ambali Parambil and Jaloliddin Rustamov and Soha Galalaldin Ahmed and Zahiriddin Rustamov and Ali Ismail Awad and Nazar Zaki and Fady Alnajjar},
  year = {2024},
  journal = {Computers and Education: Artificial Intelligence},
  volume = {7},
  pages = {100327},
  doi = {https://doi.org/10.1016/j.caeai.2024.100327},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X24001309},
  abstract = {The rapid adoption of online learning in higher education has resulted in significant cybersecurity challenges. As educational institutions increasingly rely on digital platforms, they are facing cyber threats that can compromise sensitive data and disrupt operations. This systematic literature review explores the integration of artificial intelligence (AI) into traditional methods to address cybersecurity risks in online higher education. The review integrates a qualitative synthesis of relevant literature and a quantitative meta-analysis using PRISMA guidelines, ensuring comprehensive insights into the integration process. The most prevalent cybersecurity threats are examined, and the effectiveness of AI-based and conventional approaches in mitigating these challenges is compared. Additionally, the most effective AI techniques in cybersecurity solutions are analyzed, and their performance is compared across different contexts. Furthermore, the review considers the key ethical and technical considerations associated with integrating AI into traditional cybersecurity methods. The findings reveal that while AI-based techniques offer promising solutions for threat detection, authentication, and privacy preservation, their successful implementation requires careful consideration of data privacy, fairness, transparency, and robustness. The importance of interdisciplinary collaboration, continuous monitoring of AI models—by automated systems and humans—and the need for comprehensive guidelines to ensure responsible and ethical use of AI in cybersecurity are highlighted. The findings of this review provide actionable insights for educational institutions, educators, and students, helping to facilitate the development of secure and resilient online learning environments. The identified ethical and technical considerations can serve as a foundation for the responsible integration of AI into cybersecurity within the online higher-education sector.},
}

@article{Rodríguez-Cuenca2025,
  title = {Television usage recommendations for energy efficiency: A probabilistic methodology based on the Wasserstein distance},
  author = {Francisco Rodríguez-Cuenca and Eugenio Francisco Sánchez-Úbeda and José Portela and Antonio Muñoz and Víctor Guizien and Andrea Veiga Santiago and Alicia Mateo González},
  year = {2025},
  journal = {Energy},
  volume = {322},
  pages = {135410},
  doi = {https://doi.org/10.1016/j.energy.2025.135410},
  url = {https://www.sciencedirect.com/science/article/pii/S0360544225010527},
  abstract = {This paper presents a general and interpretable methodology for delivering personalized energy-saving recommendations to household televisions. TVs, though often overlooked, account for 7% of household energy consumption, ranking as the fourth most costly category. The methodology extracts five easy-to-understand scalar features from historical TV energy consumption data, each representing a key usage aspect: OFF consumption, ON consumption, Daily Consumption, Session Duration, and Schedule of Consumption. It then employs a probabilistic approach based on the Wasserstein Distance to compare these features across TVs. Based on this comparison, two methods—percentage and elbow— are introduced for identifying TVs with significant deviations by feature, accompanied by tailored recommendations. The methodology is applied to case studies in Spain (RC4ALL project) and the UK (REFIT dataset), with results compared. The percentage method flags 60% of TVs (15 in RC4ALL, 12 in REFIT), while the elbow method flags 56% (14 TVs) in RC4ALL and 40% (8 TVs) in REFIT. Selected TVs in RC4ALL show greater deviations, with ON power 2.5 times and OFF power 16 times above normal, compared to 2 and 7 times in REFIT. TVs’ extended daily usage and long sessions raise health concerns. This methodology can also be applied to devices beyond TVs.},
}

@article{Brenna2024,
  title = {Assessing the effectiveness of “River Morphodynamic Corridors” for flood hazard mapping},
  author = {Andrea Brenna and Giacomo Poletto and Nicola Surian},
  year = {2024},
  journal = {Geomorphology},
  volume = {467},
  pages = {109460},
  doi = {https://doi.org/10.1016/j.geomorph.2024.109460},
  url = {https://www.sciencedirect.com/science/article/pii/S0169555X24004124},
  abstract = {In dynamic rivers the assessment of flood hazard related to geomorphological dynamics assumes crucial importance. Recently, some geomorphological approaches have been developed to assess channel dynamics in response to floods. In this study we explore the River Morphodynamic Corridors which include the current active channel and areas of the alluvial plain that may be (re)activated by channel dynamics during floods. In order to assess the effectiveness of such corridors for mapping flood hazard, we delineated the morphodynamic corridors along a river network of 74 km in the Cordevole River catchment (Italy) and compared the corridors with the channel changes that were triggered by a high-magnitude event (Vaia Storm) that hit the study area in October 2018. We observed that the morphodynamic corridors are capable to define areas where channel dynamics are most likely to occur and have proven satisfactorily effective in predicting channel widenings triggered by a severe flood. The results show that it is crucial to adopt different procedures for corridor delineation based on channel width, i.e., for small streams (width < 30 m) a more precautionary approach should be adopted that considers the entire alluvial plain as an area potentially affected by channel dynamics. The study highlighted an inherent limitation of the approach, since erosion that can affect valley slopes or fluvial terraces (i.e., widening of the alluvial plain) is not taken into account. This application represents the first validation of the River Morphodynamic Corridors approach through comparison with observed flood channel changes. Through this analysis, it can be inferred that, notwithstanding its relatively simple procedure, the approach allows a robust mapping and delineation of flood hazard due to river channel dynamics. The River Morphodynamic Corridors, when applied jointly with hydraulic model for assessing inundation processes, allow an overall assessment of flood hazard, particularly in dynamic river contexts.},
}

@article{Heo2024,
  title = {Early life air pollution exposures and thyroid function in children: A prospective cohort study},
  author = {You Joung Heo and Yun Jeong Lee and Soon Tae Kim and Dong Wook Lee and Johanna Inhyang Kim and Bung Nyun Kim and Yun Chul Hong and Choong Ho Shin and Young Ah Lee and Youn Hee Lim},
  year = {2024},
  journal = {Environmental Pollution},
  volume = {363},
  pages = {125092},
  doi = {https://doi.org/10.1016/j.envpol.2024.125092},
  url = {https://www.sciencedirect.com/science/article/pii/S0269749124018098},
  abstract = {Studies on early life ambient air pollution exposures and childhood thyroid function are scarce. This study aimed to evaluate the relationships between early life fine particulate matter (≤2.5 μm; PM2.5) and nitrogen dioxide (NO2) exposures and thyroid function in children. We measured the levels of thyrotropin, triiodothyronine, and free thyroxine in children (n = 684) residing in a rural Korean area at age 2, 4, 6, or 8 years from 2012 to 2020 in the Environment and Development of Children cohort. The relationship between residential average exposure levels of PM2.5 and NO2 during pregnancy and 1-year average levels before visit and thyroid function during childhood were analyzed. Inverse association between increases of 10 μg/m3 in PM2.5 during the first trimester and thyrotropin levels at aged 4 (β, −0.12; 95% CI: −0.22, −0.02) and 6 years (β, −0.16; 95% CI: −0.26, −0.06) were observed. No association was found between PM2.5 exposure during the second and third trimester and childhood TSH levels. Childhood PM2.5 exposure was positively associated with thyrotropin rise at aged 4 (β, 0.2; 95% CI: 0.06, 0.35) and 6 years (β, 0.16; 95% CI: 0.02, 0.29) and inversely related with free thyroxine levels at aged 8 years (β, −0.04; 95% CI: −0.07, −0.01). No relationship between NO2 exposure and thyroid function was found. In conclusion, association between PM2.5 exposure and childhood thyrotropin levels varied depending on exposure timing. Early gestational exposure showed an inverse relationship, whereas childhood exposure were positively associated with childhood thyrotropin levels. The long-term effects of early life air pollution exposure and underlying mechanisms should be investigated in future studies.},
}

@article{Wang2024_06,
  title = {EarthVQANet: Multi-task visual question answering for remote sensing image understanding},
  author = {Junjue Wang and Ailong Ma and Zihang Chen and Zhuo Zheng and Yuting Wan and Liangpei Zhang and Yanfei Zhong},
  year = {2024},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {212},
  pages = {422-439},
  doi = {https://doi.org/10.1016/j.isprsjprs.2024.05.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0924271624001990},
  abstract = {Monitoring and managing Earth’s surface resources is critical to human settlements, encompassing essential tasks such as city planning, disaster assessment, etc. To accurately recognize the categories and locations of geographical objects and reason about their spatial or semantic relations , we propose a multi-task framework named EarthVQANet, which jointly addresses segmentation and visual question answering (VQA) tasks. EarthVQANet contains a hierarchical pyramid network for segmentation and semantic-guided attention for VQA, in which the segmentation network aims to generate pixel-level visual features and high-level object semantics, and semantic-guided attention performs effective interactions between visual features and language features for relational modeling. For accurate relational reasoning, we design an adaptive numerical loss that incorporates distance sensitivity for counting questions and mines hard-easy samples for classification questions, balancing the optimization. Experimental results on the EarthVQA dataset (city planning for Wuhan, Changzhou, and Nanjing in China), RSVQA dataset (basic statistics for general objects), and FloodNet dataset (disaster assessment for Texas in America attacked by Hurricane Harvey) show that EarthVQANet surpasses 11 general and remote sensing VQA methods. EarthVQANet simultaneously achieves segmentation and reasoning, providing a solid benchmark for various remote sensing applications. Data is available at http://rsidea.whu.edu.cn/EarthVQA.htm},
}

@article{Heumann2025,
  title = {Factors influencing the usage of shared micromobility: Implications from Berlin},
  author = {Maximilian Heumann and Tobias Kraschewski and Philipp Otto and Lukas Tilch and Tim Brauner and Michael H. Breitner},
  year = {2025},
  journal = {Journal of Cycling and Micromobility Research},
  volume = {4},
  pages = {100063},
  doi = {https://doi.org/10.1016/j.jcmr.2025.100063},
  url = {https://www.sciencedirect.com/science/article/pii/S2950105925000075},
  abstract = {The popularity of urban micromobility has steadily grown in cities worldwide. There is a lack of comparative studies investigating factors influencing the travel behavior of shared micromobility in Europe. From this, we investigate shared bicycle, e-scooter, and e-moped usage in Berlin based on trip data from September 2019 to March 2022. We incorporate a comprehensive set of spatial, temporal, weather-, fleet size-, and COVID-19-lockdown-related factors. To account for significant over-dispersion in our hourly resolved panel dataset for 542 traffic analysis zones, we employ a functional spatiotemporal regression model to estimate variables of trip counts for the three micromobility modes. Our descriptive results reveal spatiotemporal characteristics of shared bicycle, e-scooter, and e-moped usage and significant growth of operating fleet sizes in Berlin in recent years. We provide evidence that fleet expansion does not lead to a proportional increase in trips, implying competitive effects among operators limit potential growth. As urban space is scarce and regulations on fleet sizes are lacking, urban planners and service providers use these findings and complementary studies to plan fleets and their allocation optimally. Impacts associated with land use vary between modes and allow for demand-based planning. Precipitation is the most impactful factor among the weather variables and shows a pronounced adverse effect on all three modes. COVID-19-lockdown phases had no significant effect on e-mopeds. While bicycles were moderately affected, e-scooter trips decreased significantly. The findings can help policymakers and micromobility operators further optimize sharing mobility services and facilitate evidence-based strategies for the spatial and temporal design of micromobility.},
}

@article{Jaiswal2024,
  title = {Addendum: Using #ActuallyAutistic on Twitter for Precision Diagnosis of Autism Spectrum Disorder: Machine Learning Study},
  author = {Aditi Jaiswal and Aekta Shah and Christopher Harjadi and Erik Windgassen and Peter Washington},
  year = {2024},
  journal = {JMIR Formative Research},
  volume = {8},
  doi = {https://doi.org/10.2196/59349},
  url = {https://www.sciencedirect.com/science/article/pii/S2561326X24004384},
}

@article{Hu2023,
  title = {UPDExplainer: An interpretable transformer-based framework for urban physical disorder detection using street view imagery},
  author = {Chuanbo Hu and Shan Jia and Fan Zhang and Changjiang Xiao and Mindi Ruan and Jacob Thrasher and Xin Li},
  year = {2023},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  volume = {204},
  pages = {209-222},
  doi = {https://doi.org/10.1016/j.isprsjprs.2023.08.017},
  url = {https://www.sciencedirect.com/science/article/pii/S0924271623002320},
  abstract = {Urban Physical Disorder (UPD), such as old or abandoned buildings, broken sidewalks, litter, and graffiti, has a negative impact on residents’ quality of life. They can also increase crime rates, cause social disorder, and pose a public health risk. Currently, there is a lack of efficient and reliable methods for detecting and understanding UPD. To bridge this gap, we propose UPDExplainer, an interpretable transformer-based framework for UPD detection. We first develop a UPD detection model based on the Swin Transformer architecture, which leverages readily accessible street view images to learn discriminative representations. To provide clear and comprehensible evidence and analysis, we subsequently introduce a UPD factor identification and ranking module that combines visual explanation maps with semantic segmentation maps. This novel integrated approach enables us to identify the exact objects within street view images that are responsible for physical disorders and gain insight into the underlying causes. Experimental results on the re-annotated Place Pulse 2.0 dataset demonstrate promising detection performance of the proposed method, surpassing the state-of-the-art with an increase of 6.54% in Accuracy and 7.05% in F1 score. For a comprehensive evaluation of the method’s ranking performance, we provide both quantitative and qualitative results, highlighting the model’s precise identification of one, two, three, and four UPD factors, respectively. We also present a case study of detecting and ranking physical disorders in the southern region of downtown Los Angeles, California, to demonstrate the practicality and effectiveness of our framework.},
}

@article{Kerschbaum2025,
  title = {Methods for analysing renewable energy potentials in energy system modelling: A review},
  author = {Alina Kerschbaum and Lennart Trentmann and Andreas Hanel and Sebastian Fendt and Hartmut Spliethoff},
  year = {2025},
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {215},
  pages = {115559},
  doi = {https://doi.org/10.1016/j.rser.2025.115559},
  url = {https://www.sciencedirect.com/science/article/pii/S1364032125002321},
  abstract = {With increasing demand for expanding renewable energies, there is also a growing need to assess the potential of renewable energy sources. For every technology that utilizes renewable sources, there is a variety of methodologies which can be used for potential analysis. This review aims to provide an overview of different methods and definitions commonly applied to determine various types of renewable energy potential. The focus is on solar, wind, biomass and geothermal energy. Furthermore, a summary of specific potential analysis methods for the theoretical and technical potential of solar photovoltaic, on- and offshore wind turbines, as well as electricity supply from biomass and geothermal sources is provided. As a main conclusion, for most technologies, spatially-explicit models can be considered state-of-the-art with an overarching trend to include more high-resolution data and temporal fluctuations. General overall research directions include the usage of artificial intelligence, accounting for land use competition, determining synergies and expanding the potential to formerly excluded areas. Key aspects of this work involve a comparison of terminology as well as global technical potentials. It is found that different terminology is used in regards to renewable energy potentials, which decreases comparability. Therefore, common definitions are compared, and a cross-technological view of potentials is proposed. Furthermore, estimates of the global technical potential for electricity supply are reviewed. A total range of values up to three magnitudes in difference is observed, which can be attributed to the differing assumptions and definitions of the technical potential in the evaluated papers.},
}

@article{Dakakni2023,
  title = {Artificial intelligence in the L2 classroom: Implications and challenges on ethics and equity in higher education: A 21st century Pandora's box},
  author = {Deema Dakakni and Nehme Safa},
  year = {2023},
  journal = {Computers and Education: Artificial Intelligence},
  volume = {5},
  pages = {100179},
  doi = {https://doi.org/10.1016/j.caeai.2023.100179},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000589},
  abstract = {The purpose of this research was to investigate attitudes of both students and teachers concerning Artificial Intelligence (AI) tools in the L2 classroom. The study was a descriptive, qualitative, mixedmethods case study whose data were taken from a purposive, convenient sample at a private, English-speaking university during the Summer Semester 2023 in Beirut, Lebanon. Data collection primarily involved an online survey on Google forms which was given to a sample of 49 students taking a research-based English 202 course of which 46 were completed. Afterwards, six English teachers and six students were chosen based on their voluntary will to participate in individual interviews for the former and semi-structured focus group interviews for the latter. The findings revealed that approximately 85% of students did indeed use AI unethically to get ideas for their assignments, assist them in their projects' “blue-prints” or do their assignments/projects altogether. The findings also revealed that a “love/hate” relationship seemed to dictate students' relationships with AI, where students did indeed make use of AI but were distrusting of it for privacy and equity concerns. Finally, findings also revealed that most of the interviewed instructors' readiness to undergo training for AI was more to monitor students' potential misuse of it. The article purposes a suggestive revamping of course learning objectives due to students' inclinations to misuse AI to do their coursework with 89.4% of students willing to use AI to complete their coursework should university punitive measures be removed; furthermore, the article equally proposes future research investigating the impact and use of AI in the higher educational classroom on student performance and that it be used with a “grain of salt” as it may unleash a Pandora's box of future generations graduating without the necessary know-how in delicate professions of medicine, nursing, engineering, architecture among others.},
}

@article{Vasconcelos2024,
  title = {Nature-based climate shelters? Exploring urban green spaces as cooling solutions for older adults in a warming city},
  author = {Luma Vasconcelos and Johannes Langemeyer and Helen V.S. Cole and Francesc Baró},
  year = {2024},
  journal = {Urban Forestry & Urban Greening},
  volume = {98},
  pages = {128408},
  doi = {https://doi.org/10.1016/j.ufug.2024.128408},
  url = {https://www.sciencedirect.com/science/article/pii/S1618866724002061},
  abstract = {As cities grapple with the escalating challenges of urban heat and its impacts on vulnerable populations, particularly older adults, green spaces are increasingly promoted as effective urban cooling solutions. However, despite the extensive literature on people’s access to and preferences for urban green spaces, little is known about the perception and use of these spaces as nature-based climate shelters on hot days, especially by older adults. This study focuses on Barcelona, a Mediterranean city facing rising temperatures, to explore older adults' patterns of use and preferences for urban green spaces on hot days. The research aims to: 1) analyze heat coping behaviors, emphasizing visits to urban green spaces; 2) identify crucial characteristics of green spaces for older adults; and 3) assess variations in behaviors and preferences based on socio-demographic factors. The study leverages survey data from 291 older adult residents, combining face-to-face and online formats. Results indicate that 54 % of older adults use urban green spaces for cooling on hot days, with preferences for morning or evening visits. Factors influencing non-visitation include perceived lack of thermal comfort, noisiness or crowdedness, and proximity issues. Alternative heat coping strategies include staying at home, traveling to cooler areas outside the city, visiting blue spaces, or air-conditioned indoor facilities. Preferred green space characteristics include abundant shade, leafy vegetation, accessibility factors (e.g., walkability), urban furniture (e.g., benches), and water features. Socio-demographic differences reveal higher green space use among younger age groups and residents in certain districts. Mobility limitations and lower education levels influence preferences, with mobility-challenged individuals prioritizing accessibility aspects. Lower-educated respondents are more likely to report barriers to accessing green spaces. These findings highlight the need for tailored urban planning strategies, considering sociodemographic variations, to mitigate heat-related health risks for older adults. By prioritizing green space accessibility, enhancing its quality, promoting its cooling benefits, addressing cooling inequalities and integrating climate considerations in urban green planning, cities facing increasingly pressing heat challenges can create climate-resilient and inclusive green environments that prioritize the well-being of their aging populations.},
}

@article{Jaakonaho2025,
  title = {Development agendas governing the common good – Unfolding planning approaches: A case study of Vantaa, Finland},
  author = {Mari Jaakonaho and Zinette Bergman},
  year = {2025},
  journal = {Cities},
  volume = {156},
  pages = {105581},
  doi = {https://doi.org/10.1016/j.cities.2024.105581},
  url = {https://www.sciencedirect.com/science/article/pii/S0264275124007959},
  abstract = {Contemporary debates suggest that market liberalisation has led to an unfavourable urban planning environment that impedes the ability of public authorities to pursue the common good. To study the significance and potential impact of this, our central line of inquiry was to examine how the common good is manifested within statutory plans in Vantaa, Finland, between 2015 and 2019. Using a mixed methods framework coupled with a refined typology of planning approaches, our findings demonstrate that the common good is orchestrated through three development agendas: public, private and common. In doing so, this study provides insights into the scholarly discourse surrounding the impact of market liberalisation on planning and highlights the importance of developing comprehensive strategies to promote the common good in urban planning.},
}

@article{Adadi2022,
  title = {Artificial Intelligence and COVID-19: A Systematic umbrella review and roads ahead},
  author = {Amina Adadi and Mohammed Lahmer and Samia Nasiri},
  year = {2022},
  journal = {Journal of King Saud University - Computer and Information Sciences},
  volume = {34},
  pages = {5898-5920},
  doi = {https://doi.org/10.1016/j.jksuci.2021.07.010},
  url = {https://www.sciencedirect.com/science/article/pii/S1319157821001774},
  abstract = {Artificial Intelligence (AI) has played a substantial role in the response to the challenges posed by the current pandemic. The growing interest in using AI to handle Covid-19 issues has accelerated the pace of AI research and resulted in an exponential increase in articles and review studies within a very short period of time. Hence, it is becoming challenging to explore the large corpus of academic publications dedicated to the global health crisis. Even with the presence of systematic review studies, given their number and diversity, identifying trends and research avenues beyond the pandemic should be an arduous task. We conclude therefore that after the one-year mark of the declaration of Covid-19 as a pandemic, the accumulated scientific contribution lacks two fundamental aspects: Knowledge synthesis and Future projections. In contribution to fill this void, this paper is a (i) synthesis study and (ii) foresight exercise. The synthesis study aims to provide the scholars a consolidation of findings and a knowledge synthesis through a systematic review of the reviews (umbrella review) studying AI applications against Covid-19. Following the PRISMA guidelines, we systematically searched PubMed, Scopus, and other preprint sources from 1st December 2019 to 1st June 2021 for eligible reviews. The literature search and screening process resulted in 45 included reviews. Our findings reveal patterns, relationships, and trends in the AI research community response to the pandemic. We found that in the space of few months, the research objectives of the literature have developed rapidly from identifying potential AI applications to evaluating current uses of intelligent systems. Only few reviews have adopted the meta-analysis as a study design. Moreover, a clear dominance of the medical theme and the DNN methods has been observed in the reported AI applications. Based on its constructive systematic umbrella review, this work conducts a foresight exercise that tries to envision the post-Covid-19 research landscape of the AI field. We see seven key themes of research that may be an outcome of the present crisis and which advocate a more sustainable and responsible form of intelligent systems. We set accordingly a post-pandemic research agenda articulated around these seven drivers. The results of this study can be useful for the AI research community to obtain a holistic view of the current literature and to help prioritize research needs as we are heading toward the new normal.},
}

@article{Dehm2025,
  title = {Water quality within the greater Suva urban marine environment through spatial analysis of nutrients and water properties},
  author = {Jasha Dehm and Romain {Le Gendre} and Monal Lal and Christophe Menkes and Awnesh Singh},
  year = {2025},
  journal = {Marine Pollution Bulletin},
  volume = {213},
  pages = {117601},
  doi = {https://doi.org/10.1016/j.marpolbul.2025.117601},
  url = {https://www.sciencedirect.com/science/article/pii/S0025326X25000761},
  abstract = {Coastal ecosystems in Pacific Island Countries and Territories are vital to local livelihoods, yet increasingly face pressures from urbanization. In Fiji, the Greater Suva Urban Area, where one-third of the nation's population live, exemplifies these challenges. This study examines spatial and temporal water quality variations in the coastal zone, focusing on physicochemical, nutrients, and clarity parameters. Using a Seabird Scientific SBE19 CTD and Thermo Scientific Orion™ AQUAfast™ colorimeter, coupled with hierarchical clustering and principal component analysis, six water quality clusters were identified, influenced by oceanic processes, river inputs, and anthropogenic activities. Key findings highlight nutrient enrichment near urban centers particularly at the Kinoya Sewage Treatment Plant outfall, where ammonia exceeded 17.8 mg/L, and significant variation observed in nitrate (up to 0.24 ± 0.06 mg/L) and nitrite (up to 0.24 ± 0.06 mg/L) concentrations near river mouths. Seasonal runoff contributed to elevated turbidity (up to 3.5 NTU) and total suspended solids (up to 14.7 mg/L) levels during wet months. Salinity, and temperature exhibited strong spatial and seasonal variability, reflecting land-ocean interactions and restricted water exchange. These findings emphasize the need for targeted action to mitigate nutrient pollution, urban runoff, and wastewater impacts. This study provides a cost-effective monitoring framework for water quality management, offering insights for sustainable coastal resource management in Fiji and other Pacific regions amidst urbanization and climate change.},
}

@article{Wang2024_07,
  title = {The products and multi-disciplinarity of data-centric tasks: Influences on data searchers' behaviors and cognition},
  author = {Ping Wang and Jingyu Wang and Chunfeng Liu and Qiao Li},
  year = {2024},
  journal = {Library & Information Science Research},
  volume = {46},
  pages = {101302},
  doi = {https://doi.org/10.1016/j.lisr.2024.101302},
  url = {https://www.sciencedirect.com/science/article/pii/S0740818824000239},
  abstract = {The product type and multi-disciplinarity level of tasks may be associated with the behaviors of data searchers and their utilization of cognitive System 1 and System 2. To test these associations, a user experiment was conducted. Findings suggest that due to differences in the uncertainty of data needs and the difficulty of data evaluation, when executing tasks with intellectual products, users devote more effort to data evaluation rather than query optimization and rely more on System 2. Moreover, due to differences in knowledge complexity and the availability of data resources, when performing multi-discipline tasks, users devote more effort to query optimization and data evaluation but gain lower search performance. When performing intellectual-multi-discipline tasks, data searchers use System 1 and System 2 in a complementary way to support systematic and efficient evaluation. A task classification has been developed, offering a framework to identify behavioral and cognitive patterns linked to specific tasks.},
}

@article{Tiznado-Aitken2024,
  title = {Uncovering gender-based violence and harassment in public transport: Lessons for spatial and transport justice},
  author = {Ignacio Tiznado-Aitken and Thomas E. {Guerrero B.} and Lake Sagaris},
  year = {2024},
  journal = {Journal of Transport Geography},
  volume = {114},
  pages = {103766},
  doi = {https://doi.org/10.1016/j.jtrangeo.2023.103766},
  url = {https://www.sciencedirect.com/science/article/pii/S0966692323002387},
  abstract = {When suitably organized, public transport plays an important role in social inclusion and equity by providing crucial access to social, political, and economic opportunities. Notwithstanding, a growing body of evidence points to gender-based harassment and violence associated with public transport journeys as significant barriers to women's travel. This raises important issues regarding spatial and transport justice. Using a survey in Santiago, Chile, this study explores gender-based violence and harassment experienced while accessing, egressing, and using public transport. We use zero-inflated and logistic regression models to explore individual, behavioral and spatial factors associated with gender violence, particularly sexual harassment, defined as unwanted touching, filming or photography, stalking or masturbation, and rape. We examine these factors differentiating by trip segment (access to and egress from the system versus in-vehicle experience) and public transport mode (bus and Metro). Our results show that 17.5% experienced four or more harassment situations. Public masturbation or rape primarily occurred on the journey to and from public transport (8.6%) and on board a bus (7.4%). Our models indicate a spatial component of harassment which intersects with behavioral and individual factors as trip frequency, trip purpose, gender, age, and income, reducing public transit's ability to serve women, particularly those in low-income living in more peripheral areas. This article contributes to knowledge regarding how these factors interact in a major city in the Global South, which has been under-explored to date. Our findings join other studies challenging theories of spatial and transport justice, which to date pay little attention to violence in general and gender-based violence in particular. We make suggestions for reinforcing theory in order to develop stronger solutions to ensure transport systems improve equity rather than locking in current discrimination and exclusions.},
}

@article{Zare2025,
  title = {Assessing pollution and ecological risks of potentially toxic elements in surface sediments of the north-central Persian Gulf},
  author = {Fakhteh Zare and Alireza Vaezi and Vahid Tavakoli and Razyeh Lak},
  year = {2025},
  journal = {Regional Studies in Marine Science},
  pages = {104159},
  doi = {https://doi.org/10.1016/j.rsma.2025.104159},
  url = {https://www.sciencedirect.com/science/article/pii/S2352485525001501},
  abstract = {This study investigates pollution levels and ecological risks of potentially toxic elements (PTEs) in surface sediments of the North-Central Persian Gulf, a region heavily influenced by industrial activities. Sediment samples were systematically collected from 109 stations at depths ranging from 0 to 78 meters. Analysis revealed a predominance of silt and clay (~70% of total composition). Concentrations of 14 major and trace elements followed the order: Ca > Al > Fe > Sr > Mn > Ba > Cr > Ni > V > Zn > Cu > Pb > As > Cd. The modified degree of contamination (mCd) indicated generally low contamination, though elevated pollution levels were detected near Bushehr Port, the Bushehr Nuclear Power Plant, and Khark Island, likely due to anthropogenic inputs. Significant enrichment of As and Cd was observed, alongside moderate enrichment of Cr and Ni, while Cu, Fe, Mn, Pb, V, and Zn exhibited minimal enrichment. Ecological risk assessment suggested low overall risk, with localized moderate risks at certain stations. Notably, Cd contributed 56% of the total ecological risk, highlighting concerns near industrial zones. Geochemical analysis revealed that Al, Ni, Zn, Cr, V, Fe, and Cu primarily originated from terrestrial sources, reflecting regional geological background. Moderate enrichment of Ni and Cr aligns with naturally high soil concentrations in southern Iran. In contrast, elevated As and Cd levels point to significant anthropogenic influence, particularly near industrial sites, with Cd linked to oil tanker operations and industrial wastewater discharge. These findings underscore the urgent need for mitigation strategies such as (1) stricter controls on industrial and shipping-related discharges, (2) remediation of Cd and As hotspots near high-risk zones (e.g., Bushehr Port and Khark Island), and (3) long-term monitoring of benthic ecosystems to assess bioaccumulation risks.},
}

@article{Ghodeswar2025,
  title = {Quantifying the economic costs of power outages owing to extreme events: A systematic review},
  author = {Archana Ghodeswar and Mahabir Bhandari and Bruce Hedman},
  year = {2025},
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {207},
  pages = {114984},
  doi = {https://doi.org/10.1016/j.rser.2024.114984},
  url = {https://www.sciencedirect.com/science/article/pii/S136403212400710X},
  abstract = {Quantifying the economic cost of long-duration power outages is crucial to justifying investments in resiliency and reliability improvements. However, extensive study on the subject complicates the identification of power outage costs and determining the most suitable approach to quantify them for an individual, specific facility, particularly in the context of extreme events. This research provides a systematic review of economic studies estimating the impact of environmental disasters at the microeconomic and macroeconomic levels. Of 326 articles, evaluating the costs of power outages in extreme events, this work identified 22 studies that attempted to quantify the economic costs. These findings indicate that quantifying power outage costs lacks standardization, posing challenges for comparing different studies. Most analyses aiming to quantify these costs for utilities, sectors, and the overall economy rely on outdated survey data, which offer generalized rather than specific cost estimations. The costs of power outages exhibit a significant dependence on factors such as the sector involved, the type of customer affected, and the outage duration. To quantify industry costs, the research in this study suggests that using the National Renewable Energy Laboratory's online, open-access Customer Damage Function Calculator is the best option for individual-level assessments of industries, hospitals, offices, education centers, and similar facilities. However, the Interruption Cost Estimate Calculator can estimate outage costs across industrial, commercial, and residential sectors for macroeconomic outcomes. Finally, this article discusses the relative strengths of these methods and tools and the potential directions for future research.},
}

@article{Trajanov2023,
  title = {Review of Natural Language Processing in Pharmacology},
  author = {Dimitar Trajanov and Vangel Trajkovski and Makedonka Dimitrieva and Jovana Dobreva and Milos Jovanovik and Matej Klemen and Aleš Žagar and Marko Robnik-Šikonja},
  year = {2023},
  journal = {Pharmacological Reviews},
  volume = {75},
  pages = {714-738},
  doi = {https://doi.org/10.1124/pharmrev.122.000715},
  url = {https://www.sciencedirect.com/science/article/pii/S0031699724007762},
  abstract = {Natural language processing (NLP) is an area of artificial intelligence that applies information technologies to process the human language, understand it to a certain degree, and use it in various applications. This area has rapidly developed in the past few years and now employs modern variants of deep neural networks to extract relevant patterns from large text corpora. The main objective of this work is to survey the recent use of NLP in the field of pharmacology. As our work shows, NLP is a highly relevant information extraction and processing approach for pharmacology. It has been used extensively, from intelligent searches through thousands of medical documents to finding traces of adversarial drug interactions in social media. We split our coverage into five categories to survey modern NLP: methodology, commonly addressed tasks, relevant textual data, knowledge bases, and useful programming libraries. We split each of the five categories into appropriate subcategories, describe their main properties and ideas, and summarize them in a tabular form. The resulting survey presents a comprehensive overview of the area, useful to practitioners and interested observers. Significance Statement The main objective of this work is to survey the recent use of NLP in the field of pharmacology in order to provide a comprehensive overview of the current state in the area after the rapid developments that occurred in the past few years. The resulting survey will be useful to practitioners and interested observers in the domain.},
}

@article{Ifenthaler2025,
  title = {Adaptive serious games assessment: The case of the blood transfusion game in nursing education},
  author = {Dirk Ifenthaler and Muhittin Sahin and Ivan Boo and Darshini Devi Rajasegeran and Ang Shin Yuh},
  year = {2025},
  journal = {Computers and Education: Artificial Intelligence},
  volume = {8},
  pages = {100351},
  doi = {https://doi.org/10.1016/j.caeai.2024.100351},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X24001541},
}

@article{Barratt2023,
  title = {The joys of sharing: andrology trailblazes in data transparency – an example using the World Health Organization 2021 reference ranges},
  author = {Christopher LR Barratt},
  year = {2023},
  journal = {Reproductive BioMedicine Online},
  volume = {47},
  pages = {103230},
  doi = {https://doi.org/10.1016/j.rbmo.2023.05.002},
  url = {https://www.sciencedirect.com/science/article/pii/S1472648323002833},
}

@article{Hasan2024,
  title = {Bangladesh's pathways to net-zero transition: Reassessing country's solar PV potential with high-resolution GIS data},
  author = {A.S.M. Mominul Hasan and Prin Kesapabutr and Bernd Möller},
  year = {2024},
  journal = {Energy for Sustainable Development},
  volume = {81},
  pages = {101511},
  doi = {https://doi.org/10.1016/j.esd.2024.101511},
  url = {https://www.sciencedirect.com/science/article/pii/S0973082624001376},
  abstract = {Solar photovoltaic (PV) technology stands out as a cornerstone in Bangladesh's journey towards achieving net-zero emissions, representing a crucial building block in the country's sustainable energy transition plan. However, rapid land use change and the lack of suitable land for developing PV pose significant barriers to achieving Bangladesh's renewable energy targets and decarbonisation goals towards a net-zero transition. Our analysis of the predevelopment land use state of ten existing solar PV plants in Bangladesh reveals a substantial use of scarce agricultural land for their establishment. Therefore, to identify pathways for overcoming the challenges, this study reassesses Bangladesh's geographic and technical potential for solar PV using geospatial modelling by considering local contexts. Our investigation encompasses Rooftop PV (RPV), Ground-mounted PV (GPV), Floating PV (FPV), and Agrivoltaic (APV) systems. To identify suitable areas and quantify potential, we employ a comprehensive exclusion model and system-specific suitability models using the QGIS platform. Utilising the latest spatial datasets, including footprint data comprising approximately 20 million buildings, a 10 metre (m) resolution land cover map, and bathymetry data, our study provides a robust analysis. The results of our models present a holistic view of Bangladesh's solar PV potential, estimating about 30 GWp for RPV, 9 GWp for GPV, 5 GWp for FPV, and 81 GWp for APV applications. Given the escalating urbanisation in Bangladesh, our findings recommend diversifying solar PV deployment with a focus on RPV and other PV systems that offer dual use of land to facilitate a smoother energy transition towards sustainable development.},
}

@article{Fernandes2025,
  title = {Children’s data protection in education: A case study of Google Workspace for Education in the European Economic Area},
  author = {Elora Fernandes},
  year = {2025},
  journal = {International Journal of Educational Development},
  volume = {114},
  pages = {103237},
  doi = {https://doi.org/10.1016/j.ijedudev.2025.103237},
  url = {https://www.sciencedirect.com/science/article/pii/S0738059325000355},
  abstract = {Digital technologies are increasingly embedded in education, transforming not only the means of delivery, but also its values and purposes. The COVID-19 pandemic has caused a significant surge in the edtech industry, resulting in greater involvement of private entities in shaping the future of education and processing extensive volumes of children’s data. Challenges related to protecting children’s data in this realm have already been raised by several authorities across Europe. This paper analyzes cases related to Google Workspace for Education through a data colonialism lens, seeking to understand the strengths and limitations of public authorities’ positions and whether the measures taken are adequate to address data protection issues associated with data colonialism. The findings suggest that an adequate enforcement of the General Data Protection Regulation is possible and powerful to protect children’s data. However, this demands strong political and economic power to ensure compliance. It also falls short of addressing broader challenges posed by the prevailing business model, including familiarizing children with technologies that process data for predatory commercial purposes outside the school environment, as well as concerns regarding competition, public procurement, and data sovereignty.},
}

@article{Wang2024_08,
  title = {Downscaling occupational employment data from the state to the Census tract level},
  author = {Sicheng Wang and Shubham Agrawal and Elizabeth A. Mack and Nidhi Kalani and Shelia R. Cotten and Chu-Hsiang Chang and Peter T. Savolainen},
  year = {2024},
  journal = {Applied Geography},
  volume = {170},
  pages = {103349},
  doi = {https://doi.org/10.1016/j.apgeog.2024.103349},
  url = {https://www.sciencedirect.com/science/article/pii/S0143622824001541},
  abstract = {The lack of detailed occupational employment data at more granular geographic levels presents significant challenges in forecasting and analyzing local and regional employment changes in the era of the new technological revolution. This study aims to develop detailed occupational employment data by downscaling state-level employment information to the Census tract level. We introduce two downscaling algorithms that leverage employment, population, and sociodemographic composition data sourced from the American Community Survey, the Current Population Survey, and the Occupational Employment and Wage Statistics. This approach allows us to create a tract-level employment dataset covering 808 occupations. Such data are crucial for examining the effects of expected technological and demographic shifts on employment at this scale, which is critical for understanding tax base implications and job mobility opportunities. We demonstrate the value of these datasets by examining employment projections for two occupations anticipated to decline due to technological advancements in the near future.},
}

@article{Zhang2025_04,
  title = {Inferring building type using textual data and Natural Language Processing for urban building energy modelling},
  author = {Shihong Zhang and Ya Zhou and Liutao Chen and Yixin Huang and Zhe Wang},
  year = {2025},
  journal = {Building and Environment},
  volume = {269},
  pages = {112428},
  doi = {https://doi.org/10.1016/j.buildenv.2024.112428},
  url = {https://www.sciencedirect.com/science/article/pii/S0360132324012708},
  abstract = {ABSTRACT Building type is among the most important inputs for building energy model. However, the information of building type is always missing in urban scale building energy modeling. This paper presents a novel approach to infer building type from building name. First, we created the building name text dataset through the fusion of GIS spatial data. A rule-based method was developed to estimate building types using naming features. We then trained five machine learning classifiers, including four transformer models and one Multilayer Perceptron model, to predict building types. Finally, we leveraged the inferred building type information for building energy consumption simulation, addressing the crucial data scarcity issue in urban-scale building energy models. Experimental results indicated that our rule-based classification method achieved a precision of 84.3%. The RoBERTa model, the best-performing natural language processing (NLP) model, reached a precision of 91.6% with both Chinese and English names as NLP model inputs, showcasing a 1.3% enhancement compared to solely utilizing the Chinese dataset and a 1.8% improvement compared to solely utilizing the English dataset. This research proposes a useful framework to infer building type by leveraging the state-of-art NLP techniques, paving the way for more accurate and efficient urban-scale building energy modelling.},
}

@article{Duong2024,
  title = {A novel framework for crash frequency prediction: Geographic support vector regression based on agent-based activity models in Greater Melbourne},
  author = {Quynh Duong and Hulya Gilbert and Hien Nguyen},
  year = {2024},
  journal = {Accident Analysis & Prevention},
  volume = {207},
  pages = {107747},
  doi = {https://doi.org/10.1016/j.aap.2024.107747},
  url = {https://www.sciencedirect.com/science/article/pii/S0001457524002926},
  abstract = {The field of spatial analysis in traffic crash studies can often enhance predictive performance by addressing the inherent spatial dependence and heterogeneity in crash data. This research introduces the Geographical Support Vector Regression (GSVR) framework, which incorporates generated distance matrices, to assess spatial variations and evaluate the influence of a wide range of factors, including traffic, infrastructure, socio-demographic, travel demand, and land use, on the incidence of total and fatal-or-serious injury (FSI) crashes across Greater Melbourne’s zones. Utilizing data from the Melbourne Activity-Based Model (MABM), the study examines 50 indicators related to peak hour traffic and various commuting modes, offering a detailed analysis of the multifaceted factors affecting road safety. The study shows that active transportation modes such as walking and cycling emerge as significant indicators, reflecting a disparity in safety that heightens the vulnerability of these road users. In contrast, car commuting, while a consistent factor in crash risks, has a comparatively lower impact, pointing to an inherent imbalance in the road environment. This could be interpreted as an unequal distribution of risk and safety measures among different types of road users, where the infrastructure and policies may not adequately address the needs and vulnerabilities of pedestrians and cyclists compared to those of car drivers. Public transportation generally offers safer travel, yet associated risks near train stations and tram stops in city center areas cannot be overlooked. Tram stops profoundly affect total crashes in these areas, while intersection counts more significantly impact FSI crashes in the broader metropolitan area. The study also uncovers the contrasting roles of land use mix in influencing FSI versus total crashes. The proposed framework presents an approach for dynamically extracting distance matrices of varying sizes tailored to the specific dataset, providing a fresh method to incorporate spatial impacts into the development of machine learning models. Additionally, the framework extends a feature selection technique to enhance machine learning models that typically lack comprehensive feature selection capabilities.},
}

@article{Pereira2024,
  title = {Mapping the evolution of seawater desalination research (2000–2024): Bibliometric and co-word analysis of 11,000+ publications},
  author = {Gustavo Leite Dias Pereira and Veeriah Jegatheesan},
  year = {2024},
  journal = {Desalination},
  volume = {591},
  pages = {118029},
  doi = {https://doi.org/10.1016/j.desal.2024.118029},
  url = {https://www.sciencedirect.com/science/article/pii/S0011916424007409},
  abstract = {Seawater desalination (SWD) can partially mitigate the increasing freshwater needs globally. Although, SWD is multifaceted and involves processes with environmental and economic challenges, research is often analyzed through Literature Reviews (LRs) in specific contexts that may miss general trends. Bibliometric Analyzes (BAs), however, provide researchers with an overview through Co-occurrence Networks (CNs), Strategic Diagrams (SDs), Thematic Subnetworks (TSs), and Thematic Evolution Diagrams (TEDs). Nevertheless, their use in SWD research has been limited, with minimal attention given to them. Thus, we created a bibliometric dataset, compared it with other BAs, and developed CNs, SDs, TSs, and a TED for SWD. Furthermore, key term searches (Energy, Solar, Reverse Osmosis, Modeling and Optimization, Membrane Distillation, Environmental, Economics, Fouling, Nanofiltration, and Ultrafiltration) and their literature are discussed. Geographical analysis shows China and the US lead SWD research, shifting from process design to membrane fabrication and solar energy. Furthermore, RO remains the leading technique despite high energy demands. Solar desalination shows promise but faces cost and scalability challenges. Environmental and economic concerns are discussed, as well as emerging solutions regarding, solar desalination, blue energy, “blue carbon”, and zero liquid discharge. Research gaps include fouling effects and pretreatment incorporation in optimizations are also highlighted.},
}

@article{Sala}2024,
  title = {The future of science publishing},
  author = {S. {Della Sala} and J. Bathelt and H. Buchtel and A. Tavano and C. Press and B. Love and I. Croy and R. Morris and S. Kotz and M.D. Kopelman and M.I. Coco and P. Reber and S.J. Forkel and S.R. Schweinberger},
  year = {2024},
  journal = {Cortex},
  volume = {181},
  pages = {93-100},
  doi = {https://doi.org/10.1016/j.cortex.2024.10.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0010945224002727},
}

@article{Um-e-Habiba2024,
  title = {A review on enhancing energy efficiency and adaptability through system integration for smart buildings},
  author = { Um-e-Habiba and Ijaz Ahmed and Muhammad Asif and Hassan Haes Alhelou and Muhammad Khalid},
  year = {2024},
  journal = {Journal of Building Engineering},
  volume = {89},
  pages = {109354},
  doi = {https://doi.org/10.1016/j.jobe.2024.109354},
  url = {https://www.sciencedirect.com/science/article/pii/S2352710224009227},
  abstract = {The increasing need for reducing carbon emissions and promoting smart, energy-saving buildings is fueling the rising trend of sophisticated control systems. This study provides comprehensive evaluations of advanced controls specifically designed for such buildings. The topics covered are diverse, encompassing big data, data collection, automation in construction, digitalization of energy, and modeling of building energy. The subjects covered include controls that prioritize the needs of occupants, ensuring energy security, providing flexibility and dependability, and utilizing machine learning for control systems. Furthermore, the study explores controllers that make use of artificial intelligence, adaptation to climate change, and the challenges and opportunities associated with achieving a balance between complexity and control performance. This study deepens our understanding and application of advanced controls in the creation of eco-friendly, sustainable structures. Moreover, this review will offer academics, architects, and legislators valuable information for the development of smart control technologies and the need for smart and sustainable buildings.},
}

@article{Hemerijckx2025,
  title = {Future scenarios for urban agriculture and food security in sub-Saharan Africa: Modelling the urban land-food system in an agent-based approach},
  author = {Lisa-Marie Hemerijckx and Koen {De Vos} and Joseph Oseko Kaunda and Anton {Van Rompaey}},
  year = {2025},
  journal = {Computers, Environment and Urban Systems},
  volume = {118},
  pages = {102258},
  doi = {https://doi.org/10.1016/j.compenvurbsys.2025.10225},
  url = {https://www.sciencedirect.com/science/article/pii/S0198971525000110},
  abstract = {Food systems in sub-Saharan African cities are increasingly pressured by rapid urban sprawl and socio-economic changes. While land conversion from cropland to built-up area is limiting the opportunity for urban agriculture, food demand is rising because of population growth and changing diets. Meanwhile, socio-economic segregation – often associated with urbanization - can hinder access to food. For the case study of Kampala (Uganda), we spatiotemporally model the land-food system using an agent-based approach. Based on 747 household surveys, we recalibrated the Agent-based model of Social Segregation and Urban Expansion (ASSURE) by Vermeiren et al. (2016) and included food system dynamics to assess future trajectories (2020–2040) of Kampala's dependency on urban agriculture. While food that is both produced and consumed within the city is often considered the most resilient food source in times of crisis, we show that it is particularly this source that is threatened. Overall, about 10 % of the urban and peri-urban agricultural land in Kampala is projected to disappear by 2040. This may lead to decreased levels of food security and dietary diversity, particularly for households that strongly rely on urban agriculture. Information on the links between urban growth and local food provision is essential for planners who aim to develop strategies for more secure, just and sustainable African urban food systems.},
}

@article{Taveekitworachai2024,
  title = {A systematic review of major evaluation metrics for simulator-based automatic assessment of driving after stroke},
  author = {Pittawat Taveekitworachai and Gunt Chanmas and Pujana Paliyawan and Ramita Thawonmas and Chakarida Nukoolkit and Piyapat Dajpratham and Ruck Thawonmas},
  year = {2024},
  journal = {Heliyon},
  volume = {10},
  pages = {e32930},
  doi = {https://doi.org/10.1016/j.heliyon.2024.e32930},
  url = {https://www.sciencedirect.com/science/article/pii/S2405844024089618},
  abstract = {Background: Simulator-based driving assessments (SA) have recently been used and studied for various purposes, particularly for post-stroke patients. Automating such assessment has potential benefits especially on reducing financial cost and time. Nevertheless, there currently exists no clear guideline on assessment techniques and metrics available for SA for post-stroke patients. Therefore, this systematic review is conducted to explore such techniques and establish guidelines for evaluation metrics. Objective: This review aims to find: (a) major evaluation metrics for automatic SA in post-stroke patients and (b) assessment inputs and techniques for such evaluation metrics. Methods: The study follows the PRISMA guideline. Systematic searches were performed on PubMed, Web of Science, ScienceDirect, ACM Digital Library, and IEEE Xplore Digital Library for articles published from January 1, 2010, to December 31, 2023. This review targeted journal articles written in English about automatic performance assessment of simulator-based driving by post-stroke patients. A narrative synthesis was provided for the included studies. Results: The review included six articles with a total of 239 participants. Across all of the included studies, we discovered 49 distinct assessment inputs. Threshold-based, machine-learning-based, and driving simulator calculation approaches are three primary types of assessment techniques and evaluation metrics identified in the review. Discussion: Most studies incorporated more than one type of input, indicating the importance of a comprehensive evaluation of driving abilities. Threshold-based techniques and metrics were the most commonly used in all studies, likely due to their simplicity. An existing relevant review also highlighted the limited number of studies in this area, underscoring the need for further research to establish the validity and effectiveness of simulator-based automatic assessment of driving (SAAD). Conclusions: More studies should be conducted on various aspects of SAAD to explore and validate this type of assessment.},
}

@article{Islam2024,
  title = {A comprehensive review of deep learning techniques for salt dome segmentation in seismic images},
  author = {Muhammad Saif Ul Islam and Aamir Wali},
  year = {2024},
  journal = {Journal of Applied Geophysics},
  volume = {230},
  pages = {105504},
  doi = {https://doi.org/10.1016/j.jappgeo.2024.105504},
  url = {https://www.sciencedirect.com/science/article/pii/S0926985124002209},
  abstract = {Salt dome detection in seismic images is a critical aspect of hydrocarbon exploration and production. Salt domes are subsurface structures formed from the accumulation of salt deposits and can trap oil and gas reservoirs. Seismic imaging techniques are used to visualize the subsurface structures and identify the presence of salt domes. Historically, the process of detecting salt domes in seismic images was done manually, which was time-consuming and required the input of domain experts. However, in recent years, automated methods using seismic attributes and machine learning algorithms have been developed to improve the efficiency of salt dome detection. Deep learning-based methods have shown promising results in salt body segmentation, and several techniques have been proposed in recent years. This review examines recent deep-learning architectures for salt body segmentation in seismic images, offering a concise overview of the various models proposed in the literature. It delves into established benchmark datasets, highlighting potential limitations and emphasizing the importance of data quality for robust models. It explores performance evaluation metrics used in the literature to capture a more comprehensive picture of segmentation performance. This paper identifies several promising areas for further research and development opportunities to refine and enhance the current state-of-the-art salt body segmentation in seismic images. This comprehensive analysis provides a valuable roadmap for researchers and practitioners interested in understanding how deep learning can be utilized for salt body classification in seismic exploration.},
}

@article{Heredia2025,
  title = {Research and Collaborative Working and Sharing Online},
  author = {Ana Heredia and Eloisa Viggiani},
  year = {2025},
  pages = {155-167},
  doi = {https://doi.org/10.1016/B978-0-323-95689-5.00111-5},
  publisher = {Academic Press},
  url = {https://www.sciencedirect.com/science/article/pii/B9780323956895001115},
  abstract = {In today׳s rapidly evolving digital landscape, the way research is conducted, and how collaboration happens has undergone a significant transformation. The advent of online sharing platforms and collaborative tools has revolutionized how individuals and teams work together, communicate, and share knowledge. This entry explores the impact of online sharing and collaboration on research, sheds light on the benefits and discusses the challenges of these dynamic virtual environments. Moreover, it explores the potential impacts of these new research practices to lower current global asymmetries and inequalities in the access and dissemination of research.},
}

@article{Bjurner2025,
  title = {Study protocol for a triple-blind randomised controlled trial evaluating a machine learning-based predictive clinical decision support tool for internet-delivered cognitive behaviour therapy (ICBT) for depression and anxiety},
  author = {Pontus Bjurner and Nils Hentati Isacsson and Fehmi Ben Abdesslem and Magnus Boman and Erik Forsell and Viktor Kaldo},
  year = {2025},
  journal = {Internet Interventions},
  volume = {40},
  pages = {100816},
  doi = {https://doi.org/10.1016/j.invent.2025.100816},
  url = {https://www.sciencedirect.com/science/article/pii/S221478292500017X},
  abstract = {Introduction Therapist-supported internet-based Cognitive Behavioural Therapy (ICBT) has strong scientific support, but all patients are not helped, and further improvements are needed. Personalized medicine could enhance ICBT. One promising approach uses a Machine learning (ML) based predictive decision support tool (DST) to help therapists identify patients at risk of treatment failure and adjust their treatments accordingly. ICBT is a suitable clinical context for developing and testing such predictive DST's, since its delivery is quite flexible and can quickly be adapted for probable non-responders, for example by increasing the level and nature of therapist support, to avoid treatment failures and improve overall outcomes. This type of strategy has never been tested in a triple-blind randomised controlled trial (RCT) and has rarely been studied in ICBT. The aim of this protocol is to expand on previous registered protocols with more detailed descriptions of methods and analyses before analyses is being conducted. Methods and analysis A triple blind RCT comparing ICBT with a DST (DST condition), to ICBT as usual (TAU condition). The primary objective is to evaluate if the DST condition is superior to the TAU condition in decreasing diagnose-specific symptoms among patients identified to be at risk of failure. Secondary objectives are to evaluate if the DST improves functioning, interaction, adherence, patient satisfaction, and therapist time efficiency and decreases the number of failed treatments. Additionally, we will investigate the therapists' experience of using the DST. Patients and therapists have been recruited nationally. They were randomised and given a sham rationale for the trial to ensure allocation blindness. The total number of patients included was 401, and assessments were administered pre-treatment, weekly during treatment, at post-treatment and at 12-month follow-up. Primary outcome is one of the three diagnosis-specific symptom rating scales for respective treatment and primary analysis is difference in change from pre- to post-treatment for at-risk patients on these scales. Human ethics and consent to participate Informed consent to participate in the study was obtained from all participants. Both therapists and patients are participants in this trial. For patients, informed consent to participate in the study was obtained when they registered interest for the study via the study's secure web platform and carried out initial screening before the diagnostic and fit for treatment assessment, they first received the research subject information and were asked for consent by digitally signing that they had read and understood the information. For therapists who were part of the study, consent was requested after they had registered their interest. Therapists then received an email with a link to the study's secure web platform with the research person's information and were asked for consent by digitally signing that they had read and understood the information. All documents are stored in secure, locked filing cabinets on the clinic's premises or on a secure digital consent database. Approval committee Approved by the Swedish Ethical Review Authority (SERA), record number 2020–05772.},
}

@article{Atmakuru2025,
  title = {Artificial intelligence-based suicide prevention and prediction: A systematic review (2019–2023)},
  author = {Anirudh Atmakuru and Alen Shahini and Subrata Chakraborty and Silvia Seoni and Massimo Salvi and Abdul Hafeez-Baig and Sadaf Rashid and Ru San Tan and Prabal Datta Barua and Filippo Molinari and U Rajendra Acharya},
  year = {2025},
  journal = {Information Fusion},
  volume = {114},
  pages = {102673},
  doi = {https://doi.org/10.1016/j.inffus.2024.102673},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253524004512},
  abstract = {Suicide is a major global public health concern, and the application of artificial intelligence (AI) methods, such as natural language processing (NLP), machine learning (ML), and deep learning (DL), has shown promise in advancing suicide prediction and prevention efforts. Recent advancements in AI – particularly NLP and DL have opened up new avenues of research in suicide prediction and prevention. While several papers have reviewed specific detection techniques like NLP or DL, there has been no recent study that acts as a one-stop-shop, providing a comprehensive overview of all AI-based studies in this field. In this work, we conduct a systematic literature review to identify relevant studies published between 2019 and 2023, resulting in the inclusion of 156 studies. We provide a comprehensive overview of the current state of research conducted on AI-driven suicide prevention and prediction, focusing on different data types and AI techniques employed. We discuss the benefits and challenges of these approaches and propose future research directions to improve the practical application of AI in suicide research. AI is highly capable of improving the accuracy and efficiency of risk assessment, enabling personalized interventions, and enhancing our understanding of risk and protective factors. Multidisciplinary approaches combining diverse data sources and AI methods can help identify individuals at risk by analyzing social media content, patient histories, and data from mobile devices, enabling timely intervention. However, challenges related to data privacy, algorithmic bias, model interpretability, and real-world implementation must be addressed to realize the full potential of these technologies. Future research should focus on integrating prediction and prevention strategies, harnessing multimodal data, and expanding the scope to include diverse populations. Collaboration across disciplines and stakeholders is essential to ensure that AI-driven suicide prevention and prediction efforts are ethical, culturally sensitive, and person-centered.},
}

@article{Çiriş2024,
  title = {Investigating the influence of spatial characteristics on cycling volume: A multi-scale geographic weighted regression approach},
  author = {Seçkin Çiriş and Mert Akay and Ece Tümer},
  year = {2024},
  journal = {Transportation Research Interdisciplinary Perspectives},
  volume = {26},
  pages = {101160},
  doi = {https://doi.org/10.1016/j.trip.2024.101160},
  url = {https://www.sciencedirect.com/science/article/pii/S2590198224001465},
  abstract = {Cycling has seen a remarkable rise, signifying a paradigmatic move towards sustainable, eco-friendly, and efficient commuting alternatives in the contemporary urban setting. Cities also encourage this trend by establishing cycle lanes, bike-sharing programs, and incentives for frequent riders. To enhance these motivations from an urbanistic perspective, it is essential to comprehend the influence of urban characteristics on cycling volume and to incorporate this understanding into data-driven decision-making processes. This research examines the Bicification project data from Istanbul with a spatial perspective. Utilising a comprehensive array of spatial big data, the study explores the impact of urban land use, transport services, land morphology, and sociodemographic factors on cycling volume through a Multi-scale Geographically Weighted Regression (MGWR). With an Adj R2 value of 0.68, the model demonstrates a strong relation between cycling volume and several factors, including biking park stations, park and ride points, pier stops, rail stops, transfer points, main roads, elevation, population, industrial facilities, health facilities, sports areas, and residential areas. The findings will serve to develop a data-driven strategic approach to promote cycling in Istanbul.},
}

@article{Dissanayaka2025,
  title = {Developing airport management practices towards net zero emissions: Experiences from the Australian aviation industry},
  author = {Manori Dissanayaka and Tim Ryley and Bojana Spasojevic and Savindi Caldera},
  year = {2025},
  journal = {Heliyon},
  volume = {11},
  pages = {e41201},
  doi = {https://doi.org/10.1016/j.heliyon.2024.e41201},
  url = {https://www.sciencedirect.com/science/article/pii/S2405844024172325},
  abstract = {Emissions from airport sources degrade air quality impacting community health. While some airports assess air pollution, others assess broader environmental effects, including CO2 emissions and noise. Utilising a transition management approach, this paper examines Australian airport practices and develops key sustainable strategies to reduce environmental impacts. After reviewing environmental policies and reports from eight major airports and conducting in-depth interviews with 18 sustainable aviation experts, five key strategies are proposed: 1) Collaborating data-sharing among stakeholders, including airport operators, ground handlers, airlines and air traffic controllers; 2) Evaluating emissions from aircraft ground idling delays; 3) Advancing in the Airport Carbon Accreditation program; 4) Assessing air pollutant emissions directly emitted from airport sources; 5) Maintaining an air pollutant emissions inventory. Airports should integrate these strategies into their environmental policies to support their long-term sustainable goal of reaching net zero emissions by 2050.},
}

@article{Colther2024,
  title = {Artificial intelligence: Driving force in the evolution of human knowledge},
  author = {Cristian Colther and Jean Pierre Doussoulin},
  year = {2024},
  journal = {Journal of Innovation & Knowledge},
  volume = {9},
  pages = {100625},
  doi = {https://doi.org/10.1016/j.jik.2024.100625},
  url = {https://www.sciencedirect.com/science/article/pii/S2444569X24001641},
  abstract = {This article proposes that artificial intelligence (AI) is positioned as a key driver of a new evolutionary stage of human knowledge, complementing human intelligence and facilitating the creation and development of sophisticated collective intelligence, defined as the noosphere, understood as the sphere of collective human thought. The study reveals several key insights into the transformative potential of AI, including its capacity to accelerate, mediate, and diffuse human knowledge. It concludes that AI not only catalyzes the existence of the noosphere but also redefines the structures and mechanisms through which human knowledge is expanded and democratized. Additionally, the document presents potential risks and significant ethical, social, and legal challenges of an AI-mediated noosphere, offering recommendations and a research agenda around the topic, and limitations and proposals for improvement to be considered in the future.},
}

@article{Wang2024_09,
  title = {Extensive growth of inventions: Evidence from U.S. patenting},
  author = {Jieshu Wang and José Lobo},
  year = {2024},
  journal = {Technological Forecasting and Social Change},
  volume = {207},
  pages = {123586},
  doi = {https://doi.org/10.1016/j.techfore.2024.123586},
  url = {https://www.sciencedirect.com/science/article/pii/S0040162524003822},
  abstract = {Despite the seemingly fast development and wide diffusion of technologies in recent decades, concerns have been raised as to whether invention is slowing down. A question has also arisen as to whether the vast accumulation of technical knowledge, instead of speeding up the productivity of subsequent knowledge creation, has, on the contrary, become a “burden of knowledge” that makes it harder to find new ideas. We engage with these concerns by examining nearly 7 million utility patents granted by the U.S. Patent Office and characterizing the growth process of patenting from 1976 to 2018. Although the rate of patenting has steadily increased, patenting productivity as measured as patents per distinct inventor has continuously declined in utility patents in general and for technological frontier fields of biotechnology, climate change mitigation and adaptation, and artificial intelligence. The rapid growth rate of new patents can be credited to an increase in the number of individuals engaged in inventive activity rather than improved productivity. In the U.S., the proportion of the population engaging in patenting has grown significantly. Nevertheless, the growth of the inventive labor force and new patents relies more heavily on experienced inventors than new inventors. As the size of patenting teams keeps growing, the typical inventor participates in a growing number of patents while representing a declining proportion of the inventive labor responsible for patented inventions. We find evidence that as the stock of accumulated patented inventions grows, patenting productivity declines, suggesting that past invention makes it harder for inventors to find new knowledge. In the language of economics, invention (as tracked by patenting) has experienced extensive growth driven by the increase of the inventive labor force with declining productivity and a growing division of labor.},
}

@article{Shennan2024,
  title = {A review of spaceborne synthetic aperture radar for invasive alien plant research},
  author = {Glen Shennan and Richard Crabbe},
  year = {2024},
  journal = {Remote Sensing Applications: Society and Environment},
  volume = {36},
  pages = {101358},
  doi = {https://doi.org/10.1016/j.rsase.2024.101358},
  url = {https://www.sciencedirect.com/science/article/pii/S2352938524002222},
  abstract = {Recently, a strong international focus has been placed on invasive species and their ecological, economic, and social impacts. Satellite remote sensing (SRS) for the detection of invasive alien plants (IAPs) is a promising and actively researched application of satellite-derived earth observation data. Despite its all-day, all-weather detection and mapping capability, synthetic aperture radar (SAR) data is underrepresented in these efforts. This review discussed the foundational elements and capabilities of spaceborne SAR for IAP monitoring and investigated the current state of the scientific literature concerning the detection and monitoring of IAPs by spaceborne SAR. Twenty-six published articles were discovered and analysed for trends. The analysis revealed several key findings regarding the current state of SAR in the detection and monitoring of IAPs. Data fusion techniques, especially those combining SAR with multispectral data, are gaining popularity due to their improved performance compared to single-sensor approaches. However, the full potential of SAR imagery, particularly polarimetric SAR (PolSAR), remains underutilised in multi-sensor studies. SAR analyses demonstrated strong performance in scenarios where the IAP structure exhibited distinct characteristics compared to its surroundings, such as plants isolated on water surfaces or palms displacing mangroves, due to the unique interactions of microwave radiation with the structural characteristics of targets. Several key principles in the deployment of SAR were identified, including band and polarisation selection, basic techniques such as grey-level thresholding, and more advanced analyses such as polarimetry. Also noted are the capabilities of SAR in enabling indirect methods, such as inundation mapping and soil modelling. Suggestions are made for future directions in consideration of recently launched and forthcoming spaceborne SAR sensors. Significant among these are fully polarimetric systems which will provide freely accessible data, offering huge opportunities for sophisticated PolSAR analyses. This data will need to be fully exploited to advance species-level IAP detection and monitoring. Examples of IAPs which may benefit from SAR approaches are given, with special attention paid to the Australian Weeds of National Significance (WoNS).},
}

@article{Hong2024,
  title = {Benign water quality and phytoplankton status during the operation of Guishan offshore wind farm in the Pearl River Estuary, China},
  author = {Yuankai Hong and Ziyun Liu and Shubing Li and Yan Long and Jiong Gao and Ren Hu and Jinshao Ye},
  year = {2024},
  journal = {Regional Studies in Marine Science},
  volume = {77},
  pages = {103637},
  doi = {https://doi.org/10.1016/j.rsma.2024.103637},
  url = {https://www.sciencedirect.com/science/article/pii/S2352485524002706},
  abstract = {Wind power is a clean and renewable source of energy that plays a crucial role in the global effort to combat climate change. Yet, the effects of offshore wind farms on the marine environment remain poorly understood and there is a lack of field data analysis in this area. This study aims to address these gaps by conducting a year-long examination of water quality and phytoplankton communities around the Guishan offshore wind farm (GS-OWF) in the Lingdingyang Bay, Pearl River Estuary, China. The analysis of water quality indicators reveals repelling effect on algae dispersion but no significant contamination resulting from the running of the wind farm over the course of one year. These indicators encompassed various parameters, such as water depth (4.1 − 17.4 m), salinity (16.9 – 28.4 ‰), pH (7.41 – 8.35), temperature (20.5 − 31.2 °C), transparency (⁓ 1.5 m), dissolved oxygen (⁓ 7.5 mg L−1), chemical oxygen demand (⁓ 0.81 mg L−1), chlorophyll-a (14.7 − 0.139 μg L−1), petroleum (0 – 0.0527 mg L−1), and various heavy metals. The cell composition (4 phyla, 158 species, dominated by Bacillariophyta) and density (3 – 1809 cells m−3) demonstrate a healthy phytoplankton community. These results indicate a benign estuary environment after the construction of GS-OWF. This study provides valuable insights into the environmental effects of offshore wind farms, principles for site selection, and the integration of aquaculture convergence with offshore wind farms.},
}

@article{Fenlon2025,
  title = {Digital Humanities},
  author = {Katrina Fenlon and Emily Frazier and Trevor Muñoz},
  year = {2025},
  pages = {501-510},
  doi = {https://doi.org/10.1016/B978-0-323-95689-5.00140-1},
  publisher = {Academic Press},
  url = {https://www.sciencedirect.com/science/article/pii/B9780323956895001401},
  abstract = {Digital humanities is a domain of research and practice focused on the application of digital technologies to knowledge production within and across humanistic disciplines. Through interdisciplinary collaborations, the field fosters research applying digital technologies to humanities inquiry, including technologies for representing and analyzing evidence, and technologies for scholarly communication and knowledge sharing. Critical studies of technology and its applications, novel methods and tools for humanities scholarship, and the transformation of knowledge production are core areas of focus in the digital humanities. Contemporary digital humanities research and practice aim to increase inclusion and diversity in the field, grow global equity and collaboration, engage communities beyond the academy, increase public impact toward social justice, ensure sustainability for the field and digital modes of production, and advance radical transformation for broader systems of cultural knowledge production.},
}

@article{AbuRaed2024,
  title = {Long COVID Discourse in Canada, the United States, and Europe: Topic Modeling and Sentiment Analysis of Twitter Data},
  author = {Ahmed Ghassan Tawfiq AbuRaed and Emil Azuma Prikryl and Giuseppe Carenini and Naveed Zafar Janjua},
  year = {2024},
  journal = {Journal of Medical Internet Research},
  volume = {26},
  doi = {https://doi.org/10.2196/59425},
  url = {https://www.sciencedirect.com/science/article/pii/S143888712400921X},
  abstract = {Background Social media serves as a vast repository of data, offering insights into public perceptions and emotions surrounding significant societal issues. Amid the COVID-19 pandemic, long COVID (formally known as post–COVID-19 condition) has emerged as a chronic health condition, profoundly impacting numerous lives and livelihoods. Given the dynamic nature of long COVID and our evolving understanding of it, effectively capturing people’s sentiments and perceptions through social media becomes increasingly crucial. By harnessing the wealth of data available on social platforms, we can better track the evolving narrative surrounding long COVID and the collective efforts to address this pressing issue. Objective This study aimed to investigate people’s perceptions and sentiments around long COVID in Canada, the United States, and Europe, by analyzing English-language tweets from these regions using advanced topic modeling and sentiment analysis techniques. Understanding regional differences in public discourse can inform tailored public health strategies. Methods We analyzed long COVID–related tweets from 2021. Contextualized topic modeling was used to capture word meanings in context, providing coherent and semantically meaningful topics. Sentiment analysis was conducted in a zero-shot manner using Llama 2, a large language model, to classify tweets into positive, negative, or neutral sentiments. The results were interpreted in collaboration with public health experts, comparing the timelines of topics discussed across the 3 regions. This dual approach enabled a comprehensive understanding of the public discourse surrounding long COVID. We used metrics such as normalized pointwise mutual information for coherence and topic diversity for diversity to ensure robust topic modeling results. Results Topic modeling identified five main topics: (1) long COVID in people including children in the context of vaccination, (2) duration and suffering associated with long COVID, (3) persistent symptoms of long COVID, (4) the need for research on long COVID treatment, and (5) measuring long COVID symptoms. Significant concern was noted across all regions about the duration and suffering associated with long COVID, along with consistent discussions on persistent symptoms and calls for more research and better treatments. In particular, the topic of persistent symptoms was highly prevalent, reflecting ongoing challenges faced by individuals with long COVID. Sentiment analysis showed a mix of positive and negative sentiments, fluctuating with significant events and news related to long COVID. Conclusions Our study combines natural language processing techniques, including contextualized topic modeling and sentiment analysis, along with domain expert input, to provide detailed insights into public health monitoring and intervention. These findings highlight the importance of tracking public discourse on long COVID to inform public health strategies, address misinformation, and provide support to affected individuals. The use of social media analysis in understanding public health issues is underscored, emphasizing the role of emerging technologies in enhancing public health responses.},
}

@article{Singh2025_01,
  title = {Evaluating diabetes dataset for knowledge graph embedding based link prediction},
  author = {Sushmita Singh and Manvi Siwach},
  year = {2025},
  journal = {Data & Knowledge Engineering},
  volume = {157},
  pages = {102414},
  doi = {https://doi.org/10.1016/j.datak.2025.102414},
  url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000096},
  abstract = {For doing any accurate analysis or prediction on data, a complete and well-populated dataset is required. Medical based data for any disease like diabetes is highly coupled and heterogeneous in nature, with numerous interconnections. This inherently complex data cannot be analysed by simple relational databases making knowledge graphs an ideal tool for its representation which can efficiently handle intricate relationships. Thus, knowledge graphs can be leveraged to analyse diabetes data, enhancing both the accuracy and efficiency of data-driven decision-making processes. Although substantial data exists on diabetes in various formats, the availability of organized and complete datasets is limited, highlighting the critical need for creation of a well-populated knowledge graph. Moreover while developing the knowledge graph, an inevitable problem of incompleteness is present due to missing links or relationships, necessitating the use of knowledge graph completion tasks to fill in this absent information which involves predicting missing data with various Link Prediction (LP) techniques. Among various link prediction methods, approaches based on knowledge graph embeddings have demonstrated superior performance and effectiveness. These knowledge graphs can support in-depth analysis and enhance the prediction of diabetes-associated risks in this field. This paper introduces a dataset specifically designed for performing link prediction on a diabetes knowledge graph, so that it can be used to fill the information gaps further contributing in the domain of risk analysis in diabetes. The accuracy of the dataset is assessed through validation with state-of-the-art embedding-based link prediction methods.},
}

@article{Gray2024,
  title = {A Bayesian active learning approach to comparative judgement within education assessment},
  author = {Andy Gray and Alma Rahat and Tom Crick and Stephen Lindsay},
  year = {2024},
  journal = {Computers and Education: Artificial Intelligence},
  volume = {6},
  pages = {100245},
  doi = {https://doi.org/10.1016/j.caeai.2024.100245},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000481},
  abstract = {Assessment is a crucial part of education. Traditional marking is a source of inconsistencies and unconscious bias, placing a high cognitive load on the assessors. One approach to address these issues is comparative judgement (CJ). In CJ, the assessor is presented with a pair of items of work, and asked to select the better one. Following a series of comparisons, a rank for any item may be derived using a ranking model, for example, the Bradley-Terry model, based on the pairwise comparisons. While CJ is considered to be a reliable method for conducting marking, there are concerns surrounding its transparency, and the ideal number of pairwise comparisons to generate a reliable estimation of the rank order is not known. Additionally, there have been attempts to generate a method of selecting pairs that should be compared next in an informative manner, but some existing methods are known to have created their own bias within results inflating the reliability metric used within the process. As a consequence, a random selection approach is usually deployed. In this paper, we propose a novel Bayesian approach to CJ (which we call BCJ) for determining the ranks of a range of items under scrutiny alongside a new way to select the pairs to present to the marker(s) using active learning, addressing the key shortcomings of traditional CJ. Furthermore, we demonstrate how the entire approach may provide transparency by providing the user insights into how it is making its decisions and, at the same time, being more efficient. Results from our synthetic experiments confirm that the proposed BCJ combined with entropy-driven active learning pair-selection method is superior (i.e. always equal to or significantly better) than other alternatives, for example, the traditional CJ method with differing selection methods such as uniformly random, or the popular no repeating pairs where pairs are selected in a round-robin fashion. We also find that the more comparisons that are conducted, the more accurate BCJ becomes, which solves the issue the current method has of the model deteriorating if too many comparisons are performed. As our approach can generate the complete predicted rank distribution for an item, we also show how this can be utilised in probabilistically devising a predicted grade, guided by the choice of the assessor. Finally, we demonstrate our approach on a real dataset on assessing GCSE (UK school-level) essays, highlighting the advantages of BCJ over CJ.},
}

@article{Egieya2024,
  title = {Predictive simulation of the water-energy-food nexus for the City of Cape Town},
  author = {J.M. Egieya and Y. Parker and V.S. Hofmann and B. Daher and J. Gorgens and N.J. Goosen},
  year = {2024},
  journal = {Science of The Total Environment},
  volume = {934},
  pages = {173289},
  doi = {https://doi.org/10.1016/j.scitotenv.2024.173289},
  url = {https://www.sciencedirect.com/science/article/pii/S0048969724034363},
  abstract = {The City of Cape Town (CoCT), South Africa faced a critical situation between 2015 and 2018 in which the municipal water supply was almost completely exhausted. This situation, commonly referred to as Day Zero in South Africa emanated from a decline in rainfall, resulting in one of the most severe droughts in history. The crisis was also aggravated by rapid population growth and urbanization. CoCT was on the verge of becoming the first city in the past decade to experience a complete cessation of water supply for urban and agricultural purposes. In addition to the effects of low rainfall and population surge, urban energy consumption and increased food demand impacted directly the available water resources. To evaluate the interlinkages between water utilization, water production, energy supply and demand, and food production and demand, this study employed a system dynamics modeling (SDM) approach. The model was developed as a stock and flow diagram utilizing Stella Architect and encompassed five interconnected nodes: water, energy, food, land, and population. The findings revealed that by the end of the 20-year modeling period, the volume of accessible and stored water in all the major dams will be approximately 459 million cubic meters, with residential use accounting for about 85 % of urban water use and agriculture accounting for approximately30.37 % of total water demand. The model illustrates the impacts of precipitation rate, runoff, and evaporation on variables such as land-use change and population dynamics. It is anticipated that the outcomes of this study will serve as valuable inputs for decision-making processes, not only within the CoCT as it aims to mitigate or prevent the recurrence of Day Zero, but also for other cities facing similar challenges.},
}

@article{Jin2024_02,
  title = {A comparative analysis of the spatial determinants of e-bike and e-scooter sharing link flows},
  author = {Scarlett T. Jin and Daniel Z. Sui},
  year = {2024},
  journal = {Journal of Transport Geography},
  volume = {119},
  pages = {103959},
  doi = {https://doi.org/10.1016/j.jtrangeo.2024.103959},
  url = {https://www.sciencedirect.com/science/article/pii/S0966692324001686},
  abstract = {Shared micromobility in the U.S. has rebound after the decline caused by the COVID-19 pandemic, with a substantial increase in the adoption of shared e-bikes nationwide. However, research on hybrid e-bike sharing, which combines station-based and dockless systems, is limited. This study addresses this gap by comparing spatial determinants of hybrid e-bike and dockless e-scooter sharing link flows in 32,965 street segments in Portland, Oregon during 2022, using gradient boosting decision tree (GBDT) models. Distance to the city center emerges as the most important determinant for both modes, with closer proximity to the city center associated with higher link flows. Factors such as the presence and types of bike facilities, the availability of streetlights and street trees, and job density also significantly influence e-bike and e-scooter link flows. A notable difference between the two modes is that e-scooter trips are more sensitive to distance to the city center than e-bike trips. Furthermore, bike facilities have a greater impact on e-bike link flows, whereas job density is more influential in determining e-scooter link flows. These findings offer strategies for policymakers and urban planners to promote and manage shared micromobility and optimize the built environment. These strategies include enforcing higher device availability requirements in underprivileged neighborhoods, transitioning e-scooter sharing systems into a hybrid model, expanding the off-street bike trial network and bikeway network, and augmenting the coverage of streetlights and street trees along the bikeway network.},
}

@article{Yang2024,
  title = {Assessing climate risks from satellite imagery with machine learning: A case study of flood risks in Jakarta},
  author = {Jeasurk Yang and Donghyun Ahn and Junbeom Bahk and Sungwon Park and Nurrokhmah Rizqihandari and Meeyoung Cha},
  year = {2024},
  journal = {Climate Risk Management},
  volume = {46},
  pages = {100651},
  doi = {https://doi.org/10.1016/j.crm.2024.100651},
  url = {https://www.sciencedirect.com/science/article/pii/S2212096324000688},
  abstract = {Consistent and timely assessment of climate risks is crucial for planning disaster mitigation and adaptation to climate change at the local community level. This article presents an automatized method for monitoring climate risks with machine learning on satellite imagery, specially targeting riverine and coastal floods. Our research demonstrates that disaster-related risk measurement becomes more comprehensive and multi-faceted by including the following components: hazards, exposure, and vulnerability. Our model first maps hazard-related risks with geo-spatial data, then extends the model to incorporate exposure and vulnerability. In doing so, we adopt a clustering-based supervised algorithm to sort satellite images to produce the climate risk scores at a grid-level. The developed model was tested over multiple ground-truth datasets on flood risks in the region of Jakarta, Indonesia. Results confirm that our model can assess climate risks in a granular scale and further capture potential risks in the marginalized areas (e.g., slums) that were previously hard to predict. We discuss how computational methods like ours can support humanitarian projects for developing countries.},
}

@article{Nakai2025,
  title = {Spatial optimisation for managed retreat and nature-based solutions in climate adaptation},
  author = {Fuko Nakai and Seiya Kito and Kazuaki Okubo},
  year = {2025},
  journal = {Sustainable Cities and Society},
  volume = {124},
  pages = {106246},
  doi = {https://doi.org/10.1016/j.scs.2025.106246},
  url = {https://www.sciencedirect.com/science/article/pii/S2210670725001234},
  abstract = {Managed retreat and nature-based solutions (NBSs) are major pillars of climate change adaptation, offering alternatives or complements to conventional engineering-based solutions. When an NBS is installed in retreat areas, it not only mitigates local flood risks but may also reduce risks in other regions, generating co-benefits. In contrast, when installed in inappropriate locations, NBSs may create spatial conflicts due to unacceptable costs while providing limited risk reduction. Spatial optimisation models are powerful tools for sustainable land use planning, satisfying various urban needs. However, their application in integrating managed retreat with strategic NBSs remains underexplored. This study addresses two key research gaps: (1) how managed retreat should be implemented and (2) whether/where NBSs can be installed when managed retreat is implemented. To explore these questions, we employ a spatial optimisation model aiming to maximise urban net profit by considering revenues and costs related to flood resilience, city maintenance, and land conversion, and we conduct a cost–benefit analysis. This study focuses on discontinuous levees — one of Japan’s indigenous flood control technologies — as an NBS alternative to continuous levees, an engineering-based solution widely installed in modern society. These two types of levees influence flood spatial patterns differently, affecting optimal land use allocation in the model. A case study of Toyohashi City shows that certain discontinuous levee scenarios outperform continuous levees in cost-benefit performance while requiring less retreat. These findings highlight the advantages of integrating managed retreat with NBSs, bridging the gap between land use planning and flood infrastructure management to promote sustainable urban development.},
}

@article{Mahboob2024,
  title = {Predictive modelling of mineral prospectivity using satellite remote sensing and machine learning algorithms},
  author = {Muhammad Ahsan Mahboob and Turgay Celik and Bekir Genc},
  year = {2024},
  journal = {Remote Sensing Applications: Society and Environment},
  volume = {36},
  pages = {101316},
  doi = {https://doi.org/10.1016/j.rsase.2024.101316},
  url = {https://www.sciencedirect.com/science/article/pii/S2352938524001800},
  abstract = {In today's world of falling returns on fixed exploration budgets, complex targets, and ever-increasing volumes of multi-parameter datasets, the effective management and integration of existing data are essential to any mineral exploration operation. Machine learning (ML) algorithms like Convolutional Neural Networks (CNN), Random Forest (RF), and Support Vector Machine (SVM) are powerful data-driven methods that are not implemented very often with remote sensing-derived hydrothermal alternation information and limited field datasets for mapping mineral prospectivity. The application of machine learning algorithms with satellite remote sensing data and limited field data, they have not been compared and evaluated together thoroughly in this field. A data science approach was applied to create nine predictor maps, incorporating limited field data and satellite remote sensing information. A confusion matrix, statistical measures, and a Receiver Operating Characteristic (ROC) curve were used to evaluate the prediction models efficacy on both the training and test datasets. The results suggested that the RF model exhibited the highest predictive accuracy, consistency and interpretability among the three ML models evaluated in this study. RF model also achieved the highest predictive efficiency in capturing known copper (Cu) deposits within a small prospective area. In comparison to the SVM and CNN models, the RF model outperformed them in terms of predictive accuracy and interpretability. These results imply that the RF model is the most suitable for Cu potential mapping in the Pakistan's North Waziristan region. Consequently, all the models including the RF model were used to generate a prospectivity map, which contained low to very-high potential zones, to support further exploration in the region. The newly discovered deposit inside the predicted prospective areas demonstrates the robustness and efficacy of the prospectivity modelling approach as proposed in this research for generating exploration targets.},
}

@article{Martins}2024,
  title = {Assessing the effectiveness of financial incentives on electric vehicle adoption in Europe: Multi-period difference-in-difference approach},
  author = {Edlaine {Correia Sinézio Martins} and Julien Lépine and Jacqueline Corbett},
  year = {2024},
  journal = {Transportation Research Part A: Policy and Practice},
  volume = {189},
  pages = {104217},
  doi = {https://doi.org/10.1016/j.tra.2024.104217},
  url = {https://www.sciencedirect.com/science/article/pii/S0965856424002659},
  abstract = {Electric vehicles (EVs) are considered a promising alternative to achieve a cleaner transportation sector. In the last decade, European countries have implemented financial incentive policies to boost EV adoption. This paper estimates the impacts of these policies on EV adoption in Europe using data from 30 countries from 2012 to 2021 and a multi-period difference-in-differences approach. Our results reveal that purchase incentive policies are associated with increased registrations of battery electric vehicles and plug-in hybrid vehicles, and that the effect holds over time. However, the magnitude and duration of these effects are more significant for battery electric vehicles. Ownership incentive policies do not contribute to EV registrations for either type. Further, the results suggest that policy impacts vary between countries with different levels of gross domestic product per capita and renewable energy consumption. These results contribute to the literature on evaluating financial incentive policies for EV adoption, enabling improved decision making by policymakers.},
}

@article{Wang2024_010,
  title = {The cyber-industrialization of catfishing and romance fraud},
  author = {Fangzhou Wang and Volkan Topalli},
  year = {2024},
  journal = {Computers in Human Behavior},
  volume = {154},
  pages = {108133},
  doi = {https://doi.org/10.1016/j.chb.2023.108133},
  url = {https://www.sciencedirect.com/science/article/pii/S0747563223004843},
  abstract = {We examine a new form of online fraud, which we refer to as Intimacy Manipulated Fraud Industrialization (IMFI). This type of fraud bears a strong resemblance to traditional online romance fraud and catfishing, but is “industrialized” through enterprise business practices, software platforms, and customer service processes. To gain a better understanding of this operation, we conducted an inductive analysis of publicly available testimonial and review data provided by current and prior employees of a specific company in the online customer service contract space. Companies hire individuals online to work as “chat moderators” or “customer service providers” who are told that their task is to advance engagement on social media platforms. In fact, they are being recruited as “sexting” workers, paid on a per-text basis to engage in intimate chatting with clients who believe they're interacting with individuals of the opposite gender on a dating site. The process is mediated via client management processes that monitor employee productivity and monetize all interactions between “clients” and “workers.” The company executes these processes with great efficiency by algorithmically assigning multiple workers to individual clients and assembling background files on clients in real time. We find that workers serve as both exploiters of their clients as well as victims of the company they work for. The implications of our study could significantly impact how we address AI-generated online fraud in the future, shedding light on the complex dynamics at play within these fraudulent enterprises.},
}

@article{Jaiswal2024_01,
  title = {Ethics of the Use of Social Media as Training Data for AI Models Used for Digital Phenotyping},
  author = {Aditi Jaiswal and Aekta Shah and Christopher Harjadi and Erik Windgassen and Peter Washington},
  year = {2024},
  journal = {JMIR Formative Research},
  volume = {8},
  doi = {https://doi.org/10.2196/59794},
  url = {https://www.sciencedirect.com/science/article/pii/S2561326X24003962},
  abstract = {Digital phenotyping, or personal sensing, is a field of research that seeks to quantify traits and characteristics of people using digital technologies, usually for health care purposes. In this commentary, we discuss emerging ethical issues regarding the use of social media as training data for artificial intelligence (AI) models used for digital phenotyping. In particular, we describe the ethical need for explicit consent from social media users, particularly in cases where sensitive information such as labels related to neurodiversity are scraped. We also advocate for the use of community-based participatory design principles when developing health care AI models using social media data.},
}

@article{Islam2024_01,
  title = {Diagnostic and Prognostic Electrocardiogram-Based Models for Rapid Clinical Applications},
  author = {Md Saiful Islam and Sunil Vasu Kalmady and Abram Hindle and Roopinder Sandhu and Weijie Sun and Nariman Sepehrvand and Russell Greiner and Padma Kaul},
  year = {2024},
  journal = {Canadian Journal of Cardiology},
  volume = {40},
  pages = {1788-1803},
  doi = {https://doi.org/10.1016/j.cjca.2024.07.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0828282X24005233},
  abstract = {Leveraging artificial intelligence (AI) for the analysis of electrocardiograms (ECGs) has the potential to transform diagnosis and estimate the prognosis of not only cardiac but, increasingly, noncardiac conditions. In this review, we summarize clinical studies and AI-enhanced ECG-based clinical applications in the early detection, diagnosis, and estimating prognosis of cardiovascular diseases in the past 5 years (2019-2023). With advancements in deep learning and the rapid increased use of ECG technologies, a large number of clinical studies have been published. However, most of these studies are single-centre, retrospective, proof-of-concept studies that lack external validation. Prospective studies that progress from development toward deployment in clinical settings account for < 15% of the studies. Successful implementations of ECG-based AI applications that have received approval from the Food and Drug Administration have been developed through commercial collaborations, with approximately half of them being for mobile or wearable devices. The field is in its early stages, and overcoming several obstacles is essential, such as prospective validation in multicentre large data sets, addressing technical issues, bias, privacy, data security, model generalizability, and global scalability. This review concludes with a discussion of these challenges and potential solutions. By providing a holistic view of the state of AI in ECG analysis, this review aims to set a foundation for future research directions, emphasizing the need for comprehensive, clinically integrated, and globally deployable AI solutions in cardiovascular disease management. Résumé L’utilisation de l’intelligence artificielle (IA) pour analyser les électrocardiogrammes (ECG) pourrait transformer le diagnostic et l’estimation du pronostic non seulement pour les maladies cardiaques, mais également pour les maladies non cardiaques. Dans cette analyse, nous résumons les études cliniques et les applications cliniques de l’ECG amélioré par l’IA dans le dépistage précoce, le diagnostic et l’estimation du pronostic des maladies cardiovasculaires (MCV) au cours des cinq dernières années (2019-2023). Avec les progrès de l’apprentissage profond et l’utilisation croissante des technologies d’ECG, un grand nombre d’études cliniques ont été publiées. Cependant, une majorité de ces dernières sont des études de validation de principe, rétrospectives et monocentriques qui n’ont pas été soumises à une validation externe. Les études prospectives qui portent tant sur le développement que sur le déploiement de ces technologies dans un contexte clinique comptent pour moins de 15 % des études. Des implantations réussies d’applications d’IA pour l’ECG qui ont fait l’objet d’une approbation par la Food and Drug Administration (FDA) sont issues de collaborations commerciales, et environ la moitié d’entre elles portent sur des appareils mobiles ou portables. Ce domaine en est à ses balbutiements et plusieurs obstacles doivent être surmontés, comme la validation prospective dans des ensembles de données multicentriques, la correction des problèmes techniques, les biais, la confidentialité, la sécurité des données, le caractère généralisable des modèles et l’extensibilité à l’échelle mondiale. Nous terminons par un survol des problèmes et des solutions possibles. En fournissant une perspective holistique sur la situation de l’IA dans l’analyse des ECG, cette étude vise à jeter les bases de recherches futures, en mettant l’accent sur le besoin de solutions d’IA complètes, cliniquement intégrées et transposables partout dans le monde pour la prise en charge des MCV.},
}

@article{Uribe-Velázquez2025,
  title = {From waste to value: Mitigating the environmental impact of whey in Jalisco, Mexico},
  author = {Tlalli Uribe-Velázquez and Diego Díaz-Vázquez and Paloma Barajas-Álvarez and Martín Esteban González-López and Misael Sebastián Gradilla-Hernández and Luis Eduardo Garcia-Amezquita and Danay Carrillo-Nieves and Tomás García-Cayuela},
  year = {2025},
  journal = {Journal of Cleaner Production},
  volume = {501},
  pages = {145334},
  doi = {https://doi.org/10.1016/j.jclepro.2025.145334},
  url = {https://www.sciencedirect.com/science/article/pii/S0959652625006845},
  abstract = {Whey, a liquid by-product of cheese and yogurt production, represents a substantial environmental challenge in the dairy industry. This study focuses on Jalisco, Mexico, a major dairy-producing region, to assess the environmental impact of whey disposal. Through a combination of physicochemical analysis, geographic information systems, and Life Cycle Assessment (LCA), we investigated the current whey management practices and their associated environmental burdens. Whey samples from 19 dairy processing units were analyzed, revealing high organic loads with biological oxygen demand ranging from 9.42 to 50.05 g/L, and total nitrogen and phosphorus levels of 1019.93 mg/L and 171.72 mg/L, respectively, exceeding local and international discharge limits. The region produces approximately 322,942 m3 of whey annually, with 67 % concentrated in 10 municipalities. LCA compared three whey management scenarios: direct disposal (DD), anaerobic digestion (AD), and whey powder (WP) production. DD contributed to freshwater and marine eutrophication (0.26 kg P eq and 0.23 kg N eq, respectively). AD emerged as the most sustainable option, reducing greenhouse gas emissions and generating biogas as a renewable energy source. WP production, while adding economic and nutritional value, had a global warming potential of 96.89 Kg CO2 eq due to energy-intensive processing. Centralized treatment systems were identified as a viable solution for small producers, enabling shared infrastructure and cost reduction. These findings provide a scalable framework for sustainable whey management strategies, applicable to other dairy-producing regions.},
}

@article{Bernasconi2024,
  title = {Information and Communication Technology to Enhance the Implementation of the Integrated Management of Childhood Illness: A Systematic Review and Meta-Analysis},
  author = {Andrea Bernasconi and Marco Landi and Clarence S. Yah and Marianne A.B. {van der Sande}},
  year = {2024},
  journal = {Mayo Clinic Proceedings: Digital Health},
  volume = {2},
  pages = {438-452},
  doi = {https://doi.org/10.1016/j.mcpdig.2024.06.005},
  url = {https://www.sciencedirect.com/science/article/pii/S294976122400066X},
  abstract = {Objective To evaluate the impact of Information and Communication Technology (ICT) on the implementation of Integrated Management of Childhood Illness (IMCI) and integrated Community Case Management (iCCM) through a systematic review and meta-analysis (PROSPERO registration number: CRD42024517375). Methods We searched MEDLINE, EMBASE, Cochrane Library, and gray literature from January 2010 to February 2024, focusing on IMCI/iCCM-related terms (Integrated Management of Childhood Illness, IMCI, integrated Community Case Management, iCCM) and excluding non-ICT interventions. A meta-analysis synthesized the effect of ICT on clinical assessment, disease classification, therapy, and antibiotic prescription through odds ratio (OR; 95% CI) employing a random effects model for significant heterogeneity (I2>50%) and conducting subgroup analyses. Results Of 1005 initial studies, 44 were included, covering 8 interventions for IMCI, 7 for iCCM, and 2 for training. All digital interventions except 1 outperformed traditional paper-based methods. Pooling effect sizes from 16 studies found 5.7 OR for more complete clinical assessments (95% CI, 1.7-19.1; I2, 95%); 2.0 for improved disease classification accuracy (95% CI, 0.9-4.4; I2, 93%); 1.4 for more appropriate therapy (95% CI, 0.8-2.2; I2, 93%); and 0.2 for reduced antibiotic use (95% CI, 0.06-0.55; I2 99%). Conclusion This review is the first to comprehensively quantify the effect of ICT on the implementation of IMCI/iCCM programs, confirming both the benefits and limitations of these technologies. The customization of digital tools for IMCI/iCCM can serve as a model for other health programs. As ICT increasingly supports the achievement of sustainable development goals, the effective digital interventions identified in this review can pave the way for future innovations.},
}

@article{Cosentino2024,
  title = {Measuring the psychosocial impact of COVID-19 by means of the “international student well-being study questionnaire”: Evidence on Italian university students},
  author = {Chiara Cosentino and Annavittoria Sarli and Massimo Guasconi and Fabio Mozzarelli and Chiara Foà and Rosangela {De Simone} and Dimitris Argiropoulos and Giovanna Artioli and Antonio Bonacaro},
  year = {2024},
  journal = {Heliyon},
  volume = {10},
  pages = {e28342},
  doi = {https://doi.org/10.1016/j.heliyon.2024.e28342},
  url = {https://www.sciencedirect.com/science/article/pii/S2405844024043731},
  abstract = {Background The COVID-19 pandemic appeared as an unpredictable disruption of daily activities. This situation produced a unique mental health impact for the general population, youth, and vulnerable groups. A documented consequence has been alcohol abuse and impaired mental health. To our knowledge, no published study has yet evaluated the rates of depressive symptoms, academic frustration, and substance abuse in the Italian student population in the COVID era linking them to sociodemographic variables. Aims To investigate the incidence of depressive symptoms, academic frustration, and substance abuse students developed in one university in Northern Italy during the first COVID-19 outbreak, using a student wellbeing framework borrowed from Allardt's and Bronfenbrenner's theories. Methods Descriptive statistics, correlations, Wilcoxon test and factorial ANOVA were performed on data gathered through an online questionnaire sent to a convenience sample of university students. Data collection occurred between 18 May and June 21, 2020. Results According to the framework used, Being was the dimension of sociodemographic variables, Having was economic support and Loving was social support. Students experienced academic frustration and related depressive symptoms. Male gender seemed to be less affected by the depressive syndrome. With regard to economic support and related repercussions on depressive symptoms, students who reported that they could not obtain a loan showed significantly higher depressive symptoms. Students benefiting from greater financial support showed less academic frustration. Age appeared to be a predictor of academic frustration since young students proved to be the most frustrated. The examined students’ population showed a significant decrease in alcohol and cigarettes consumption. Conclusions Having and Being dimensions influenced depression and academic frustration. Having and Loving dimensions were related with use of substances. The study findings support the implementation of interventions to help students, at a psychological, social, financial level aiming at fostering the quality of students’ educational life.},
}

@article{Jiang2024_01,
  title = {Generative urban design: A systematic review on problem formulation, design generation, and decision-making},
  author = {Feifeng Jiang and Jun Ma and Christopher John Webster and Alain J.F. Chiaradia and Yulun Zhou and Zhan Zhao and Xiaohu Zhang},
  year = {2024},
  journal = {Progress in Planning},
  volume = {180},
  pages = {100795},
  doi = {https://doi.org/10.1016/j.progress.2023.100795},
  url = {https://www.sciencedirect.com/science/article/pii/S0305900623000569},
  abstract = {Urban design is the process of designing and shaping the physical forms of cities, towns, and suburbs. It involves the arrangement and design of street systems, groups of buildings, public spaces, and landscapes, to make the urban environment performative and sustainable. The typical design process, reliant on manual work and expert experience has unavoidable low efficiency in generating high-performing design solutions due to the involvement of complex social, institutional, and economic contexts and the trade-off between conflicting preferences of different stakeholder groups. Taking advantage of artificial intelligence (AI) and computational capacity, generative urban design (GUD) has been developed as a trending technical direction to narrow the gaps and produce design solutions with high efficiency at early design stages. It uses computer-aided generative methods, such as evolutionary optimization and deep generative models, to efficiently explore complex solution spaces and automatically generate design options that satisfy conflicting objectives and various constraints. GUD experiments have attracted much attention from academia, practitioners, and public authorities in recent years. However, a systematic review of the current stage of GUD research is lacking. This study, therefore, reports on a systematic investigation of the existing literature according to the three key stages in the GUD process: (1) design problem formulation, (2) design option generation, and (3) decision-making. For each stage, current trends, findings, and limitations from GUD studies are examined. Future directions and potential challenges are discussed and presented. The review is highly interdisciplinary and involves articles from urban study, computer science, social science, management, and other fields. It reports what scholars have found in GUD experiments and organizes a diverse and complicated technical agenda into something accessible to all stakeholders. The results and discoveries will serve as a holistic reference for GUD developers and users in both academia and industry and form a baseline for the field of GUD development in the coming years.},
}

@article{Petrucci2025,
  title = {Strategic prioritization of sewersheds to mitigate combined sewer overflows under climate change},
  author = {J. Petrucci and J. Jalbert and S. Dorner and N. McQuaid and F. Bichai},
  year = {2025},
  journal = {Environmental Challenges},
  volume = {18},
  pages = {101088},
  doi = {https://doi.org/10.1016/j.envc.2025.101088},
  url = {https://www.sciencedirect.com/science/article/pii/S2667010025000095},
  abstract = {The impact of combined sewer overflows (CSOs) on water bodies is well documented: they pose severe threats to water quality, ecosystems, and public health. Exposure to contaminants from overflows can lead to waterborne diseases, emphasizing the critical need for effective stormwater management. Mitigating the effects of CSOs can be achieved through various solutions, including blue-green infrastructure (BGI). However, the implementation of these solutions often occurs opportunistically rather than strategically, depending on the opportunities that arise. In addition, simulations under climate change predict a surge in extreme events, necessitating adaptation in urban planning and infrastructure design. This paper proposes a prioritization index to support the location choice for mitigation measures under current conditions and projected climate scenarios. The model's effectiveness is validated, and simulated precipitations generated by the Canadian Regional Climate Model version 5 (CRCM5) are used, revealing an exponential increase in CSO events over time due to climate change. The importance of spatial location in prioritizing urban catchments for mitigation measures implementation is emphasized, providing valuable insights for urban planners to navigate climate-induced challenges and protect water bodies.},
}

@article{Promma2025,
  title = {The influence of AI literacy on complex problem-solving skills through systematic thinking skills and intuition thinking skills: An empirical study in Thai gen Z accounting students},
  author = {Watcharawat Promma and Narinthon Imjai and Berto Usman and Somnuk Aujirapongpan},
  year = {2025},
  journal = {Computers and Education: Artificial Intelligence},
  volume = {8},
  pages = {100382},
  doi = {https://doi.org/10.1016/j.caeai.2025.100382},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000220},
  abstract = {The rapid integration of artificial intelligence (AI) into various sectors has heightened the need for understanding its impact on cognitive and problem-solving skills. This study investigates the influence of AI literacy (AIL) on complex problem-solving skills (CPS) by examining the mediating roles of systematic thinking skills (STS) and intuitive thinking skills (ITS) among Generation Z accounting students in Thailand. Utilizing structural equation modeling (SEM) to analyze the relationships between these constructs, the research draws on a sample of 420 final-year undergraduate accounting students from both public and private universities, selected through convenience sampling. The findings reveal that AIL significantly enhances both STS and ITS, which in turn play a crucial mediating role in the development of CPS. These results underscore the importance of integrating AIL into educational curricula, particularly to foster deeper cognitive abilities that are essential for effective problem-solving in the context of AI technology. Moreover, the study advocates for the implementation of training programs designed to cultivate these skills within the workforce, thereby preparing individuals to navigate the challenges of the digital era where technology is increasingly pivotal. The implications of this research extend to the formulation of educational policies, the design of training initiatives, and the development of human resource strategies aimed at enhancing the nation's competitive advantage on a global scale.},
}

@article{Evans2024,
  title = {Developments and applications of the OPTIMADE API for materials discovery, design, and data exchange††Electronic supplementary information (ESI) available: Copy of Table 1 with web links. See DOI: https://doi.org/10.1039/d4dd00039k},
  author = {Matthew L. Evans and Johan Bergsma and Andrius Merkys and Casper W. Andersen and Oskar B. Andersson and Daniel Beltrán and Evgeny Blokhin and Tara M. Boland and Rubén {Castañeda Balderas} and Kamal Choudhary and Alberto {Díaz Díaz} and Rodrigo {Domínguez García} and Hagen Eckert and Kristjan Eimre and María Elena {Fuentes Montero} and Adam M. Krajewski and Jens Jørgen Mortensen and José Manuel {Nápoles Duarte} and Jacob Pietryga and Ji Qi and Felipe de Jesús {Trejo Carrillo} and Antanas Vaitkus and Jusong Yu and Adam Zettel and Pedro Baptista {de Castro} and Johan Carlsson and Tiago F. T. Cerqueira and Simon Divilov and Hamidreza Hajiyani and Felix Hanke and Kevin Jose and Corey Oses and Janosh Riebesell and Jonathan Schmidt and Donald Winston and Christen Xie and Xiaoyu Yang and Sara Bonella and Silvana Botti and Stefano Curtarolo and Claudia Draxl and Luis Edmundo {Fuentes Cobas} and Adam Hospital and Zi-Kui Liu and Miguel A. L. Marques and Nicola Marzari and Andrew J. Morris and Shy},
  year = {2024},
  journal = {Digital Discovery},
  volume = {3},
  pages = {1509-1533},
  doi = {https://doi.org/10.1039/d4dd00039k},
  url = {https://www.sciencedirect.com/science/article/pii/S2635098X24001219},
  abstract = {The Open Databases Integration for Materials Design (OPTIMADE) application programming interface (API) empowers users with holistic access to a growing federation of databases, enhancing the accessibility and discoverability of materials and chemical data. Since the first release of the OPTIMADE specification (v1.0), the API has undergone significant development, leading to the v1.2 release, and has underpinned multiple scientific studies. In this work, we highlight the latest features of the API format, accompanying software tools, and provide an update on the implementation of OPTIMADE in contributing materials databases. We end by providing several use cases that demonstrate the utility of the OPTIMADE API in materials research that continue to drive its ongoing development.},
}

@article{Olenik2025,
  title = {Machine Learning and Omic Data for Prediction of Health and Chronic Diseases},
  author = {Mark Olenik and Handan Melike Dönertaş},
  year = {2025},
  pages = {365-388},
  doi = {https://doi.org/10.1016/B978-0-323-95502-7.00284-0},
  publisher = {Elsevier},
  url = {https://www.sciencedirect.com/science/article/pii/B9780323955027002840},
  abstract = {Machine learning (ML) combined with diverse omic datasets offers transformative potential for predicting health outcomes and chronic diseases. By leveraging diverse omic data, ML models can identify biomarkers, enhance diagnostic accuracy, and enable personalized treatments. This chapter introduces the fundamental concepts of ML, key omic data sources, and the challenges associated with ML and omic-based disease prediction. Advances in technology, large-scale datasets, interpretable ML algorithms, and the digitization of healthcare are poised to revolutionize medical science, paving the way for precision medicine and early disease detection.},
}

@article{Sharifi2025,
  title = {The metaverse as a future form of smart cities: A systematic literature review of co-benefits and trade-offs for sustainable development goals},
  author = {Ayyoob Sharifi and Melika Amirzadeh and Amir Reza Khavarian-Garmsir},
  year = {2025},
  journal = {Cities},
  volume = {161},
  pages = {105879},
  doi = {https://doi.org/10.1016/j.cities.2025.105879},
  url = {https://www.sciencedirect.com/science/article/pii/S0264275125001799},
  abstract = {The United Nations launched the Sustainable Development Goals (SDGs) in 2015 to address a multitude of complex challenges facing today's world. Concurrently, the concept of smart cities has evolved to embrace advanced digital technologies and artificial intelligence for urban management and sustainability. Within this context, the Metaverse has emerged as a virtual extension of smart cities, offering a revolutionary space to reimagine urban life and governance. This systematic review explores how the Metaverse, as a virtual form of smart cities, aligns with the SDGs. It focuses on identifying potential co-benefits and trade-offs between the Metaverse and SDGs. The results indicate that research primarily focuses on SDGs related to health, education, innovation, sustainable cities, and responsible consumption and production. In contrast, less attention is given to SDGs related to reducing inequalities, climate change, environmental sustainability, peace, and partnership. The synthesis highlights the potential of the Metaverse to enhance smart cities' operational efficiency, transparency, and evidence-based decision-making through big data analytics and virtual collaboration. However, it also identifies risks such as exacerbating disparities and increasing environmental burdens. While the Metaverse can offer transformative solutions for smart urban environments, its benefits must be balanced with equitable access and environmental sustainability to effectively contribute to the SDGs. This requires addressing the digital and technological divide and environmental concerns to foster inclusive urban societies that uphold peace, justice, and strong institutions. The review underscores the need for strategic policy frameworks to mitigate risks and maximize the positive impact of the Metaverse on sustainable and resilient urban development.},
}

@article{Dwivedi2024,
  title = {“Real impact”: Challenges and opportunities in bridging the gap between research and practice – Making a difference in industry, policy, and society},
  author = {Yogesh K. Dwivedi and Anand Jeyaraj and Laurie Hughes and Gareth H. Davies and Manju Ahuja and Mousa Ahmed Albashrawi and Adil S. Al-Busaidi and Salah Al-Sharhan and Khalid Ibrahim Al-Sulaiti and Levent Altinay and Shem Amalaya and Sunil Archak and María Teresa Ballestar and Shonil A. Bhagwat and Anandhi Bharadwaj and Amit Bhushan and Indranil Bose and Pawan Budhwar and Deborah Bunker and Alexandru Capatina and Lemuria Carter and Ioanna Constantiou and Crispin Coombs and Tom Crick and Csaba Csáki and Yves Darnige and Rahul Dé and Rick Delbridge and Rameshwar Dubey and Robin Gauld and Ravi Kumar Gutti and Marié Hattingh and Arve Haug and Leeya Hendricks and Airo Hino and Cathy H.C. Hsu and Netta Iivari and Marijn Janssen and Ikram Jebabli and Paul Jones and Iris Junglas and Abhishek Kaushik and Deepak Khazanchi and Mitsuru Kodama and Sascha Kraus and Vikram Kumar and Christian Maier and Tegwen Malik and Machdel Matthee and Ian P. McCarthy and Marco Meier and Bhimaraya Metri and Adrian M},
  year = {2024},
  journal = {International Journal of Information Management},
  volume = {78},
  pages = {102750},
  doi = {https://doi.org/10.1016/j.ijinfomgt.2023.102750},
  url = {https://www.sciencedirect.com/science/article/pii/S0268401223001317},
  abstract = {Achieving impact from academic research is a challenging, complex, multifaceted, and interconnected topic with a number of competing priorities and key performance indicators driving the extent and reach of meaningful and measurable benefits from research. Academic researchers are incentivised to publish their research in high-ranking journals and academic conferences but also to demonstrate the impact of their outputs through metrics such as citation counts, altmetrics, policy and practice impacts, and demonstrable institutional decision-making influence. However, academic research has been criticized for: its theoretical emphasis, high degree of complexity, jargon-heavy language, disconnect from industry and societal needs, overly complex and lengthy publishing timeframe, and misalignment between academic and industry objectives. Initiatives such as collaborative research projects and technology transfer offices have attempted to deliver meaningful impact, but significant barriers remain in the identification and evaluation of tangible impact from academic research. This editorial focusses on these aspects to deliver a multi-expert perspective on impact by developing an agenda to deliver more meaningful and demonstrable change to how “impact” can be conceptualized and measured to better align with the aims of academia, industry, and wider society. We present the 4D model - Design, Deliver, Disseminate, and Demonstrate - to provide a structured approach for academia to better align research endeavors with practice and deliver meaningful, tangible benefits to stakeholders.},
}

@article{Himeur2024,
  title = {Edge AI for Internet of Energy: Challenges and perspectives},
  author = {Yassine Himeur and Aya Nabil Sayed and Abdullah Alsalemi and Faycal Bensaali and Abbes Amira},
  year = {2024},
  journal = {Internet of Things},
  volume = {25},
  pages = {101035},
  doi = {https://doi.org/10.1016/j.iot.2023.101035},
  url = {https://www.sciencedirect.com/science/article/pii/S254266052300358X},
  abstract = {The digital landscape of the Internet of Energy (IoE) is on the brink of a revolutionary transformation with the integration of edge Artificial Intelligence (AI). This comprehensive review elucidates the promise and potential that edge AI holds for reshaping the IoE ecosystem. Commencing with a meticulously curated research methodology, the article delves into the myriad of edge AI techniques specifically tailored for IoE. The myriad benefits, spanning from reduced latency and real-time analytics to the pivotal aspects of information security, scalability, and cost-efficiency, underscore the indispensability of edge AI in modern IoE frameworks. As the narrative progresses, readers are acquainted with pragmatic applications and techniques, highlighting on-device computation, secure private inference methods, and the avant-garde paradigms of AI training on the edge. A critical analysis follows, offering a deep dive into the present challenges including security concerns, computational hurdles, and standardization issues. However, as the horizon of technology ever expands, the review culminates in a forward-looking perspective, envisaging the future symbiosis of 5G networks, federated edge AI, deep reinforcement learning, and more, painting a vibrant panorama of what the future beholds. For anyone vested in the domains of IoE and AI, this review offers both a foundation and a visionary lens, bridging the present realities with future possibilities.},
}

@article{Wu2025_03,
  title = {How well can ChatGPT forecast tourism demand?},
  author = {Doris Chenguang Wu and Wenjia Li and Ji Wu and Mingming Hu and Shujie Shen},
  year = {2025},
  journal = {Tourism Management},
  volume = {108},
  pages = {105119},
  doi = {https://doi.org/10.1016/j.tourman.2024.105119},
  url = {https://www.sciencedirect.com/science/article/pii/S0261517724002383},
  abstract = {ChatGPT has demonstrated remarkable capabilities across various natural language processing (NLP) tasks. However, its potential for forecasting tourism demand from temporal data, specifically historical tourism arrivals data, remains an unexplored frontier. This research presents the first attempt to conduct an extensive Zero-shot and Chain-of-Thought analysis of ChatGPT's capabilities in tourism demand forecasting, under various temporal scenarios. Based on the Macau inbound tourism arrivals dataset, our empirical findings indicate that the predictive capability of ChatGPT-4 is noteworthy compared to the three benchmark time series models (Naïve, Exponential Smoothing, SARIMA) and the three benchmark machine learning models (Random Forest, Multi-Layer Perceptron, Long Short-Term Memory), especially when the forecast horizon is relatively short. Furthermore, compared to Zero-shot prompts, engaging in continuous dialogue can enhance the forecast accuracy of ChatGPT-4. This performance of ChatGPT highlights its potential for quantitative data prediction as a new user-friendly and cost-effective management tool.},
}

@article{D’Rosario2024,
  title = {Open Innovation},
  author = {Michael D’Rosario},
  year = {2024},
  doi = {https://doi.org/10.1016/B978-0-443-13701-3.00556-9},
  publisher = {Elsevier},
  url = {https://www.sciencedirect.com/science/article/pii/B9780443137013005569},
  abstract = {Open innovation is a multidisciplinary concept that underscores the importance of collaborative ecosystems in accelerating innovation. Coined by Henry Chesbrough, it involves the strategic management of knowledge dissemination across organizational boundaries. Unlike closed innovation, which relies solely on internal resources, open innovation is a decentralized and inclusive innovation strategy that incorporates both internal and external ideas and technologies to accelerate the development and diffusion of ideas, products, and services. This shift is evident in various industries, fostering bidirectional innovation flows that benefit firms, consumers, and society.},
}

@article{Li2024_04,
  title = {On the value of orderly electric vehicle charging in carbon emission reduction},
  author = {Zhi Li and Zhibin Chen and Hailong Li and ChengHe Guan and Minghui Zhong},
  year = {2024},
  journal = {Transportation Research Part D: Transport and Environment},
  volume = {135},
  pages = {104383},
  doi = {https://doi.org/10.1016/j.trd.2024.104383},
  url = {https://www.sciencedirect.com/science/article/pii/S1361920924003407},
  abstract = {In this study, a bi-level model is developed to quantify the value of orderly electric vehicle (EV) charging in carbon reduction. Specifically, the upper-level model optimizes each EV driver’s charging schedule to diminish the total carbon emissions without impacting their travel plans, and the lower-level problem aims to fulfill electricity demands with minimal electricity dispatch cost. Based on real-world operation data obtained from 3,777 battery EVs (BEVs) in Shanghai over 11 months and local power plant data, the total carbon emissions generated by BEVs in Shanghai is calculated as 1,176,637 tons over this period, averaging 73 gCO2/km per BEV. By administering charging control to all BEVs in Shanghai, the above emission could be curtailed by 39%. Sensitivity analyses uncover that augmenting battery capacity and integrating wind power can significantly enhance emission reductions, while increasing the flexibility of the power plant might diminish the effectiveness of orderly EV charging.},
}

@article{Gattiglia2025,
  title = {Managing Artificial Intelligence in Archeology. An overview},
  author = {Gabriele Gattiglia},
  year = {2025},
  journal = {Journal of Cultural Heritage},
  volume = {71},
  pages = {225-233},
  doi = {https://doi.org/10.1016/j.culher.2024.11.020},
  url = {https://www.sciencedirect.com/science/article/pii/S1296207424002516},
  abstract = {The integration of AI in archaeology poses several risks due to the oversimplification of complex archaeological data for computational ease. This reductionist approach fosters a deterministic view, treating provisional classifications as definitive truths and influencing subsequent interpretations. The reliance on legacy data and Big Data for AI training risks perpetuating outdated ideas and frameworks. As AI expands from automating tasks to interpreting and creating reconstructions, archaeologists must adopt a critical approach to avoid biased and harmful outputs. The deterministic view of AI hinders informed debate. Archaeologists should engage in discussions that address the classificatory, and ethical aspects as well as the materiality of AI. The accumulation of data in AI mimics storytelling but lacks the interpretative depth needed to understand historical human perspectives. Developing theories and narrative practices is essential to making archaeological data meaningful. The shift from a representational to a co-creative view of data is necessary to understand its re-use and the power dynamics involved. Finally, to normalise AI in archaeology, a critical and sceptical approach is needed to integrate AI into the real world and understand its implications and ethical considerations.},
}

@article{Sathe2025,
  title = {An Analysis of Political Contributions by the American College of Surgeons Professional Association (SurgeonsPAC)},
  author = {Tejas S. Sathe and Thomas A. Sorrentino and Hanmin Lee and Andre R. Campbell},
  year = {2025},
  journal = {Journal of Surgical Research},
  volume = {305},
  pages = {313-321},
  doi = {https://doi.org/10.1016/j.jss.2024.11.021},
  url = {https://www.sciencedirect.com/science/article/pii/S0022480424007637},
  abstract = {Introduction In 2001, The American College of Surgeons (ACS) Professional Association launched a political action committee called SurgeonsPAC. SurgeonsPAC donates money to candidates for the United States Congress in an effort to advocate for the interests of surgeons and surgical patients. In this study, we analyzed the political distribution of SurgeonsPAC donations over time and assessed how contributions correlate with candidates' voting records on issues important to the ACS. Methods We queried OpenSecrets to identify candidates and organizations supported by SurgeonsPAC between 2012 and 2022. We calculated the number of recipients and dollar amounts given to each party overall and in each election cycle. Next, we identified bills on which the ACS lobbied and attempted to identify whether the ACS supported or opposed those bills. Using the Congressional Record, we identified the voting records of congressional candidates receiving SurgeonsPAC support and cross-referenced those records with ACS positions, ultimately identifying the proportion of SurgeonsPAC-funded candidates in each party who supported ACS priorities. Results From 2012 to 2022, SurgeonsPAC donated $2,832,000 (57%) to 491 Republican candidates and organizations and $2,097,650 (43%) to 386 Democratic candidates and organizations. SurgeonsPAC donated more to Republican candidates from 2012 to 2018 and more to Democratic candidates in 2020 and 2022. We found that Republican and Democratic recipients of SurgeonsPAC funds voted about evenly on ACS priorities in the Republican-led 115th Congress. Contrastingly, Democrats voted for ACS priorities more frequently than Republicans in the Democratic-led 117th Congress (98.1% versus 29.2%). Conclusions Here, we present the first descriptive analysis of SurgeonsPAC distributions over a 10 y period. Greater transparency into which candidates receive financial support from the ACS and how they vote on ACS priorities may help inform the effectiveness of political contributions and incentivize more surgeons to engage in advocacy efforts.},
}

@article{Gangwal2024,
  title = {Current strategies to address data scarcity in artificial intelligence-based drug discovery: A comprehensive review},
  author = {Amit Gangwal and Azim Ansari and Iqrar Ahmad and Abul Kalam Azad and Wan Mohd Azizi {Wan Sulaiman}},
  year = {2024},
  journal = {Computers in Biology and Medicine},
  volume = {179},
  pages = {108734},
  doi = {https://doi.org/10.1016/j.compbiomed.2024.108734},
  url = {https://www.sciencedirect.com/science/article/pii/S0010482524008199},
  abstract = {Artificial intelligence (AI) has played a vital role in computer-aided drug design (CADD). This development has been further accelerated with the increasing use of machine learning (ML), mainly deep learning (DL), and computing hardware and software advancements. As a result, initial doubts about the application of AI in drug discovery have been dispelled, leading to significant benefits in medicinal chemistry. At the same time, it is crucial to recognize that AI is still in its infancy and faces a few limitations that need to be addressed to harness its full potential in drug discovery. Some notable limitations are insufficient, unlabeled, and non-uniform data, the resemblance of some AI-generated molecules with existing molecules, unavailability of inadequate benchmarks, intellectual property rights (IPRs) related hurdles in data sharing, poor understanding of biology, focus on proxy data and ligands, lack of holistic methods to represent input (molecular structures) to prevent pre-processing of input molecules (feature engineering), etc. The major component in AI infrastructure is input data, as most of the successes of AI-driven efforts to improve drug discovery depend on the quality and quantity of data, used to train and test AI algorithms, besides a few other factors. Additionally, data-gulping DL approaches, without sufficient data, may collapse to live up to their promise. Current literature suggests a few methods, to certain extent, effectively handle low data for better output from the AI models in the context of drug discovery. These are transferring learning (TL), active learning (AL), single or one-shot learning (OSL), multi-task learning (MTL), data augmentation (DA), data synthesis (DS), etc. One different method, which enables sharing of proprietary data on a common platform (without compromising data privacy) to train ML model, is federated learning (FL). In this review, we compare and discuss these methods, their recent applications, and limitations while modeling small molecule data to get the improved output of AI methods in drug discovery. Article also sums up some other novel methods to handle inadequate data.},
}

@article{Franciscatto2025,
  title = {A CBR-based conversational architecture for situational data management},
  author = {Maria Helena Franciscatto and Luis Carlos Erpen {de Bona} and Celio Trois and Marcos Didonet {Del Fabro}},
  year = {2025},
  journal = {Computer Speech & Language},
  volume = {92},
  pages = {101779},
  doi = {https://doi.org/10.1016/j.csl.2025.101779},
  url = {https://www.sciencedirect.com/science/article/pii/S088523082500004X},
  abstract = {This paper introduces a conversational Case-Based Reasoning (CBR) architecture, aimed at improving situational data management by incorporating user feedback into the process. The core of the architecture is a “human-in-the-loop” approach implemented through a conversational agent, which facilitates interaction between the user and the system. The CBR-based approach leverages a historical knowledge base that is dynamically updated based on user feedback, allowing for a more responsive and adaptive system. This feedback plays a crucial role in the processes of case retrieval, review, and retention within the CBR cycle, enabling the system to evolve based on user interactions. An empirical study involving 22 participants was conducted to assess the impact of user feedback on system recommendations. This study included both static and dynamic test scenarios, focusing on aspects such as visibility, support, usefulness, and data integration. The results highlighted a general preference for recommendations that were influenced by user input, indicating the effectiveness of incorporating human feedback in the decision-making process. The research contributes to situational data management by illustrating how a conversational CBR framework, integrated with user feedback, can improve processes such as data integration and data discovery. In addition, it highlights the importance of user involvement in enhancing the functionality of conversational systems for complex data management, pointing to the potential for further development in this area.},
}

@article{Yu2023,
  title = {What is uncertainty in today’s practice of data science?},
  author = {Bin Yu},
  year = {2023},
  journal = {Journal of Econometrics},
  volume = {237},
  pages = {105519},
  doi = {https://doi.org/10.1016/j.jeconom.2023.105519},
  url = {https://www.sciencedirect.com/science/article/pii/S030440762300235X},
}

@article{Porras-Salazar2023,
  title = {The public perception of urban vegetation in metropolitan regions of Costa Rica},
  author = {Jose Ali Porras-Salazar and Jan-Frederik Flor and Sergio Contreras-Espinoza and Melissa Soto-Arce and Rene Castro-Salazar},
  year = {2023},
  journal = {Environmental Advances},
  volume = {13},
  pages = {100422},
  doi = {https://doi.org/10.1016/j.envadv.2023.100422},
  url = {https://www.sciencedirect.com/science/article/pii/S2666765723000807},
  abstract = {Urban vegetation makes cities more liveable, provides essential ecosystem services, and is relevant for sustainable development. We investigated the public perception of urban vegetation in the metropolitan area of Costa Rica, Central America. Through an online survey, we collected 1264 responses from Costa Rican residents on their attitudes and opinions towards urban vegetation's environmental services and disservices. After selecting those participants residing in the Greater Metropolitan Area and applying a data cleaning process, we derived 811 valid responses. Poststratification techniques were employed to fit the sample to the general population distribution. We found that the majority viewed urban vegetation favorably, with 80 % believing that urban greenery contributes more benefits than negative effects to air quality, shading, and wildlife. In contrast, up to 20 % think urban greenery is harmful, asserting that it encourages crime, promotes pests, and damages infrastructure. Perceptions changed according to demographic variables such as gender, age group, and education level. We compared our results to a previous study in Singapore, Southeast Asia, which showed similar trends despite cultural and economic differences. This paper offers a starting point for priority-setting and decision-making in city planning by delivering insights into how people in the tropics perceive urban vegetation.},
}

@article{González-Fernández2025,
  title = {Environmental, socio-cultural, and economic sustainability in care facilities: Evaluating the impact of person-centered building renovation in Aragon, Spain},
  author = {Irene González-Fernández and Lucía C. Pérez-Moreno},
  year = {2025},
  journal = {Environmental Impact Assessment Review},
  volume = {112},
  pages = {107822},
  doi = {https://doi.org/10.1016/j.eiar.2025.107822},
  url = {https://www.sciencedirect.com/science/article/pii/S0195925525000198},
  abstract = {The increasing aging population, life expectancy, and dependency rates in Spain have led to a growing demand for long-term care places, presenting a significant challenge for care facilities for older people. Transitioning from institutional care homes to a person-centered care model is essential for improving the well-being and quality of life of residents. This paper focuses on Aragon, a region in Spain facing territorial disparities and aging-related issues. The study proposes a holistic sustainability approach that encompasses socio-cultural, environmental, and economic dimensions to evaluate the impact of renovating institutional long-term care facilities into person-centered care environments. The research methodology involves creating a holistic database of care facilities in Aragon and assessing their sustainability through socio-cultural indicators (size, living units, and single rooms) and environmental factors (construction year, energy consumption, and carbon emissions). Several design measures, such as implementing living units and improving insulation, are proposed to evaluate economic sustainability. The findings reveal that 25 % of care facilities need to reduce their capacity, with only 7.7 % having adopted living units. Additionally, over 80 % of the building stock is energy inefficient. Renovating these care facilities in line with a sustainable, person-centered approach is estimated to cost between 101 and 170 million euros. The paper concludes that a holistic approach combining socio-cultural, environmental, and economic sustainability is crucial for implementing a person-centered care model in Aragon's facilities. Furthermore, effective public-private collaboration is needed to ensure equitable access to care services and uphold quality standards that safeguard the well-being of older adults.},
}

@article{Jaiswal2024_02,
  title = {Using #ActuallyAutistic on Twitter for Precision Diagnosis of Autism Spectrum Disorder: Machine Learning Study},
  author = {Aditi Jaiswal and Peter Washington},
  year = {2024},
  journal = {JMIR Formative Research},
  volume = {8},
  doi = {https://doi.org/10.2196/52660},
  url = {https://www.sciencedirect.com/science/article/pii/S2561326X24003056},
  abstract = {Background The increasing use of social media platforms has given rise to an unprecedented surge in user-generated content, with millions of individuals publicly sharing their thoughts, experiences, and health-related information. Social media can serve as a useful means to study and understand public health. Twitter (subsequently rebranded as “X”) is one such social media platform that has proven to be a valuable source of rich information for both the general public and health officials. We conducted the first study applying Twitter data mining to autism screening. Objective We aimed to study the feasibility of autism screening from Twitter data and discuss the ethical implications of such models. Methods We developed a machine learning model to attempt to distinguish individuals with autism from their neurotypical peers based on the textual patterns from their public communications on Twitter. We collected 6,515,470 tweets from users’ self-identification with autism using “#ActuallyAutistic” and a separate control group. To construct the data set, we targeted English-language tweets using the search query “#ActuallyAutistic” posted from January 1, 2014 to December 31, 2022. We encrypted all user IDs and stripped the tweets of identifiable information such as the associated email address prior to analysis. From these tweets, we identified unique users who used keywords such as “autism” OR “autistic” OR “neurodiverse” in their profile description and collected all the tweets from their timelines. To build the control group data set, we formulated a search query excluding the hashtag “#ActuallyAutistic” and collected 1000 tweets per day during the same time period. We trained a word2vec model and an attention-based, bidirectional long short-term memory model to validate the performance of per-tweet and per-profile classification models. We deleted the data set and the models after our analysis. Results Our tweet classifier reached a 73% accuracy, a 0.728 area under the receiver operating characteristic curve score, and an 0.71 F1-score using word2vec representations fed into a logistic regression model, while the user profile classifier achieved an 0.78 area under the receiver operating characteristic curve score and an F1-score of 0.805 using an attention-based, bidirectional long short-term memory model. Conclusions We have shown that it is feasible to train machine learning models using social media data to predict use of the #ActuallyAutistic hashtag, an imperfect proxy for self-reported autism. While analyzing textual differences in naturalistic text has the potential to help clinicians screen for autism, there remain ethical questions that must be addressed for such research to move forward and to translate into the real world. While machine learning has the potential to improve behavioral research, there are still a plethora of ethical issues in digital phenotyping studies using social media with respect to user consent of marginalized populations. Achieving this requires a more inclusive approach during the model development process that involves the autistic community directly in the ideation and consent processes.},
}

@article{Liu2024_02,
  title = {Integration of data science with product design towards data-driven design},
  author = {Ang Liu and Stephen Lu and Fei Tao and Nabil Anwer},
  year = {2024},
  journal = {CIRP Annals},
  volume = {73},
  pages = {509-532},
  doi = {https://doi.org/10.1016/j.cirp.2024.06.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0007850624001252},
  abstract = {This paper aims to investigate the scientific integration of data science with product design towards data-driven design (D3). Data science has potential to facilitate design decision-making through insight extraction, predictive analytics, and automatic decisions. A systematic scoping review is conduced to converge various D3 applications in four dimensions: the design dimension about design operations, the data dimension about popular data sources and common data-related challenges, the method dimension about the methodological foundations, and the social/ethical dimension about social/ethical considerations and implications. Based on the state-of-the-art, this paper also highlights potential future research avenues in this dynamic field.},
}

@article{Rohde2024,
  title = {Broadening the perspective for sustainable artificial intelligence: sustainability criteria and indicators for Artificial Intelligence systems},
  author = {Friederike Rohde and Josephin Wagner and Andreas Meyer and Philipp Reinhard and Marcus Voss and Ulrich Petschow and Anne Mollen},
  year = {2024},
  journal = {Current Opinion in Environmental Sustainability},
  volume = {66},
  pages = {101411},
  doi = {https://doi.org/10.1016/j.cosust.2023.101411},
  url = {https://www.sciencedirect.com/science/article/pii/S1877343523001586},
  abstract = {The increased use of Artificial intelligence systems (AI systems) is associated with multifaceted social, environmental, and economic consequences. These include nontransparent decision-making processes, discrimination, increasing inequalities, rising energy consumption and greenhouse gas emissions in AI model development and application, and an increasing concentration of economic power. By considering the multidimensionality of sustainability, this paper takes steps toward substantiating the call for an overarching perspective on ‘sustainable AI.’ It presents the Sustainability Criteria and Indicators for Artificial Intelligence Systems (SCAIS) Framework, an assessment framework that contains a set of 19 sustainability criteria for sustainable AI and 67 indicators that are based on the results of a critical literature review, and expert workshops. Its interdisciplinary approach contributes a unique holistic perspective to facilitate and structure the discourse on sustainable AI. Further, it provides a concrete assessment framework that lays the foundation for developing standards and tools to support the conscious development and application of AI systems.},
}

@article{Pelser2025,
  title = {ETHOS.REFLOW: An open-source workflow for reproducible renewable energy potential assessments},
  author = {Tristan Pelser and Jann Michael Weinand and Patrick Kuckertz and Detlef Stolten},
  year = {2025},
  journal = {Patterns},
  volume = {6},
  pages = {101172},
  doi = {https://doi.org/10.1016/j.patter.2025.101172},
  url = {https://www.sciencedirect.com/science/article/pii/S2666389925000200},
  abstract = {Summary Accurate renewable energy resource assessments are necessary for energy system planning to meet climate goals, yet inconsistencies in methods and data can produce significant differences in results. This paper introduces ETHOS.REFLOW, a Python-based workflow manager that ensures transparency and reproducibility in energy potential assessments. The tool enables reproducible analyses with minimal effort by automating the entire workflow, from data acquisition to reporting. We demonstrate its functionality by estimating the technical offshore wind potential of the North Sea, for fixed-foundation and mixed-technology (including floating turbines) scenarios. Two methods for turbine siting (explicit placement vs. uniform power density) and wind datasets are compared. Results show a maximum installable capacity of 768–861 GW and an annual yield of 2,961–3,047 TWh, with capacity factors between 41% and 46% and significant temporal variability. ETHOS.REFLOW offers a robust framework for reproducible energy potential studies, enabling energy system modelers to build on existing work and fostering trust in findings.},
}

@article{Boeters2025,
  title = {A dynamic framework for calculating the biomass of fattening pigs with an application in estimating the burden of porcine reproductive and respiratory syndrome in the Netherlands},
  author = {Marloes Boeters and Wilma Steeneveld and Beatriz Garcia-Morante and Jonathan Rushton and Gerdien {van Schaik}},
  year = {2025},
  journal = {Preventive Veterinary Medicine},
  volume = {234},
  pages = {106383},
  doi = {https://doi.org/10.1016/j.prevetmed.2024.106383},
  url = {https://www.sciencedirect.com/science/article/pii/S0167587724002691},
  abstract = {Gaining insight into the size and composition of national pig populations can support decisions on disease control, welfare, and environmental sustainability. However, if one needs to draw meaningful comparisons between the performance of various production systems or countries, a method for standardization is required. One approach to achieve this is by means of biomass estimation. The objective of this study was to develop a biomass estimation framework that can provide detailed and reliable estimates of fattening pig biomass disaggregated by pig life stage (suckling, weaning and fattening), while accounting for the dynamic nature of pig populations. The framework was developed on publicly accessible data pertaining to pig production in the Netherlands, and we additionally assessed availability of required data for several other European countries (Spain, Germany, and Great Britain). Three distinct life stages—suckling piglets, weaning pigs, and fattening pigs—are considered in the framework. Demographic and movement data, including yearly imports, exports, and slaughter numbers, along with standing populations, were collected from official governmental sources. Required production parameters were sourced from representative surveys, with missing parameters supplemented by private industry reports or expert elicitation. The results from the framework for the Netherlands yield insights into the Dutch pig sector. In 2020, 156 million kg, 552 million kg, and 1654 million kg of biomass were produced in the suckling, weaning, and fattening stages, respectively. The evaluation against census data indicated the framework's reliability, with deviations mostly below 10 %. Data availability assessments for Spain, Germany and Great Britain reveal variations in data completeness and underscore the importance of local contacts and language expertise when extending the framework to other countries. The framework's relevance was further demonstrated through an illustrative application, assessing the impact of porcine reproductive and respiratory syndrome on pig biomass in the Netherlands. In the most severe disease scenario, the produced biomass decreased by 13 %, 17 %, and 66 % in the suckling, weaning, and fattening stages, respectively. Beyond disease burden estimation, the biomass estimates can be used as a denominator for various purposes to provide efficiency metrics, such as the amount of antibiotics used or the volume of greenhouse gases emitted per kilogram of pig biomass produced. While the framework could benefit from further refinement regarding resource use and economic values, its current iteration provides a robust and unique foundation for estimating biomass disaggregated by pig life stage, aiding decision-makers in the agricultural and veterinary sector.},
}

@article{Khalid2024,
  title = {Repairing raw metadata for metadata management},
  author = {Hiba Khalid and Esteban Zimányi},
  year = {2024},
  journal = {Information Systems},
  volume = {122},
  pages = {102344},
  doi = {https://doi.org/10.1016/j.is.2024.102344},
  url = {https://www.sciencedirect.com/science/article/pii/S0306437924000024},
  abstract = {With the exponential growth of data production, the generation of metadata has become an integral part of the process. Metadata plays a crucial role in facilitating enhanced data analytics, data integration, and resource management by offering valuable insights. However, inconsistencies arise due to deviations from standards in metadata recording, including missing attribute information, publishing URLs, and provenance. Furthermore, the recorded metadata may exhibit inconsistencies, such as varied value formats, special characters, and inaccurately entered values. Addressing these inconsistencies through metadata preparation can greatly enhance the user experience during data management tasks. This paper introduces MDPrep, a system that explores the usability and applicability of data preparation techniques in improving metadata quality. Our approach involves three steps: (1) detecting and identifying problematic metadata elements and structural issues, (2) employing a keyword-based approach to enhance metadata elements and a syntax-based approach to rectify structural metadata issues, and (3) comparing the outcomes to ensure improved readability and reusability of prepared metadata files.},
}

@article{Bakmohammadi2025,
  title = {A holistic framework to optimize embedding PV systems into building façades},
  author = {Parnian Bakmohammadi and Nima Narjabadifam and Maziar Jamshidi and Mustafa Gül},
  year = {2025},
  journal = {Applied Energy},
  volume = {382},
  pages = {125288},
  doi = {https://doi.org/10.1016/j.apenergy.2025.125288},
  url = {https://www.sciencedirect.com/science/article/pii/S0306261925000182},
  abstract = {In addressing fossil fuel supply concerns and their environmental impacts, the building sector, as a major energy consumer, offers an opportunity for renewable energy integration. Among renewable energy sources, solar energy through photovoltaic (PV) panels on building façades stands out as a notable option, though fully realizing their potential remains a challenge. This study introduces a framework for the automated design of PV panels integrated into the façades of existing buildings, enabling thorough assessment based on energy efficiency, economic feasibility, and environmental impact. The process involves capturing the geometry of building envelopes, a deep learning model to identify façade surfaces for PV installation, and simulations to model PV generation and energy demand. An evolutionary multi-objective optimization algorithm is then employed to determine the PV system design parameters. The results of applying the framework to two university buildings in Alberta, Canada, are presented. For these cases, when equal weights are given to economic, environmental, and energy efficiency objectives, the optimal PV layout can achieve electricity self-sufficiency of 5.16 % and 6.78 %, with greenhouse gas emission rates of 18.26 and 15.69 g CO2-eq./kWh, respectively. The analysis illustrates that adjusting objective priorities yields different optimized solutions to balance competing factors. For example, prioritizing self-sufficiency increases the number of panels while focusing on financial return results in fewer panels and shorter payback periods. Although the financial feasibility of PV systems in Alberta's energy market is currently constrained by low electricity prices, the analysis highlights opportunities for improvement through government incentives and potential electricity price increases.},
}

@article{Yan2023,
  title = {Research on the impact of trends related to ChatGPT},
  author = {Yunxi Yan and Biao Li and Jinyuan Feng and Yang Du and Zhichen Lu and Manling Huang and Youyuan Li},
  year = {2023},
  journal = {Procedia Computer Science},
  volume = {221},
  pages = {1284-1291},
  doi = {https://doi.org/10.1016/j.procs.2023.08.117},
  url = {https://www.sciencedirect.com/science/article/pii/S187705092300875X},
  abstract = {Since ChatGPT was launched, it has attracted great attention across society. Especially in non-professional fields, ChatGPT can answer follow-up questions, reject inappropriate requests, challenge erroneous assumptions, and admit mistakes from a user's experience. It has many emergent capabilities such as high-quality dialogue, complex reasoning, chains of thought (CoT), zero/low-shot learning (contextual learning), cross-task generalization, code understanding/generation, etc. The emergence of ChatGPT has brought a profound impact on the development of all aspects, and brought huge changes to the social economy and living environment.},
}

@article{Dai2024,
  title = {Assessing the proficiency of large language models in automatic feedback generation: An evaluation study},
  author = {Wei Dai and Yi-Shan Tsai and Jionghao Lin and Ahmad Aldino and Hua Jin and Tongguang Li and Dragan Gašević and Guanliang Chen},
  year = {2024},
  journal = {Computers and Education: Artificial Intelligence},
  volume = {7},
  pages = {100299},
  doi = {https://doi.org/10.1016/j.caeai.2024.100299},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X24001024},
  abstract = {Assessment feedback is important to student learning. Learning analytics (LA) powered by artificial intelligence exhibits profound potential in helping instructors with the laborious provision of feedback. Inspired by the recent advancements made by Generative Pre-trained Transformer (GPT) models, we conducted a study to examine the extent to which GPT models hold the potential to advance the existing knowledge of LA-supported feedback systems towards improving the efficiency of feedback provision. Therefore, our study explored the ability of two versions of GPT models – i.e., GPT-3.5 (ChatGPT) and GPT-4 – to generate assessment feedback on students' writing assessment tasks, common in higher education, with open-ended topics for a data science-related course. We compared the feedback generated by GPT models (namely GPT-3.5 and GPT-4) with the feedback provided by human instructors in terms of readability, effectiveness (content containing effective feedback components), and reliability (correct assessment on student performance). Results showed that (1) both GPT-3.5 and GPT-4 were able to generate more readable feedback with greater consistency than human instructors, (2) GPT-4 outperformed GPT-3.5 and human instructors in providing feedback containing information about effective feedback dimensions, including feeding-up, feeding-forward, process level, and self-regulation level, and (3) GPT-4 demonstrated higher reliability of feedback compared to GPT-3.5. Based on our findings, we discussed the potential opportunities and challenges of utilising GPT models in assessment feedback generation.},
}

@article{Saeedikiya2024,
  title = {Toward a dynamic capability perspective of digital transformation in SMEs: A study of the mobility sector},
  author = {Mehrzad Saeedikiya and Sandeep Salunke and Marek Kowalkiewicz},
  year = {2024},
  journal = {Journal of Cleaner Production},
  volume = {439},
  pages = {140718},
  doi = {https://doi.org/10.1016/j.jclepro.2024.140718},
  url = {https://www.sciencedirect.com/science/article/pii/S0959652624001653},
  abstract = {Digital Transformation (DT) has been considered a critical structural change for SMEs to be responsive and agile in today's dynamic and turbulent business environments. This significance is more evident in mobility and transport systems in which the sector is undergoing radical technological changes in the context of smart cities. However, SMEs must possess some capabilities to successfully leverage DT toward better performance and gain a competitive advantage. The current research is a step forward toward achieving this goal. Using a multiple case study of mobility sector SMEs in Australia, the authors explore the nature of Dynamic Capabilities (DC) in SMEs and discover digital transformation's enablers, barriers, and performance outcomes.},
}

@article{Amirzadeh2024,
  title = {The evolutionary path of place making: From late twentieth century to post-pandemic cities},
  author = {Melika Amirzadeh and Ayyoob Sharifi},
  year = {2024},
  journal = {Land Use Policy},
  volume = {141},
  pages = {107124},
  doi = {https://doi.org/10.1016/j.landusepol.2024.107124},
  url = {https://www.sciencedirect.com/science/article/pii/S0264837724000760},
  abstract = {Since its emergence in the latter half of the 20th century, the concept of place making has undergone substantial evolution. To fully comprehend how this concept has transformed and encountered diverse trajectories, it is beneficial to examine its evolutionary path over time. Therefore, this paper aims to conduct a thorough review of the literature surrounding the expanding matrix of place making through a combination of bibliometric analysis and selective review. The paper seeks to undertake a comprehensive evaluation of nine interrelated paradigms of place making as a subfield of urban planning and design. These are, namely, late 20th century place making, sustainable place making, digital place making, democratic place making, creative place making, strategic place making, healthy place making, resilient place making, and post-pandemic place making. Results reveal that there has been a transition toward community-based participation and a stronger emphasis on social and environmental elements. Also, there has been a shift from emphasizing functional spaces to promoting community building, social connectivity, healthy, and resilient environments via long-term strategies as well as the deployment of smart city and digital technologies.},
}

@article{Leone2025,
  title = {A holistic asset-level modelling framework for a comprehensive multi-hazard risk/impact assessment: Insights from the ICARIA project},
  author = {Mattia Federico Leone and Giulio Zuccaro and Daniela {De Gregorio} and Agnese Turchi and Amanda Tedeschi and Marianne Büegelmayer-Blaschek and Athanasios Sfetsos and Ioannis Zarikos and Alex de la Cruz Coronas and Beniamino Russo},
  year = {2025},
  journal = {International Journal of Disaster Risk Reduction},
  volume = {119},
  pages = {105319},
  doi = {https://doi.org/10.1016/j.ijdrr.2025.105319},
  url = {https://www.sciencedirect.com/science/article/pii/S2212420925001438},
  abstract = {The frequency and intensity of climate- and weather-related phenomena have significantly increased over the past two decades, with future projections suggesting further escalation due to climate change. Compound events, involving coincident or consecutive hazards, and their cascading effects, often exacerbate the severity of disasters, resulting in greater damage than would result from isolated hazards. However, risk/impact assessments have predominantly used single-hazard approaches, limiting understanding of how multi-hazard interactions affect socio-eco-technological systems. This paper presents a comprehensive asset-level modelling framework developed within the EU-funded Horizon Europe project ICARIA. The framework aims to assess risks/impacts and resilience to a wide range of natural phenomena, including droughts, heatwaves, extreme winds, wildfires, floods and landslides, as well as the potential cascading effects due to impacts on interdependent infrastructure systems. It enables the development of multi-hazard scenarios, data harmonisation, and the characterisation of exposure and vulnerability for different categories of elements at risk, particularly critical infrastructures and related services, thereby facilitating the estimation of direct and indirect damage. Furthermore, the framework incorporates coping, adaptive and transformative capacities as key-components of resilience, as well as human behavioural factors, into the modelling process. Examples from initial testing of the framework on ICARIA case study regions are introduced to highlight the operational steps for its application, including the identification of reference multi-hazard risk/impact scenarios through event trees, the inventory of relevant modelling data and the interconnection of single hazard/impact models to determine the consequences of complex multi-hazard events on exposed assets and services.},
}

@article{Kotiranta2024,
  title = {Digitalization as a growth driver for social enterprises},
  author = {Annu Kotiranta and Kaisu Puumalainen and Helena Sjögren and Léo-Paul Dana},
  year = {2024},
  journal = {Technological Forecasting and Social Change},
  volume = {209},
  pages = {123837},
  doi = {https://doi.org/10.1016/j.techfore.2024.123837},
  url = {https://www.sciencedirect.com/science/article/pii/S0040162524006358},
  abstract = {Social enterprises' motivations for growth arguably stem from their social missions, which can result in moderate business growth due to conflicting interests and the trade-off costs of impact scaling and growing their business. Digitalization has been suggested as one method of enabling the simultaneous growth of business and social or environmental impact. In this study, we analyze the digital orientation of social enterprises and test whether our hypotheses regarding the superior business benefits of digitalization for social enterprises can be empirically confirmed. Our results show social enterprises as early adopters of digitalization, who have higher expectations that digitalization will benefit them and tend to invest more in digital technologies and capabilities than commercial companies do. However, the strong digital orientation of social enterprises does not manifest better business growth. Furthermore, the findings suggest that social enterprises' investment in social media has hampered their productivity. Our findings challenge current theoretical arguments that claim that digitalization has particular benefits for social enterprises, and we suggest that the digital antecedents of social enterprise growth are, after all, very similar to those of other small and medium-sized enterprises.},
}

@article{Kuziemsky2024,
  title = {AI Quality Standards in Health Care: Rapid Umbrella Review},
  author = {Craig E Kuziemsky and Dillon Chrimes and Simon Minshall and Michael Mannerow and Francis Lau},
  year = {2024},
  journal = {Journal of Medical Internet Research},
  volume = {26},
  doi = {https://doi.org/10.2196/54705},
  url = {https://www.sciencedirect.com/science/article/pii/S1438887124002607},
  abstract = {Background In recent years, there has been an upwelling of artificial intelligence (AI) studies in the health care literature. During this period, there has been an increasing number of proposed standards to evaluate the quality of health care AI studies. Objective This rapid umbrella review examines the use of AI quality standards in a sample of health care AI systematic review articles published over a 36-month period. Methods We used a modified version of the Joanna Briggs Institute umbrella review method. Our rapid approach was informed by the practical guide by Tricco and colleagues for conducting rapid reviews. Our search was focused on the MEDLINE database supplemented with Google Scholar. The inclusion criteria were English-language systematic reviews regardless of review type, with mention of AI and health in the abstract, published during a 36-month period. For the synthesis, we summarized the AI quality standards used and issues noted in these reviews drawing on a set of published health care AI standards, harmonized the terms used, and offered guidance to improve the quality of future health care AI studies. Results We selected 33 review articles published between 2020 and 2022 in our synthesis. The reviews covered a wide range of objectives, topics, settings, designs, and results. Over 60 AI approaches across different domains were identified with varying levels of detail spanning different AI life cycle stages, making comparisons difficult. Health care AI quality standards were applied in only 39% (13/33) of the reviews and in 14% (25/178) of the original studies from the reviews examined, mostly to appraise their methodological or reporting quality. Only a handful mentioned the transparency, explainability, trustworthiness, ethics, and privacy aspects. A total of 23 AI quality standard–related issues were identified in the reviews. There was a recognized need to standardize the planning, conduct, and reporting of health care AI studies and address their broader societal, ethical, and regulatory implications. Conclusions Despite the growing number of AI standards to assess the quality of health care AI studies, they are seldom applied in practice. With increasing desire to adopt AI in different health topics, domains, and settings, practitioners and researchers must stay abreast of and adapt to the evolving landscape of health care AI quality standards and apply these standards to improve the quality of their AI studies.},
}

@article{Dissanayake2025,
  title = {The state-of-the-art of crowdsourcing systems: A computational literature review and future research agenda using a text analytics approach},
  author = {Indika Dissanayake and Sridhar P. Nerur and Roman Lukyanenko and Minoo Modaresnezhad},
  year = {2025},
  journal = {Information & Management},
  volume = {62},
  pages = {104098},
  doi = {https://doi.org/10.1016/j.im.2025.104098},
  url = {https://www.sciencedirect.com/science/article/pii/S0378720625000011},
  abstract = {Crowdsourcing effectively harnesses diverse skills and perspectives of crowds beyond organizational, geographical, and cultural boundaries. Organizations are gaining invaluable insights through crowdsourcing across diverse domains. This study reviews the growing academic literature on crowdsourcing using advanced topic modeling, an approach to unraveling key themes latent in the literature. Following a systems approach, we adopted inter- and intra-systems perspectives to identify distinct crowdsourcing models and their interrelated components based on a text analysis of the crowdsourcing literature. The paper elucidates the intellectual foundations of crowdsourcing as represented in the literature and offers suggestions for pursuing research that will extend its conceptual boundaries.},
}

@article{Qian2025,
  title = {Initializing a Public Repository for Hosting Benchmark Datasets to Facilitate Machine Learning Model Development in Food Safety},
  author = {Chenhao Qian and Huan Yang and Jayadev Acharya and Jingqiu Liao and Renata Ivanek and Martin Wiedmann},
  year = {2025},
  journal = {Journal of Food Protection},
  volume = {88},
  pages = {100463},
  doi = {https://doi.org/10.1016/j.jfp.2025.100463},
  url = {https://www.sciencedirect.com/science/article/pii/S0362028X25000158},
  abstract = {While there is clear potential for artificial intelligence (AI) and machine learning (ML) models to help improve food safety, the development and deployment of these models in the food safety domain are by and large lacking. The absence of publicly available databases that host well-curated datasets that can be used to develop and validate AI /ML models represents one likely barrier. Thus, we took three previously published datasets, which we further cleaned and annotated, and made them publicly available in a repository called Cornell Food Safety ML Repository. The selected datasets include (i) presence or absence of Listeria spp. in soil samples collected across the U.S. with paired metadata for soil properties, geolocation, climate, and surrounding land use, (ii) presence or absence of Salmonella and Campylobacter in young chicken carcasses tested in processing facilities with associated meteorological and temporal metadata, and (iii) presence or absence of fecal contamination as well as E. coli concentration in New York watersheds with associated metadata for land use, water attributes, and meteorological factors. These datasets can serve as benchmark datasets for developing ML models. To demonstrate the utility of the repository, we developed customizable scripts as well as LazyPredict (a quick screening method) scripts for training different types of ML models using the shared datasets. While this repository provides an important starting point that will allow for the development and testing of ML models to predict foodborne pathogens contamination in different sources, the inclusion of further datasets is clearly needed to advance this field. This paper thus includes a call to action for the deposit of well-curated datasets that can be used for further development of predictive models in food safety. This paper will also discuss the benefits of such public databases, including the assessment of data-sharing scenarios using existing privacy-preserving techniques.},
}

@article{Pan}2024,
  title = {Understanding electric vehicle ownership using data fusion and spatial modeling},
  author = {Meiyu {(Melrose) Pan} and Majbah Uddin and Hyeonsup Lim},
  year = {2024},
  journal = {Transportation Research Part D: Transport and Environment},
  volume = {127},
  pages = {104075},
  doi = {https://doi.org/10.1016/j.trd.2024.104075},
  url = {https://www.sciencedirect.com/science/article/pii/S1361920924000324},
  abstract = {The global shift toward electric vehicles (EVs) for climate sustainability lacks comprehensive insights into the impact of the built environment on EV ownership, especially in varying spatial contexts. This study, focusing on New York State, integrates data fusion techniques across diverse datasets to examine the influence of socioeconomic and built environmental factors on EV ownership. The utilization of spatial regression models reveals consistent coefficient values, highlighting the robustness of the results, with the Spatial Lag model better at capturing spatial autocorrelation. Results underscore the significance of charging stations within a 10-mile radius, indicative of a preference for convenient charging options influencing EV ownership decisions. Factors like higher education levels, lower rental populations, and concentrations of older population align with increased EV ownership. Utilizing publicly available data offers a more accessible avenue for understanding EV ownership across regions, complementing traditional survey approaches.},
}

@article{Hevner2024,
  title = {Transparency in design science research},
  author = {Alan R. Hevner and Jeffrey Parsons and Alfred Benedikt Brendel and Roman Lukyanenko and Verena Tiefenbeck and Monica Chiarini Tremblay and Jan {vom Brocke}},
  year = {2024},
  journal = {Decision Support Systems},
  volume = {182},
  pages = {114236},
  doi = {https://doi.org/10.1016/j.dss.2024.114236},
  url = {https://www.sciencedirect.com/science/article/pii/S0167923624000691},
  abstract = {Research transparency promotes openness and trust in the process, evidence, contributions, and implications of scientific inquiry. Information Systems (IS), as a pluralistic research community, must address transparency in relation to its use of multiple research methods appropriate to complex socio-technical contexts and challenging research questions. This commentary presents a set of important transparency challenges and actionable guidance for the Design Science Research (DSR) community. We propose a DSR Transparency Framework containing six forms of transparency: process, problem space, solution space, build, evaluation, and contribution. For each, we discuss challenges with guidance to achieve effective DSR transparency throughout the publication process.},
}

@article{Andries2023,
  title = {Alexa doesn't have that many feelings: Children's understanding of AI through interactions with smart speakers in their homes},
  author = {Valentina Andries and Judy Robertson},
  year = {2023},
  journal = {Computers and Education: Artificial Intelligence},
  volume = {5},
  pages = {100176},
  doi = {https://doi.org/10.1016/j.caeai.2023.100176},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000553},
  abstract = {As voice-based Conversational Assistants (CAs), including Alexa, Siri, Google Home, have become commonly embedded in households, many children now routinely interact with Artificial Intelligence (AI) systems. It is important to research children's experiences with consumer devices which use AI techniques because these shape their understanding of AI and its capabilities. We conducted a mixed-methods study (questionnaires and interviews) with primary-school children aged 6–11 in Scotland to establish children's understanding of how voice-based CAs work, how they perceive their cognitive abilities, agency and other human-like qualities, their awareness and trust of privacy aspects when using CAs and what they perceive as appropriate verbal interactions with CAs. Most children overestimated the CAs' intelligence and were uncertain about the systems' feelings or agency. They also lacked accurate understanding of data privacy and security aspects, and believed it was wrong to be rude to conversational assistants. Exploring children's current understanding of AI-supported technology has educational implications; such findings will enable educators to develop appropriate materials to address the pressing need for AI literacy.},
}

@article{Hoseini2024,
  title = {A survey on semantic data management as intersection of ontology-based data access, semantic modeling and data lakes},
  author = {Sayed Hoseini and Johannes Theissen-Lipp and Christoph Quix},
  year = {2024},
  journal = {Journal of Web Semantics},
  volume = {81},
  pages = {100819},
  doi = {https://doi.org/10.1016/j.websem.2024.100819},
  url = {https://www.sciencedirect.com/science/article/pii/S1570826824000052},
  abstract = {In recent years, data lakes emerged as a way to manage large amounts of heterogeneous data for modern data analytics. One way to prevent data lakes from turning into inoperable data swamps is semantic data management. Such approaches propose the linkage of metadata to knowledge graphs based on the Linked Data principles to provide more meaning and semantics to the data in the lake. Such a semantic layer may be utilized not only for data management but also to tackle the problem of data integration from heterogeneous sources, in order to make data access more expressive and interoperable. In this survey, we review recent approaches with a specific focus on the application within data lake systems and scalability to Big Data. We classify the approaches into (i) basic semantic data management, (ii) semantic modeling approaches for enriching metadata in data lakes, and (iii) methods for ontology-based data access. In each category, we cover the main techniques and their background, and compare latest research. Finally, we point out challenges for future work in this research area, which needs a closer integration of Big Data and Semantic Web technologies.},
}

@article{Zhang2025_05,
  title = {U.S. college students’ acceptability and educational benefits of ChatGPT from a digital divide perspective},
  author = {Ceciley (Xinyi) Zhang and Laurent H. Wang and Ronald E. Rice},
  year = {2025},
  journal = {Computers and Education: Artificial Intelligence},
  volume = {8},
  pages = {100385},
  doi = {https://doi.org/10.1016/j.caeai.2025.100385},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000256},
  abstract = {ChatGPT has diffused widely and rapidly, with diverse positive and negative implications. In educational settings it is important to understand students' perceptions of the acceptability of ChatGPT for various learning activities and to examine whether prior digital divide concerns pertain to this digital innovation, in order to provide guidance for users, inform policymakers and other stakeholders, and extend digital divide research. The purpose is to investigate the associations of socioeconomic status (SES, both family and student), gender, and race/ethnicity with students’ perceived acceptability of common ChatGPT activities as well as their opinion about how beneficial ChatGPT is for college education, and additionally, whether such relationships differ based on adoption experience. We analyzed survey data quantitatively in two phases (N = 360 and 1267), applying measurement reliability and validity, correlations, and structural equation modeling. The results indicate that students with higher family SES and lower individual SES, and females, tend to view the acceptability of ChatGPT uses more positively, although racial/ethnic minorities are more critical of displacement activities. ChatGPT adopters perceive two dimensions of ChatGPT activities (academic support and academic displacement) as more acceptable than do non-adopters, and they also perceive uses for academic support more positively than for displacement. Moreover, adoption is a significant moderator of some of these associations. At this early stage of ChatGPT diffusion, these digital divide influences on acceptability and general opinion are weak and variable. The discussion further considers theoretical and practical implications for digital education in the AI era.},
}

@article{Huang2025_01,
  title = {Risk propagation mechanisms in railway systems under extreme weather: A knowledge graph-based unsupervised causation chain approach},
  author = {Yujie Huang and Zhipeng Zhang and Hao Hu},
  year = {2025},
  journal = {Reliability Engineering & System Safety},
  volume = {260},
  pages = {110976},
  doi = {https://doi.org/10.1016/j.ress.2025.110976},
  url = {https://www.sciencedirect.com/science/article/pii/S0951832025001796},
  abstract = {Frequent and intensive adverse weathers can cause severe rail accidents through domino effect, posing significant challenges to railway safety and operational reliability. A detailed elucidation of the risk propagation mechanism across hazardous events is critical for effective risk management in rail transportation. Risk pathways involve various meteorological factors, infrastructure vulnerabilities, and consequences, in which each exhibits distinct causation strengths, trigger probabilities, severity levels, and high-impact points. To disclose the characteristics of weather-related railway domino effect accidents, this paper develops a novel railway causation analysis methodology based on an event logic graph. This framework enhances existing knowledge graph-based methodologies by emphasizing the evolution and logical progression of sequential hazardous events. Besides, an unsupervised accident causation chain linking technique is proposed, which integrates historical accident data into the knowledge graph to build a comprehensive graph database. It facilitates data-driven analysis of both structured and unstructured accident records without requiring laborious annotations. By thoroughly evaluating topological features and statistical indicators via a real-world dataset of weather-related railway accidents, key risk propagation patterns such as risk path dependence, path convergence, and risk escalation curves are recognized. Critical nodes including risk amplifiers, critical junctures, and marginal risk contributors within six critical domino chains are identified. These findings inform targeted risk mitigation strategies to prevent risk propagation and escalation. The proposed methodology and results offer theoretical support and actionable insights for enhancing safety and reliability management of railway systems under extreme weather conditions.},
}

@article{Bekkum}2025,
  title = {Using sensitive data to de-bias AI systems: Article 10(5) of the EU AI act},
  author = {Marvin {van Bekkum}},
  year = {2025},
  journal = {Computer Law & Security Review},
  volume = {56},
  pages = {106115},
  doi = {https://doi.org/10.1016/j.clsr.2025.106115},
  url = {https://www.sciencedirect.com/science/article/pii/S026736492500010X},
  abstract = {In June 2024, the EU AI Act came into force. The AI Act includes obligations for the provider of an AI system. Article 10 of the AI Act includes a new obligation for providers to evaluate whether their training, validation and testing datasets meet certain quality criteria, including an appropriate examination of biases in the datasets and correction measures. With the obligation comes a new provision in Article 10(5) AI Act, allowing providers to collect sensitive data to fulfil the obligation. Article 10(5) AI Act aims to prevent discrimination. In this paper, I investigate the scope and implications of Article 10(5) AI Act. The paper primarily concerns European Union law, but may be relevant in other parts of the world, as policymakers aim to regulate biases in AI systems.},
}

@article{Jian2025,
  title = {Inclusive beyond the swings and slides: Exploring access and equity in Hong Kong's playground},
  author = {Izzy Yi Jian and Terry Yepeng Yao and Kar Him Mo and Pengfei Chen and Weixuan Chen and Yue Yu},
  year = {2025},
  journal = {Habitat International},
  volume = {156},
  pages = {103276},
  doi = {https://doi.org/10.1016/j.habitatint.2024.103276},
  url = {https://www.sciencedirect.com/science/article/pii/S0197397524002765},
  abstract = {Play is critical for children's growth and well-being, with playgrounds serving as primary venues for facilitating their advancement. While research has extensively studied inclusive playground design, addressing factors that may limit play experiences, the spatial accessibility of playgrounds remains understudied, particularly in high-density urban environments where space constraints and competing land uses create unique challenges for equitable provision. Through comprehensive spatial analysis, this study investigates playground accessibility using Hong Kong's New Towns and metropolitan areas as a case study, revealing significant disparities across different socio-demographic contexts. The results demonstrate substantial inequalities in playground accessibility, with complex relationships emerging between income levels, population density, and access patterns at different spatial scales. The study further reveals the role of public housing estates as a basic but essential guarantee for playground provision and underscores the need for targeted metrics and transparency in planning standards. These findings necessitate a targeted and responsive prioritisation in future urban facility planning to ensure equitable accessibility. This research lays the groundwork for urban planning principles that advocate for more balanced, engaging, and suitable playground planning, acknowledging children as essential urban dwellers and working toward making cities more habitable and delightful for everyone.},
}

@article{Huang2023_01,
  title = {Artificial Intelligence in academic library strategy in the United Kingdom and the Mainland of China},
  author = {Yingshen Huang and Andrew M. Cox and John Cox},
  year = {2023},
  journal = {The Journal of Academic Librarianship},
  volume = {49},
  pages = {102772},
  doi = {https://doi.org/10.1016/j.acalib.2023.102772},
  url = {https://www.sciencedirect.com/science/article/pii/S0099133323001118},
  abstract = {There is growing recognition of the value of applying Artificial Intelligence (AI) in libraries. This study explores how academic libraries have responded to this opportunity at the level of strategy, what is the status of the application of AI, if any, and what are the different emphases of development comparing the UK and China. The data for the study was strategy documentation from high-ranking universities and their libraries. The sample consisted of the top 25 universities from the United Kingdom and top 25 from the Mainland of China according to the QS world university rankings. Explicit mention of Artificial Intelligence and related technologies is rarely found in strategic plans of universities in the UK but most Chinese universities mention them in their vision statements which focus on the development of new majors and research of the technology. Though several libraries have already implemented applications based on AI or claim to be “smart” or “intelligent” most academic library strategic plans or agendas do not emphasize AI. This is one of the first studies to explore the current status of AI applied in academic libraries as a sector and to compare experiences internationally.},
}

@article{Lünich2024,
  title = {Diverging perceptions of artificial intelligence in higher education: A comparison of student and public assessments on risks and damages of academic performance prediction in Germany},
  author = {Marco Lünich and Birte Keller and Frank Marcinkowski},
  year = {2024},
  journal = {Computers and Education: Artificial Intelligence},
  volume = {7},
  pages = {100305},
  doi = {https://doi.org/10.1016/j.caeai.2024.100305},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X24001085},
  abstract = {The integration of Artificial Intelligence (AI) into higher education, particularly through Academic Performance Prediction (APP), promises enhanced educational outcomes. However, it simultaneously raises concerns regarding data privacy, potential biases, and broader socio-technical implications. Our study, focusing on Germany–a pivotal player in shaping the European Union's AI policies–seeks to understand prevailing perceptions of APP among students and the general public. Initial findings of a large standardized online survey suggest a divergence in perceptions: While students, in comparison to the general population, do not attribute a higher risk to APP in a general risk assessment, they do perceive higher societal and, in particular, individual damages from APP. Factors influencing these damage perceptions include trust in AI and personal experiences with discrimination. Students further emphasize the importance of preserving their autonomy by placing high value on self-determined data sharing and explaining their individual APP. Recognizing these varied perceptions is crucial for educators, policy-makers, and higher education institutions as they navigate the intricate ethical landscape of AI in education. This understanding can inform strategies that accommodate both the potential benefits and concerns associated with AI-driven educational tools.},
}

@article{Kennedy2024,
  title = {Asia–Pacific developments},
  author = {Gabriela Kennedy},
  year = {2024},
  journal = {Computer Law & Security Review},
  volume = {54},
  pages = {106026},
  doi = {https://doi.org/10.1016/j.clsr.2024.106026},
  url = {https://www.sciencedirect.com/science/article/pii/S026736492400092X},
  abstract = {This column provides a country by country analysis of the latest legal developments, cases and issues relevant to the IT, media and telecommunications' industries in key jurisdictions across the Asia Pacific region. The articles appearing in this column are intended to serve as ‘alerts’ and are not submitted as detailed analyses of cases or legal developments.},
}

@article{King2024,
  title = {Using the system of environmental-economic accounting ecosystem accounting for policy: A case study on forest ecosystems},
  author = {S. King and R. Agra and A. Zolyomi and H. Keith and E. Nicholson and X. {de Lamo} and R. Portela and C. Obst and M. Alam and M. Honzák and R. Valbuena and P.A.L.D. Nunes and F. Santos-Martin and M. Equihua and O. Pérez-Maqueo and M. Javorsek and A. Alfieri and C. Brown},
  year = {2024},
  journal = {Environmental Science & Policy},
  volume = {152},
  pages = {103653},
  doi = {https://doi.org/10.1016/j.envsci.2023.103653},
  url = {https://www.sciencedirect.com/science/article/pii/S1462901123003027},
  abstract = {Robust, regular and integrated evidence on the environment and its relationship with the economy and human well-being is needed to deliver effective environmental policy. This paper highlights the role the United Nations System of Environmental-Economic Accounting Ecosystem Accounting (SEEA EA) can play in delivering this ‘policy-ready’ evidence. We demonstrate this using forest ecosystems as a policy theme of high international concern, via structured reviews of evidence needs for two case studies: the EU Green Deal; and, Liberia’s forest policy framework. The EU Green Deal case study highlights evidence gaps in a proposed regulation on environmental-economic accounting that are policy relevant and could be met using the SEEA EA. These gaps concern old growth forest extent, carbon storage, biodiversity, water regulation and erosion control ecosystem services. The Liberia case study highlights evidence needs for policy concerning the extent of natural forests important for biodiversity and ecosystem services of timber provisioning, global climate regulation and non-wood forest products, which could be met by the SEEA EA. Starting from these policy perspectives is critical to establishing evidence needs that the SEEA EA should be compiled to meet. This address concerns that the compilation of SEEA EA accounts has often been an exercise in best organising available data, rather than a demand driven exercise in response to policy evidence needs. We argue that addressing clear policy needs is essential for the SEEA EA to deliver on its potential to mainstream the many benefits from natural, as well as managed forests, into development planning.},
}

@article{Jaafari2025,
  title = {Spatiotemporal dynamics of social vulnerability to natural hazards: Trends and projections from 2002 to 2030 in northwestern Iran},
  author = {Abolfazl Jaafari and Davood Mafi-Gholami and Bahram Choubin},
  year = {2025},
  journal = {Sustainable Cities and Society},
  volume = {120},
  pages = {106172},
  doi = {https://doi.org/10.1016/j.scs.2025.106172},
  url = {https://www.sciencedirect.com/science/article/pii/S2210670725000502},
  abstract = {The significant economic and human toll of natural hazards underscores the need for robust management strategies, particularly county- and community-level vulnerability initiatives to mitigate the devastating impacts of these natural hazards on vulnerable populations. Here, we assessed and quantified the social vulnerability to natural hazards, specifically floods, in 17 northwestern Iran counties from 2002 to 2022, with projections for 2030. We modeled and mapped flood vulnerability, then used expert opinion to weigh exposure, sensitivity, and adaptive capacity indicators, forming a Social Vulnerability Index (SoVI). Experts identified population density, age demographics, and healthcare infrastructure as the most important variables contributing to social vulnerability to natural hazards. Spatiotemporal analysis showed a 20-year upward SoVI trend for 24 % of counties, while 76 % decreased. Projections for 2030 suggested a decrease in vulnerability for 65 % of counties, with an increase for the remaining 35 %. Our study revealed that social vulnerability is context-specific and warrants individual assessments across various counties to effectively address the disparities in vulnerability indicators among different socio-economic sectors. While we primarily concentrated on a specific province in Iran, the findings and strategic insights hold global relevance, potentially enhancing the efficacy of social vulnerability assessments worldwide.},
}

@article{Sońta-Drączkowska2024,
  title = {Co-creating innovations with users: A systematic literature review and future research agenda for project management},
  author = {Ewa Sońta-Drączkowska and Marzenna Cichosz and Patrycja Klimas and Tomasz Pilewicz},
  year = {2024},
  journal = {European Management Journal},
  doi = {https://doi.org/10.1016/j.emj.2024.07.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0263237324000902},
  abstract = {This study aimed to systematically review the extensive literature on user innovation co-creation and connect the findings to the project management domain. It focused specifically on new product development, offering a domain-based systematic review of methods, tools, and user types involved in the co-creation process. Analyzing a total of 266 articles, the authors synthesize the types of users and methods discussed in the domain of user innovation, aligning them across the new product development cycle and specific phases of co-innovation. Additionally, the authors formulate research questions and propositions that may inspire the project management domain. This study provides insights into innovation-oriented, exploratory project management and enhances the configuration approach to project management. From a practical perspective, it provides a comprehensive overview of methods that can enrich a managerial toolkit for leading innovative projects.},
}

@article{Cain2023,
  title = {Navigating Design, Data, and Decision in an Age of Uncertainty},
  author = {John Cain and Zach Pino},
  year = {2023},
  journal = {She Ji: The Journal of Design, Economics, and Innovation},
  volume = {9},
  pages = {197-212},
  doi = {https://doi.org/10.1016/j.sheji.2023.07.002},
  url = {https://www.sciencedirect.com/science/article/pii/S2405872623000448},
  abstract = {The Future of Design Education working group on technical systems argues that the approach to handling data—the methods used, and the expectations for outcomes—can transform design practice. In contrast to design’s past defined by a lack of accessible data, today’s rapidly evolving age of data abundance informs the choices available—the decision space—with far-reaching consequences for organizational, social, and environmental well-being. The shifting design landscape requires new tools and techniques to navigate this data age effectively. This paper proposes a new curricular approach that intersects data, technology, and design to create an environment where students can evaluate their roles and impact, and interact effectively through data with humans and computational collaborators. This data-oriented curriculum includes foundational technical skills proficiency, data analytical skills, rhetorical skills for arguing with data, interdisciplinary design studies, and a focus on designing for society. It embraces the complexities and opportunities of the data age, and acknowledges the inherent uncertainty in this new landscape. The aim is to prepare the next generation of designers to create data-informed, human-centered, ethical, and sustainable designs, thereby fostering an inclusive, equitable, and sustainable future.},
}

@article{Ollerenshaw2024,
  title = {A smart agriculture information system delivering research data for adoption by the Australian grains industry},
  author = {A. Ollerenshaw and N. Robinson and A. Chadha and J. Channon},
  year = {2024},
  journal = {Smart Agricultural Technology},
  volume = {9},
  pages = {100610},
  doi = {https://doi.org/10.1016/j.atech.2024.100610},
  url = {https://www.sciencedirect.com/science/article/pii/S2772375524002156},
  abstract = {Online Farm Trials (OFT) is a bespoke online information system providing current and historical grains trials research data and information in Australia. It represents a technology that supports smart agriculture by enabling discoveries in the data. Its impact on users has been established, supporting industry knowledge and decision making and leading to improved work practices. The aim of this research is to identify the contribution of OFT, as a tool that supports smart agriculture, and to explore the current adoption, barriers, and opportunities for expanding the system for the Australian grains industry. In-depth, semi-structured interviews were conducted with leaders and innovators from the agriculture industry (N = 16). Thematic analysis of the qualitative data reveals widespread value of OFT as an important resource for accessing industry-dedicated, current and historical research trials data. Barriers to adoption were identified which included incomplete data, and a lack of industry-wide awareness. Opportunities to consolidate and expand the data, together with improved industry awareness, are essential to improving industry-wide adoption. Key learnings were identified in the research that offers benefit for the industry, and consideration for the adoption of technology in smart agriculture. This includes functionality, audience, completeness of information/data, consideration of new technologies and the promotion of the system through the delivery of real world examples to encourage industry adoption. Recommendations to support the future design and development of digital platforms in agriculture are also offered.},
}

@article{Zhang2024_04,
  title = {Data preparation for Deep Learning based Code Smell Detection: A systematic literature review},
  author = {Fengji Zhang and Zexian Zhang and Jacky Wai Keung and Xiangru Tang and Zhen Yang and Xiao Yu and Wenhua Hu},
  year = {2024},
  journal = {Journal of Systems and Software},
  volume = {216},
  pages = {112131},
  doi = {https://doi.org/10.1016/j.jss.2024.112131},
  url = {https://www.sciencedirect.com/science/article/pii/S0164121224001766},
  abstract = {Code Smell Detection (CSD) plays a crucial role in improving software quality and maintainability. And Deep Learning (DL) techniques have emerged as a promising approach for CSD due to their superior performance. However, the effectiveness of DL-based CSD methods heavily relies on the quality of the training data. Despite its importance, little attention has been paid to analyzing the data preparation process. This systematic literature review analyzes the data preparation techniques used in DL-based CSD methods. We identify 36 relevant papers published by December 2023 and provide a thorough analysis of the critical considerations in constructing CSD datasets, including data requirements, collection, labeling, and cleaning. We also summarize seven primary challenges and corresponding solutions in the literature. Finally, we offer actionable recommendations for preparing and accessing high-quality CSD data, emphasizing the importance of data diversity, standardization, and accessibility. This survey provides valuable insights for researchers and practitioners to harness the full potential of DL techniques in CSD.},
}

@article{Huang2024_03,
  title = {Examining the relationship between the L2 motivational self system and technology acceptance model post ChatGPT introduction and utilization},
  author = {Jerry Huang and Atsushi Mizumoto},
  year = {2024},
  journal = {Computers and Education: Artificial Intelligence},
  volume = {7},
  pages = {100302},
  doi = {https://doi.org/10.1016/j.caeai.2024.100302},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X2400105X},
  abstract = {Since the introduction of the L2 Motivational Self System (L2MSS), numerous studies worldwide have highlighted its effectiveness in elucidating Second Language Acquisition. However, the influence of generative artificial intelligence (GenAI) technology on this model remains largely unexplored. The Technology Acceptance Model (TAM) is a widely employed framework for examining the impact of a new technology, and this study explores the intercorrelation when these two models are considered together. Conducted with 35 s-year university English as a foreign language (EFL) students in humanities, the study involved two sessions of instructor-led ChatGPT usage writing workshops, followed by the collection of survey responses. Data analysis unveiled a notable correlation between the L2 Motivational Self System and the Technology Acceptance Model. Particularly noteworthy is the finding that Ought-to L2 Self positively predict Actual Usage. The study discusses pedagogical and theoretical implications, along with suggesting future research directions.},
}

@article{Bautista-Hernández2024,
  title = {Who uses transit in the journey to work? Multimodality, equity, and planning implications in México City},
  author = {Dorian Antonio Bautista-Hernández and Alejandra {Trejo Nieto}},
  year = {2024},
  journal = {Journal of Transport Geography},
  volume = {117},
  pages = {103871},
  doi = {https://doi.org/10.1016/j.jtrangeo.2024.103871},
  url = {https://www.sciencedirect.com/science/article/pii/S0966692324000802},
  abstract = {Increasing mobility needs in developing countries demand the coordination and development of robust urban transit systems. However, several factors lead to the emergence of fragile and disintegrated systems, where several subsystems work under different schemes of formality. In this context, transit travel tends to be highly multimodal, which at the trip level has been scarcely studied. In this study, the metropolitan geography of the primary forms of Transit Mode Combinations (TMCs) is explored for the Mexico City Metropolitan Area (MCMA) based on data from the 2017 Household Origin-Destination Survey (HODS17). Transit travel is disaggregated to analyze its multimodality, its link with location, as well as its sociodemographic profiles. The most frequent TMCs are compared regarding average travel distances and speed, and their ridership rates are mapped. A regression model is used to test the socioeconomic and urban form correlates of the TMCs. The results show that transit multimodality has a specific geographical pattern by TMCs. Most Metro users are not those residing close to its stations but those having these stations at their trip destination. Travel by Metro shows a stronger association with decreasing Colectivo ridership than with travel by Bus Rapid Transit (BRT). These findings are discussed based on issues of equity and the emerging debates about the path that transit systems must follow in the developing world.},
}

@article{Gudivada2025,
  title = {Chapter 2 - Data analytics: fundamentals},
  author = {Venkat N. Gudivada},
  year = {2025},
  pages = {27-66},
  doi = {https://doi.org/10.1016/B978-0-443-13878-2.00012-6},
  publisher = {Elsevier},
  url = {https://www.sciencedirect.com/science/article/pii/B9780443138782000126},
  abstract = {This chapter provides a comprehensive and unified view of data analytics fundamentals. Four functional facets of data analytics—descriptive, diagnostic, predictive, and prescriptive—are described. The evolution of data analytics from SQL analytics, business analytics, visual analytics, and big data analytics to cognitive analytics is presented. Data science as the foundational discipline for the current generation of data analytics systems is explored in this chapter. Data lifecycle and data quality issues are outlined. Open-source tools and resources for developing data analytics systems are listed. The chapter concludes by indicating emerging trends in data analytics.},
}

@article{Auñón2024,
  title = {Evaluation and utilisation of privacy enhancing technologies—A data spaces perspective},
  author = {J.M. Auñón and D. Hurtado-Ramírez and L. Porras-Díaz and B. Irigoyen-Peña and S. Rahmian and Y. Al-Khazraji and J. Soler-Garrido and A. Kotsev},
  year = {2024},
  journal = {Data in Brief},
  volume = {55},
  pages = {110560},
  doi = {https://doi.org/10.1016/j.dib.2024.110560},
  url = {https://www.sciencedirect.com/science/article/pii/S2352340924005274},
  abstract = {Data sharing has facilitated the digitisation of society. We can access our bank accounts or make an appointment with our doctor anytime and anywhere. To achieve this, we have to share certain information, whether personal, professional, etc. This may seem like a minor cost for an individual user, but actually the data economy as the backbone of a digital transformation that is reshaping all aspects of human life. However, one of the major concerns arises regarding what happens to such individual data; once shared, control over it is often lost. For that reason, users and companies are reluctant to share their data. The European Union, through its European Strategy for Data, is establishing a policy and legal framework for establishing a single market for data in Europe by improving the trust and fairness of the data economy. Data spaces are a commitment to sharing data in a reliable and secure way, but this endeavour should, of course, not be at the expense of privacy rights. In recent years, Privacy-Enhancing Technologies (PETs) have emerged to achieve data sharing and privacy preservation that can address the requirements of data spaces around sensitive citizen and business data. In this work, we review existing PETs and assess their relevance, technological maturity, and applicability in the context of common European data spaces. Finally, we illustrate the benefits of secure data sharing via Federated Learning in a healthcare use case, where the preservation of privacy is a primer requirement and is therefore to be guaranteed.},
}

@article{Biber-Freudenberger2025,
  title = {Impacts of road development in sub-Saharan Africa: A call for holistic perspectives in research and policy},
  author = {Lisa Biber-Freudenberger and Christina Bogner and Georg Bareth and Michael Bollig and Peter Dannenberg and Javier Revilla Diez and Clemens Greiner and Philipo Jacob Mtweve and Britta Klagge and Tanja Kramm and Detlef Müller-Mahn and Vincent Moseti and Nicodemus Nyamari and Dennis Otieno Ochuodho and Elias Kuntashula and Theobald Theodory and Jessica Paula Rose Thorn and Jan Börner},
  year = {2025},
  journal = {iScience},
  volume = {28},
  pages = {111913},
  doi = {https://doi.org/10.1016/j.isci.2025.111913},
  url = {https://www.sciencedirect.com/science/article/pii/S2589004225001737},
  abstract = {Summary This perspective explores the multifaceted development challenges related to road network expansion in sub-Saharan Africa, where recent infrastructure investments reflect transformative ambitions but also imply socio-ecological tradeoffs. Roads can boost economic growth by facilitating trade, tourism, and access to essential services, yet they simultaneously contribute to ecosystem fragmentation, biodiversity loss, and human-wildlife conflicts. Looking at the history of Africa’s road development, we find that mega-projects—often funded by international donors—reshape political and economic landscapes while altering rural livelihoods and ecosystems. We synthesize literature and case studies to reveal critical trends and propose solutions, urging for a shift toward sustainable, evidence-based infrastructure strategies that balance development with environmental stewardship. We further advocate for transdisciplinary approaches and community engagement to align road expansion with long-term stakeholder needs so as to minimize adverse impacts on Africa’s socio-ecological systems.},
}

@article{Sato2024,
  title = {A systematic survey of environmental DNA in Palau's lakes and waterfalls reveals an increase in Leptospira levels after flooding},
  author = {Yukuto Sato and Kaori Tsurui-Sato and Yoichiro Uchima and Cheryl-Ann Udui and Osiro Lorin and Kashgar Rengulbai and Claudia Toma and Ryo Suzuki},
  year = {2024},
  journal = {One Health},
  volume = {19},
  pages = {100898},
  doi = {https://doi.org/10.1016/j.onehlt.2024.100898},
  url = {https://www.sciencedirect.com/science/article/pii/S2352771424002246},
  abstract = {Objective Leptospirosis is an important bacterial zoonosis which is widespread in tropical and subtropical islands and influences human and animal health which has secondary economic effects. Although leptospirosis is endemic in Palau, an Oceanian Pacific Island country, few systematic surveys of potential risk factors for Leptospira infection, such as weather and host animals, have been conducted in the natural environment. We used environmental DNA metabarcoding to assess the distribution, species diversity, and abundance of pathogenic Leptospira in this endemic region to investigate the potential environmental risks. Methods Forty-two paired water samples, representing fine and rainy weather conditions, were collected from four representative waterfalls and lakes on Babeldaob Island, the largest island in Palau. High-throughput sequencing analysis was conducted for polymerase chain reaction products of leptospiral 16S rRNA and vertebrate animal mitochondrial 12S rRNA genes. Results We revealed greater Leptospira diversity and abundance in samples collected after continuous rain, particularly in the presence of flooding, compared with samples collected under typhoon, monsoon, or fine weather conditions. From same samples, six mammalian species including cats (Felis catus), mice (Mus musculus), Yap flying fox (Pteropus yapensis), rats (Rattus spp.), and pigs (Sus scrofa) were repeatedly detected. These may be candidates of host animals of Leptospira in Palau; however, their detection was not clearly correlated with that of Leptospira. Conclusion We repeatedly detected several species of pathogenic Leptospira from water samples of a wide region of Babeldaob Island. We confirmed that Leptospira contamination in freshwater environments increased under rainy conditions, particularly in the presence of flooding. This information could be used to improve public health control measures in this region.},
}

@article{Moulin2025,
  title = {Evaluating the effectiveness of two congestion limitation policies in Milan: Charge increase and vehicle type},
  author = {Léonard Moulin and Valeria Maria Urbano},
  year = {2025},
  journal = {Transport Policy},
  volume = {165},
  pages = {17-27},
  doi = {https://doi.org/10.1016/j.tranpol.2025.02.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0967070X25000630},
  abstract = {Congestion pricing, also referred to as road pricing, is a form of Pigouvian taxation designed to limit or reduce vehicular traffic within a specific area. These systems aim to encourage changes in driving behavior and the choice of transportation mode. An example of a congestion pricing system is the Milan Area C charging zone, which operates on a fixed-rate basis. In recent years, two changes to the system have been introduced: (i) restrictions on the vehicle types allowed to enter the zone, and (ii) a 50 percent increase in the congestion charge. This study introduces a novel and replicable approach to evaluate the effectiveness of these policy changes, by studying the ratio of vehicles entering Area C to the total number of unique individuals within it, leveraging mobile phone data on user presence. Using fixed-effects models to control for unobserved heterogeneity across time, this study analyzes the impact of these two policy changes in the Area C congestion pricing system. The findings indicate that both policy changes influenced individuals’ choice of transportation mode, with vehicle type restrictions having a greater impact than price increases. This shows the effectiveness of the two types of measures, offering insights for policymakers on how to enhance congestion charging system effectiveness through refined pricing strategies and vehicle limitations. Additionally, demographic characteristics of users present in the area, as captured through mobile phone data, such as the proportion of women and elderly individuals, significantly influence transportation choices. Recognizing these factors is essential for policymakers, as it highlights the need for equitable policies that improve acceptance and effectiveness among vulnerable groups. Additionally, demographic characteristics of users present in the area, as captured through mobile phone data—such as the proportion of women and elderly individuals—significantly influence transportation choices.},
}

@article{Khan2025_01,
  title = {Chapter 11 - AI-driven drug discovery and development},
  author = {Sameer Mohommed Khan},
  year = {2025},
  pages = {233-255},
  doi = {https://doi.org/10.1016/B978-0-443-33584-6.00011-6},
  publisher = {Academic Press},
  url = {https://www.sciencedirect.com/science/article/pii/B9780443335846000116},
  abstract = {The process of identifying a medicine from its early phases of discovery to its commercial distribution is referred to as drug development and discovery. The process of creating, discovering, and developing new drugs is a difficult, dangerous, high-octane, and maybe extremely gratifying and illuminating undertaking. The biopharmaceutical sector is poised for a revolutionary shift as artificial intelligence (AI) becomes a game-changing factor in drug discovery. AI has the potential to speed up and lower the cost of bringing new medications to market, and this promise is starting to materialize. The purpose of this chapter is to provide readers with a general understanding of the many phases involved in the drug discovery and development process. Additionally, it informs the readers about the technology and function of artificial intelligence in this procedure. The benefits and challenges of using AI in drug discovery are also mentioned at the end of the chapter.},
}

@article{Tropsha2025,
  title = {The Six Ds of Exponentials and drug discovery: A path toward reversing Eroom’s law},
  author = {Alexander Tropsha and Holli-Joi Martin and Artem Cherkasov},
  year = {2025},
  journal = {Drug Discovery Today},
  pages = {104341},
  doi = {https://doi.org/10.1016/j.drudis.2025.104341},
  url = {https://www.sciencedirect.com/science/article/pii/S1359644625000546},
  abstract = {Many technological sectors underwent recent exponential growth because of digital disruption, a phenomenon Peter Diamantis characterized as the ‘Six Ds of Exponentials’: digitization, deception, disruption, demonetization, dematerialization, and democratization. In contrast, drug discovery has been marked by rising costs and modest growth, if any, of annual drug approvals. We argue that the exponential growth of drug discovery can be also achieved through digital disruption brought by data expansion, mature artificial intelligence (AI), automation of experiments, public–private partnerships, and open science. We detected the emergence of all ‘Six Ds of Exponentials’ within modern drug discovery and discuss how each of the ‘Six Ds’ can further empower the field and forcefully address the societal demand for novel, potent, affordable, and accessible medicines.},
}

@article{Zhu2024,
  title = {Establish an information integration and data sharing platform for national quality infrastructure (NQI) in China},
  author = {Yiwei Zhu and Xingchuang Xiong and Zilong Liu and Dabo Li and Jianhua Liu},
  year = {2024},
  journal = {Measurement: Sensors},
  pages = {101486},
  doi = {https://doi.org/10.1016/j.measen.2024.101486},
  url = {https://www.sciencedirect.com/science/article/pii/S2665917424004628},
  abstract = {The UNESCO “Open Science Recommendation” published in 2021 signals a global consensus on open science, emphasizing the importance of open science infrastructures such as data sharing platform and scientific data repositories. China is building a National Quality Infrastructures (NQI) system and has made continuous progress in metrology, standard, testing, certification and accreditation. National Institute of Metrology, China (NIM) is leading the planning and establishment of an information integration and data sharing platform for the NQI in China, in order to solve the problems of dispersed NQI information resources and insufficient sharing of NQI scientific research results, and to realize information integration and data sharing within the NQI field. This paper briefly describes the establishment program, system architecture and core functions of the platform project, as well as the strategies planned to provide services for different target groups, including researchers in the NQI system, managers of research institutions and funders. Through the establishment of this data sharing platform, it is expected to simplify the process of accessing, sharing and reusing data and information for researchers in the field of NQI, and improve the efficiency of utilizing NQI data resources, thus promoting the development of Digital NQI, ensuring that the data in the field of NQI complies with the FAIR principles (Findable, Accessible, Interoperable, and Reusable) and accelerating the scientific research innovation and sharing of results in the field of NQI in China.},
}

@article{Ma2025,
  title = {Systematically visualizing ChatGPT used in higher education: Publication trend, disciplinary domains, research themes, adoption and acceptance},
  author = {Ting Ma},
  year = {2025},
  journal = {Computers and Education: Artificial Intelligence},
  volume = {8},
  pages = {100336},
  doi = {https://doi.org/10.1016/j.caeai.2024.100336},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X24001395},
  abstract = {Since it was released in November 2022, ChatGPT has been exerting revolutionary influence on the realm of higher education. In order to obtain a comprehensive understanding of the research landscape, we conduct a systematic literature review on the studies of ChatGPT used in higher education. Both quantitative and qualitative methods were adopted to bibliometrically examine the included literature selected from Web of Science and Scopus through the PRISMA protocol. Tools of VOSviewer and CitNetExplorer were employed to visualize the citation information. Our findings showed that the recent two years witnessed an ever-growing popularity of this research theme. Citation information analysis reveals the most influential authors, countries, sources, organizations and four focused topics. The disciplinary distribution of related research indicates a wide range of categories. More importantly, ChatGPT was found to be versatile in assisting teachers, students and researchers with a variety of tasks, and the factors influencing the acceptance of this technology among college students could be investigated through models like TAM, UTAUT and their extensions. We suggest future studies to focus on the ways to address the limitations and ethical issues of ChatGPT through AI literacy cultivation and joint efforts of all stakeholders.},
}

@article{Przeybilovicz2024,
  title = {Governing in the digital age: The emergence of dynamic smart urban governance modes},
  author = {Erico Przeybilovicz and Maria Alexandra Cunha},
  year = {2024},
  journal = {Government Information Quarterly},
  volume = {41},
  pages = {101907},
  doi = {https://doi.org/10.1016/j.giq.2023.101907},
  url = {https://www.sciencedirect.com/science/article/pii/S0740624X23001077},
  abstract = {There is growing concern that implementing effective governance constitutes a significant element in cities becoming ‘smart’ due to its multidisciplinarity, complexity of urban challenges and multi-stakeholder involvement. It is assumed that in smart city initiatives, new governance modes arise through the interplay of technological artefacts and political and social factors, viewed through a sociotechnical perspective. We also argue that traditional urban governance modes help explain emerging modes and the nature of citizen-government interactions. Thus, a combination of the sociotechnical view with the modes of urban governance as a theoretical approach was used to understand the dynamics of emerging governance modes in smart city initiatives. Two cases were studied using a longitudinal qualitative case study and temporal bracketing analysis for an in-depth understanding. Our findings evidenced that the configuration of the elements, governance mode, information and communication technology (ICT) and types of citizen-government interaction varies from one initiative to another and changes over time, across multiple sociotechnical networks in practice, which leads to emerging new governance modes. We highlight that a new understanding of smart urban governance for sustainable development in the digital age needs to be developed as a dynamic process. Moreover, we identified two emerging governance modes and proposed a dynamic approach to investigate smart urban governance in other contexts.},
}

@article{Soosaar}2025,
  title = {Bridging the gap: Unravelling local government data sharing barriers in Estonia and beyond},
  author = {Katrin {Rajamäe Soosaar} and Anastasija Nikiforova},
  year = {2025},
  journal = {Computer Law & Security Review},
  volume = {56},
  pages = {106099},
  doi = {https://doi.org/10.1016/j.clsr.2024.106099},
  url = {https://www.sciencedirect.com/science/article/pii/S026736492400164X},
  abstract = {Open Government Data (OGD) plays a crucial role in transforming smart cities into sustainable and intelligent entities by enabling analytics, real-time monitoring, and informed decision-making. However, local administrative data remain underutilized due to organizational, technological, and legal barriers, even in advanced countries like Estonia. While Estonia is globally recognized for its digital governance success, its local governments face persistent challenges in OGD adoption. This study explores barriers preventing Estonian municipalities from sharing data, using a qualitative approach through interviews with Estonian municipalities. Drawing on the OGD-adapted Innovation Resistance Theory (IRT) model, it highlights current issues such as limited awareness, skills gaps, and data quality. By identifying overlooked weaknesses in Estonia's open data ecosystem and providing actionable recommendations, this research contributes to a more resilient and sustainable open data ecosystem development. Additionally, by validating the OGD-adapted Innovation Resistance Theory model and proposing a revised version tailored for local government contexts, the study advances theoretical frameworks on data sharing resistance. Ultimately, this study serves as a call to action for policymakers and practitioners to prioritize local OGD initiatives, ensuring the full utilization of OGD in smart city development.},
}

@article{Kopanaki2025,
  title = {Designing an information system to support family policy making – the digital transformation of the policy making process},
  author = {Evangelia Kopanaki and Dimitrios S. Stamoulis},
  year = {2025},
  journal = {Procedia Computer Science},
  volume = {256},
  pages = {142-149},
  doi = {https://doi.org/10.1016/j.procs.2025.02.106},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050925004636},
  abstract = {Policy making in the public sector is mainly supported by databases, as well as by the dispersed use of software tools. Information resources, not matter how extensive they may be, do not constitute information systems support for policy making. Drawing on the analogy of ERP systems for the private sector, information systems in the public sector are absolute necessary for evidence-based, data-driven policy making. Recognizing the complexities and multifaceted nature of family-related issues, the family policy domain is used to exemplify the design and usage of such information systems. Therefore, this paper proposes the conceptual design of a family policy information system (FPIS), which can be generalized for other policy making areas. By leveraging advanced digital technologies and methodologies, such as social analytics and stakeholder engagement tools, the proposed conceptual architecture of an FPIS facilitates evidence-based, data driven decision making to address the diverse needs of policy making support in an integrated manner. The design of an FPIS exemplifies the digital transformation of the policy making process in the public sector.},
}

@article{Monlezun2024,
  title = {7 - Our common home: artificial intelligence + global public health ecosystem},
  author = {Dominique J. Monlezun},
  year = {2024},
  pages = {215-243},
  doi = {https://doi.org/10.1016/B978-0-443-21597-1.00007-X},
  publisher = {Morgan Kaufmann},
  url = {https://www.sciencedirect.com/science/article/pii/B978044321597100007X},
  abstract = {Chapter 7 unites the different dimensions explored in each of the earlier chapters into a cohesive whole to understand the artificial intelligence (AI)-powered global public health ecosystem as humanity’s common home: its decentralized organic design (financing and integral sustainable development), framework (data architecture and political economics), inhabitants (culture and demographics), and foundation (ethics and human security balancing national security). It summarizes the key findings for these domains from the earlier chapters while highlighting emblematic AI case uses. It considers financing advances, including in universal health coverage, public–private partnerships, digital global health diplomacy, finance tracking, and value-based health. This chapter moves on to integral sustainable development advances, including in AI for the sustainable development goals, precision agriculture, climate change, affordable clean energy, equity, and generative AI (including ChatGPT). It then considers data architecture advances, including in the United Kingdom’s hybrid data architecture, India’s federated data architecture, swarm learning, gossip learning, blockchain, edge computing, application programming interfaces, augmented public health intelligence, quantum computing, zero-trust security, blockchain, and data solidarity. This chapter then considers political economic advances particularly from the perspective of Political Liberalism–bridging democracies and autocracies, including in data governance models (spanning Europe’s general data protection regulation and Japan’s agile governance), managed strategic competition, and World Health Organization coordination. Finally, this chapter considers AI ethics for the health ecosystem. Particular emphasis is given to how population aging, multicultural diversity, and human security requires more inclusive discussion of diverse perspectives, as through Personalist Social Contract ethics to generate and sustain substantive convergence on the unifying values of human dignity, rights, and sovereignty that then give rise to effective and equitable collective action. This chapter concludes by applying the abovesaid dimensions to concrete AI use cases for the global public health ecosystem that illustrates this integral approach, including ethics by design or embedded AI ethics (within existing ecosystem operations and structures), democratizing AI or personalizing AI (as with end-to-end AI platforms and edge computing expanding and interlinking free and affordable AI services for larger audiences), and ecosystem interoperability (uniting political economic interoperability, data interoperability, and moral interoperability to leverage global resources and insights for local communities leading their own projects).},
}

@article{Chang2025,
  title = {Are they psychologically prepared? Examining psychological preparedness for disasters among frontline civil servants in Taiwan},
  author = {Kaiju Chang},
  year = {2025},
  journal = {International Journal of Disaster Risk Reduction},
  volume = {117},
  pages = {105189},
  doi = {https://doi.org/10.1016/j.ijdrr.2025.105189},
  url = {https://www.sciencedirect.com/science/article/pii/S2212420925000135},
  abstract = {Psychological preparedness for disasters among frontline civil servants still has not been fully explored in current disaster management research. To fill this gap, this study focuses on frontline civil servants tasked with disaster management to examine the extent of their psychological preparedness for disasters and explore influential factors to increase such preparedness. This research surveyed frontline civil servants in district offices across six special municipalities in Taiwan to collect and analyze data, garnering 926 valid responses. The empirical findings reveal that compared to that for earthquakes, psychological preparedness for floods scored higher on average. The regression analyses indicate the significance of disaster management job self-efficacy, trust in government, serving as primary staff, and having psychological preparedness-related training. With respect to trust in government, confidence in the disaster management capabilities of affiliated district offices and local police and fire stations is crucial. Future studies should keep exploring differences in risk perception, sex, and educational level in psychological preparedness for disasters.},
}

@article{Lnenicka2024_01,
  title = {Understanding the development of public data ecosystems: From a conceptual model to a six-generation model of the evolution of public data ecosystems},
  author = {Martin Lnenicka and Anastasija Nikiforova and Mariusz Luterek and Petar Milic and Daniel Rudmark and Sebastian Neumaier and Karlo Kević and Anneke Zuiderwijk and Manuel Pedro {Rodríguez Bolívar}},
  year = {2024},
  journal = {Telematics and Informatics},
  volume = {94},
  pages = {102190},
  doi = {https://doi.org/10.1016/j.tele.2024.102190},
  url = {https://www.sciencedirect.com/science/article/pii/S0736585324000947},
  abstract = {There is a lack of understanding of the elements that constitute different types of value-adding public data ecosystems and how these elements form and shape the development of these ecosystems over time, which can lead to misguided efforts to develop future public data ecosystems. The aim of the study is twofold: (1) to explore how public data ecosystems have developed over time and (2) to identify the value-adding elements and formative characteristics of public data ecosystems. Using an exploratory retrospective analysis and a deductive approach, we systematically review 148 studies published between 1994 and 2023. Based on the results, this study presents a typology of public data ecosystems and develops a conceptual model of elements and formative characteristics that contribute most to value-adding public data ecosystems. Moreover, this study develops a conceptual model of the evolutionary generation of public data ecosystems represented by six generations that differ in terms of (a) components and relationships, (b) stakeholders, (c) actors and their roles, (d) data types, (e) processes and activities, and (f) data lifecycle phases. Finally, three avenues for a future research agenda are proposed. This study is relevant for practitioners suggesting what elements of public data ecosystems have the most potential to generate value and should thus be part of public data ecosystems. As a scientific contribution, this study integrates conceptual knowledge about the elements of public data ecosystems, the evolution of these ecosystems, defines a future research agenda, and thereby moves towards defining public data ecosystems of the new generation.},
}

@article{Jackson2025,
  title = {An assessment of Kenya's forest policy and law on participatory forest management for sustainable forest management: Insights from Mt. Kenya Forest Reserve},
  author = {Colbert M. Jackson and Olufemi S. Durowoju and Samuel A. Adelabu and Sunday A. Adeniyi},
  year = {2025},
  journal = {Trees, Forests and People},
  volume = {19},
  pages = {100770},
  doi = {https://doi.org/10.1016/j.tfp.2024.100770},
  url = {https://www.sciencedirect.com/science/article/pii/S2666719324002760},
  abstract = {The management of state forests is increasingly adopting participatory forest management (PFM), a collaborative approach involving various stakeholders. PFM aims to enhance transparency in forest governance by involving local communities in decision-making and processes. The Forest Act of 2005 formalized PFM in Kenya, granting community forest associations (CFAs) the right to collaborate with the Kenya Forest Service (KFS). This framework empowers CFAs with both responsibilities and rights related to forest management and benefit sharing. This study evaluated the effectiveness of PFM in Kenya by analyzing the forest policy, legislative framework, and institutional mechanisms that support the transition to sustainable forest management (SFM). It also aimed to identify and address key challenges in forest law enforcement and governance. The study focused on three key questions: (i) How has Forest Act of 2005 and subsequent policies influenced PFM? (ii) How effective are CFAs in managing forests? (iii) What challenges in Forest Law Enforcement and Governance (FLEG) hinder PFM, and what strategies can address these issues? A mixed-methods approach was used, combining qualitative and quantitative data for cross-validation. The methodology included a comprehensive literature review, alongside data from key informant interviews (KIIs), household surveys, and participant observation. The analysis revealed gaps in forest governance, including conflicting legislation, weak enforcement, insufficient KFS funding, limited CFA involvement in decision-making, and ambiguous CFA roles. Other challenges included illegal logging, inadequate judicial understanding of forestry law, insufficient PFM benefits for the local communities, and the need for more inclusive practices to improve forest conservation and management.},
}

@article{Daniel2024,
  title = {Applying model-driven engineering to the domain of chatbots: The Xatkit experience},
  author = {Gwendal Daniel and Jordi Cabot},
  year = {2024},
  journal = {Science of Computer Programming},
  volume = {232},
  pages = {103032},
  doi = {https://doi.org/10.1016/j.scico.2023.103032},
  url = {https://www.sciencedirect.com/science/article/pii/S0167642323001144},
  abstract = {Chatbots are becoming a common component of many types of software systems. But they are typically developed as a side feature using ad-hoc tools and custom integrations. Moreover, current frameworks are efficient only when designing simple chatbot applications while they still require advanced technical knowledge to define complex interactions and are difficult to evolve along with the company needs. In addition, the deployment of a chatbot application usually requires a deep understanding of the targeted platforms, especially back-end connections, increasing the development and maintenance costs. In this paper, we discuss our experiences building, evolving and distributing the Xatkit framework. Xatkit is a model-based framework built around a Domain-Specific Language to define chatbots (and voicebots and bots in general) in a platform-independent way. Xatkit also comes with a runtime engine that automatically deploys the chatbot application and manages the defined conversation logic over the platforms of choice. Xatkit has significantly evolved since its initial release. This paper focuses on describing the evolution and the reasons (technical and non-technical) that triggered them. We believe our lessons learned can be useful to any other initiative trying to build a successful industrial-level chatbot platform, and in general, any type of model-based solution.},
}

@article{Downing2025,
  title = {Harnessing Internet Search Data as a Potential Tool for Medical Diagnosis: Literature Review},
  author = {Gregory J Downing and Lucas M Tramontozzi and Jackson Garcia and Emma Villanueva},
  year = {2025},
  journal = {JMIR Mental Health},
  volume = {12},
  doi = {https://doi.org/10.2196/63149},
  url = {https://www.sciencedirect.com/science/article/pii/S2368795925000216},
  abstract = {Background The integration of information technology into health care has created opportunities to address diagnostic challenges. Internet searches, representing a vast source of health-related data, hold promise for improving early disease detection. Studies suggest that patterns in search behavior can reveal symptoms before clinical diagnosis, offering potential for innovative diagnostic tools. Leveraging advancements in machine learning, researchers have explored linking search data with health records to enhance screening and outcomes. However, challenges like privacy, bias, and scalability remain critical to its widespread adoption. Objective We aimed to explore the potential and challenges of using internet search data in medical diagnosis, with a specific focus on diseases and conditions such as cancer, cardiovascular disease, mental and behavioral health, neurodegenerative disorders, and nutritional and metabolic diseases. We examined ethical, technical, and policy considerations while assessing the current state of research, identifying gaps and limitations, and proposing future research directions to advance this emerging field. Methods We conducted a comprehensive analysis of peer-reviewed literature and informational interviews with subject matter experts to examine the landscape of internet search data use in medical research. We searched for published peer-reviewed literature on the PubMed database between October and December 2023. Results Systematic selection based on predefined criteria included 40 articles from the 2499 identified articles. The analysis revealed a nascent domain of internet search data research in medical diagnosis, marked by advancements in analytics and data integration. Despite challenges such as bias, privacy, and infrastructure limitations, emerging initiatives could reshape data collection and privacy safeguards. Conclusions We identified signals correlating with diagnostic considerations in certain diseases and conditions, indicating the potential for such data to enhance clinical diagnostic capabilities. However, leveraging internet search data for improved early diagnosis and health care outcomes requires effectively addressing ethical, technical, and policy challenges. By fostering interdisciplinary collaboration, advancing infrastructure development, and prioritizing patient engagement and consent, researchers can unlock the transformative potential of internet search data in medical diagnosis to ultimately enhance patient care and advance health care practice and policy.},
}

@article{Pack2024,
  title = {Large language models and automated essay scoring of English language learner writing: Insights into validity and reliability},
  author = {Austin Pack and Alex Barrett and Juan Escalante},
  year = {2024},
  journal = {Computers and Education: Artificial Intelligence},
  volume = {6},
  pages = {100234},
  doi = {https://doi.org/10.1016/j.caeai.2024.100234},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000353},
  abstract = {Advancements in generative AI, such as large language models (LLMs), may serve as a potential solution to the burdensome task of essay grading often faced by language education teachers. Yet, the validity and reliability of leveraging LLMs for automatic essay scoring (AES) in language education is not well understood. To address this, we evaluated the cross-sectional and longitudinal validity and reliability of four prominent LLMs, Google's PaLM 2, Anthropic's Claude 2, and OpenAI's GPT-3.5 and GPT-4, for the AES of English language learners' writing. 119 essays taken from an English language placement test were assessed twice by each LLM, on two separate occasions, as well as by a pair of human raters. GPT-4 performed the best, demonstrating excellent intrarater reliability and good validity. All models, with the exception of GPT-3.5, improved over time in their intrarater reliability. The interrater reliability of GPT-3.5 and GPT-4, however, decreased slightly over time. These findings indicate that some models perform better than others in AES and that all models are subject to fluctuations in their performance. We discuss potential reasons for such variability, and offer suggestions for prospective avenues of research.},
}

@article{Lima}2024,
  title = {Automating COVID-19 epidemiological situation reports based on multiple data sources, the Netherlands, 2020 to 2023},
  author = {Priscila {de Oliveira Bressane Lima} and Jan {van de Kassteele} and Maarten Schipper and Naomi Smorenburg and Martijn {S․ van Rooijen} and Janneke Heijne and Rolina {D․ van Gaalen}},
  year = {2024},
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {257},
  pages = {108436},
  doi = {https://doi.org/10.1016/j.cmpb.2024.108436},
  url = {https://www.sciencedirect.com/science/article/pii/S0169260724004292},
  abstract = {Background During the COVID-19 pandemic, the National Institute for Public Health and the Environment in the Netherlands developed a pipeline of scripts to automate and streamline the production of epidemiological situation reports (epi‑sitrep). The pipeline was developed for the Automation of Data Import, Summarization, and Communication (hereafter called the A-DISC pipeline). Objective This paper describes the A-DISC pipeline and provides a customizable scripts template that may be useful for other countries wanting to automate their infectious disease surveillance processes. Methods The A-DISC pipeline was developed using the open-source statistical software R. It is organized in four modules: Prepare, Process data, Produce report, and Communicate. The Prepare scripts set the working environment (e.g., load packages). The (data-specific) Process data scripts import, validate, verify, transform, save, analyze, and summarize data as tables and figures and store these data summaries. The Produce report scripts gather summaries from multiple data sources and integrate them into a RMarkdown document – the epi‑sitrep. The Communicate scripts send e-mails to stakeholders with the epi‑sitrep. Results As of March 2023, up to ten data sources were automatically summarized into tables and figures by A-DISC. These data summaries were featured in routine extensive COVID-19 epi‑sitreps, shared as open data, plotted on RIVM's website, sent to stakeholders and submitted to European Centre for Disease Prevention and Control via the European Surveillance System -TESSy [38]. Discussion In the face of an unprecedented high number of cases being reported during the COVID-19 pandemic, the A-DISC pipeline was essential to produce frequent and comprehensive epi‑sitreps. A-DISC's modular and intuitive structure allowed for the integration of data sources of varying complexities, encouraged collaboration among people with various R-scripting capabilities, and improved data lineage. The A-DISC pipeline remains under active development and is currently being used in modified form for the automatization and professionalization of various other disease surveillance processes at the RIVM, with high acceptance from the participant epidemiologists. Conclusion The A-DISC pipeline is an open-source, robust, and customizable tool for automating epi‑sitreps based on multiple data sources.},
}

@article{Gwagwa2024,
  title = {How could the United Nations Global Digital Compact prevent cultural imposition and hermeneutical injustice?},
  author = {Arthur Gwagwa and Warmhold Jan Thomas Mollema},
  year = {2024},
  journal = {Patterns},
  volume = {5},
  pages = {101078},
  doi = {https://doi.org/10.1016/j.patter.2024.101078},
  url = {https://www.sciencedirect.com/science/article/pii/S266638992400237X},
  abstract = {Summary As the geopolitical superpowers race to regulate the digital realm, their divergent rights-centered, market-driven, and social-control-based approaches require a global compact on digital regulation. If diverse regulatory jurisdictions remain, forms of domination entailed by cultural imposition and hermeneutical injustice related to AI legislation and AI systems will follow. We argue for consensual regulation on shared substantive issues, accompanied by proper standardization and coordination. Failure to attain consensus will fragment global digital regulation, enable regulatory capture by authoritarian powers or bad corporate actors, and deepen the historical geopolitical power asymmetries between the global South and the global North. To prevent an unjust regulatory landscape where the global South’s cultural and hermeneutic resources are absent, two principles for the Global Digital Compact to counter these prospective harms are proposed and discussed: (1) “recognitive consensus on key substantive benefits and harms” and (2) “procedural consensus on global coordination and essential standards.”},
}

@article{Berezvai2024,
  title = {The impact of a short-term fuel price cap on market prices after its removal: Evidence from Hungary},
  author = {Zombor Berezvai and Dániel Helfrich},
  year = {2024},
  journal = {Energy Strategy Reviews},
  volume = {54},
  pages = {101472},
  doi = {https://doi.org/10.1016/j.esr.2024.101472},
  url = {https://www.sciencedirect.com/science/article/pii/S2211467X24001792},
  abstract = {In 2022, energy prices in the European Union reached record highs, accompanied by a significant increase in fuel prices. In response, the Hungarian government imposed a price cap on retail fuel prices from November 15, 2021 to December 6, 2022. This study empirically examines the effect of the elimination of the price cap on fuel prices. The aim is to determine whether the removal of the price cap resulted in subsequent price increases. The study employed a synthetic control method, comparing actual observed prices with prices estimated by the synthetic control after the intervention. In the ten-month period following the removal of the price cap (December 2022 to September 2023), the actual price was on average 12 % higher than the price projected by the synthetic control. The largest difference was observed in January 2023, with a price premium of 16 %. There was a moderation in the difference between February and September, but even during these months, the gap remained slightly higher than 11 %. This difference may be attributed to distortions of competition due to the price cap.},
}

@article{Wang2025_03,
  title = {Implementation path and reference model for Multilateral Data Circulation System (MDCS) in Datacentric Product-Service System (DPSS): from an industrial practice survey},
  author = {Chengjun Wang and Xinguo Ming and Xinming Gao and Xianyu Zhang},
  year = {2025},
  journal = {Advanced Engineering Informatics},
  volume = {64},
  pages = {103085},
  doi = {https://doi.org/10.1016/j.aei.2024.103085},
  url = {https://www.sciencedirect.com/science/article/pii/S1474034624007365},
  abstract = {With the digital transformation of enterprises and the development of digital infrastructure (smart sensors, 5G/6G, IoT, Industrial Internet, etc.), large amounts of data are generated in various stages of the product life cycle. The value of data in the Product-Service System is becoming prominent. However, through literature review and industrial practice survey, it has been observed that there is a lack of systematic investigation into the processes of data circulation and utilization within PSS. Additionally, within the existing research on data circulation, scholars focus on partial points such as data privacy computing, data sharing and data transaction, lacking an overall reference model for the data circulation in the Product-Service System and the path of implementing a multilateral data circulation platform in the industry. This paper aims to use the industrial practice survey method, based on the literature review, to propose the Datacentric Product-Service System (DPSS) for the first time, and study the main processes of data circulation in the DPSS. The study of the reference model and industrial implementation path of the multilateral data circulation system that meets the industry’s needs in the Datacentric Product-Service System. It provides a reference for the government and industry to design, implement and regulate the domain data circulation platform. In addition, the proposed data circulation system reference model and implementation path can enhance the value symbiosis among enterprises and increase industry benefits.},
}

@article{Osorio2025,
  title = {Promoting free flows via competition law: An AI industry blueprint for Southeast Asia},
  author = {Chad Patrick Osorio and Jamlech Iram {Gojo Cruz}},
  year = {2025},
  journal = {Telecommunications Policy},
  pages = {102953},
  doi = {https://doi.org/10.1016/j.telpol.2025.102953},
  url = {https://www.sciencedirect.com/science/article/pii/S0308596125000503},
  abstract = {Concerns about multinational technology companies and their antitrust activities are increasing, particularly in the US and the European Union. However, this issue has not been given much attention in the context of ASEAN, despite being the fifth largest economy in the world. In this article, we discuss how a common competition law framework for ASEAN presents multiple benefits for the AI industry, spanning regulatory, economic, and innovation-related aspects. We argue that harmonizing competition laws across the region through a unified framework—promoting the free flow of data, compute, and AI models—would streamline regulatory compliance for AI firms and create a level playing field, preventing monopolistic practices and enhancing consumer protection. To support this, we propose a set of technical rules for the AI industry to be integrated in the common legal framework, which can stimulate not only regional AI governance, but drive responsible AI development and regional economic growth.},
}

@article{Chen2024_04,
  title = {A survey of large language models for cyber threat detection},
  author = {Yiren Chen and Mengjiao Cui and Ding Wang and Yiyang Cao and Peian Yang and Bo Jiang and Zhigang Lu and Baoxu Liu},
  year = {2024},
  journal = {Computers & Security},
  volume = {145},
  pages = {104016},
  doi = {https://doi.org/10.1016/j.cose.2024.104016},
  url = {https://www.sciencedirect.com/science/article/pii/S0167404824003213},
  abstract = {With the increasing complexity of cyber threats and the expanding scope of cyberspace, there exist progressively more challenges in cyber threat detection. It is proven that most previous threat detection models may become inadequate due to the escalation of hacker attacks. However, recent research has shown that some of these problems can be effectively addressed by Large Language Models (LLMs) directly or indirectly. Nowadays, a growing number of security researchers are adopting LLMs for analyzing various cyber threats. According to the investigation, we found that while there are numerous emerging reviews on the utilization of LLMs in some fields of cyber security, there is currently a lack of a comprehensive review on the application of LLMs in the threat detection stage. Through retrieving and collating existing works in recent years, we examined various threat detection and monitoring tasks for which LLMs may be well-suited, including cyber threat intelligence, phishing email detection, threat prediction, logs analysis, and so on. Additionally, the review explored the specific stages of different detection tasks in which LLMs are involved, evaluating the points at which LLMs are optimized. For instance, LLMs have been found to enhance the interpretability of log analysis in real-time anomaly event discovery. Additionally, we discussed some tasks where LLMs may not be suitable and explored future directions and challenges in this field. By providing a detailed status update and comprehensive insights, this review aims to assist security researchers in leveraging LLMs to enhance existing detection frameworks or develop domain-specific LLMs.},
}

@article{Dashkevych2024,
  title = {How can generative AI help in different parts of research? An experiment study on smart cities’ definitions and characteristics},
  author = {Oleg Dashkevych and Boris A. Portnov},
  year = {2024},
  journal = {Technology in Society},
  volume = {77},
  pages = {102555},
  doi = {https://doi.org/10.1016/j.techsoc.2024.102555},
  url = {https://www.sciencedirect.com/science/article/pii/S0160791X24001039},
  abstract = {Artificial intelligence (AI) engines, such as ChatGPT, InferKit, and DeepAI, are very popular today and new AI engines, such as Google Bard, Chinchilla AI DeepMind, and GPT-4, constantly emerge. However, question remains how these new data management tools can assist scholars in improving the research design and implementation. In an attempt to answer this question, we focus on one particular research field – definition and identification of smart cities (SCs), – and compare the answers provided by different AI engines with the answers given in a sequence of research papers, prepared without the use of AI and recently published by these authors. In particular, the following aspects of the original studies were re-analysed here using the AI input: a) problem definition; b) summary of current knowledge; c) identification of unknowns; d) research strategy, and e) recommendations for research and practice. As the study reveals, the recommendations of AI engines are, at times, inconsistent and data sources cited are often inaccurate. However, as such engines scan multiple open sources and retrieve relevant information, they can help to bridge gaps in the summary of background studies and streamline the research design, by supplementing missing or overlooked information.},
}

@article{Wu2024_02,
  title = {Unveiling security, privacy, and ethical concerns of ChatGPT},
  author = {Xiaodong Wu and Ran Duan and Jianbing Ni},
  year = {2024},
  journal = {Journal of Information and Intelligence},
  volume = {2},
  pages = {102-115},
  doi = {https://doi.org/10.1016/j.jiixd.2023.10.007},
  url = {https://www.sciencedirect.com/science/article/pii/S2949715923000707},
  abstract = {This paper delves into the realm of ChatGPT, an AI-powered chatbot that utilizes topic modeling and reinforcement learning to generate natural responses. Although ChatGPT holds immense promise across various industries, such as customer service, education, mental health treatment, personal productivity, and content creation, it is essential to address its security, privacy, and ethical implications. By exploring the upgrade path from GPT-1 to GPT-4, discussing the model's features, limitations, and potential applications, this study aims to shed light on the potential risks of integrating ChatGPT into our daily lives. Focusing on security, privacy, and ethics issues, we highlight the challenges these concerns pose for widespread adoption. Finally, we analyze the open problems in these areas, calling for concerted efforts to ensure the development of secure and ethically sound large language models.},
}

@article{Ng2025,
  title = {Opportunities, challenges and school strategies for integrating generative AI in education},
  author = {Davy Tsz Kit Ng and Eagle Kai Chi Chan and Chung Kwan Lo},
  year = {2025},
  journal = {Computers and Education: Artificial Intelligence},
  volume = {8},
  pages = {100373},
  doi = {https://doi.org/10.1016/j.caeai.2025.100373},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X2500013X},
  abstract = {The increasing accessibility of Generative Artificial Intelligence (GenAI) tools has led to their exploration and adoption in education. This qualitative study investigates the opportunities and challenges associated with integrating GenAI in education, and the strategies that encourage teachers and students to embrace GenAI in school settings. We recruited 76 educators in Canada to participate in a professional training seminar about GenAI and expressed their views through online surveys. Through written reflections, an optimistic outlook on GenAI's role in education was identified among the teachers, and some discipline-specific ideas were proposed. Thematic analysis reveals three key practices of AI implementation: teaching/learning, administration and assessments. However, three major challenges are also identified: school's readiness, teachers' AI competencies, and students' AI literacy and ethics. Teachers suggest several strategies to motivate GenAI integration, including professional development, clear guidelines, and access to AI software and technical support. Finally, Singh's Teach AI Global Initiative Guidance and Socio-ecological Model are adapted and proposed to support schools in becoming AI-ready by addressing teachers' and students' needs, facilitating organizational learning, and promoting improvement and transformation to foster their literacy development. Recommendations were provided for developing effective strategies to embrace GenAI in education.},
}

@article{Cortés-Cediel2023,
  title = {Trends and challenges of e-government chatbots: Advances in exploring open government data and citizen participation content},
  author = {María E. Cortés-Cediel and Andrés Segura-Tinoco and Iván Cantador and Manuel Pedro {Rodríguez Bolívar}},
  year = {2023},
  journal = {Government Information Quarterly},
  volume = {40},
  pages = {101877},
  doi = {https://doi.org/10.1016/j.giq.2023.101877},
  url = {https://www.sciencedirect.com/science/article/pii/S0740624X23000771},
  abstract = {In this paper, we propose a conceptual framework composed of a number of e-government, implementation and evaluation-oriented variables, with which we jointly analyze chatbots presented in the research literature and chatbots deployed as public services in Spain at national, regional and local levels. As a result of our holistic analysis, we identify and discuss current trends and challenges in the development and evaluation of chatbots in the public administration sector, such as focusing the use of the conversational agents on the search for government information, documents and services –leaving citizen consultation and collaboration aside–, and conducting preliminary evaluations of prototypes in limited studies, lacking experiments on deployed systems, with metrics beyond effectiveness and usability –e.g., metrics related to the generation of public values. Addressing some of the identified challenges, we build and evaluate two novel chatbots that present advances in the access to open government data and citizen participation content. Moreover, we come up with additional, potential research lines that may be considered in the future for a new generation of e-government chatbots.},
}

@article{Kaynak2025,
  title = {Geo-WC: Custom web components for earth science organizations and agencies},
  author = {Sümeyye Kaynak and Baran Kaynak and Carlos Erazo Ramirez and Ibrahim Demir},
  year = {2025},
  journal = {Environmental Modelling & Software},
  volume = {185},
  pages = {106328},
  doi = {https://doi.org/10.1016/j.envsoft.2025.106328},
  url = {https://www.sciencedirect.com/science/article/pii/S136481522500012X},
  abstract = {The development of web technologies and their integration into various fields has allowed a new era in data-driven decision-making and public data accessibility, especially through their adoption of monitoring and quantification environmental resources provided by governmental institutions. The use of web technologies has made it possible to create applications that can be accessed and used by a wide user base. However, dealing with the complexity of environmental data and non-standard data formats remains a hindering issue. To overcome these challenges and obtain up-to-date information from different institutions, we present Geo-WC: a web component framework specifically designed for earth and environmental sciences, serving as a bridge across various scientific domains. The Geo-WC utilizes a developer-friendly approach through simple HTML declarative syntax to bring together data in a single interface that is easy for developers to work with, making it accessible to users of varying skill levels. The framework integrates widely used web technologies, facilitating client-side data analysis, visualization, and accessibility within web browsers.},
}

@article{Yang2024_01,
  title = {Application of question answering systems for intelligent agriculture production and sustainable management: A review},
  author = {Tian Yang and Yupeng Mei and Ling Xu and Huihui Yu and Yingyi Chen},
  year = {2024},
  journal = {Resources, Conservation and Recycling},
  volume = {204},
  pages = {107497},
  doi = {https://doi.org/10.1016/j.resconrec.2024.107497},
  url = {https://www.sciencedirect.com/science/article/pii/S0921344924000910},
  abstract = {The increasing application of artificial intelligence in agriculture production and management has generated a large amount of data, leading to a demand for processing this data. This review focuses on the knowledge storage approaches in agricultural question answering systems, namely corpora, knowledge graphs, and large language models. These systems are built on massive amounts of data and aim to process and retrieve information effectively in the context of sustainable agriculture. Corpora refer to large collections of diverse documents that serve as foundational resources for training and fine-tuning question answering systems. Knowledge graphs capture structured and interconnected knowledge by representing entities, relationships, and attributes, enabling efficient organization and querying of information. Large language models, such as GPT-4, enhance the capacity of question answering systems to provide accurate and relevant responses. By exploring these three prominent knowledge storage approaches, this review analyses the methodology and impact of agricultural question answering systems, highlighting their applications in the production process. The findings provide important implications for future research in agriculture, and potential directions for further exploration.},
}

@article{Huang2025_02,
  title = {Generative spatial artificial intelligence for sustainable smart cities: A pioneering large flow model for urban digital twin},
  author = {Jeffrey Huang and Simon Elias Bibri and Paul Keel},
  year = {2025},
  journal = {Environmental Science and Ecotechnology},
  volume = {24},
  pages = {100526},
  doi = {https://doi.org/10.1016/j.ese.2025.100526},
  url = {https://www.sciencedirect.com/science/article/pii/S2666498425000043},
  abstract = {Rapid urbanization, alongside escalating resource depletion and ecological degradation, underscores the critical need for innovative urban development solutions. In response, sustainable smart cities are increasingly turning to cutting-edge technologies—such as Generative Artificial Intelligence (GenAI), Foundation Models (FMs), and Urban Digital Twin (UDT) frameworks—to transform urban planning and design practices. These transformative tools provide advanced capabilities to analyze complex urban systems, optimize resource management, and enable evidence-based decision-making. Despite recent progress, research on integrating GenAI and FMs into UDT frameworks remains scant, leaving gaps in our ability to capture complex urban flows and multimodal dynamics essential to achieving environmental sustainability goals. Moreover, the lack of a robust theoretical foundation and real-world operationalization of these tools hampers comprehensive modeling and practical adoption. This study introduces a pioneering Large Flow Model (LFM), grounded in a robust foundational framework and designed with GenAI capabilities. It is specifically tailored for integration into UDT systems to enhance predictive analytics, adaptive learning, and complex data management functionalities. To validate its applicability and relevance, the Blue City Project in Lausanne City is examined as a case study, showcasing the ability of the LFM to effectively model and analyze urban flows—namely mobility, goods, energy, waste, materials, and biodiversity—critical to advancing environmental sustainability. This study highlights how the LFM addresses the spatial challenges inherent in current UDT frameworks. The LFM demonstrates its novelty in comprehensive urban modeling and analysis by completing impartial city data, estimating flow data in new locations, predicting the evolution of flow data, and offering a holistic understanding of urban dynamics and their interconnections. The model enhances decision-making processes, supports evidence-based planning and design, fosters integrated development strategies, and enables the development of more efficient, resilient, and sustainable urban environments. This research advances both the theoretical and practical dimensions of AI-driven, environmentally sustainable urban development by operationalizing GenAI and FMs within UDT frameworks. It provides sophisticated tools and valuable insights for urban planners, designers, policymakers, and researchers to address the complexities of modern cities and accelerate the transition towards sustainable urban futures.},
}

@article{Koley2025,
  title = {Inhibitors in ridesharing firms from developing Nations: A novel Integrated MCDM – Text Mining approach using Large-Scale data},
  author = {Souradeep Koley and Mukesh {Kumar Barua} and Arnab Bisi},
  year = {2025},
  journal = {Transportation Research Part E: Logistics and Transportation Review},
  volume = {193},
  pages = {103832},
  doi = {https://doi.org/10.1016/j.tre.2024.103832},
  url = {https://www.sciencedirect.com/science/article/pii/S136655452400423X},
  abstract = {Our study identifies major impediments (or inhibitors) faced by Transportation Network Companies (TNCs) such as Uber, Lyft, and Ola within the context of developing nations. While existing studies on TNCs centered on passenger adoption and drivers’ perspectives, we quantitively assess the inhibitors and provide mitigation strategies. To achieve this, we use machine learning methods, particularly Latent Dirichlet Allocation (LDA) and emotion analysis on large-scale public data, to understand and classify consumer perspectives on TNCs into multiple themes. The latent theme helps experts of different ridesharing firms get a holistic perspective of riders on TNCs, assisting them in identifying the inhibitors. Using the Delphi method, we were able to achieve a consensus in identifying six primary and nineteen secondary inhibitors. We rank the primary inhibitors based on the optimal weight obtained using the Bayesian Best Worst Method. To minimize uncertainty and imprecise judgment in decision-making, we combine the grey theory with the Decision-Making Trial and Evaluation Laboratory (Grey-DEMATEL) to identify the interrelationships among the secondary inhibitors. Moreover, we perform sensitivity analysis to show the robustness of our solution. Contrary to conventional perception, our findings indicate that the government is the primary inhibitor for TNCs due to current policy and discrepancies in regulations between central and states. Additionally, our studies introduce five new inhibitors to the literature, which include drivers inciting trip cancellation to avoid commission, internal coalition of drivers, commission miscomprehension among drivers, limited infrastructure for cashless operation, and internal conflict and dysfunction within the department. The findings from large-scale data analysis, coupled with group decision-making, offer various managerial implications that can guide future managers and policymakers to enhance the operational efficiency of firms.},
}

@article{Caliskan2023,
  title = {Metadata integrity in bioinformatics: Bridging the gap between data and knowledge},
  author = {Aylin Caliskan and Seema Dangwal and Thomas Dandekar},
  year = {2023},
  journal = {Computational and Structural Biotechnology Journal},
  volume = {21},
  pages = {4895-4913},
  doi = {https://doi.org/10.1016/j.csbj.2023.10.006},
  url = {https://www.sciencedirect.com/science/article/pii/S2001037023003616},
  abstract = {In the fast-evolving landscape of biomedical research, the emergence of big data has presented researchers with extraordinary opportunities to explore biological complexities. In biomedical research, big data imply also a big responsibility. This is not only due to genomics data being sensitive information but also due to genomics data being shared and re-analysed among the scientific community. This saves valuable resources and can even help to find new insights in silico. To fully use these opportunities, detailed and correct metadata are imperative. This includes not only the availability of metadata but also their correctness. Metadata integrity serves as a fundamental determinant of research credibility, supporting the reliability and reproducibility of data-driven findings. Ensuring metadata availability, curation, and accuracy are therefore essential for bioinformatic research. Not only must metadata be readily available, but they must also be meticulously curated and ideally error-free. Motivated by an accidental discovery of a critical metadata error in patient data published in two high-impact journals, we aim to raise awareness for the need of correct, complete, and curated metadata. We describe how the metadata error was found, addressed, and present examples for metadata-related challenges in omics research, along with supporting measures, including tools for checking metadata and software to facilitate various steps from data analysis to published research.},
}

@article{Cordella2025,
  title = {Policymaking in the digital era: Exploring techno-legal assemblages and their impact on policy formulation},
  author = {Antonio Cordella and Francesco Gualdi},
  year = {2025},
  journal = {Government Information Quarterly},
  volume = {42},
  pages = {102023},
  doi = {https://doi.org/10.1016/j.giq.2025.102023},
  url = {https://www.sciencedirect.com/science/article/pii/S0740624X25000176},
  abstract = {This paper contributes to the literature by shedding light on the impact of digital technologies on the policymaking process. Specifically, it focuses on the formulation phase of policymaking, where policymakers discuss, draft, and approve formal legislation that directly or indirectly involves digital technologies. By drawing on the assemblage theory, the paper argues that the assemblages of existing technological and legal systems significantly influence the policymaking process during the formulation phase. Through a case study of the Italian reform of the Digital Administration Code (DAC), the paper offers a new framework that unpacks the various dimensions – organizational, normative, political, and technological – of the policy formulation phase impacted by techno-legal assemblages. This research provides valuable insights for policymakers tasked with discussing, drafting, and approving policies to digitize relevant public administration sectors.},
}

@article{Novelli2024,
  title = {Generative AI in EU law: Liability, privacy, intellectual property, and cybersecurity},
  author = {Claudio Novelli and Federico Casolari and Philipp Hacker and Giorgio Spedicato and Luciano Floridi},
  year = {2024},
  journal = {Computer Law & Security Review},
  volume = {55},
  pages = {106066},
  doi = {https://doi.org/10.1016/j.clsr.2024.106066},
  url = {https://www.sciencedirect.com/science/article/pii/S0267364924001328},
  abstract = {The complexity and emergent autonomy of Generative AI systems introduce challenges in predictability and legal compliance. This paper analyses some of the legal and regulatory implications of such challenges in the European Union context, focusing on four areas: liability, privacy, intellectual property, and cybersecurity. It examines the adequacy of the existing and proposed EU legislation, including the Artificial Intelligence Act (AIA), in addressing the challenges posed by Generative AI in general and LLMs in particular. The paper identifies potential gaps and shortcomings in the EU legislative framework and proposes recommendations to ensure the safe and compliant deployment of generative models.},
}

@article{Williams2025,
  title = {Academic Libraries and Technological Advancement},
  author = {Caroline A. Williams},
  year = {2025},
  pages = {144-151},
  doi = {https://doi.org/10.1016/B978-0-323-95689-5.00242-X},
  publisher = {Academic Press},
  url = {https://www.sciencedirect.com/science/article/pii/B978032395689500242X},
  abstract = {This entry provides a brief overview of technological advancement broadly and its impact on academic institutions and a history of technological advancement and academic libraries. It sets out the systems and technology landscape of academic libraries, and key technical functions of the academic library including delivery of information search and repository infrastructure for (wide ranging) information resources. It looks to the future and provides a summary of the rise of Artificial Intelligence (AI). While this entry discusses generally the academic library and technological advancement, it is important to note that the literature reviewed which is largely from the US, the UK, and Australia.},
}

@article{Sabovčík2024,
  title = {Leveraging open-source data to study solar-wind complementarity in the global perspective},
  author = {Robert Sabovčík and Ján {Mykhalchyk Hradický} and Martin Šinka},
  year = {2024},
  journal = {Renewable Energy Focus},
  volume = {50},
  pages = {100583},
  doi = {https://doi.org/10.1016/j.ref.2024.100583},
  url = {https://www.sciencedirect.com/science/article/pii/S1755008424000474},
  abstract = {Renewable energy sources, particularly solar and wind power, offer promising solutions for sustainable electricity generation. However, their inherent dependency on natural conditions and resulting intermittent generation pose challenges to the electricity grid. This study investigates the strategy of wind-solar complementarity to partly mitigate this issue, leveraging open-source data from the Slovak Republic. Our analysis reveals that combined solar and wind generation aligns more closely with real consumption compared to individual solar generation. We employ two quantitative methodologies, ordinary least squares (OLS) and least absolute deviations (LAD) regressions, to demonstrate the consistency of our findings. Additionally, we assess the prevalence of dunkelflaute events using real-generation data, finding them to be infrequent and posing a minimal risk in the context of Central Europe. We show that complementarity can be studied using open-source data for virtually any country in the world and thus quantitative methods can be used to advocate for renewable energy in general and balanced building of both wind and solar energy in particular. This research contributes to the broader understanding of renewable energy integration strategies and informs policymakers and stakeholders on optimizing energy systems for sustainability and cost-effectiveness.},
}

@article{Zou2025,
  title = {Deep learning for cross-domain data fusion in urban computing: Taxonomy, advances, and outlook},
  author = {Xingchen Zou and Yibo Yan and Xixuan Hao and Yuehong Hu and Haomin Wen and Erdong Liu and Junbo Zhang and Yong Li and Tianrui Li and Yu Zheng and Yuxuan Liang},
  year = {2025},
  journal = {Information Fusion},
  volume = {113},
  pages = {102606},
  doi = {https://doi.org/10.1016/j.inffus.2024.102606},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253524003841},
  abstract = {As cities continue to burgeon, Urban Computing emerges as a pivotal discipline for sustainable development by harnessing the power of cross-domain data fusion from diverse sources (e.g., geographical, traffic, social media, and environmental data) and modalities (e.g., spatio-temporal, visual, and textual modalities). Recently, we are witnessing a rising trend that utilizes various deep-learning methods to facilitate cross-domain data fusion in smart cities. To this end, we propose the first survey that systematically reviews the latest advancements in deep learning-based data fusion methods tailored for urban computing. Specifically, we first delve into data perspective to comprehend the role of each modality and data source. Secondly, we classify the methodology into four primary categories: feature-based, alignment-based, contrast-based, and generation-based fusion methods. Thirdly, we further categorize multi-modal urban applications into seven types: urban planning, transportation, economy, public safety, society, environment, and energy. Compared with previous surveys, we focus more on the synergy of deep learning methods with urban computing applications. Furthermore, we shed light on the interplay between Large Language Models (LLMs) and urban computing, postulating future research directions that could revolutionize the field. We firmly believe that the taxonomy, progress, and prospects delineated in our survey stand poised to significantly enrich the research community. The summary of the comprehensive and up-to-date paper list can be found at https://github.com/yoshall/Awesome-Multimodal-Urban-Computing.},
}

@article{Haque2025,
  title = {Leveraging LLMs for optimised feature selection and embedding in structured data: A case study on graduate employment classification},
  author = {Radiah Haque and Hui-Ngo Goh and Choo-Yee Ting and Albert Quek and M.D. Rakibul Hasan},
  year = {2025},
  journal = {Computers and Education: Artificial Intelligence},
  volume = {8},
  pages = {100356},
  doi = {https://doi.org/10.1016/j.caeai.2024.100356},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X24001590},
  abstract = {The application of Machine Learning (ML) for predicting graduate student employability is a growing area of research, driven by the need to align educational outcomes with job market requirements. In this context, this paper investigates the application of Large Language Models (LLMs) for tabular data transformation and embedding, specifically using Bidirectional Encoder Representations from Transformers (BERT), to enhance the performance of ML models in binary classification tasks for student employability prediction. The primary objective is to determine whether converting structured data into text format improves model accuracy. The study involves several ML models including Artificial Neural Networks (ANN), CatBoost, and BERT classifier. The focus is on predicting the employment status of graduate students based on demographic, academic, and graduate tracer study data, collected from over 4000 university graduates. Feature selection methods, including Boruta and Extra Tree Classifier (ETC) are employed to identify the optimal feature set, guided by a sliding window algorithm for automatic feature selection. The models are trained in four stages: 1) original dataset without feature selection or word embedding, 2) dataset with selected optimal features, 3) transformed data with word embedding, and 4) transformed data with feature selection applied both before and after word embedding. The baseline model (without feature selection and embedding) achieved the highest accuracy with the ANN model (79%). Subsequently, applying ETC for feature selection improved accuracy, with CatBoost achieving 83%. Further transformation with BERT-based embeddings raised the highest accuracy to 85% using the BERT classifier. Finally, the optimal accuracy of 88% was obtained by applying feature selection before and after embedding, with the BERT-Boruta model. The findings from this study demonstrate that using the dual-stage feature selection approach in combination with BERT embedding significantly increases the classification accuracy. This highlights the potential of LLMs in transforming tabular data for enhanced graduate employment prediction.},
}

@article{Kano2024,
  title = {Extracting skills for promoting local government digital transformation (DX) using text generative AI},
  author = {Eiji Kano and Kazuhiko Tsuda},
  year = {2024},
  journal = {Procedia Computer Science},
  volume = {246},
  pages = {463-472},
  doi = {https://doi.org/10.1016/j.procs.2024.09.426},
  url = {https://www.sciencedirect.com/science/article/pii/S1877050924024669},
  abstract = {This study presents a method for identifying the skills that local governments need to promote digital transformation (DX) using case information and text generative AI. By using text generative AI, it is possible to abstract the meaning of a prompt based on information and context that is not directly included in the prompt, and form a response based on the broader context and related knowledge. Verifying the reliability and usefulness of output results, this study shows the potential to efficiently identify the skills needed at organizational, departmental, and business unit levels for promoting local government DX.},
}

@article{Montagnani2024,
  title = {The EU Regulatory approach(es) to AI liability, and its Application to the financial services market},
  author = {Maria Lillà Montagnani and Marie-Claire Najjar and Antonio Davola},
  year = {2024},
  journal = {Computer Law & Security Review},
  volume = {53},
  pages = {105984},
  doi = {https://doi.org/10.1016/j.clsr.2024.105984},
  url = {https://www.sciencedirect.com/science/article/pii/S0267364924000517},
  abstract = {The continued progress of Artificial Intelligence (AI) can benefit different aspects of society and various fields of the economy, yet pose crucial risks to both those who offer such technologies and those who use them. These risks are emphasized by the unpredictability of developments in AI technology (such as the increased level of autonomy of self-learning systems), which renders it even more difficult to build a comprehensive legal framework accounting for all potential legal and ethical issues arising from the use of AI. As such, enforcement authorities are facing increased difficulties in checking compliance with applicable legislation and assessing liability, due to the specific features of AI, – namely: complexity, opacity, autonomy, unpredictability, openness, data-drivenness, and vulnerability. These problems are particularly significant in areas, such as financial markets, in which consequences arising from malfunctioning of AI systems are likely to have a major impact both in terms of individuals' protection, and of overall market stability. This scenario challenges policymaking in an increasingly digital and global context, where it becomes difficult for regulators to predict and face the impact of AI systems on economy and society, to make sure that they are human-centric, ethical, explainable, sustainable and respectful of fundamental rights and values. The European Union has been dedicating increased attention to filling the gap between the existing legal framework and AI. Some of the legislative proposals in consideration call for preventive legislation and introduce obligations on different actors – such as the AI Act – while others have a compensatory scope and seek to build a liability framework – such as the proposed AI Liability Directive and revised Product Liability Directive. At the same time, cross-sectorial regulations shall coexist with sector-specific initiatives, and the rules they establish. The present paper starts by assessing the fit of the existing European liability regime(s) with the constantly evolving AI landscape, by identifying the normative foundations on which a liability regime for such technology should be built on. It then addresses the proposed additions and revisions to the legislation, focusing on how they seek to govern AI systems, with a major focus on their implications on highly-regulated complex systems such as financial markets. Finally, it considers potential additional measures that could continue to strike a balance between the interests of all parties, namely by seeking to reduce the inherent risks that accompany the use of AI and to leverage its major benefits for our society and economy.},
}

@article{Benjira2025,
  title = {Automated mapping between SDG indicators and open data: An LLM-augmented knowledge graph approach},
  author = {Wissal Benjira and Faten Atigui and Bénédicte Bucher and Malika Grim-Yefsah and Nicolas Travers},
  year = {2025},
  journal = {Data & Knowledge Engineering},
  volume = {156},
  pages = {102405},
  doi = {https://doi.org/10.1016/j.datak.2024.102405},
  url = {https://www.sciencedirect.com/science/article/pii/S0169023X24001290},
  abstract = {Meeting the Sustainable Development Goals (SDGs) presents a large-scale challenge for all countries. SDGs established by the United Nations provide a comprehensive framework for addressing global issues. To monitor progress towards these goals, we need to develop key performance indicators and integrate and analyze heterogeneous datasets. The definition of these indicators requires the use of existing data and metadata. However, the diversity of data sources and formats raises major issues in terms of structuring and integration. Despite the abundance of open data and metadata, its exploitation remains limited, leaving untapped potential for guiding urban policies towards sustainability. Thus, this paper introduces a novel approach for SDG indicator computation, leveraging the capabilities of Large Language Models (LLMs) and Knowledge Graphs (KGs). We propose a method that combines rule-based filtering with LLM-powered schema mapping to establish semantic correspondences between diverse data sources and SDG indicators, including disaggregation. Our approach integrates these mappings into a KG, which enables indicator computation by querying graph’s topology. We evaluate our method through a case study focusing on the SDG Indicator 11.7.1 about accessibility of public open spaces. Our experimental results show significant improvements in accuracy, precision, recall, and F1-score compared to traditional schema mapping techniques.},
}

@article{Hannah2025,
  title = {On the legal implications of Large Language Model answers: A prompt engineering approach and a view beyond by exploiting Knowledge Graphs},
  author = {George Hannah and Rita T. Sousa and Ioannis Dasoulas and Claudia d’Amato},
  year = {2025},
  journal = {Journal of Web Semantics},
  volume = {84},
  pages = {100843},
  doi = {https://doi.org/10.1016/j.websem.2024.100843},
  url = {https://www.sciencedirect.com/science/article/pii/S1570826824000295},
  abstract = {With the recent surge in popularity of Large Language Models (LLMs), there is the rising risk of users blindly trusting the information in the response. Nevertheless, there are cases where the LLM recommends actions that have potential legal implications and this may put the user in danger. We provide an empirical analysis on multiple existing LLMs showing the urgency of the problem. Hence, we propose a first short-term solution, consisting in an approach for isolating these legal issues through prompt engineering. We prove that this solution is able to stem some risks related to legal implications, nonetheless we also highlight some limitations. Hence, we argue on the need for additional knowledge-intensive resources and specifically Knowledge Graphs for fully solving these limitations. For the purpose, we draw our proposal aiming at designing and developing a solution powered by a legal Knowledge Graph (KG) that, besides capturing and alerting the user on possible legal implications coming from the LLM answers, is also able to provide actual evidence for them by supplying citations of the interested laws. We conclude with a brief discussion on the issues that may be needed to solve for building a comprehensive legal Knowledge Graph},
}

@article{Meroño-Peñuela2025,
  title = {KG.GOV: Knowledge graphs as the backbone of data governance in AI},
  author = {Albert Meroño-Peñuela and Elena Simperl and Anelia Kurteva and Ioannis Reklos},
  year = {2025},
  journal = {Journal of Web Semantics},
  volume = {85},
  pages = {100847},
  doi = {https://doi.org/10.1016/j.websem.2024.100847},
  url = {https://www.sciencedirect.com/science/article/pii/S1570826824000337},
  abstract = {As (generative) Artificial Intelligence continues to evolve, so do the challenges associated with governing the data that powers it. Ensuring data quality, privacy, security, and ethical use become more and more challenging due to the increasing volume and variety of the data, the complexity of AI models, and the rapid pace of technological advancement. Knowledge graphs have the potential to play a significant role in enabling data governance in AI, as we move beyond their traditional use as data organisational systems. To address this, we present KG.gov, a framework that positions KGs at a higher abstraction level within AI workflows, and enables them as a backbone of AI data governance. We illustrate the three dimensions of KG.gov: modelling data, alternative representations, and describing behaviour; and describe the insights and challenges of three use cases implementing them: Croissant, a vocabulary to model and document ML datasets; WikiPrompts, a collaborative KG of prompts and prompt workflows to study their behaviour at scale; and Multimodal transformations, an approach for multimodal KGs harmonisation and completion aiming at broadening access to knowledge.},
}

@article{Alexakis2025,
  title = {Genetic algorithm-based multi-objective optimisation for energy-efficient building retrofitting: A systematic review},
  author = {Konstantinos Alexakis and Vasilis Benekis and Panagiotis Kokkinakos and Dimitris Askounis},
  year = {2025},
  journal = {Energy and Buildings},
  volume = {328},
  pages = {115216},
  doi = {https://doi.org/10.1016/j.enbuild.2024.115216},
  url = {https://www.sciencedirect.com/science/article/pii/S037877882401332X},
  abstract = {The operation of buildings accounts for approximately 40% of global energy consumption and 36% of greenhouse gas emissions. Retrofitting existing buildings has become essential to mitigating these impacts, yet the process presents complex optimisation challenges that require balancing conflicting objectives, such as minimising energy consumption, reducing costs, and improving thermal comfort. This systematic review investigates the application of Genetic Algorithms (GAs) to building retrofits, with a particular focus on their use in multi-objective optimisation. By analysing 175 studies − 118 new publications and 57 from a prior review – this work identifies key trends, challenges, and advancements in GA-based retrofitting approaches. The findings reveal that NSGA-II remains the most widely adopted GA, demonstrating strengths in computational efficiency and solution quality. An increasing integration of life-cycle analysis and dynamic simulation tools, such as EnergyPlus, reflects the growing sophistication of optimisation strategies. However, significant challenges persist, including prolonged computation times, a lack of open data, and limited consideration of occupant preferences and indoor environmental quality. This review systematically highlights these limitations in optimisation formulation, such as stakeholder conflicts, model sensitivity to climate variability, and scalability across different building types. By presenting these insights, this review offers a practical framework for researchers and practitioners to navigate the complexities of GA-based optimisation for building retrofitting. Recommendations include enhancing the accessibility of tools, incorporating emerging materials and technologies, and addressing climate change through tailored retrofit simulations. This work aims to advance the theoretical and practical application of GAs, contributing to the development of efficient, scalable, and equitable retrofit strategies.},
}

@article{Zhang2024_05,
  title = {Accelerating drug discovery, development, and clinical trials by artificial intelligence},
  author = {Yilun Zhang and Mohamed Mastouri and Yang Zhang},
  year = {2024},
  journal = {Med},
  volume = {5},
  pages = {1050-1070},
  doi = {https://doi.org/10.1016/j.medj.2024.07.026},
  url = {https://www.sciencedirect.com/science/article/pii/S2666634024003088},
  abstract = {Summary Artificial intelligence (AI) has profoundly advanced the field of biomedical research, which also demonstrates transformative capacity for innovation in drug development. This paper aims to deliver a comprehensive analysis of the progress in AI-assisted drug development, particularly focusing on small molecules, RNA, and antibodies. Moreover, this paper elucidates the current integration of AI methodologies within the industrial drug development framework. This encompasses a detailed examination of the industry-standard drug development process, supplemented by a review of medications presently undergoing clinical trials. Conclusively, the paper tackles a predominant obstacle within the AI pharmaceutical sector: the absence of AI-conceived drugs receiving approval. This paper also advocates for the adoption of large language models and diffusion models as a viable strategy to surmount this challenge. This review not only underscores the significant potential of AI in drug discovery but also deliberates on the challenges and prospects within this dynamically progressing field.},
}

@article{Alkhalaf2024,
  title = {Applying generative AI with retrieval augmented generation to summarize and extract key clinical information from electronic health records},
  author = {Mohammad Alkhalaf and Ping Yu and Mengyang Yin and Chao Deng},
  year = {2024},
  journal = {Journal of Biomedical Informatics},
  volume = {156},
  pages = {104662},
  doi = {https://doi.org/10.1016/j.jbi.2024.104662},
  url = {https://www.sciencedirect.com/science/article/pii/S1532046424000807},
  abstract = {Background Malnutrition is a prevalent issue in aged care facilities (RACFs), leading to adverse health outcomes. The ability to efficiently extract key clinical information from a large volume of data in electronic health records (EHR) can improve understanding about the extent of the problem and developing effective interventions. This research aimed to test the efficacy of zero-shot prompt engineering applied to generative artificial intelligence (AI) models on their own and in combination with retrieval augmented generation (RAG), for the automating tasks of summarizing both structured and unstructured data in EHR and extracting important malnutrition information. Methodology We utilized Llama 2 13B model with zero-shot prompting. The dataset comprises unstructured and structured EHRs related to malnutrition management in 40 Australian RACFs. We employed zero-shot learning to the model alone first, then combined it with RAG to accomplish two tasks: generate structured summaries about the nutritional status of a client and extract key information about malnutrition risk factors. We utilized 25 notes in the first task and 1,399 in the second task. We evaluated the model’s output of each task manually against a gold standard dataset. Result The evaluation outcomes indicated that zero-shot learning applied to generative AI model is highly effective in summarizing and extracting information about nutritional status of RACFs’ clients. The generated summaries provided concise and accurate representation of the original data with an overall accuracy of 93.25%. The addition of RAG improved the summarization process, leading to a 6% increase and achieving an accuracy of 99.25%. The model also proved its capability in extracting risk factors with an accuracy of 90%. However, adding RAG did not further improve accuracy in this task. Overall, the model has shown a robust performance when information was explicitly stated in the notes; however, it could encounter hallucination limitations, particularly when details were not explicitly provided. Conclusion This study demonstrates the high performance and limitations of applying zero-shot learning to generative AI models to automatic generation of structured summarization of EHRs data and extracting key clinical information. The inclusion of the RAG approach improved the model performance and mitigated the hallucination problem.},
}

@article{Alkamli2024,
  title = {Understanding privacy concerns in ChatGPT: A data-driven approach with LDA topic modeling},
  author = {Shahad Alkamli and Reham Alabduljabbar},
  year = {2024},
  journal = {Heliyon},
  volume = {10},
  pages = {e39087},
  doi = {https://doi.org/10.1016/j.heliyon.2024.e39087},
  url = {https://www.sciencedirect.com/science/article/pii/S2405844024151183},
  abstract = {This study investigates privacy concerns associated with ChatGPT, a prominent generative AI model, through a data-driven approach combining Twitter data analysis and a user survey. Leveraging Latent Dirichlet Allocation (LDA) topic modeling and data categorization techniques, the research identifies key areas of concern: 1) Privacy Leakage Due to Public Data Exploitation, 2) Privacy Leakage Due to Personal Input Exploitation, and 3) Privacy Leakage Due to Unauthorized Access. Twitter data analysis of over 500k tweets, supplemented by a survey of 67 ChatGPT users, reveals nuanced user perceptions and experiences regarding privacy risks. A Python program was used to improve a dataset of 500k tweets referencing “ChatGPT” during the data preparation stage. To get a refined collection of terms, steps included converting text to lowercase, eliminating mentions and hyperlinks, tokenizing, eliminating stopwords, and keyword matching to extract tweets about ChatGPT's privacy features. Once preprocessing was completed, there were 11k refined tweets. Results highlight significant apprehensions, particularly regarding unauthorized access, underscoring the importance of robust privacy measures in AI systems. The study contributes to understanding user concerns, informing policy decisions, and guiding future research on privacy in generative AI. These studies might improve ChatGPT and other AI systems' security and privacy. The public, corporations, researchers, lawmakers, and AI developers may all benefit from the useful information it provides in better understanding and managing privacy threats.},
}

@article{Lin2025_01,
  title = {Public water risk concerns triggered by energy-transition-mineral mining},
  author = {Zipeng Lin and Peng Wang and Linbin Tang and Zilin Wang and Jon Mckechnie and Bo Li and Wei-Qiang Chen and Faith Ka Shun Chan},
  year = {2025},
  journal = {Resources, Environment and Sustainability},
  volume = {19},
  pages = {100196},
  doi = {https://doi.org/10.1016/j.resenv.2025.100196},
  url = {https://www.sciencedirect.com/science/article/pii/S2666916125000088},
  abstract = {The intensifying demand for energy transition minerals (ETMs) has triggered global concern over water-related issues in mining regions. However, localized and generalizable metrics are lacking to help companies and governments manage social licenses to operate (SLO). In this study, we propose an analytical method that combines digital media data from the Global Database of Events, Language, and Tone (GDELT) with high-resolution mining data to analyze social awareness. LightGBM with Shapley additive explanations models are introduced to uncover key factors influencing public sentiment. This approach was applied to analyze media attention and public sentiment on five categories of water issues across 12 mineral types and 511 mines from 2016 to 2023. Our findings show a 40% increase in water-related events linked to ETM mining since 2020. Regions such as East and Southeast Asia, and Central and South America exhibit rising but negative sentiment, while public discontent in Southern Africa remains consistently high. Cobalt, platinum, and vanadium have the most negative sentiment, particularly concerning water quality and pollution. Manganese shows the most negative sentiment due to concerns over drought and desertification. Model results indicate that the Goldstein scale of events, which reflects the magnitude of cooperation or conflict, was the most influential factor in shaping public sentiment. Precipitation has a significant positive impact on sentiment in drought- and flood-related events, while higher runoff improved sentiment in drought events but negatively affected flood- and water quality-related events. Socio-economic factors, such as educational expenditure and unemployment rates, also demonstrated varied effects across categories. Finally, this study introduces the water sentiment index (WSI) as a proxy for water-related SLO concerns, offering a new tool to track social awareness in ETM regions and providing actionable insights for policymakers and stakeholders to mitigate social risks and ensure sustainable mining practices.},
}

@article{Brewer2024,
  title = {Navigating the challenges of generative technologies: Proposing the integration of artificial intelligence and blockchain},
  author = {Jordan Brewer and Dhru Patel and Dennie Kim and Alex Murray},
  year = {2024},
  journal = {Business Horizons},
  volume = {67},
  pages = {525-535},
  doi = {https://doi.org/10.1016/j.bushor.2024.04.011},
  url = {https://www.sciencedirect.com/science/article/pii/S0007681324000569},
  abstract = {The transformative impact of generative AI (GenAI), extending beyond traditional AI, raises numerous concerns including the replacement of human roles and AI misuse in an array of industries. This article introduces blockchain technology as a complementary technological safeguard to address some of these challenges. We emphasize blockchain’s role in promoting transparency, verifiability, and decentralization in AI development and usage, thereby offering potential solutions for four distinct challenges: (1) AI toxicity, biases, hallucinations, (2) AI interest misalignment, (3) AI as a black box, and (4) AI misuse. This article proposes ways to ensure responsible and transparent AI usage through the integration of blockchain. We position the convergence of AI and blockchain as a means to manage AI’s societal impact and unlock its benefits—contingent upon collaborative efforts among various stakeholders such as businesses, developers, and regulatory bodies. We contribute to the discourse on ethical AI usage and the potential of blockchain to enhance AI’s reliability and accountability for organizations.},
}

@article{Ito2024,
  title = {Understanding urban perception with visual data: A systematic review},
  author = {Koichi Ito and Yuhao Kang and Ye Zhang and Fan Zhang and Filip Biljecki},
  year = {2024},
  journal = {Cities},
  volume = {152},
  pages = {105169},
  doi = {https://doi.org/10.1016/j.cities.2024.105169},
  url = {https://www.sciencedirect.com/science/article/pii/S0264275124003834},
  abstract = {Visual characteristics of the built environment affect how people perceive and experience cities. For a long time, many studies have examined visual perception in cities. Such efforts have accelerated in recent years due to advancements in technologies and the proliferation of relevant data (e.g., street view imagery, geo-tagged photos, videos, virtual reality, and aerial imagery). There has not been a comprehensive systematic review paper on this topic to reveal an overarching set of research trends, limitations, and future research opportunities. Such omission is plausibly due to the difficulty in reviewing a large number of relevant papers on this popular topic. In this study, we utilized machine learning techniques (i.e., natural language processing and large language models) to semi-automate the review process and reviewed 393 relevant papers. Through the review, we found that these papers can be categorized into the physical aspects of cities: greenery and water, street design, building design, landscape, public space, and the city as a whole. We also revealed that many studies conducted quantitative analyses with a recent trend of increasingly utilizing big data and advanced technologies, such as combinations of street view imagery and deep learning models. Limitations and research gaps were also identified as follows: (1) a limited scope in terms of study areas, sample size, and attributes; (2) low quality of subjective and visual data; and (3) the need for more controlled and sophisticated methods to infer more closely examined impacts of visual features on human perceptions. We suggest that future studies utilize and contribute to open data and take advantage of existing data and technologies to examine the causality of visual features on human perception. The approach developed to accelerate this review proved to be accurate, efficient, and insightful. Considering its novelty, we also describe it to enable replications in the future.},
}

@article{Renwick2025,
  title = {Data and Information, an Overview},
  author = {Jason D. Renwick and Shamin Renwick},
  year = {2025},
  pages = {283-297},
  doi = {https://doi.org/10.1016/B978-0-323-95689-5.00229-7},
  publisher = {Academic Press},
  url = {https://www.sciencedirect.com/science/article/pii/B9780323956895002297},
  abstract = {This entry provides an overview of the terms, information and data, their origin, definitions, and concepts as well as the relationship between the two. Types of data as well as the processes involved in the data ecosystem which allows for data to be useful, and when processed have productive output are described. The relevance of the data ecosystem to libraries and librarians is illustrated. Future trends with regard to data and libraries are highlighted.},
}

@article{UÇKAN2025,
  title = {A hybrid model for extractive summarization: Leveraging graph entropy to improve large language model performance},
  author = {Taner UÇKAN},
  year = {2025},
  journal = {Ain Shams Engineering Journal},
  volume = {16},
  pages = {103348},
  doi = {https://doi.org/10.1016/j.asej.2025.103348},
  url = {https://www.sciencedirect.com/science/article/pii/S2090447925000899},
  abstract = {Extractive text summarization models focus on condensing large texts by selecting key sentences rather than generating new ones. Recently, studies have utilized large language models (LLMs) for effective summarization solutions. However, limitations like cost and time in using LLMs make achieving high performance challenging. This study introduces a hybrid model that combines graph entropy with LLMs to improve summarization accuracy and time efficiency. Initially, the text is represented as a graph, with each sentence as a node. Using Karci Entropy (KE) to measure each sentence’s information, the model selects the most valuable sentences, which are then processed by LLMs like BERT, RoBERTa, and XLNet, to create summaries of 400 words, 200 words, and 3 sentences. Testing on Duc2002 and CNN Daily datasets shows significant gains in both accuracy and processing speed, highlighting the proposed model’s effectiveness.},
}

@article{Rossello}2025,
  title = {Artificial intelligence for digital citizen participation: Design principles for a collective intelligence architecture},
  author = {Nicolas {Bono Rossello} and Anthony Simonofski and Annick Castiaux},
  year = {2025},
  journal = {Government Information Quarterly},
  volume = {42},
  pages = {102020},
  doi = {https://doi.org/10.1016/j.giq.2025.102020},
  url = {https://www.sciencedirect.com/science/article/pii/S0740624X25000140},
  abstract = {The challenges posed by digital citizen participation and the amount of data generated by Digital Participation Platforms (DPPs) create an ideal context for the implementation of Artificial Intelligence (AI) solutions. However, current AI solutions in DPPs focus mainly on technical challenges, often neglecting their social impact and not fully exploiting AI's potential to empower citizens. The goal of this paper is thus to investigate how to design digital participation platforms that integrate technical AI solutions while considering the social context in which they are implemented. Using Collective Intelligence as kernel theory, and through a literature review and a focus group, we generate design principles for the development of a socio-technically aware AI architecture. These principles are then validated by experts from the field of AI and citizen participation. The principles suggest optimizing the alignment of AI solutions with project goals, ensuring their structured integration across multiple levels, enhancing transparency, monitoring AI-driven impacts, dynamically allocating AI actions, empowering users, and balancing cognitive disparities. These principles provide a theoretical basis for future AI-driven artifacts, and theories in digital citizen participation.},
}

@article{Quek2024,
  title = {Dynamic knowledge graph applications for augmented built environments through “The World Avatar”},
  author = {Hou Yee Quek and Markus Hofmeister and Simon D. Rihm and Jingya Yan and Jiawei Lai and George Brownbridge and Michael Hillman and Sebastian Mosbach and Wilson Ang and Yi-Kai Tsai and Dan N. Tran and Soon Kang, William Tan and Markus Kraft},
  year = {2024},
  journal = {Journal of Building Engineering},
  volume = {91},
  pages = {109507},
  doi = {https://doi.org/10.1016/j.jobe.2024.109507},
  url = {https://www.sciencedirect.com/science/article/pii/S2352710224010751},
  abstract = {The proliferation of digital building models in recent years has led to a corresponding rise in specialised, non-interoperable models. These models impede sustainable developments by forming data silos that hinder cross-application data exchange and knowledge discovery processes. Although Semantic Web solutions hold promise in addressing these silos, current approaches primarily focus on developing novel ontologies, yielding similar outcomes. But it is unclear how these methodologies could support broader knowledge discovery processes and application requirements. This paper addresses these research challenges by introducing a dynamic knowledge graph as implemented within The World Avatar for interoperable building models. We demonstrate its value through two distinct applications in urban energy management and laboratory automation. The dynamic knowledge graph revolves around a comprehensive structured knowledge model constructed from ontologies and agents. Ontologies semantically annotate data and represent domain knowledge and their relationships with standardised definitions. When augmented with an agent architecture, the resulting knowledge model can align stakeholder perspectives and accommodate the dynamic and scalable nature of urban data. Moreover, the dynamic knowledge graph fosters innovative human-machine interactions through visualisation interfaces to augment knowledge discovery processes in the built environment for greater efficiencies and innovation. As the knowledge model expands, users gain access to a broader spectrum of private and public data sources and technologies, while reducing integration barriers. This is especially pertinent for smaller and less influential entities like municipal and local governments with limited resources, who can realise substantial benefits at reduced costs.},
}

@article{Dorta-González2024,
  title = {Generative artificial intelligence usage by researchers at work: Effects of gender, career stage, type of workplace, and perceived barriers},
  author = {Pablo Dorta-González and Alexis Jorge López-Puig and María Isabel Dorta-González and Sara M. González-Betancor},
  year = {2024},
  journal = {Telematics and Informatics},
  volume = {94},
  pages = {102187},
  doi = {https://doi.org/10.1016/j.tele.2024.102187},
  url = {https://www.sciencedirect.com/science/article/pii/S0736585324000911},
  abstract = {The integration of generative artificial intelligence technology into research environments has become increasingly common in recent years, representing a significant shift in the way researchers approach their work. This paper seeks to explore the factors underlying the frequency of use of generative AI amongst researchers in their professional environments. As survey data may be influenced by a bias towards scientists interested in AI, potentially skewing the results towards the perspectives of these researchers, this study uses a regression model to isolate the impact of specific factors such as gender, career stage, type of workplace, and perceived barriers to using AI technology on the frequency of use of generative AI. It also controls for other relevant variables such as direct involvement in AI research or development, collaboration with AI companies, geographic location, and scientific discipline. Our results show that researchers who face barriers to AI adoption experience an 11 % increase in tool use, while those who cite insufficient training resources experience an 8 % decrease. Female researchers experience a 7 % decrease in AI tool usage compared to men, while advanced career researchers experience a significant 19 % decrease. Researchers associated with government advisory groups are 45 % more likely to use AI tools frequently than those in government roles. Researchers in for-profit companies show an increase of 19 %, while those in medical research institutions and hospitals show an increase of 16 % and 15 %, respectively. This paper contributes to a deeper understanding of the mechanisms driving the use of generative AI tools amongst researchers, with valuable implications for both academia and industry.},
}

@article{Zhang2025_06,
  title = {Opportunities of applying Large Language Models in building energy sector},
  author = {Liang Zhang and Zhelun Chen},
  year = {2025},
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {214},
  pages = {115558},
  doi = {https://doi.org/10.1016/j.rser.2025.115558},
  url = {https://www.sciencedirect.com/science/article/pii/S136403212500231X},
  abstract = {In recent years, the rapid advancement and impressive capabilities of Large Language Models have been evident across various engineering domains. This paper explores the application, implications, and potential of Large Language Models in building energy sectors, especially energy efficiency and decarbonization studies, based on an extensive literature review and a survey from building engineers and scientists. The paper explores how LLMs can enhance intelligent control systems, automate code generation for software and modeling tools, optimize data infrastructure, and refine analysis of technical reports and papers. Additionally, the paper discusses the role of LLMs in improving regulatory compliance, supporting building lifecycle management, and revolutionizing education and training practices within the sector. Despite the promising potential of Large Language Models, challenges including complex and expensive computation, data privacy, security and copyright, complexity in fine-tuned Large Language Models, and self-consistency are discussed. The paper concludes with a call for future research focused on the enhancement of LLMs for domain-specific tasks, multi-modal LLMs, and collaborative research between AI and energy experts.},
}

@article{Silva2024_01,
  title = {Improving dense retrieval models with LLM augmented data for dataset search},
  author = {Levy Silva and Luciano Barbosa},
  year = {2024},
  journal = {Knowledge-Based Systems},
  volume = {294},
  pages = {111740},
  doi = {https://doi.org/10.1016/j.knosys.2024.111740},
  url = {https://www.sciencedirect.com/science/article/pii/S0950705124003757},
  abstract = {Data augmentation for training supervised models has achieved great results in different areas. With the popularity of Large Language Models (LLMs), a research area has emerged focused on applying LLMs for text data augmentation. This approach is particularly beneficial for low-resource tasks, whereby the availability of labeled data is very scarce. Dataset search is an information retrieval task that aims to retrieve relevant datasets based on user queries. However, due to the lack of labeled data tailored explicitly for this task, developing accurate retrieval models becomes challenging. In this paper, we target LLMs to create training examples for retrieval models in the dataset search task. Specifically, we propose a new pipeline that generates synthetic queries from dataset descriptions using LLMs. The query-description pairs are utilized to fine-tune dense retrieval approaches for re-ranking, which we assume as soft matches to our task. We evaluated our pipeline using fine-tuned embedding models for semantic search over dataset search benchmarks (NTCIR and ACORDAR). We tuned these models in the dataset search task using the synthetic data generated by our solution and compared their performance with the original models. The results show the models tuned on the synthetic data statistically outperform the baselines at different normalized discounted cumulative gain levels.},
}

@article{Caccavale2025,
  title = {ChatGMP: A case of AI chatbots in chemical engineering education towards the automation of repetitive tasks},
  author = {Fiammetta Caccavale and Carina L. Gargalo and Julian Kager and Steen Larsen and Krist V. Gernaey and Ulrich Krühne},
  year = {2025},
  journal = {Computers and Education: Artificial Intelligence},
  volume = {8},
  pages = {100354},
  doi = {https://doi.org/10.1016/j.caeai.2024.100354},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X24001577},
  abstract = {Artificial Intelligence (AI) is rapidly and consistently becoming more integrated in various aspects of our lives. One of the areas where these systems are increasingly used is education. In fact, it is both being incorporated into specific curricula, allowing students the possibility to acquire skills within this field, and more recently AI has been used as a tool to facilitate the teaching and learning process. However, an increased demand and availability of these tools do not imply a successful switch from traditional to AI-supported learning. In this work, ChatGMP, a chatbot leveraging a Large Language Model (LLM) able to conduct an interview exercise in a Master's Degree course taught at the Technical University of Denmark (DTU), is introduced. The exercise consists in a student interview of a fictitious company, represented by the teachers or ChatGMP, regarding its Good Manufacturing Practices (GMP). The aim is for the students to ask sensible and well-reasoned questions to acquire the necessary documentation to make an exhaustive report indicating whether the company is a potential fit for business. To evaluate the initiative, we compare the performance of ChatGMP to the one of the physical teachers of the course, as well as the perception of the students towards it. The results show no significant difference in the information provided by the teachers and the model, enabling the students to achieve similar learning. The students that interacted with ChatGMP are satisfied with the initiative and would likely recommend future students to perform the audit with the digital tool. This initial experiment and its positive results lay the foundation for opening the discussion on how to use LLMs in education, the opportunities they could provide, as well as their limitations and drawbacks.},
}

@article{Dong2024,
  title = {Meta-Regulation: An ideal alternative to the primary responsibility as the regulatory model of generative AI in China},
  author = {Huijuan Dong and Junkai Chen},
  year = {2024},
  journal = {Computer Law & Security Review},
  volume = {54},
  pages = {106016},
  doi = {https://doi.org/10.1016/j.clsr.2024.106016},
  url = {https://www.sciencedirect.com/science/article/pii/S0267364924000827},
  abstract = {Generative AI with stronger responsiveness and emergent abilities has triggered a global boom and is facing challenges such as data compliance risks during the pretraining process and risks of generating fake information, which has raised concerns among global regulatory authorities. The European Union, United States, United Kingdom, and other countries and regions are gradually establishing risk-based, scenario-based, and outcome-based governance models for generative AI. China recently introduced new regulations for the management of generative AI, which adopt a governance model focusing on generative AI service providers. It suggests that China is continuing the principle of primary responsibility in Internet governance, which encompasses legal responsibility, contractual obligations, and ethical responsibility. However, the governance model based on primary responsibility emphasizes the accountability of generative AI model service providers, with relatively limited regulation on other important entities such as users and large-scale dissemination platforms, which may not be conducive to achieving China's regulatory goals for the AI industry. In comparison, the Meta-Regulation model could be an ideal alternative for China. As a classic theory explaining the public-private relationship, the ‘Meta-Regulation’ aligns with the generative AI governance requirements. Based on the Meta-Regulation theory, the governance of generative AI in China should move towards a direction of emphasizing safety, transparency, collaborative governance, and accountability. In line with this, it is necessary to include users and large-scale dissemination platforms within the regulatory scope and establish overarching governance objectives that ensure the responsible distribution of duties among stakeholders, with regulatory authorities assuming ultimate oversight responsibility and technical coordination. At the level of specific improvement measures, it is possible to integrate the three stages of model development, usage, and content dissemination of generative AI. During the model development stage, generative AI providers have specific transparency obligations. In the usage stage, a self-regulatory system centered around platform autonomy should be constructed. In the content dissemination stage, the proactive notification obligations of the dissemination platforms should be clearly defined. Additionally, the enforcement of technical interoperability requirements is necessary, thereby promoting the orderly development of generative AI applications.},
}

@article{Kahn2025,
  title = {More than 50 years of consumer behavior research: What will the future look like?},
  author = {Barbara E. Kahn and Anne V. Wilson},
  year = {2025},
  journal = {Journal of Business Research},
  volume = {186},
  pages = {115027},
  doi = {https://doi.org/10.1016/j.jbusres.2024.115027},
  url = {https://www.sciencedirect.com/science/article/pii/S0148296324005319},
  abstract = {To understand how consumer behavior research has evolved and what the future might hold, we first summarize the intellectual trajectory of scholarship in the area and briefly describe the research paradigms that developed over time. We report on the trends in research topics over the years and the “hot topics” projected for the near future. We also discuss the internal and external forces that fundamentally shape research and scholars. These forces provide both constraints and opportunities that will define the future of the field. We predict that some forces, unfortunately, incentivize scholars to examine more minor, less influential research questions. However, new sources of data and areas of inquiry are simultaneously providing opportunities for innovation and creativity in exploring how consumer behavior will change or evolve in response to macroeconomic factors, such as social issues, political movements, or rapid technological advances.},
}

@article{Wamba2024,
  title = {How emerging technologies can solve critical issues in organizational operations: An analysis of blockchain-driven projects in the public sector},
  author = {Samuel Fosso Wamba and Serge-Lopez Wamba-Taguimdje and Qihui Lu and Maciel M. Queiroz},
  year = {2024},
  journal = {Government Information Quarterly},
  volume = {41},
  pages = {101912},
  doi = {https://doi.org/10.1016/j.giq.2024.101912},
  url = {https://www.sciencedirect.com/science/article/pii/S0740624X24000042},
  abstract = {Blockchain technology emerged as a concrete and disruptive application in all sectors. Even if the public sector witnessed this technology's first applications and implementations, it took a while to spread even in that environment. Previous studies have shown that blockchain technologies are a powerful, essential, and effective lever for transforming government processes and procedures and improving the management of public benefits and policies. Following an analysis of a sample of 167 blockchain-oriented projects in the public sector, we explore in this article the extent of the effects of blockchain on fundamental public governance functions, and we further explore the information technology and strategic management literature in this regard. As a result, our study shows concrete evidence of how blockchain improves several government core functions: (1) public service governance, administrative efficiency, and open government capabilities; (2) process innovation in public services; and operational and administrative performance improvement. Via a fsQCA analysis, we explored how indicators characterizing blockchain-based transformation projects in the public sector led to radical transformations in public services. Our findings move forward the blockchain perspective on the public sector by enriching the literature, bringing insights to policymakers, and opening new research directions to scholars and practitioners.},
}

@article{Collie2024,
  title = {Teachers’ motivation and engagement to harness generative AI for teaching and learning: The role of contextual, occupational, and background factors},
  author = {Rebecca J. Collie and Andrew J. Martin},
  year = {2024},
  journal = {Computers and Education: Artificial Intelligence},
  volume = {6},
  pages = {100224},
  doi = {https://doi.org/10.1016/j.caeai.2024.100224},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000250},
  abstract = {Since their release in late 2022, generative AI (genAI) tools have led to widespread use, including among teachers. The aim of our study is to examine several factors that may be implicated in teachers' motivation and engagement to harness genAI in teaching and learning. We examined contextual (i.e., autonomy-supportive leadership, autonomy-thwarting leadership), occupational experience (i.e., professional growth striving, change-related stress), and background factors (i.e., gender, age, teaching experience, contract length, class size, school level) as predictors of motivation (i.e., genAI valuing) and, in turn, engagement (i.e., integration in teaching-related work and student learning activities). Among 339 Australian teachers, our findings revealed that perceived autonomy-supportive leadership, professional growth striving, and change-related stress were linked with greater genAI valuing. In turn, genAI valuing was associated with greater genAI integration in both teaching-related work and student learning activities. Perceived autonomy-thwarting leadership was directly linked with greater genAI integration in student learning activities, and professional growth striving was directly associated with greater genAI integration in teaching-related work. Teachers’ gender and school level were also linked with the motivation and engagement factors, and there were several indirect associations as well. Our results pinpoint areas of focus for future research, policy, and practice to support genAI and its application in schools.},
}

@article{Bakator2024,
  title = {The three pillars of tomorrow: How Marketing 5.0 builds on Industry 5.0 and impacts Society 5.0?},
  author = {Mihalj Bakator and Dragan Ćoćkalo and Vesna Makitan and Sanja Stanisavljev and Milan Nikolić},
  year = {2024},
  journal = {Heliyon},
  volume = {10},
  pages = {e36543},
  doi = {https://doi.org/10.1016/j.heliyon.2024.e36543},
  url = {https://www.sciencedirect.com/science/article/pii/S2405844024125741},
  abstract = {In today's environment, the connections between Marketing 5.0, Industry 5.0, and Society 5.0 are gaining increasing attention. Governments and businesses are eager to explore how they can boost both economic competitiveness and societal well-being through strategic initiatives. It is important to ensure that technology adoption, ethical governance, and human capital development all align and are in-sync. This review dives into this challenge, aiming to create a theoretical model that provides significant insight on how Marketing 5.0 influences Society 5.0 through Industry 5.0. By analyzing a broad range of literature, the aim was to offer practical suggestions and guidelines for enhancing competitiveness and societal welfare. 48 studies were analyzed studies to discuss the complexities of the relationships between these three domains. The findings suggest actionable steps and strategies for both businesses and policymakers. Finally, the paper serves as a foundation for future research in this area, exploring the synergy between Marketing 5.0, Industry 5.0, and Society 5.0.},
}

@article{Luo2024,
  title = {BC4LLM: A perspective of trusted artificial intelligence when blockchain meets large language models},
  author = {Haoxiang Luo and Jian Luo and Athanasios V. Vasilakos},
  year = {2024},
  journal = {Neurocomputing},
  volume = {599},
  pages = {128089},
  doi = {https://doi.org/10.1016/j.neucom.2024.128089},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231224008609},
  abstract = {In recent years, artificial intelligence (AI) and machine learning (ML) are reshaping society's production methods and productivity, and also changing the paradigm of scientific research. Among them, the AI language model represented by ChatGPT has made great progress. Such large language models (LLMs) serve people in the form of AI-generated content (AIGC) and are widely used in consulting, healthcare, and education. However, it is difficult to guarantee the authenticity and reliability of AIGC learning data. In addition, there are also hidden dangers of privacy disclosure in distributed AI training. Moreover, the content generated by LLMs is difficult to identify and trace, and it is difficult to cross-platform mutual recognition. The above information security issues in the coming era of AI powered by LLMs will be infinitely amplified and affect everyone's life. Therefore, we consider empowering LLMs using blockchain technology with superior security features to propose a vision for trusted AI. This survey mainly introduces the motivation and technical route of blockchain for LLM (BC4LLM), including reliable learning corpus, secure training process, and identifiable generated content. Meanwhile, this survey also reviews the potential applications and future challenges, especially in the frontier communication networks field, including network resource allocation, dynamic spectrum sharing, and semantic communication. Based on the above work combined with the prospect of blockchain and LLMs, it is expected to help the early realization of trusted AI and provide guidance for the academic community.},
}

@article{Phippen2025_01,
  title = {Digital Literacy},
  author = {Andy Phippen},
  year = {2025},
  pages = {125-132},
  doi = {https://doi.org/10.1016/B978-0-323-95689-5.00097-3},
  publisher = {Academic Press},
  url = {https://www.sciencedirect.com/science/article/pii/B9780323956895000973},
  abstract = {Digital literacy is a crucial skill set in the contemporary era, encompassing technical proficiency, information and media literacy, data literacy, and more. There are further disciplines that are incorporated into the broad concept of digital literacy, including cybersecurity, online safety, and responsible communication. The importance of critical thinking in digital contexts and the emerging field of digital wellbeing are addressed. There are challenges in achieving digital literacy including the lack of common frameworks and diverse barriers such as access to technology, affordability, and cultural differences. Ultimately digital literacy is something that requires the input of various stakeholders, including educators, governments, technology providers, and community organizations. There is a clear need for a collaborative, multi-pronged approach to address these challenges and the need for common agreement on what digital literacy is.},
}

@article{Narula2025,
  title = {The impact of mega-mergers on uninsured depositors: Evidence from Indian commercial banks},
  author = {Sakshi Narula},
  year = {2025},
  journal = {International Review of Economics & Finance},
  pages = {103983},
  doi = {https://doi.org/10.1016/j.iref.2025.103983},
  url = {https://www.sciencedirect.com/science/article/pii/S1059056025001467},
  abstract = {This paper investigates how depositors fare in banking mega-mergers. The theoretical section demonstrates that the banks communicate their risk profile to depositors using an interest rate signalling model. These signalling efforts become particularly strong during mega-mergers when depositors’ concerns regarding deposit interest rates intensify. The empirical section examines the impact of the 2019 Indian commercial bank mega-merger on acquiring bank depositors from 2000 to 2023. The study leverages the heterogeneity in the bank’s ownership structure to address two empirical questions (i) Did the 2019 mega-merger affect Indian depositors? (ii) Do depositors associate with the interest ratios following a mega-merger? The study finds that mega-mergers have a positive effect on acquiring bank depositors. The private benefits given to high-value depositors outweigh the public information provided to average depositors. The study reports that average depositors of acquiring banks are positively associated with interest spread post-mega-merger. High-value depositors do not associate with interest ratios during mega-mergers.},
}

@article{Ketola2024,
  title = {Document structure-driven investigative information retrieval},
  author = {Tuomas Ketola and Thomas Roelleke},
  year = {2024},
  journal = {Information Systems},
  volume = {121},
  pages = {102315},
  doi = {https://doi.org/10.1016/j.is.2023.102315},
  url = {https://www.sciencedirect.com/science/article/pii/S0306437923001515},
  abstract = {Data-driven investigations are increasingly dealing with non-moderated, non-standard and even manipulated information Whether the field in question is journalism, law enforcement, or insurance fraud it is becoming more and more difficult for investigators to verify the outcomes of various black-box systems To contribute to this need of discovery methods that can be used for verification, we introduce a methodology for document structure-driven investigative information retrieval (InvIR) InvIR is defined as a subtask of exploratory IR, where transparency and reasoning take centre stage The aim of InvIR is to facilitate the verification and discovery of facts from data and the communication of those facts to others From a technical perspective, the methodology applies recent work from structured document retrieval (SDR) concerned with formal retrieval constraints and information content-based field weighting (ICFW) Using ICFW, the paper establishes the concept of relevance structures to describe the document structure-based relevance of documents These contexts are then used to help the user navigate during their discovery process and to rank entities of interest The proposed methodology is evaluated using a prototype search system called Relevance Structure-based Entity Ranker (RSER) in order to demonstrate its the feasibility This methodology represents an interesting and important research direction in a world where transparency is becoming more vital than ever.},
}

@article{Yigitcanlar2024,
  title = {Artificial intelligence and the local government: A five-decade scientometric analysis on the evolution, state-of-the-art, and emerging trends},
  author = {Tan Yigitcanlar and Sajani Senadheera and Raveena Marasinghe and Simon Elias Bibri and Thomas Sanchez and Federico Cugurullo and Renee Sieber},
  year = {2024},
  journal = {Cities},
  volume = {152},
  pages = {105151},
  doi = {https://doi.org/10.1016/j.cities.2024.105151},
  url = {https://www.sciencedirect.com/science/article/pii/S0264275124003652},
  abstract = {In recent years, the rapid advancement of artificial intelligence (AI) technologies has significantly impacted various sectors, including public governance at the local level. However, there exists a limited understanding of the overarching narrative surrounding the adoption of AI in local governments and its future. Therefore, this study aims to provide a comprehensive overview of the evolution, current state-of-the-art, and emerging trends in the adoption of AI in local government. A comprehensive scientometric analysis was conducted on a dataset comprising 7112 relevant literature records retrieved from the Scopus database in October 2023, spanning over the last five decades. The study findings revealed the following key insights: (a) exponential technological advancements over the last decades ushered in an era of AI adoption by local governments; (b) the primary purposes of AI adoption in local governments include decision support, automation, prediction, and service delivery; (c) the main areas of AI adoption in local governments encompass planning, analytics, security, surveillance, energy, and modelling; and (d) under-researched but critical research areas include ethics of and public participation in AI adoption in local governments. This study informs research, policy, and practice by offering a comprehensive understanding of the literature on AI applications in local governments, providing valuable insights for stakeholders and decision-makers.},
}

@inproceedings{Kabal2024,
  title = {Enhancing Domain-Independent Knowledge Graph Construction through OpenIE Cleaning and LLMs Validation},
  author = {Kabal, Othmane and Harzallah, Mounira and Guillet, Fabrice and Ichise, Ryutaro},
  year = {2024},
  journal = {Procedia Computer Science},
  volume = {246},
  pages = {2617 – 2626},
  doi = {10.1016/j.procs.2024.09.436},
  publisher = {Elsevier B.V.},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213339134&doi=10.1016%2fj.procs.2024.09.436&partnerID=40&md5=2055d155a77810cedf773e0f34e2426e},
  abstract = {In the challenging context of Knowledge Graph (KG) construction from text, traditional approaches often rely on Open Information Extraction (OpenIE) pipelines. However, they are prone to generating many incorrect triplets. While domain specific Named Entity Recognition (NER) is commonly used to enhance the results, it compromises the domain independence and misses crucial triplets. To address these limitations, we introduce G-T2KG, a novel pipeline for KG construction that aims to preserve the domain independence while reducing incorrect triplets, thus offering a cost-effective solution without the need for domain-specific adaptations. Our pipeline utilizes state-of-the-art OpenIE combined with both a noun phrase-based cleaning and a LLMs based validation. It is evaluated using gold standards in two distinct domains (i.e., computer science and music) that we have constructed in the context of this study. On computer science corpus, the experimental results demonstrate a higher recall as compared to state-of-the-art approaches, and a higher precision notably increased by the integration of LLMs. Experiments on the music corpus show good performance, underscoring the versatility and effectiveness of G-T2KG in domain-independent KG construction. © 2024 The Authors.},
}

@article{Knight2023,
  title = {Generative AI in the Australian education system: An open data set of stakeholder recommendations and emerging analysis from a public inquiry},
  author = {Knight, Simon and Dickson-Deane, Camille and Heggart, Keith and Kitto, Kirsty and Kozanoğlu, Dilek Cetindamar and Maher, Damian and Naraya, Bhuva and Zarrabi, Forooq},
  year = {2023},
  journal = {Australasian Journal of Educational Technology},
  volume = {39},
  pages = {101 – 124},
  doi = {10.14742/ajet.8922},
  publisher = {Australasian Society for Computers in Learning in Tertiary Education (ASCILITE)},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184257904&doi=10.14742%2fajet.8922&partnerID=40&md5=e51b99910dcbe5a647879018a7a759a3},
  abstract = {The launch of new tools in late 2022 heralded significant growth in attention to the impacts of generative AI (GenAI) in education. Claims of the potential impact on education are contested, but there are clear risks of inappropriate use particularly where GenAI aligns poorly with learning aims. In response, in mid-2023, the Australian Federal Government held an inquiry, calling for public submissions. This inquiry offers a lens onto the policy framing of GenAI in education and provides the object of investigation for this paper. We use the inquiry submissions, extracting structured claims from each. This extraction is provided as an open data set for further research, while this paper focuses on our analysis of the policy recommendations made. Implications for practice or policy • For practitioners, policymakers, and researchers. the paper provides an overview and synthesis of submission recommendations and their themes, by source type. • For respondents to the inquiry (sources), the paper supports reflection regarding synergies and gaps in recommendations, pointing to opportunity for collaboration and policy development. • For stakeholders with responsibility for aspects of policy delivery and/or those applying a critical lens to the inquiry and recommendation framing(s), the paper offers actionable insight. © Articles published in the Australasian Journal of Educational Technology (AJET) are available under Creative Commons Attribution Non-Commercial No Derivatives Licence (CC BY-NC-ND 4.0). Authors retain copyright in their work and grant AJET right of first publication under CC BY-NC-ND 4.0.},
}

@inproceedings{Kumar2025_01,
  title = {KGFakeNet: A Knowledge Graph-Enhanced Model for Fake News Detection},
  author = {Kumar, Anuj and Kumar, Pardeep and Yadav, Abhishek and Ahlawat, Satyadev and Prasad, Yamuna},
  year = {2025},
  journal = {Proceedings - International Conference on Computational Linguistics, COLING},
  volume = {2025-January},
  pages = {109 – 122},
  publisher = {Association for Computational Linguistics (ACL)},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217717624&partnerID=40&md5=ec10c2e05cbb1794c5f45d765c091a86},
  abstract = {The proliferation of fake news on social media has intensified the spread of misinformation, promoting societal biases, hate, and violence. While recent advancements in Generative AI (GenAI), particularly large language models (LLMs), have shown promise, these models often need more structured representation for accurate verification, as they rely on pre-trained data patterns without access to real-time or validated information. This study presents a framework that utilizes Open Information Extractor 6 (OpenIE6) to extract triplet relationships (subject-predicate-object) from statements and justifications to compute the cosine similarity between the Knowledge Graphs (KGs) of the statements and their supporting justification to precisely measure the relevance and alignment between them. This similarity feature is integrated with an attention mechanism over GenAI-generated embeddings to enhance the model’s ability to capture semantic features accurately. In addition, a Multi-Layer Perceptron (MLP) classifier is employed to integrate all features, resulting in a 4% improvement in accuracy and a 5% increase in F1-score over state-of-the-art LLM-based approaches. ©2025 International Committee on Computational Linguistics (ICCL)},
}

@inproceedings{Sebbag2025,
  title = {AdminSet and AdminBERT: a Dataset and a Pre-trained Language Model to Explore the Unstructured Maze of French Administrative Documents},
  author = {Sebbag, Thomas and Quiniou, Solen and Stucky, Nicolas and Morin, Emmanuel},
  year = {2025},
  journal = {Proceedings - International Conference on Computational Linguistics, COLING},
  volume = {Part F206484-1},
  pages = {392 – 406},
  publisher = {Association for Computational Linguistics (ACL)},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218496207&partnerID=40&md5=248127e31536c70803b77e8e6a35f3ff},
  abstract = {In recent years, Pre-trained Language Models (PLMs) have been widely used to analyze various documents, playing a crucial role in Natural Language Processing (NLP). However, administrative texts have rarely been used in information extraction tasks, even though this resource is available as open data in many countries. Most of these texts contain many specific domain terms. Moreover, especially in France, they are unstructured because many administrations produce them without a standardized framework. Due to this fact, current language models do not process these documents correctly. In this paper, we propose AdminBERT, the first French pre-trained language model for the administrative domain. Since interesting information in such texts corresponds to named entities and the relations between them, we compare this PLM with general domain language models, fine-tuned on the Named Entity Recognition (NER) task applied to administrative texts, as well as to a Large Language Model (LLM) and to a language model with an architecture different from the BERT one. We show that taking advantage of a PLM for French administrative data increases the performance in the administrative and general domains, on these texts. We also release AdminBERT as well as AdminSet, the pre-training corpus of administrative texts in French and the subset AdminSet-NER, the first NER dataset consisting exclusively of administrative texts in French. © 2025 Association for Computational Linguistics.},
}

@inproceedings{Peng2024,
  title = {PocketLLM: Enabling On-Device Fine-Tuning for Personalized LLMs},
  author = {Peng, Dan and Fu, Zhihui and Wang, Jun},
  year = {2024},
  journal = {PrivateNLP 2024 - 5th Workshop on Privacy in Natural Language Processing, Proceedings of the Workshop},
  pages = {91 – 96},
  publisher = {Association for Computational Linguistics (ACL)},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204498544&partnerID=40&md5=7f5e76b2d15d2c1ae86ae1a4b0ece483},
  abstract = {Recent advancements in large language models (LLMs) have indeed showcased their impressive capabilities. On mobile devices, the wealth of valuable, non-public data generated daily holds great promise for locally fine-tuning personalized LLMs, while maintaining privacy through on-device processing. However, the constraints of mobile device resources pose challenges to direct on-device LLM fine-tuning, mainly due to the memory-intensive nature of derivative-based optimization required for saving gradients and optimizer states. To tackle this, we propose employing derivative-free optimization techniques to enable on-device fine-tuning of LLM, even on memory-limited mobile devices. Empirical results demonstrate that the RoBERTa-large model and OPT-1.3B can be fine-tuned locally on the OPPO Reno 6 smartphone using around 4GB and 6.5GB of memory respectively, using derivative-free optimization techniques. This highlights the feasibility of on-device LLM fine-tuning on mobile devices, paving the way for personalized LLMs on resource-constrained devices while safeguarding data privacy. © 2024 Association for Computational Linguistics.},
}

@inproceedings{Ahmed2023_01,
  title = {Reimagining open data ecosystems: a practical approach using AI, CI, and Knowledge Graphs},
  author = {Ahmed, Umair},
  year = {2023},
  journal = {CEUR Workshop Proceedings},
  volume = {3514},
  pages = {235 – 249},
  publisher = {CEUR-WS},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176118205&partnerID=40&md5=3aa98b44917bcdc9c7a8b2629ebf5cea},
  abstract = {Open data promotes transparency, facilitates innovation, and enables informed decision-making. In the information age, despite the abundance of data, issues related to findability, accessibility, usability, and value creation continue to be significant challenges. This study focuses on ways to tackle those challenges within the realm of open data ecosystem. It particularly investigates the utilization of AI (Artificial Intelligence) and CI (Collective Intelligence) to enhance the open data ecosystem in the aforementioned aspects. It also navigates through fitting knowledge representation methodologies for open data, which promote semantic reasoning and make it conducive for AI and CI to work more effectively. Given the main objectives of this study, in the preliminary stage, apart from the literature review, we surveyed multiple open data portals to find the state of functional traits currently. We found the state to be significantly lacking in terms of the aforementioned objectives. Initially, we focused on the problem of missing metadata. We explored state-of-the-art AI methodologies such as BERT, YAKE, RAKE, TextRank, ChatGPT and proposed BRYT (a hybrid methodology) for automated metadata extraction. We proposed to extract keywords, themes/categories, and descriptions of data sets using AI to fill in the missing metadata and recommend them to publishers while uploading new data sets. Following metadata extraction, we proposed to explore the idea of constructing a representative knowledge graph from open data sets and investigate how it aids with the objectives of this study. To address this, we chose Open Street Maps as our source of geographical open data and GTFS as a layer of mobility data on top of it. Following it, we propose to employ intuitive search algorithms and recommender systems on top of it to enhance the open data ecosystem. In association with OSM and GTFS, we also plan to focus on the problem of optimal route recommendation, improved navigation, and emergency response planning. c},
}

@article{Mohapatra2023,
  title = {Leveraging Large Language Models (LLM) for the Plastic Surgery Resident Training: Do They Have a Role?},
  author = {Mohapatra, Devi Prasad and Thiruvoth, Friji Meethale and Tripathy, Satyaswarup and Rajan, Sheeja and Vathulya, Madhubari and Lakshmi, Palukuri and Singh, Veena K. and Haq, Ansar Ul},
  year = {2023},
  journal = {Indian Journal of Plastic Surgery},
  volume = {56},
  pages = {413 – 420},
  doi = {10.1055/s-0043-1772704},
  publisher = {Georg Thieme Verlag},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178156684&doi=10.1055%2fs-0043-1772704&partnerID=40&md5=085fd8d2c31ca5dc8dd3e230a276f17c},
  abstract = {Introduction Large language models (LLMs) are designed for recognizing, summarizing, translating, predicting, and generating text-based content from knowledge gained from extensive data sets. ChatGPT4 (Generative Pre-trained Transformer 4) (OpenAI, San Francisco, California, United States) is a transformer-based LLM model pretrained on public data as well as data obtained from third-party sources using deep learning techniques of fine tuning and reinforcement learning from human feedback to predict the next text. We wanted to explore the role of LLM as a teaching assistant (TA) in plastic surgery. Material and Methods TA roles were first identified in available literature, and based on the roles, a list of suitable tasks was created where LLM could be used to perform the task. Prompts designed to be fed in to the LLM (specifically ChatGPT) to generate appropriate output, were then created and fed to the ChatGPT model. The outputs generated were scored by evaluators and compared for interobserver agreement. Results A final set of eight TA roles were identified where a LLM could be utilized to generate content. These contents were scored for usefulness and accuracy. These were scored independently by the eight study authors in a scoring sheet created for the study. Interobserver agreements for content accuracy, usefulness, and clarity were 100% for content generated for the following: interactive case studies (generation), simulation of preoperative consultations, and generation of ethical considerations. Discussion LLMs in general and ChatGPT (on which this study is based) in specific, can generate answers to questions and prompts based on huge amount of text fed into the model for training the underlying language model. The answers generated have been found to be accurate, readable, and even indistinguishable from human-generated text. This capability of automated content synthesis can be exploited to generate summaries to text, answer short and long answers, and generate case scenarios. We could identify a few such scenarios where the LLM could in general be utilized to play the role of a TA and aid plastic surgery residents in particular. In addition, these models could also be used by students to obtain feedback and gain reflection which itself stimulates critical thinking. Conclusion Incorporating LLMs into the educational arsenal of plastic surgery residency programs can provide a dynamic, interactive, and individualized learning experience for residents and prove to be worthy TAs of future. © 2023 Georg Thieme Verlag. All rights reserved.},
}

@article{Takefuji2024,
  title = {Unveiling inequality: A deep dive into racial and gender disparities in US court case closures},
  author = {Takefuji, Yoshiyasu},
  year = {2024},
  journal = {Cities},
  volume = {154},
  doi = {10.1016/j.cities.2024.105398},
  publisher = {Elsevier Ltd},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201451945&doi=10.1016%2fj.cities.2024.105398&partnerID=40&md5=ff33c9a9585a41979b0d599ba6badaf5},
  abstract = {This study uses generative AI and public datasets to examine racial and gender disparities in US court case closures from 2019 to 2024. It finds significant disparities using ANOVA, Chi-Square, and Fisher's method, with an increasing trend over time. Gender disparity, though less significant in 2024, persists. Further research is needed to identify root causes and develop targeted interventions. Continuous monitoring is essential to evaluate their effectiveness. Promoting transparency, investing in research, and implementing robust monitoring systems are crucial steps towards a fairer justice system for all. © 2024 Elsevier Ltd},
}

@article{Zhao2023,
  title = {A search-based geographic metadata curation pipeline to refine sequencing institution information and support public health},
  author = {Zhao, Kun and Farrell, Katie and Mashiku, Melchizedek and Abay, Dawit and Tang, Kevin and Oberste, M. Steven and Burns, Cara C.},
  year = {2023},
  journal = {Frontiers in Public Health},
  volume = {11},
  doi = {10.3389/fpubh.2023.1254976},
  publisher = {Frontiers Media SA},
  url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178031636&doi=10.3389%2ffpubh.2023.1254976&partnerID=40&md5=5e94e15f9f6285942beae317a122f81a},
  abstract = {Background: The National Center for Biotechnology Information (NCBI) Sequence Read Archive (SRA) has amassed a vast reservoir of genetic data since its inception in 2007. These public data hold immense potential for supporting pathogen surveillance and control. However, the lack of standardized metadata and inconsistent submission practices in SRA may impede the data’s utility in public health. Methods: To address this issue, we introduce the Search-based Geographic Metadata Curation (SGMC) pipeline. SGMC utilized Python and web scraping to extract geographic data of sequencing institutions from NCBI SRA in the Cloud and its website. It then harnessed ChatGPT to refine the sequencing institution and location assignments. To illustrate the pipeline’s utility, we examined the geographic distribution of the sequencing institutions and their countries relevant to polio eradication and categorized them. Results: SGMC successfully identified 7,649 sequencing institutions and their global locations from a random selection of 2,321,044 SRA accessions. These institutions were distributed across 97 countries, with strong representation in the United States, the United Kingdom and China. However, there was a lack of data from African, Central Asian, and Central American countries, indicating potential disparities in sequencing capabilities. Comparison with manually curated data for U.S. institutions reveals SGMC’s accuracy rates of 94.8% for institutions, 93.1% for countries, and 74.5% for geographic coordinates. Conclusion: SGMC may represent a novel approach using a generative AI model to enhance geographic data (country and institution assignments) for large numbers of samples within SRA datasets. This information can be utilized to bolster public health endeavors. Copyright © 2023 Zhao, Farrell, Mashiku, Abay, Tang, Oberste and Burns.},
}

@article{Ostojic2025,
  title = {Systematic Literature Review of Optimization Algorithms forP||Cmax Problem},
  author = {Ostojic, Dragutin and Ramljak, Dusan and Urosevic, Andrija and Jolovic,Marija and Draskovic, Radovan and Kakka, Jainil and Kruger, TatjanaJaksic and Davidovic, Tatjana},
  year = {2025},
  journal = {SYMMETRY-BASEL},
  volume = {17},
  doi = {10.3390/sym17020178},
  abstract = {In the era of open data and open science, it is important that, before announcing their new results, authors consider all previous studies and ensure that they have competitive material worth publishing. To save time, it is popular to replace the exhaustive search of online databases with the utilization of generative Artificial Intelligence (AI). However, especially for problems in niche domains, generative AI results may not be precise enough and sometimes can even be misleading. A typical example is P||Cmax, an important scheduling problem studied mainly in a wider context of parallel machine scheduling. As there is an uncovered symmetry between P||Cmax and other similar optimization problems, it is not easy for generative AI tools to include all relevant results into search. Therefore, to provide the necessary background data to support researchers and generative AI learning, we critically discuss comparisons between algorithms for P||Cmax that have been presented in the literature. Thus, we summarize and categorize the ``state-of-the-art{''} methods, benchmark test instances, and compare methodologies, all over a long time period. We aim to establish a framework for fair performance evaluation of algorithms for P||Cmax, and according to the presented systematic literature review, we uncovered that it does not exist. We believe that this framework could be of wider importance, as the identified principles apply to a plethora of combinatorial optimization problems.},
}

@article{Li2024_05,
  title = {MT4CrossOIE: Multi-stage tuning for cross-lingual openinformation extraction},
  author = {Li, Tongliang and Wang, Zixiang and Chai, Linzheng and Yang, Jian andBai, Jiaqi and Yin, Yuwei and Liu, Jiaheng and Guo, Hongcheng and Yang,Liqun and El-abidine, Hebboul Zine and Li, Zhoujun},
  year = {2024},
  journal = {EXPERT SYSTEMS WITH APPLICATIONS},
  volume = {255},
  doi = {10.1016/j.eswa.2024.124760},
  abstract = {Cross-lingual open information extraction aims to extract structured information from raw text across multiple languages. Previous work uses a shared cross-lingual pre-trained model to handle the different languages but underuses the potential of the language-specific representation. In this paper, we propose an effective multistage tuning framework called Mr4CrossOIE, designed for enhancing cross-lingual open information extraction by injecting language-specific knowledge into the shared model. Specifically, the cross-lingual pre-trained model is first tuned in a shared semantic space (e.g., embedding matrix) in the fixed encoder and then other components are optimized in the second stage. After enough training, we freeze the pre-trained model and tune the multiple extra low-rank language-specific modules using mixture of LoRAs for model-based cross- lingual transfer. In addition, we leverage two-stage prompting to encourage the large language model (LLM) to annotate the multi-lingual raw data for data-based cross-lingual transfer. The model is trained with multilingual objectives on our proposed dataset OpenIE4++ by combining the model-based and data-based transfer techniques. Experimental results on various benchmarks emphasize the importance of aggregating multiple plug-in-and-play language-specific modules and demonstrate the effectiveness of Mr4CrossOIE in cross-lingual OIE.2},
}

@article{Knight2023_01,
  title = {Generative AI in the Australian education system: An open data set ofstakeholder recommendations and emerging analysis from a public inquiry},
  author = {Knight, Simon and -Deane, Camille Dickson and Heggart, Keith and Kitto,Kirsty and Kozanoglu, Dilek Cetindamar and Maher, Damian and Narayan,Bhuva and Zarrabi, Forooq},
  year = {2023},
  journal = {AUSTRALASIAN JOURNAL OF EDUCATIONAL TECHNOLOGY},
  volume = {39},
  pages = {101-124},
  doi = {10.14742/ajet.8922},
  abstract = {The launch of new tools in late 2022 heralded significant growth in attention to the impacts of generative AI (GenAI) in education. Claims of the potential impact on education are contested, but there are clear risks of inappropriate use particularly where GenAI aligns poorly with learning aims. In response, in mid-2023, the Australian Federal Government held an inquiry, calling for public submissions. This inquiry offers a lens onto the policy framing of GenAI in education and provides the object of investigation for this paper. We use the inquiry submissions, extracting structured claims from each. This extraction is provided as an open data set for further research, while this paper focuses on our analysis of the policy recommendations made.},
}

@inproceedings{Siebenlist2023_01,
  title = {Approaches towards using ChatGPT as an open data companion},
  author = {Siebenlist, Tobias},
  year = {2023},
  pages = {674-675},
  doi = {10.1145/3598469.3598554},
  abstract = {Many challenges are associated with the public's use of open data. Often, there needs to be more skills and knowledge to develop ideas from published raw data, implement specific projects, and generate added value. The emergence of generative AI tools can help remedy this. This work-in-progress article presents approaches towards using ChatGPT as a companion for interested users who want to deal with open data and points out exemplary applications.},
}

@article{Duan2024,
  title = {Flexible and Effective Cellular Traffic Data Synthesis with Large Language Model},
  author = {Duan, Sijing and Lyu, Feng and Cen, Jinfeng and Ren, Ju and Yang, Peng and Zhang, Yaoxue},
  year = {2024},
  pages = {5223-5228},
  doi = {10.1109/GLOBECOM52923.2024.10901834},
  abstract = {Cellular traffic data hold significant potential for applications such as network planning, traffic prediction, mobility modeling, and personalized recommendations. However, limited data accessibility hinders more open data-driven research. Previous studies have explored data synthesis, while exhibiting flexible limitations in supporting conditional traffic synthesis, and are vulnerable to multidimensional data modeling. In this paper, we present LLMCell, a flexible and effective framework that leverages the arbitrary conditioning and contextual understanding capabilities of the large language model (LLM) to generate high-quality synthetic cellular traffic data. The LLMCell comprises three key components: i) a textual encoder for converting raw cellular traffic data into textual representations, ii) a generative model learner to fine-tune pre-trained LLM based on encoded textual representation for cellular traffic generation, and iii) a synthetic data sampling module for final synthetic data sampling and textual-to-data transformation. Experiments conducted on a large-scale dataset demonstrate the superior fidelity and utility of LLMCell over state-of-the-art baselines, and the synthetic data can effectively preserve user privacy. We release our synthetic dataset to the public to benefit future research in the wireless network community1.},
}

@article{Zhu2021,
  title = {DSI-Net: Deep Synergistic Interaction Network for Joint Classification and Segmentation With Endoscope Images},
  author = {Zhu, Meilu and Chen, Zhen and Yuan, Yixuan},
  year = {2021},
  journal = {IEEE Transactions on Medical Imaging},
  volume = {40},
  pages = {3315-3325},
  doi = {10.1109/TMI.2021.3083586},
  abstract = {Automatic classification and segmentation of wireless capsule endoscope (WCE) images are two clinically significant and relevant tasks in a computer-aided diagnosis system for gastrointestinal diseases. Most of existing approaches, however, considered these two tasks individually and ignored their complementary information, leading to limited performance. To overcome this bottleneck, we propose a deep synergistic interaction network (DSI-Net) for joint classification and segmentation with WCE images, which mainly consists of the classification branch (C-Branch), the coarse segmentation (CS-Branch) and the fine segmentation branches (FS-Branch). In order to facilitate the classification task with the segmentation knowledge, a lesion location mining (LLM) module is devised in C-Branch to accurately highlight lesion regions through mining neglected lesion areas and erasing misclassified background areas. To assist the segmentation task with the classification prior, we propose a category-guided feature generation (CFG) module in FS-Branch to improve pixel representation by leveraging the category prototypes of C-Branch to obtain the category-aware features. In such way, these modules enable the deep synergistic interaction between these two tasks. In addition, we introduce a task interaction loss to enhance the mutual supervision between the classification and segmentation tasks and guarantee the consistency of their predictions. Relying on the proposed deep synergistic interaction mechanism, DSI-Net achieves superior classification and segmentation performance on public dataset in comparison with state-of-the-art methods. The source code is available at https://github.com/CityU-AIM-Group/DSI-Net.},
}

@article{Genest2025,
  title = {OWNER — Toward Unsupervised Open-World Named Entity Recognition},
  author = {Genest, Pierre-Yves and Portier, Pierre-Edouard and Egyed-Zsigmond, Előd and Lovisetto, Martino},
  year = {2025},
  journal = {IEEE Access},
  volume = {13},
  pages = {50077-50105},
  doi = {10.1109/ACCESS.2025.3552122},
  abstract = {Named Entity Recognition (NER) is a crucial task in Natural Language Processing (NLP), traditionally addressed through supervised learning, which requires extensive annotated corpora. This requirement poses challenges, particularly in specialized domains with limited labeled data. In response, the field has shifted towards lower-resource approaches, such as few-shot and zero-shot learning, which reduce the dependency on annotated data. However, even zero-shot models require prior knowledge of entity types, limiting their applicability in exploratory scenarios. In this context, we introduce OWNER, our unsupervised and open-world NER model, designed to operate without annotated documents or predefined entity types. OWNER leverages Encoder-only Language Models like BERT to infer and organize entities into dynamic entity types through a two-step process: mention detection and entity typing. Mention detection employs a BIO sequence labeling approach to locate entities, while entity typing uses BERT-based embeddings, refined through contrastive learning, for clustering and naming entity types. This method allows OWNER to automatically identify and structure unknown entity types, offering advantages for exploratory dataset analysis and knowledge graph construction. Our experimental evaluation on 13 domain-specific datasets demonstrates that OWNER surpasses existing LLM-based open-world NER models and remains competitive with more supervised and closed-world zero-shot models. OWNER’s architecture provides a lightweight, easily deployable solution that advances the state of the art in unsupervised and open-world NER. The source code of OWNER is publicly available at https://github.com/alteca/OWNER, facilitating future research in this domain.},
}

@article{Plikynas2025,
  title = {Systematic Review of Fake News, Propaganda, and Disinformation: Examining Authors, Content, and Social Impact Through Machine Learning},
  author = {Plikynas, Darius and Rizgelienė, Ieva and Korvel, Gražina},
  year = {2025},
  journal = {IEEE Access},
  volume = {13},
  pages = {17583-17629},
  doi = {10.1109/ACCESS.2025.3530688},
  abstract = {In recent years, the world has witnessed a global outbreak of fake news, propaganda and disinformation (FNPD) flows on online social networks (OSN). In the context of information warfare and the capabilities of generative AI, FNPDs have proliferated. They have become a powerful and quite effective tool for influencing people’s social identities, attitudes, opinions and even behavior. Ad hoc malicious social media accounts and organized networks of trolls and bots target countries, societies, social groups, political campaigns and individuals. As a result, conspiracy theories, echo chambers, filter bubbles and other processes of fragmentation and marginalization are polarizing, radicalizing, and disintegrating society in terms of coherent politics, governance, and social networks of trust and cooperation. This systematic review aims to explore advances in using machine and deep learning to detect FNPD in OSNs effectively. We present the results of a combined PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) review in three analysis domains: 1) propagators (authors, trolls, and bots), 2) textual content, 3) social impact. This systemic research framework integrates meta-analyses of three research domains, providing an overview of the wider research field and revealing important relationships between these research domains. It not only addresses the most promising ML/DL research methodologies and hybrid approaches in each domain, but also provides perspectives and insights on future research directions.},
}

@article{Hu2024,
  title = {Intelligent Tourism Plan Making System Based on Machine Learning Technology},
  author = {Hu, Wenyue},
  year = {2024},
  pages = {140-146},
  doi = {10.1109/ICRSS65752.2024.00032},
  abstract = {Generative artificial intelligence will conduct in-depth analysis and mining of massive data in the tourism industry to help tourism enterprises and government departments make more accurate and scientific decisions. In order to further improve the user experience of tourist terminals, this study proposes a personalized tourist route generation modeling method based on user interest model. Moreover, after the theoretical research of this method is completed, its application value is analyzed, and the expected design goal is achieved. Therefore, in the future research, this model can be used to solve the route planning problem, and provide users with more personal tourism routes and tourism schemes. Through the experimental results, it can be seen that the route generation result of this model is good, which proves that this model has high application value. In addition, generative artificial intelligence technology imitates human creative thinking to generate a series of data, images, text or audio content that tourists need, thereby effectively improving the user experience.},
}

@article{Xiao2023,
  title = {Inform the Uninformed: Improving Online Informed Consent Reading with an AI-Powered Chatbot},
  author = {Xiao, Ziang and Li, Tiffany Wenting and Karahalios, Karrie and Sundaram, Hari},
  year = {2023},
  doi = {10.1145/3544548.3581252},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3544548.3581252},
  abstract = {Informed consent is a core cornerstone of ethics in human subject research. Through the informed consent process, participants learn about the study procedure, benefits, risks, and more to make an informed decision. However, recent studies showed that current practices might lead to uninformed decisions and expose participants to unknown risks, especially in online studies. Without the researcher’s presence and guidance, online participants must read a lengthy form on their own with no answers to their questions. In this paper, we examined the role of an AI-powered chatbot in improving informed consent online. By comparing the chatbot with form-based interaction, we found the chatbot improved consent form reading, promoted participants’ feelings of agency, and closed the power gap between the participant and the researcher. Our exploratory analysis further revealed the altered power dynamic might eventually benefit study response quality. We discussed design implications for creating AI-powered chatbots to offer effective informed consent in broader settings.},
}

@article{Myers2024,
  title = {Pick, Click, Flick! The Story of Interaction Techniques},
  author = {Myers, Brad A.},
  year = {2024},
  volume = {57},
  publisher = {Association for Computing Machinery},
  abstract = {This book provides a comprehensive study of the many ways to interact with computers and computerized devices. An “interaction technique” starts when the user performs an action that causes an electronic device to respond, and includes the direct feedback from the device to the user. Examples include physical buttons and switches, on-screen menus and scrollbars operated by a mouse, touchscreen widgets, gestures such as flick-to-scroll, text entry on computers and touchscreens, input for virtual reality systems, interactions with conversational agents such as Apple Siri, Google Assistant, Amazon Alexa, and Microsoft Cortana, and adaptations of all of these for people with disabilities. Pick, Click, Flick! is written for anyone interested in interaction techniques, including computer scientists and designers working on human-computer interaction, as well as implementers and consumers who want to understand and get the most out of their digital devices.REVIEWS“Pick, Click, Flick! is an impressive reference manual of the many years of interaction design development. It is a reference book, invaluable when questions arise, whether while you are busy designing something, or learning, or teaching, where assigning sections of the reference will be a valuable resource and learning tool for students. Brad Myers has provided a great service to the interaction community.” ‐ Don Norman, Distinguished Prof. Emeritus, Design Lab, University of California, San Diego“Every UX professional should immerse themselves in this book. Not only does it unravel the fascinating and complex history of GUI widgets that will captivate any user interface nerd, but it also stands as the definitive guide to an incredibly diverse array of interaction techniques. This is not just an engaging read; it’s an essential toolkit.” ‐ Jakob Nielsen, Principal, Nielsen Design Group},
}

@article{Klein2024,
  title = {Proceedings of the 2023 ACM/IEEE Joint Conference on Digital Libraries},
  author = {Klein, Martin and Ben-David, Anat and J\"{a}schke, Robert and Kelly, Mat},
  year = {2024},
  publisher = {IEEE Press},
}

@article{Zhao2024_03,
  title = {A Comprehensive Survey on Relation Extraction: Recent Advances and New Frontiers},
  author = {Zhao, Xiaoyan and Deng, Yang and Yang, Min and Wang, Lingzhi and Zhang, Rui and Cheng, Hong and Lam, Wai and Shen, Ying and Xu, Ruifeng},
  year = {2024},
  journal = {ACM Comput. Surv.},
  volume = {56},
  doi = {10.1145/3674501},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3674501},
  abstract = {Relation extraction (RE) involves identifying the relations between entities from underlying content. RE serves as the foundation for many natural language processing (NLP) and information retrieval applications, such as knowledge graph completion and question answering. In recent years, deep neural networks have dominated the field of RE and made noticeable progress. Subsequently, the large pre-trained language models (PLMs) have taken the state-of-the-art RE to a new level. This survey provides a comprehensive review of existing deep learning techniques for RE. First, we introduce RE resources, including datasets and evaluation metrics. Second, we propose a new taxonomy to categorize existing works from three perspectives, i.e., text representation, context encoding, and triplet prediction. Third, we discuss several important challenges faced by RE and summarize potential techniques to tackle these challenges. Finally, we outline some promising future directions and prospects in this field. This survey is expected to facilitate researchers’ collaborative efforts to address the challenges of real-world RE systems.},
}

@article{Liu2024_03,
  title = {PreCurious: How Innocent Pre-Trained Language Models Turn into Privacy Traps},
  author = {Liu, Ruixuan and Wang, Tianhao and Cao, Yang and Xiong, Li},
  year = {2024},
  pages = {3511–3524},
  doi = {10.1145/3658644.3690279},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3658644.3690279},
  abstract = {The pre-training and fine-tuning paradigm has demonstrated its effectiveness and has become the standard approach for tailoring language models to various tasks. Currently, community-based platforms offer easy access to various pre-trained models, as anyone can publish without strict validation processes. However, a released pre-trained model can be a privacy trap for fine-tuning datasets if it is carefully designed. In this work, we propose PreCurious framework to reveal the new attack surface where the attacker releases the pre-trained model and gets a black-box access to the final fine-tuned model. PreCurious aims to escalate the general privacy risk of both membership inference and data extraction on the fine-tuning dataset. The key intuition behind PreCurious is to manipulate the memorization stage of the pre-trained model and guide fine-tuning with a seemingly legitimate configuration. While empirical and theoretical evidence suggests that parameter-efficient and differentially private fine-tuning techniques can defend against privacy attacks on a fine-tuned model, PreCurious demonstrates the possibility of breaking up this invulnerability in a stealthy manner compared to fine-tuning on a benign pre-trained model. While DP provides some mitigation for membership inference attack, by further leveraging a sanitized dataset, PreCurious demonstrates potential vulnerabilities for targeted data extraction even under differentially private tuning with a strict privacy budget e.g. ε=0.05. Thus, PreCurious raises warnings for users on the potential risks of downloading pre-trained models from unknown sources, relying solely on tutorials or common-sense defenses, and releasing sanitized datasets even after perfect scrubbing.},
}

@article{Zhang2023,
  title = {ScienceBenchmark: A Complex Real-World Benchmark for Evaluating Natural Language to SQL Systems},
  author = {Zhang, Yi and Deriu, Jan and Katsogiannis-Meimarakis, George and Kosten, Catherine and Koutrika, Georgia and Stockinger, Kurt},
  year = {2023},
  journal = {Proc. VLDB Endow.},
  volume = {17},
  pages = {685–698},
  doi = {10.14778/3636218.3636225},
  publisher = {VLDB Endowment},
  url = {https://doi.org/10.14778/3636218.3636225},
  abstract = {Natural Language to SQL systems (NL-to-SQL) have recently shown improved accuracy (exceeding 80%) for natural language to SQL query translation due to the emergence of transformer-based language models, and the popularity of the Spider benchmark. However, Spider mainly contains simple databases with few tables, columns, and entries, which do not reflect a realistic setting. Moreover, complex real-world databases with domain-specific content have little to no training data available in the form of NL/SQL-pairs leading to poor performance of existing NL-to-SQL systems.In this paper, we introduce ScienceBenchmark, a new complex NL-to-SQL benchmark for three real-world, highly domain-specific databases. For this new benchmark, SQL experts and domain experts created high-quality NL/SQL-pairs for each domain. To garner more data, we extended the small amount of human-generated data with synthetic data generated using GPT-3. We show that our benchmark is highly challenging, as the top performing systems on Spider achieve a very low performance on our benchmark. Thus, the challenge is many-fold: creating NL-to-SQL systems for highly complex domains with a small amount of hand-made training data augmented with synthetic data. To our knowledge, ScienceBenchmark is the first NL-to-SQL benchmark designed with complex real-world scientific databases, containing challenging training and test data carefully validated by domain experts.},
}

@article{Chora\'{s}2019,
  title = {SocialTruth Project Approach to Online Disinformation (Fake News) Detection and Mitigation},
  author = {Chora\'{s}, Micha\l{} and Pawlicki, Marek and Kozik, Rafa\l{} and Demestichas, Konstantinos and Kosmides, Pavlos and Gupta, Manik},
  year = {2019},
  doi = {10.1145/3339252.3341497},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3339252.3341497},
  abstract = {The extreme growth and adoption of Social Media, in combination with their poor governance and the lack of quality control over the digital content being published and shared, has led information veracity to a continuous deterioration. Current approaches entrust content verification to a single centralised authority, lack resilience towards attempts to successfully "game" verification checks, and make content verification difficult to access and use. In response, our ambition is to create an open, democratic, pluralistic and distributed ecosystem that allows easy access to various verification services (both internal and third-party), ensuring scalability and establishing trust in a completely decentralized environment. In fact, this is the ambition of the EU H2020 SocialTruth project. In this paper, we present the innovative project approach and the vision of effective online disinformation detection for various practical use-cases.},
}

@article{Zhang2025_07,
  title = {MCAP: Low-Pass GNNs with Matrix Completion for Academic Recommendations},
  author = {Zhang, Dan and Zheng, Shaojie and Zhu, Yifan and Yuan, Huihui and Gong, Jibing and Tang, Jie},
  year = {2025},
  journal = {ACM Trans. Inf. Syst.},
  volume = {43},
  doi = {10.1145/3698193},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3698193},
  abstract = {Graph neural networks (GNNs) are commonly used and have shown promising performance in recommendation systems. A major branch, heterogeneous GNNs, models heterogeneous information by leveraging side information for academic paper recommendations. These networks use message passing and high-order propagation to learn representations for users and items. However, existing recommendation methods perform high-order propagation, leading to sub-optimal representation learning. To address this issue, this article proposes a framework called MCAP, which uses relation-aware GNNs and executes low-pass propagation with matrix completion to enhance academic paper recommendations. The framework uses an attention mechanism to learn top- (U)  relationships by constructing a user–user relation graph based on common authors and venues from interacted items. To efficiently and effectively capture semantic-aware similar items, MCAP builds an item–item relation graph by fusing side information of papers using text embedding models (e.g., Mistral) and large language models (e.g., GPT-3.5-Turbo, GLM-4). Finally, the relation-aware user–user and item–item graphs are incorporated into existing GNN-based models to generate representations of users and papers to enhance academic paper recommendations. The effectiveness of the MCAP is validated using four academic datasets, AMiner-PC, AMiner-WeChat, CiteULike, and DBLP, with user–item interactions and side information of papers. Comprehensive experiments show that the MCAP outperforms state-of-the-art models in terms of Recall@5, NDCG@5, and HR@5 with 69.2%, 70.5%, and 77.6% on the AMiner-WeChat dataset. The code for MCAP is available at .},
}

@article{Lei2024,
  title = {RecExplainer: Aligning Large Language Models for Explaining Recommendation Models},
  author = {Lei, Yuxuan and Lian, Jianxun and Yao, Jing and Huang, Xu and Lian, Defu and Xie, Xing},
  year = {2024},
  pages = {1530–1541},
  doi = {10.1145/3637528.3671802},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3637528.3671802},
  abstract = {Recommender systems are widely used in online services, with embedding-based models being particularly popular due to their expressiveness in representing complex signals. However, these models often function as a black box, making them less transparent and reliable for both users and developers. Recently, large language models (LLMs) have demonstrated remarkable intelligence in understanding, reasoning, and instruction following. This paper presents the initial exploration of using LLMs as surrogate models to explaining black-box recommender models. The primary concept involves training LLMs to comprehend and emulate the behavior of target recommender models. By leveraging LLMs' own extensive world knowledge and multi-step reasoning abilities, these aligned LLMs can serve as advanced surrogates, capable of reasoning about observations. Moreover, employing natural language as an interface allows for the creation of customizable explanations that can be adapted to individual user preferences. To facilitate an effective alignment, we introduce three methods: behavior alignment, intention alignment, and hybrid alignment. Behavior alignment operates in the language space, representing user preferences and item information as text to mimic the target model's behavior; intention alignment works in the latent space of the recommendation model, using user and item representations to understand the model's behavior; hybrid alignment combines both language and latent spaces. Comprehensive experiments conducted on three public datasets show that our approach yields promising results in understanding and mimicking target models, producing high-quality, high-fidelity, and distinct explanations. Our code is available at https://github.com/microsoft/RecAI.},
}

@article{Eltabakh2023,
  title = {Cross Modal Data Discovery over Structured and Unstructured Data Lakes},
  author = {Eltabakh, Mohamed Y. and Kunjir, Mayuresh and Elmagarmid, Ahmed K. and Ahmad, Mohammad Shahmeer},
  year = {2023},
  journal = {Proc. VLDB Endow.},
  volume = {16},
  pages = {3377–3390},
  doi = {10.14778/3611479.3611533},
  publisher = {VLDB Endowment},
  url = {https://doi.org/10.14778/3611479.3611533},
  abstract = {Organizations are collecting increasingly large amounts of data for data-driven decision making. These data are often dumped into a centralized repository, e.g., a data lake, consisting of thousands of structured and unstructured datasets. Perversely, such mixture makes the problem of discovering tables or documents that are relevant to a user's query very challenging. Despite the recent efforts in data discovery, the problem remains widely open especially in the two fronts of (1) discovering relationships and relatedness across structured and unstructured datasets-where existing techniques suffer from either scalability, being customized for a specific problem type (e.g., entity matching or data integration), or demolishing the structural properties on its way, and (2) developing a holistic system for integrating various similarity measurements and sketches in an effective way to boost the discovery accuracy.In this paper, we propose a new data discovery system, named CMDL, for addressing these two limitations. CMDL supports the data discovery process over both structured and unstructured data while retaining the structural properties of tables. As a result, CMDL is the only system to date that empowers end-users to seamlessly pipeline the discovery tasks across the two modalities. We propose a novel multi-modal embedding representation that captures the similarities between text documents and tabular columns. The model training relies on labeled datasets generated though weak supervision, and thus the system is domain agnostic and easily generalizable. We evaluate CMDL on three real-world data lakes with diverse applications and show that our system is significantly more effective for cross-modality discovery compared to the search-based baseline techniques. Moreover, CMDL is more accurate and robust to different data types and distributions compared to the state-of-the-art systems that are limited to only the structured datasets.},
}

@article{Thukral2024,
  title = {Generating insights about financial asks from Reddit posts and user interactions},
  author = {Thukral, Sachin and Sangwan, Suyash and Chauhan, Vipul and Chatterjee, Arnab and Dey, Lipika},
  year = {2024},
  pages = {294–299},
  doi = {10.1145/3625007.3627313},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3625007.3627313},
  abstract = {As an increasingly large number of people turn to platforms like Reddit, YouTube, Twitter, Instagram, etc. for financial advice, generating insights about the content generated and interactions taking place within these platforms have become a key research question. This study proposes content and interaction analysis techniques for a large repository created from social media content, where people's interactions are centered around financial information exchange. We propose methods for content analysis that can generate human-interpretable insights using topic-centered clustering and multi-document abstractive summarization. We share details of insights generated from our experiments with a large repository of data gathered from subreddit for personal finance. We have also explored the use of ChatGPT and Vicuna for generating responses to queries and compared them with human responses. The methods proposed in this work are generic and applicable to all large social media platforms.},
}

@article{Petroni2024,
  title = {IR-RAG @ SIGIR24: Information Retrieval's Role in RAG Systems},
  author = {Petroni, Fabio and Siciliano, Federico and Silvestri, Fabrizio and Trappolini, Giovanni},
  year = {2024},
  pages = {3036–3039},
  doi = {10.1145/3626772.3657984},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3626772.3657984},
  abstract = {In recent years, Retrieval Augmented Generation (RAG) systems have emerged as a pivotal component in the field of artificial intelligence, gaining significant attention and importance across various domains. These systems, which combine the strengths of information retrieval and generative models, have shown promise in enhancing the capabilities and performance of machine learning applications. However, despite their growing prominence, RAG systems are not without their limitations and continue to be in need of exploration and improvement. This workshop seeks to focus on the critical aspect of information retrieval and its integral role within RAG frameworks. We argue that current efforts have undervalued the role of Information Retrieval (IR) in the RAG and have concentrated their attention on the generative part. As the cornerstone of these systems, IR's effectiveness dramatically influences the overall performance and outcomes of RAG models. We call for papers that will seek to revisit and emphasize the fundamental principles underpinning RAG systems. At the end of the workshop, we aim to have a clearer understanding of how robust information retrieval mechanisms can significantly enhance the capabilities of RAG systems. The workshop will serve as a platform for experts, researchers, and practitioners. We intend to foster discussions, share insights, and encourage research that underscores the vital role of Information Retrieval in the future of generative systems.},
}

@article{Hu2024_01,
  title = {Euphemism Identification via Feature Fusion and Individualization},
  author = {Hu, Yuxue and Wu, Mingmin and Huang, Zhongqiang and Li, Junsong and Ge, Xing and Sha, Ying},
  year = {2024},
  pages = {2383–2394},
  doi = {10.1145/3589334.3645433},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3589334.3645433},
  abstract = {Euphemisms are widely used on social media and darknet markets to evade supervision. For instance, "ice" serves as a euphemism for the target keyword "methamphetamine" in illicit transactions. Thus, euphemism identification which aims to map the euphemism to its secret meaning (target keyword) is a crucial task in ensuring social network security. However, this task poses significant challenges, including resource limitations due to the unavailable of annotated datasets and linguistic challenges arising from subtle differences in meaning between target keywords. Existing methods employed self-supervised schemes to automatically construct labeled training data, addressing the resource limitations. Yet, these methods rely on static embedding methods that fail to distinguish between target keywords with similar meanings. In addition, we observe that different euphemisms in similar contexts confuse the identification results. To overcome these obstacles, we propose a feature fusion and individualization (FFI) method for euphemism identification. First, we reformulate the task as a cloze task, making it more feasible. Next, we develop a feature fusion module to capture both dynamic global and static local features, enhancing discrimination between different euphemisms in similar contexts. Additionally, we employ a feature individualization module to ensure each target keyword has a unique feature representation by projecting features into their orthogonal space. As a result, FFI can effectively identify similar euphemisms that refer to target keywords with similar meanings. Experimental results demonstrate that our method outperforms state-of-the-art methods and large language models, providing robust support for its effectiveness.},
}

@article{Yan2024,
  title = {Exploring ChatGPT App Ecosystem: Distribution, Deployment and Security},
  author = {Yan, Chuan and Ren, Ruomai and Meng, Mark Huasong and Wan, Liuhuo and Ooi, Tian Yang and Bai, Guangdong},
  year = {2024},
  pages = {1370–1382},
  doi = {10.1145/3691620.3695510},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3691620.3695510},
  abstract = {ChatGPT has enabled third-party developers to create plugins to expand ChatGPT's capabilities. These plugins are distributed through OpenAI's plugin store, making them easily accessible to users. With ChatGPT as the backbone, this app ecosystem has illustrated great business potential by offering users personalized services in a conversational manner. Nonetheless, many crucial aspects regarding app development, deployment, and security of this ecosystem have yet to be thoroughly studied in the research community, potentially hindering a broader adoption by both developers and users. In this work, we conduct the first comprehensive study of the ChatGPT app ecosystem, aiming to illuminate its landscape for our research community. Our study examines the distribution and deployment models in the integration of LLMs and third-party apps, and assesses their security and privacy implications. We uncover an uneven distribution of functionality among ChatGPT plugins, highlighting prevalent and emerging topics. We also identify severe flaws in the authentication and user data protection for third-party app APIs integrated within LLMs, revealing a concerning status quo of security and privacy in this app ecosystem. Our work provides insights for the secure and sustainable development of this rapidly evolving ecosystem.},
}

@article{Lu2024_01,
  title = {Collaborative Training of Tiny-Large Vision Language Models},
  author = {Lu, Shichen and Guo, Longteng and Wang, Wenxuan and Zhao, Zijia and Yue, Tongtian and Liu, Jing and Liu, Si},
  year = {2024},
  pages = {4928–4937},
  doi = {10.1145/3664647.3681026},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3664647.3681026},
  abstract = {Recently, large vision language models (LVLMs) have advanced AI by integrating visual and linguistic data for tasks like visual conversation, image captioning, and visual question answering. Current LVLM research either scales up model size for performance or reduces parameters for limited computational resources. We believe both large and tiny models have unique strengths and that collaborative training yields better results than independent training. We propose Collaborative Training of Tiny-Large Vision Language Models (CTVLMs), a framework connecting large and tiny models via a projection layer and leveraging a synergistic training strategy. Our framework improves training efficiency by strengthening the interconnection between large and tiny models. Using the parameter efficiency of tiny models, we effectively align image-text features, then apply knowledge distillation to help large models better align cross-modal information. During fine-tuning, the large model's extensive knowledge enhances tiny model's performance. This collaborative approach allows models to adapt to various computational resources and outperforms existing methods in vision-language tasks.},
}

@article{Li2024_06,
  title = {Federated Learning in Large Model Era: Vision-Language Model for Smart City Safety Operation Management},
  author = {Li, Zengxiang and Hou, Zhaoxiang and Liu, Hui and Li, Tongzhi and Yang, Chengyi and Wang, Ying and Shi, Chao and Xie, Longfei and Zhang, Weishan and Xu, Liang and Liu, Zelei},
  year = {2024},
  pages = {1578–1585},
  doi = {10.1145/3589335.3651939},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3589335.3651939},
  abstract = {With the tremendous success of large language models such as ChatGPT, artificial intelligence has entered a new era of large models. Multimodal data, which can comprehensively perceive and recognize the physical world, has become an essential path towards general artificial intelligence. However, multimodal large models trained on public datasets often underperform in specific industrial domains. In this paper, we tackle the problem of building large vision-language intelligent models for specific industrial domains by leveraging the general large models and federated learning. We compare the challenges faced by federated learning in the era of small models and large models from different dimensions, and propose a technical framework for federated learning in the era of large models.Specifically, our framework mainly considers three aspects: heterogeneous model fusion, flexible aggregation methods, and data quality improvement. Based on this framework, we conduct a case study of leading enterprises contributing vision-language data and expert knowledge to city safety operation management. The preliminary experiments show that enterprises can enhance and accumulate their intelligence capabilities through federated learning, and jointly create an intelligent city model that provides high-quality intelligent services covering energy infrastructure security, residential community security and urban operation management.},
}

@article{Ye2024,
  title = {Enhancing Asymmetric Web Search through Question-Answer Generation and Ranking},
  author = {Ye, Dezhi and Liu, Jie and Fan, Jiabin and Tian, Bowen and Zhou, Tianhua and Chen, Xiang and Ma, Jin},
  year = {2024},
  pages = {6127–6136},
  doi = {10.1145/3637528.3671517},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3637528.3671517},
  abstract = {This paper addresses the challenge of the semantic gap between user queries and web content, commonly referred to as asymmetric text matching, within the domain of web search. By leveraging BERT for reading comprehension, current algorithms enable significant advancements in query understanding, but still encounter limitations in effectively resolving the asymmetrical ranking problem due to model comprehension and summarization constraints.To tackle this issue, we propose the QAGR (Question-Answer Generation and Ranking) method, comprising an offline module called QAGeneration and an online module called QARanking. The QAGeneration module utilizes large language models (LLMs) to generate high-quality question-answering pairs for each web page. This process involves two steps: generating question-answer pairs and performing verification to eliminate irrelevant questions, resulting in high-quality questions associated with their respective documents. The QARanking module combines and ranks the generated questions and web page content. To ensure efficient online inference, we design the QARanking model as a homogeneous dual-tower model, incorporating query intent to drive score fusion while balancing keyword matching and asymmetric matching. Additionally, we conduct a preliminary screening of questions for each document, selecting only the top-N relevant questions for further relevance calculation.Empirical results demonstrate the substantial performance improvement of our proposed method in web search. We achieve over 8.7% relative offline relevance improvement and over 8.5% online engagement gain compared to the state-of-the-art web search system. Furthermore, we deploy QAGR to online web search engines and share our deployment experience, including production considerations and ablation experiments. This research contributes to advancing the field of asymmetric web search and provides valuable insights for enhancing search engine performance.},
}

@article{Mildner2024,
  title = {Listening to the Voices: Describing Ethical Caveats of Conversational User Interfaces According to Experts and Frequent Users},
  author = {Mildner, Thomas and Cooney, Orla and Meck, Anna-Maria and Bartl, Marion and Savino, Gian-Luca and Doyle, Philip R and Garaialde, Diego and Clark, Leigh and Sloan, John and Wenig, Nina and Malaka, Rainer and Niess, Jasmin},
  year = {2024},
  doi = {10.1145/3613904.3642542},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3613904.3642542},
  abstract = {Advances in natural language processing and understanding have led to a rapid growth in the popularity of conversational user interfaces (CUIs). While CUIs introduce novel benefits, they also yield risks that may exploit people’s trust. Although research looking at unethical design deployed through graphical user interfaces (GUIs) established a thorough understanding of so-called dark patterns, there is a need to continue this discourse within the CUI community to understand potentially problematic interactions. Addressing this gap, we interviewed 27 participants from three cohorts: researchers, practitioners, and frequent users of CUIs. Applying thematic analysis, we construct five themes reflecting each cohort’s insights about ethical design challenges and introduce the CUI Expectation Cycle, bridging system capabilities and user expectations while considering each theme’s ethical caveats. This research aims to inform future development of CUIs to consider ethical constraints while adopting a human-centred approach.},
}

@article{El Zein2024,
  title = {Shadow Health-Related Data: Definition, Categorization, and User Perspectives},
  author = {El Zein, Yamane and Salehzadeh Niksirat, Kavous and Zufferey, No\'{e} and Humbert, Mathias and Huguenin, K\'{e}vin},
  year = {2024},
  pages = {58–76},
  doi = {10.1145/3688459.3688462},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3688459.3688462},
  abstract = {Health-related data (HRD) about individuals are increasingly generated and processed. The sources and volume of such data have grown larger over the past years, they include wearable devices, health-related mobile apps, and electronic health records. HRD are sensitive, have important privacy implications, hence hold a special status under existing privacy laws and regulations. In this work, we focus on shadow HRD: these HRD are generated and/or processed by individuals by using general-purpose digital tools outside of a professional healthcare information system. Some examples are health-related queries made by individuals on general-purpose search engines and LLM-based chatbots, or medical appointments and contact information of health professionals synced to the cloud. Such data, and the privacy risks stemming from them, are often overlooked when studying digital health. Using information from two focus group sessions (23 participants in total), we identified and categorized a broad variety of user behaviors that, including the aforementioned examples, lead to the creation of shadow HRD. Then, informed by this categorization, we designed a questionnaire and deployed it through an online survey (300 respondents) to assess the prevalence of such behaviors among the general public, as well as user awareness of (and concerns about) the privacy risks stemming from their shadow HRD. Our findings show that most respondents adopt numerous and diverse behaviors that create shadow HRD, and that very few resort to mechanisms to protect their privacy.},
}

@article{Li2023_01,
  title = {Modernization of Databases in the Cloud Era: Building Databases that Run Like Legos},
  author = {Li, Feifei},
  year = {2023},
  journal = {Proc. VLDB Endow.},
  volume = {16},
  pages = {4140–4151},
  doi = {10.14778/3611540.3611639},
  publisher = {VLDB Endowment},
  url = {https://doi.org/10.14778/3611540.3611639},
  abstract = {Utilizing cloud for common and critical computing infrastructures has already become the norm across the board. The rapid evolvement of the underlying cloud infrastructure and the revolutionary development of AI present both challenges and opportunities for building new database architectures and systems. It is crucial to modernize database systems in the cloud era, so that next generation cloud native databases may run like legos-they are adaptive, flexible, reliable, and smart towards dynamic workloads and varying requirements.That said, we observe four critical trends and requirements for the modernization of cloud databases: embracing cloud-native architecture, full integration with cloud platform and orchestration, co-design for data fabric, and moving towards being AI augmented. Modernizing database systems by adopting these critical trends and addressing key challenges associated with them provide ample opportunities for data management communities from both academia and industry to explore. We will provide an in-depth case study of how we modernize PolarDB with respect to embracing these four trends in the cloud era. Our ultimate goal is to build databases that run just like playing with legos, so that a database system fits for rich and dynamic workloads and requirements in a self-adaptive, performant, easy-/intuitive-to use, reliable, and intelligent manner.},
}

@article{Kayali2024,
  title = {Chorus: Foundation Models for Unified Data Discovery and Exploration},
  author = {Kayali, Moe and Lykov, Anton and Fountalis, Ilias and Vasiloglou, Nikolaos and Olteanu, Dan and Suciu, Dan},
  year = {2024},
  journal = {Proc. VLDB Endow.},
  volume = {17},
  pages = {2104–2114},
  doi = {10.14778/3659437.3659461},
  publisher = {VLDB Endowment},
  url = {https://doi.org/10.14778/3659437.3659461},
  abstract = {We apply foundation models to data discovery and exploration tasks. Foundation models are large language models (LLMS) that show promising performance on a range of diverse tasks unrelated to their training. We show that these models are highly applicable to the data discovery and data exploration domain. When carefully used, they have superior capability on three representative tasks: table-class detection, column-type annotation and join-column prediction. On all three tasks, we show that a foundation-model-based approach outperforms the task-specific models and so the state of the art. Further, our approach often surpasses human-expert task performance. We investigate the fundamental characteristics of this approach including generalizability to several foundation models and the impact of non-determinism on the outputs. All in all, this suggests a future direction in which disparate data management tasks can be unified under foundation models.},
}

@article{Pavlenko2024,
  title = {Vertically Autoscaling Monolithic Applications with CaaSPER: Scalable Container-as-a-Service Performance Enhanced Resizing Algorithm for the Cloud},
  author = {Pavlenko, Anna and Cahoon, Joyce and Zhu, Yiwen and Kroth, Brian and Nelson, Michael and Carter, Andrew and Liao, David and Wright, Travis and Camacho-Rodr\'{\i}guez, Jes\'{u}s and Saur, Karla},
  year = {2024},
  pages = {241–254},
  doi = {10.1145/3626246.3653378},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3626246.3653378},
  abstract = {Kubernetes has emerged as a prominent open-source platform for managing cloud applications, including stateful databases. These monolithic applications rely on vertical scaling, adjusting CPU cores based on load fluctuations. However, our analysis of Kubernetes-based Database-as-a-Service (DBaaS) offerings at Microsoft revealed that many customers consistently over-provision resources for peak workloads, neglecting cost-saving opportunities through resource scale-down. We found that there is a gap in the ability of existing vertical autoscaling tools to minimize resource slack and respond promptly to throttling, leading to increased costs and impacting crucial metrics such as throughput and availability.To address this challenge, we propose CaaSPER, a vertical autoscaling algorithm that blends reactive and proactive strategies. By dynamically adjusting CPU resources, CaaSPER minimizes resource slack, maintains optimal CPU utilization, and reduces throttling. Importantly, customers have the flexibility to prioritize either cost savings or high performance based on their preferences. Extensive testing demonstrates that CaaSPER effectively reduces throttling and keeps CPU utilization within target levels. CaaSPER is designed to be application-agnostic and platform-agnostic, with potential for extension to other applications requiring vertical autoscaling.},
}

@article{Karras2024,
  title = {Fashion-VDM: Video Diffusion Model for Virtual Try-On},
  author = {Karras, Johanna and Li, Yingwei and Liu, Nan and Zhu, Luyang and Yoo, Innfarn and Lugmayr, Andreas and Lee, Chris and Kemelmacher-Shlizerman, Ira},
  year = {2024},
  doi = {10.1145/3680528.3687623},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3680528.3687623},
  abstract = {We present Fashion-VDM, a video diffusion model (VDM) for generating virtual try-on videos. Given an input garment image and person video, our method aims to generate a high-quality try-on video of the person wearing the given garment, while preserving the person’s identity and motion. Image-based virtual try-on has shown impressive results; however, existing video virtual try-on (VVT) methods are still lacking garment details and temporal consistency. To address these issues, we propose a diffusion-based architecture for video virtual try-on, split classifier-free guidance for increased control over the conditioning inputs, and a progressive temporal training strategy for single-pass 64-frame, 512px video generation. We also demonstrate the effectiveness of joint image-video training for video try-on, especially when video data is limited. Our qualitative and quantitative experiments show that our approach sets the new state-of-the-art for video virtual try-on.},
}

@article{Qin2025,
  title = {Language Models for Online Depression Detection: A Review and Benchmark Analysis on Remote Interviews},
  author = {Qin, Ruiyang and Yang, Kai and Abbasi, Ahmed and Dobolyi, David and Seyedi, Salman and Griner, Emily and Kwon, Hyeokhyen and Cotes, Robert and Jiang, Zifan and Clifford, Gari and Cook, Ryan A.},
  year = {2025},
  journal = {ACM Trans. Manage. Inf. Syst.},
  volume = {16},
  doi = {10.1145/3673906},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3673906},
  abstract = {The use of machine learning (ML) to detect depression in online settings has emerged as an important health and wellness use case. In particular, the use of deep learning methods for depression detection from textual content posted on social media has garnered considerable attention. Conversely, there has been relatively limited evaluation of depression detection in clinical environments involving text generated from remote interviews. In this research, we review state-of-the-art feature-based ML, deep learning, and large language models for depression detection. We use a multidimensional analysis framework to benchmark various language models on a novel testbed comprising speech-to-text transcriptions of remote interviews. Our framework considers the impact of different transcription types and interview segments on depression detection performance. Finally, we summarize the key trends and takeaways from the review and benchmark evaluation and provide suggestions to guide the design of future detection methods.},
}

@article{Wu2023,
  title = {Enhanced Privacy Preservation for Recommender Systems},
  author = {Wu, Ziqing},
  year = {2023},
  pages = {1364–1368},
  doi = {10.1145/3604915.3608888},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3604915.3608888},
}

@article{Rachabatuni2024,
  title = {Context-aware chatbot using MLLMs for Cultural Heritage},
  author = {Rachabatuni, Pavan Kartheek and Principi, Filippo and Mazzanti, Paolo and Bertini, Marco},
  year = {2024},
  pages = {459–463},
  doi = {10.1145/3625468.3652193},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3625468.3652193},
  abstract = {Multi-modal Large Language Models (MLLMs) are currently an extremely active research topic for the multimedia and computer vision communities, and show a significant impact in visual analysis and text generation tasks. MLLM's are well-versed in integrated understanding, analysis of complex data from cross modalities (i.e. text-image) and text generation with chat abilities. Almost all MLLM's, focus on alignment of image features to textual features for downstream text generation tasks includes detailed image description, visual question answering, stories and poems generation, phrase grounding, etc.. However, when focusing on visual question answering, questions that are highly relevant to the context of an image may not be answered correctly with the existing MLLM's, contrary to questions that are related to visual aspects. Moreover, generating meta data (context) for an image using present day MLLM's is hard task due to hallucinating characteristic of underlying Large Language Models (LLM's), and adequate contextual information cannot be directly derived from an image based perspective.Considering the cultural heritage domain, these issues hamper the introduction of multimedia chatbots as tools to support learning and understanding artworks, since contextual information is typically needed to better understand the content of the artworks themselves, and museum curators require that scientifically accurate information is provided to the users of such systems. In this paper we present a system that combines contextual description of the artworks to enhance the contextual visual question answering task.},
}

@article{Gupta2024_07,
  title = {Low-Rank Adaptation of Time Series Foundational Models for Out-of-Domain Modality Forecasting},
  author = {Gupta, Divij and Bhatti, Anubhav and Parmar, Suraj and Dan, Chen and Liu, Yuwei and Shen, Bingjie and Lee, San},
  year = {2024},
  pages = {382–386},
  doi = {10.1145/3678957.3685724},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3678957.3685724},
  abstract = {Low-Rank Adaptation (LoRA) is a widely used technique for fine-tuning large pre-trained or foundational models across different modalities and tasks. However, its application to time series data, particularly within foundational models, remains underexplored. This paper examines the impact of LoRA on contemporary time series foundational models: Lag-Llama, MOIRAI, and Chronos. We demonstrate LoRA’s fine-tuning potential for forecasting the vital signs of sepsis patients in intensive care units (ICUs), emphasizing the models’ adaptability to previously unseen, out-of-domain modalities. Integrating LoRA aims to enhance forecasting performance while reducing inefficiencies associated with fine-tuning large models on limited domain-specific data. Our experiments show that LoRA fine-tuning of time series foundational models (TSFMs) significantly improves forecasting over zero-shot TSFMs, achieving results comparable to state-of-the-art models trained from scratch on similar modalities. We conduct comprehensive ablation studies to demonstrate the trade-offs between the number of tunable parameters and forecasting performance and assess the impact of varying LoRA matrix ranks on model performance.},
}

@article{Kobayashi2023,
  title = {Modeling and generating human mobility trajectories using transformer with day encoding},
  author = {Kobayashi, Akihiro and Takeda, Naoto and Yamazaki, Yudai and Kamisaka, Daisuke},
  year = {2023},
  pages = {7–10},
  doi = {10.1145/3615894.3628506},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3615894.3628506},
  abstract = {Modeling and predicting human mobility trajectories in urban areas is a crucial challenge with various applications. The HuMob Challenge is a competition that focuses on modeling human mobility trajectories using large open-source datasets. This paper presents one of the top 10 winning methods from the competition. In recent years, a method has been developed to apply advanced large language model (LLM) technology to model human movement. We have enhanced trajectory prediction accuracy by encoding daily characteristics, such as commuting to the office on weekdays and visiting shopping districts on weekends, to capture day-to-day variations in movement patterns.},
}

@article{Lin2024,
  title = {Open-Source AI-based SE Tools: Opportunities and Challenges of Collaborative Software Learning},
  author = {Lin, Zhihao and Ma, Wei and Lin, Tao and Zheng, Yaowen and Ge, Jingquan and Wang, Jun and Klein, Jacques and Bissyande, Tegawende and Liu, Yang and Li, Li},
  year = {2024},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  doi = {10.1145/3708529},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3708529},
  abstract = {Large Language Models (LLMs) have become instrumental in advancing software engineering (SE) tasks, showcasing their efficacy in code understanding and beyond. AI code models has demonstrated their value not only in code generating but also in defect detection, enhancing security measures, and improving overall software quality. They are emerging as crucial tools for both software development and maintaining software quality. Like traditional SE tools, open-source collaboration is key in realising the excellent products. However, with AI models, the essential need is in data. The collaboration of these AI-based SE models hinges on maximising the sources of high-quality data. However, data especially of high quality, often holds commercial or sensitive value, making it less accessible for open-source AI-based SE projects. This reality presents a significant barrier to the development and enhancement of AI-based SE tools within the software engineering community. Therefore, researchers need to find solutions for enabling open-source AI-based SE models to tap into resources by different organizations. Addressing this challenge, our position paper investigates one solution to facilitate access to diverse organizational resources for open-source AI models, ensuring privacy and commercial sensitivities are respected. We introduce a governance framework centered on federated learning (FL), designed to foster the joint development and maintenance of open-source AI code models while safeguarding data privacy and security. Additionally, we present guidelines for developers on AI-based SE tool collaboration, covering data requirements, model architecture, updating strategies, and version control. Given the significant influence of data characteristics on federated learning, our research examines the effect of code data heterogeneity on federated learning performance. We consider 6 different scenarios of data distributions and include 4 code models. We also include 4 most common federated learning algorithms.  Our experimental findings highlight the potential for employing federated learning in the collaborative development and maintenance of AI-based software engineering models. We also discuss the key issues to be addressed in the co-construction process and future research directions.},
}

@article{Georgiou2024,
  title = {SKILLAB: Creating a Skills Supply and Demand Data Space},
  author = {Georgiou, Konstantinos and Rossini, Rosaria and Jahn, Marco and Tsoukalas, Dimitrios and Voulgarakis, Vassilis and Tsekeridou, Sofia and Aluas, Mihaela and Vontas, Apostolos and Kehagias, Dionysios and Mittas, Nikolaos and Ampatzoglou, Apostolos and Kordoni, Valia and Chatzigeorgiou, Alexander and Angelis, Lefteris},
  year = {2024},
  pages = {10–17},
  doi = {10.1145/3685651.3686701},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3685651.3686701},
  abstract = {The advent of Industry 5.0 as a defining concept for the future, which advocates a human-centric coalescence of humans and technology or software, renders the skilled workforce the most important asset in any organization or business. The society is ’forced’ to adapt itself to technological change and progress for setting the necessary skillsets for the workforce. In order to follow the digital transformation, it is necessary to evoke the reshaping, evolution, or replacement of traditional and possibly obsolete processes at intra- or inter-organizational levels in multiple aspects, introducing innovative ways of re-defining the workforce. To do so, the key piece are data. In this context various platform collect and organize data, also exploiting the new era of Data Spaces (DS). SKILLAB will act as a smart tool for handling, honing, and widening the competencies of the personnel of companies, forecasting future skill gaps and providing European citizens with a tool for upskilling and reskilling, exploiting DS.},
}

@article{Majdinasab2024,
  title = {Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code},
  author = {Majdinasab, Vahid and Nikanjam, Amin and Khomh, Foutse},
  year = {2024},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  doi = {10.1145/3702980},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3702980},
  abstract = {Code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources. The recent advent of Large Language Models (LLMs) as coding assistants in the software development process poses new challenges for code auditing. The dataset for training these models is mainly collected from publicly available sources. This raises the issue of intellectual property infringement as developers’ codes are already included in the dataset. Therefore, auditing code developed using LLMs is challenging, as it is difficult to reliably assert if an LLM used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models. Given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement. To address this challenge, we propose a new approach, TraWiC; a model-agnostic and interpretable method based on membership inference for detecting code inclusion in an LLM’s training dataset. We extract syntactic and semantic identifiers unique to each program to train a classifier for detecting code inclusion. In our experiments, we observe that TraWiC is capable of detecting 83.87% of codes that were used to train an LLM. In comparison, the prevalent clone detection tool NiCad is only capable of detecting 47.64%. In addition to its remarkable performance, TraWiC has low resource overhead in contrast to pair-wise clone detection that is conducted during the auditing process of tools like CodeWhisperer reference tracker, across thousands of code snippets.},
}

@article{Papakyriakopoulos2023,
  title = {Augmented Datasheets for Speech Datasets and Ethical Decision-Making},
  author = {Papakyriakopoulos, Orestis and Choi, Anna Seo Gyeong and Thong, William and Zhao, Dora and Andrews, Jerone and Bourke, Rebecca and Xiang, Alice and Koenecke, Allison},
  year = {2023},
  pages = {881–904},
  doi = {10.1145/3593013.3594049},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3593013.3594049},
  abstract = {Speech datasets are crucial for training Speech Language Technologies (SLT); however, the lack of diversity of the underlying training data can lead to serious limitations in building equitable and robust SLT products, especially along dimensions of language, accent, dialect, variety, and speech impairment—and the intersectionality of speech features with socioeconomic and demographic features. Furthermore, there is often a lack of oversight on the underlying training data—commonly built on massive web-crawling and/or publicly available speech—with regard to the ethics of such data collection. To encourage standardized documentation of such speech data components, we introduce an augmented datasheet for speech datasets1, which can be used in addition to “Datasheets for Datasets” [78]. We then exemplify the importance of each question in our augmented datasheet based on in-depth literature reviews of speech data used in domains such as machine learning, linguistics, and health. Finally, we encourage practitioners—ranging from dataset creators to researchers—to use our augmented datasheet to better define the scope, properties, and limits of speech datasets, while also encouraging consideration of data-subject protection and user community empowerment. Ethical dataset creation is not a one-size-fits-all process, but dataset creators can use our augmented datasheet to reflexively consider the social context of related SLT applications and data sources in order to foster more inclusive SLT products downstream.},
}

@article{Cui2024,
  title = {Three Heads Are Better Than One: Suggesting Move Method Refactoring Opportunities with Inter-class Code Entity Dependency Enhanced Hybrid Hypergraph Neural Network},
  author = {Cui, Di and Wang, Jiaqi and Wang, Qiangqiang and Ji, Peng and Qiao, Minglang and Zhao, Yutong and Hu, Jingzhao and Wang, Luqiao and Li, Qingshan},
  year = {2024},
  pages = {745–757},
  doi = {10.1145/3691620.3695068},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3691620.3695068},
  abstract = {Methods implemented in incorrect classes will cause excessive reliance on other classes than their own, known as a typical code smell symptom: feature envy, which makes it difficult to maintain increased coupling between classes. Addressing this issue, several Move Method refactoring tools have been proposed, employing a two-phase process: identifying misplaced methods to move and appropriate classes to receive, and implementing the mechanics of refactoring. These tools traditionally use hard-coded metrics to measure correlations between movable methods and target classes and apply heuristic thresholds or trained classifiers to unearth refactoring opportunities. Yet, these approaches predominantly illuminate pairwise correlations between methods and classes while overlooking the complex and complicated dependencies binding multiple code entities within these methods/classes that are prevalent in real-world cases. This narrow focus can lead to refactoring suggestions that may diverge from developers' actual needs. To bridge this gap, our paper leverages the concept of inter-class code entity dependency hypergraph to model complicated dependency relationships involving multiple code entities within various methods/classes and proposes a hypergraph learning-based approach to suggest Move Method refactoring opportunities named HMove. We first construct inter-class code entity dependency hypergraphs from training samples and assign attributes to entities with a pre-trained code model. All the attributed hypergraphs are fed into a hybrid hypergraph neural network for training. Utilizing this trained neural network alongside a large language model, we construct a refactoring suggestion system. We trained HMove on a large-scale dataset and evaluated it on two real-world datasets. The results show that demonstrates an increase of 27.8% in precision, 2.5% in recall, and 18.5% in f1-measure compared to 9 state-of-the-art refactoring tools, which is more useful for 68% of participants. The results also unveil practical suggestions and new insights that benefit existing feature envy-related refactoring techniques.},
}

@article{Elsden2023,
  title = {FestForward: Participatory Design Futuring and World-Building for Equitable Digital Futures in Performing Arts Festivals},
  author = {Elsden, Chris and Jones, Vikki and Helgason, Ingi and Abernethy, Lizzie and Brown, Will},
  year = {2023},
  pages = {1424–1437},
  doi = {10.1145/3563657.3596033},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3563657.3596033},
  abstract = {FestForward is a fictional, local, cultural magazine, set in 2030, designed to stimulate conversations about equitable and sustainable digital futures in performing arts festivals. This extensive design fiction was developed through a series of participatory workshops, where creative and cultural practitioners responded to various ‘provotypes’ suggesting narrative content for the magazine. In this pictorial, we annotate and unpack the making of FestForward to reflect upon various formats and approaches to design futuring, and to offer a platform for further world-building, research and discussion on equitable digital futures in arts festivals.},
}

@article{Demelius2025,
  title = {Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic Survey},
  author = {Demelius, Lea and Kern, Roman and Tr\"{u}gler, Andreas},
  year = {2025},
  journal = {ACM Comput. Surv.},
  volume = {57},
  doi = {10.1145/3712000},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3712000},
  abstract = {Differential privacy has become a widely popular method for data protection in machine learning, especially since it allows formulating strict mathematical privacy guarantees. This survey provides an overview of the state of the art of differentially private centralized deep learning, thorough analyses of recent advances and open problems, as well as a discussion of potential future developments in the field. Based on a systematic literature review, the following topics are addressed: emerging application domains, differentially private generative models, auditing and evaluation methods for private models, protection against a broad range of threats and attacks, and improvements of privacy-utility tradeoffs.},
}

@article{Capel2023,
  title = {What is Human-Centered about Human-Centered AI? A Map of the Research Landscape},
  author = {Capel, Tara and Brereton, Margot},
  year = {2023},
  doi = {10.1145/3544548.3580959},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3544548.3580959},
  abstract = {The application of Artificial Intelligence (AI) across a wide range of domains comes with both high expectations of its benefits and dire predictions of misuse. While AI systems have largely been driven by a technology-centered design approach, the potential societal consequences of AI have mobilized both HCI and AI researchers towards researching human-centered artificial intelligence (HCAI). However, there remains considerable ambiguity about what it means to frame, design and evaluate HCAI. This paper presents a critical review of the large corpus of peer-reviewed literature emerging on HCAI in order to characterize what the community is defining as HCAI. Our review contributes an overview and map of HCAI research based on work that explicitly mentions the terms ‘human-centered artificial intelligence’ or ‘human-centered machine learning’ or their variations, and suggests future challenges and research directions. The map reveals the breadth of research happening in HCAI, established clusters and the emerging areas of Interaction with AI and Ethical AI. The paper contributes a new definition of HCAI, and calls for greater collaboration between AI and HCI research, and new HCAI constructs.},
}

@article{Cassano2024,
  title = {Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs},
  author = {Cassano, Federico and Gouwar, John and Lucchetti, Francesca and Schlesinger, Claire and Freeman, Anders and Anderson, Carolyn Jane and Feldman, Molly Q and Greenberg, Michael and Jangda, Abhinav and Guha, Arjun},
  year = {2024},
  journal = {Proc. ACM Program. Lang.},
  volume = {8},
  doi = {10.1145/3689735},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3689735},
  abstract = {Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as building blocks for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming language. Code LLMs produce impressive results on high-resource programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages that have limited training data available (e.g., OCaml, Racket, and several others).                                                                                                                                                                                                                                                                This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach, called MultiPL-T, generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. MultiPL-T translates training data from high-resource languages into training data for low-resource languages in the following way. 1) We use a Code LLM to synthesize unit tests for commented code from a high-resource source language, filtering out faulty tests and code with low test coverage. 2) We use a Code LLM to translate the code from the high-resource source language to a target low-resource language. This gives us a corpus of candidate training data in the target language, but many of these translations are wrong. 3) We use a lightweight compiler to compile the test cases generated in (1) from the source language to the target language, which allows us to filter our obviously wrong translations. The result is a training corpus in the target low-resource language where all items have been validated with test cases. We apply this approach to generate tens of thousands of new, validated training items for five low-resource languages: Julia, Lua, OCaml, R, and Racket, using Python as the source high-resource language. Furthermore, we use an open Code LLM (StarCoderBase) with open training data (The Stack), which allows us to decontaminate benchmarks, train models without violating licenses, and run experiments that could not otherwise be done.                                                                                                                                                                                                                                                                Using datasets generated with MultiPL-T, we present fine-tuned versions of StarCoderBase and Code Llama for Julia, Lua, OCaml, R, and Racket that outperform other fine-tunes of these base models on the natural language to code task. We also present Racket fine-tunes for two very recent models, DeepSeek Coder and StarCoder2, to show that MultiPL-T continues to outperform other fine-tuning approaches for low-resource languages. The MultiPL-T approach is easy to apply to new languages, and is significantly more efficient and effective than alternatives such as training longer.},
}

@article{Yue2024,
  title = {IdeoRate: Towards a Semi-automated Assessment Methodology for OSS Ideologies},
  author = {Yue, Yang and Wang, Yi and Redmiles, David},
  year = {2024},
  pages = {2478–2479},
  doi = {10.1145/3691620.3695343},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3691620.3695343},
  abstract = {Open source software (OSS) development, as any social movement, is driven by its ideologies, namely OSS ideologies [6]. Understanding OSS ideologies could provide significant insights into open source development, since OSS ideologies determine and influence the dynamics and outcomes of open source development [1, 2]. Assessing OSS ideologies within open source projects could bring various benefits to OSS practitioners, e.g., the owners and the maintainers of open source projects could identify important ideological elements that were previously ignored, and could improve open source development accordingly. Therefore, with such an assessment of OSS ideologies, institutional and individual stakeholders who are interested in open source development could make informed decisions when interacting with open source projects.},
}

@article{Zhang2024_06,
  title = {Spatial-Temporal Embodied Carbon Models for the Embodied Carbon Accounting of Computer Systems},
  author = {Zhang, Xiaoyang and Yang, Yijie and Wang, Dan},
  year = {2024},
  pages = {464–471},
  doi = {10.1145/3632775.3661939},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3632775.3661939},
  abstract = {Embodied carbon is the total amount of carbon released from the processes associated with a product from cradle to gate. In many industry sectors, embodied carbon dominates the overall carbon footprint. Embodied carbon accounting, i.e., to estimate the embodied carbon of a product, has become an important research topic. Existing studies derive the embodied carbon through life cycle analysis (LCA) reports. Current LCA reports only provide the carbon emission of a product class, e.g., 28nm CPU, yet a product instance can be manufactured from diverse regions and in diverse time periods, e.g., in the winter in Ireland (Intel). It is known that carbon emissions depend on the electricity generation process, which has spatial and temporal dynamics. Therefore, the embodied carbon of a specific product instance can largely differ from its product class. In this paper, we present new Spatial-Temporal Embodied Carbon (STEC) models for embodied carbon accounting. We observe significant differences between current embodied carbon models and STEC models, e.g., for 7nm CPU the difference can be 13.69%. We further examine the impact of STEC models on existing embodied carbon accounting schemes on key computer applications, such as Large Language Model (LLM) inference and LLM training. We observe that using STEC models leads to much greater differences in the embodied accounting of certain applications as compared to others (e.g., 32.26% vs. 6.35%). This is because the hardware requirements of certain applications allow for a wider range of hardware choices, while others have greater restrictions.},
}

@article{Deterding2023,
  title = {Inaugural Editorial: A Lighthouse for Games and Playable Media},
  author = {Deterding, Sebastian and Mitchell, Kenny and Kowert, Rachel and King, Brad},
  year = {2023},
  journal = {ACM Games},
  volume = {1},
  doi = {10.1145/3585393},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3585393},
  abstract = {In games and playable media, almost nothing is as it was at the turn of the millennium. Digital and analog games have exploded in reach, diversity, and relevance. Digital platforms and globalisation have shifted and fragmented their centres of gravity and how they are made and played. Games are converging with other media, technologies, and arts into a wide field of playable media. Games research has similarly exploded in volume and fragmented into disciplinary specialisms. All this can be deeply disorienting. The journal Games: Research and Practice wants to offer a lighthouse that helps readers orient themselves in this new, ever-shifting reality of games industry and games research.},
}

@article{Von Davier2023,
  title = {Designing for Appreciation: How Digital Spaces Can Support Art and Culture},
  author = {Von Davier, Thomas Serban},
  year = {2023},
  doi = {10.1145/3544549.3577041},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3544549.3577041},
  abstract = {Throughout history, a complex network of human actors shaped how the general public perceived art. Today, social media platforms and their algorithms influence artful experiences for billions. How has this changed the appreciation and perception of art? The role of visual art as items we use to define ourselves and our societies motivates research to explore how recommendation algorithms impact our ability to appreciate and perceive art. There are three aspects to explore: the art metadata and algorithm functionality, conversations with artworld experts, and redesigning digital art experiences. These three methods will follow open science practices and methods by releasing open-access datasets and research prototypes. Ultimately, this proposed thesis aims to contribute to theories of content base algorithmic recommendation and its role in presenting art and culture to users.},
}

@article{Jegede2023,
  title = {Challenge Accepted? A Critique of the 2021 National Institute of Justice Recidivism Forecasting Challenge},
  author = {Jegede, Tobi and Gerchick, Marissa Kumar and Mathai, Amreeta S and Horowitz, Aaron},
  year = {2023},
  doi = {10.1145/3617694.3623242},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3617694.3623242},
  abstract = {In 2021, the National Institute of Justice — the research arm of the United States Department of Justice — released the “Recidivism Forecasting Challenge” (“the Challenge”) with the stated goals of “increas[ing] public safety and improv[ing] the fair administration of justice across the United States,” providing “critical information to community corrections departments...,” and ultimately “improv[ing]” the ability to forecast recidivism using person-and place-based variables” [68]. The Challenge was also designed, in part, to encourage “non-criminal justice forecasting researchers to compete against more ‘traditional’ criminal justice researchers” [68]. Challenge contestants had the opportunity to win part of the $723,000 in prize money for their submitted models. In this work, we highlight how the Challenge was underpinned by a techno-solutionist framing (emphasizing technical interventions without addressing underlying structural problems) [66] and plagued by serious ethical and methodological issues, including (1) the choice of training data and the selection of an outcome variable extracted from racially biased and inaccurate law enforcement data systems, (2) data leakage that may have seriously compromised the Challenge, (3) the choice of a faulty fairness metric, leading to the inability of submitted models to accurately surface any bias issues in the data selected for the Challenge, (4) the inclusion of candidate variables that created the potential for feedback loops, (5) a Challenge structure that arguably incentivized exploiting the metrics used to judge entrants, leading to the development of trivial solutions that could not realistically work in practice, and (6) the participation of Challenge contestants who demonstrated a lack of understanding of basic aspects of the U.S. criminal legal system’s structure and functions. We analyze the Challenge and its shortcomings through the lens of participatory design, applying emerging principles for robust participatory design practices in artificial intelligence (AI) and machine learning (ML) development to evaluate the Challenge’s structure and results. We argue that if the Challenge’s designers had adhered to these principles, the Challenge would have looked dramatically different or would not have occurred at all. We highlight several urgent needs and potential paths forward for any future efforts of this nature, recognizing the real and significant harms of recidivism prediction tools and the need to center communities directly impacted by policing and incarceration when thinking about whether to develop risk assessment tools.},
}

@article{Ajmani2024,
  title = {Data Agency Theory: A Precise Theory of Justice for AI Applications},
  author = {Ajmani, Leah and Stapleton, Logan and Houtti, Mo and Chancellor, Stevie},
  year = {2024},
  pages = {631–641},
  doi = {10.1145/3630106.3658930},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3630106.3658930},
  abstract = {Data collection methods for AI applications have been heavily scrutinized by researchers, policymakers, and the general public. In this paper, we propose data agency theory (DAT), a precise theory of justice to evaluate and improve current consent procedures used in AI applications. We argue that data agency is systematically defined by consent policies. Therefore, data agency is a matter of justice. DAT claims data agency ought to be afforded in a way that minimizes the oppression of data contributors by data collectors. We then apply DAT to two salient consent procedures in AI applications: Reddit’s Terms of Service agreement and the United States’s IRB protocols. Through these cases, we demonstrate how our theory helps evaluate justice and generate ideas for improvement. Finally, we discuss the implications of using justice as an evaluation metric, comparing consent procedures, and adopting DAT in future research.},
}

@article{Xiao2024_01,
  title = {Formal Privacy Proof of Data Encoding: The Possibility and Impossibility of Learnable Encryption},
  author = {Xiao, Hanshen and Suh, G. Edward and Devadas, Srinivas},
  year = {2024},
  pages = {1834–1848},
  doi = {10.1145/3658644.3670277},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3658644.3670277},
  abstract = {We initiate a formal study on the concept of learnable obfuscation and aim to answer the following question: is there a type of data encoding that maintains the "learnability" of encoded samples, thereby enabling direct model training on transformed data, while ensuring the privacy of both plaintext and the secret encoding function? This long-standing open problem has prompted many efforts to design such an encryption function, for example, NeuraCrypt and TransNet. Nonetheless, all existing constructions are heuristic without formal privacy guarantees, and many successful reconstruction attacks are known on these constructions assuming an adversary with substantial prior knowledge.We present both generic possibility and impossibility results pertaining to learnable obfuscation. On one hand, we demonstrate that any non-trivial, property-preserving transformation which enables effectively learning over encoded samples cannot offer cryptographic computational security in the worst case. On the other hand, from the lens of information-theoretical security, we devise a series of new tools to produce provable and useful privacy guarantees from a set of heuristic obfuscation methods, including matrix masking, data mixing and permutation, through noise perturbation. Under the framework of PAC Privacy, we show how to quantify the leakage from the learnable obfuscation built upon obfuscation and perturbation methods against adversarial inference. Significantly sharpened utility-privacy tradeoffs are achieved compared to state-of-the-art accounting methods when measuring privacy against data reconstruction and membership inference attacks.},
}

@article{Rezwana2023,
  title = {User Perspectives on Ethical Challenges in Human-AI Co-Creativity: A Design Fiction Study},
  author = {Rezwana, Jeba and Maher, Mary Lou},
  year = {2023},
  pages = {62–74},
  doi = {10.1145/3591196.3593364},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3591196.3593364},
  abstract = {In a human-AI co-creation, AI not only categorizes, evaluates and interprets data but also generates new content and interacts with humans. As co-creative AI is a form of intelligent technology that directly involves humans, it is critical to anticipate and address ethical issues during all design stages. The open-ended nature of human-AI interactions in co-creation poses many challenges for designing ethical co-creative AI systems. Researchers have been exploring ethical issues associated with autonomous AI in recent years, but ethics in human-AI co-creativity is a relatively new research area. In order to design human-centered ethical AI, it is important to understand the perspectives, expectations, and ethical concerns of potential users. In this paper, we present a study with 18 participants to explore several ethical dilemmas and challenges in human-AI co-creation from the perspective of potential users using a design fiction (DF) methodology. DF is a speculative research method that depicts a new concept or technology through stories as an intangible prototype. We present the findings from the study as potential users’ perspectives, stances, and expectations around ethical challenges in human-AI co-creativity as a basis for designing human-centered ethical AI partners for human-AI co-creation.},
}

@article{Wu2024_03,
  title = {Integrating measures of replicability into scholarly search: Challenges and opportunities},
  author = {Wu, Chuhao and Chakravorti, Tatiana and Carroll, John M. and Rajtmajer, Sarah},
  year = {2024},
  doi = {10.1145/3613904.3643043},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3613904.3643043},
  abstract = {Challenges to reproducibility and replicability have gained widespread attention, driven by large replication projects with lukewarm success rates. A nascent work has emerged developing algorithms to estimate the replicability of published findings. The current study explores ways in which AI-enabled signals of confidence in research might be integrated into the literature search. We interview 17 PhD researchers about their current processes for literature search and ask them to provide feedback on a replicability estimation tool. Our findings suggest that participants tend to confuse replicability with generalizability and related concepts. Information about replicability can support researchers throughout the research design processes. However, the use of AI estimation is debatable due to the lack of explainability and transparency. The ethical implications of AI-enabled confidence assessment must be further studied before such tools could be widely accepted. We discuss implications for the design of technological tools to support scholarly activities and advance replicability.},
}

@article{Baloukas2024,
  title = {A Risk Assessment and Legal Compliance Framework for Supporting Personal Data Sharing with Privacy Preservation for Scientific Research},
  author = {Baloukas, Christos and Papadopoulos, Lazaros and Demestichas, Kostas and Weissenfeld, Axel and Schlarb, Sven and Aramburu, Mikel and Red\'{o}, David and Garc\'{\i}a, Jorge and Gaines, Se\'{a}n and Marquenie, Thomas and Eren, Ezgi and Erdogan Peter, Irmak},
  year = {2024},
  doi = {10.1145/3664476.3670878},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3664476.3670878},
  abstract = {In order to perform cutting-edge research like AI model training, a large amount of data needs to be accessed. However, data providers are often reluctant to share their data with researchers as these might contain personal data and thereby sharing may introduce serious risks with significant personal, institutional or societal impacts. Apart from the need to control these risks, data providers must also comply with regulations like GDPR, which creates an additional overhead that makes data sharing even less appealing to data providers. Technologies like anonymization can play a critical role when sharing data that may contain personal information by offering privacy preservation measures like face or license plate anonymization. Therefore, we propose a framework to support data sharing of personal data for research by integrating anonymization, risk assessment and automatic licence agreement generation. The framework offers a practical and efficient solution for organisations seeking to enhance data-sharing practices without compromising information security.},
}

@article{Kapoor2024,
  title = {ACM's 2024 General Election},
  author = {Kapoor, Hemangee and Pancake, Cherri and Segal, Gerald},
  year = {2024},
  journal = {Commun. ACM},
  volume = {67},
  pages = {7–21},
  doi = {10.1145/3652030},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3652030},
  abstract = {Please Take This Opportunity to Vote.Meet the candidates who introduce their plans---and stands---for the Association.},
}

@article{Khalil2025,
  title = {The lack of generalisability in learning analytics research: why, how does it matter, and where to?},
  author = {Khalil, Mohammad and Prinsloo, Paul},
  year = {2025},
  pages = {170–180},
  doi = {10.1145/3706468.3706489},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3706468.3706489},
  abstract = {Concerns about the lack of impact of learning analytics (LA) research has been part of the evolution of the field since its emergence as a research focus and practice in 2011. The preponderance of small-scale and exploratory nature of much of LA research are well-documented as contributing factors to the lack of generalisability, transferability, replicability and scalability. Through an analysis of 144 full research papers published in the conference proceedings of the Learning Analytics &amp; Knowledge (LAK) Conference '22, 23 and 24, this paper provides an overview of the extent and contours of the lack of generalisability in LA research and pointers for making LA research more generalisable. The inductive and deductive analysis of the recent three LAK conferences provide evidence that a significant percentage (46%) of the corpus papers do not refer at all to generalisability or transferability, while few papers report on the scalability of their research findings. While the crisis of replicability/reproducibility is a wider concern in the broader context of research, considering and reporting on generalisability and transferability is integral to the scientific rigour. We conclude our paper with a range of pointers for addressing the lack of generalisability in LA research including, but not limited to expanding data, methodological adaptation and the potential of open science.},
}

@article{Antonini2023,
  title = {Positive by Design: The Next Big Challenge in Rethinking Media as Agents?},
  author = {Antonini, Alessio},
  year = {2023},
  doi = {10.1145/3603163.3609068},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3603163.3609068},
  abstract = {Hypertext and Web pioneers had high aspirations and expectations about the potential positive impact of technology. However, studies in the last decades have shown how widely adopted social and intelligent media are either amplifiers or a source of adverse detrimental effects on their users. On the one hand, we now have a better understanding of these negative phenomena and strategies to identify and quantify their effects. On the other hand, as a community, we should take on the challenge of steering hypertext technologies toward positive applications. This position paper argues for a proactive role of the hypertext community in the design of agent media result of combining social media with intelligent algorithms. This silent paradigm shift introduced third-party proactive agents in a wide range of human-to-human interactions. We are today at a point where social media and global web applications cannot operate without such systems and demand, in the author's opinion, a similar proactive role of academia and scholars in understanding and driving their design for positive goals. This position paper outlines the need for this challenge to be taken on, and how and why the Hypertext community could lead in its own way this vision forward.},
}

@article{Liu2024_04,
  title = {Causal Dataset Discovery with Large Language Models},
  author = {Liu, Junfei and Sun, Shaotong and Nargesian, Fatemeh},
  year = {2024},
  pages = {1–8},
  doi = {10.1145/3665939.3665968},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3665939.3665968},
  abstract = {Causal data discovery is crucial in scientific research by uncovering causal links among a variety of observed variables. Causal dataset discovery is the task of identifying datasets that contain columns that have causal relationships with columns in a query dataset. Discovering causal links from large-scale repositories faces three major challenges: vast scale of data, inherent sparsity of causal links, and incompleteness of variables present. Identifying causal relationships among datasets is a complex and time-intensive task, especially because it requires joining datasets, to bring all variables together, before applying causal link discovery. In this paper, we introduce the Causal Dataset Discovery problem and propose a large language model (LLM)-based framework to discover potential pairwise causal links between columns from different datasets. We heuristically improve LLM's grasp of causality through prompting and fine-tuning and prevent the extreme imbalance in causal candidate distributions due to natural sparsity of causal connections. We create benchmarks specific to this task1, experimentally show that our framework achieves remarkable performance with GPT-3.5 and GPT-4. We summarize the distinctive behaviors of different LLM strategies, and discuss improvements for future research.},
}

@article{Zhang2024_07,
  title = {Data Probes as Boundary Objects for Technology Policy Design: Demystifying Technology for Policymakers and Aligning Stakeholder Objectives in Rideshare Gig Work},
  author = {Zhang, Angie and Rana, Rocita and Boltz, Alexander and Dubal, Veena and Lee, Min Kyung},
  year = {2024},
  doi = {10.1145/3613904.3642000},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3613904.3642000},
  abstract = {Despite the evidence of harm that technology can inflict, commensurate policymaking to hold tech platforms accountable still lags. This is pertinent to app-based gig workers, where unregulated algorithms continue to dictate their work, often with little human recourse. While past HCI literature has investigated workers’ experiences under algorithmic management and how to design interventions, rarely are the perspectives of stakeholders who inform or craft policy sought. To bridge this, we propose using data probes—interactive visualizations of workers’ data that show the impact of technology practices on people—exploring them in 12 semi-structured interviews with policy informers, (driver-)organizers, litigators, and a lawmaker in the rideshare space. We show how data probes act as boundary objects to assist stakeholder interactions, demystify technology for policymakers, and support worker collective action. We discuss the potential for data probes as training tools for policymakers, and considerations around data access and worker risks when using data probes.},
}

@article{Correll2024,
  title = {When the Body Became Data: Historical Data Cultures and Anatomical Illustration},
  author = {Correll, Michael and Garrison, Laura},
  year = {2024},
  doi = {10.1145/3613904.3642056},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3613904.3642056},
  abstract = {With changing attitudes around knowledge, medicine, art, and technology, the human body has become a source of information and, ultimately, shareable and analyzable data. Centuries of illustrations and visualizations of the body occur within particular historical, social, and political contexts. These contexts are enmeshed in different so-called data cultures: ways that data, knowledge, and information are conceptualized and collected, structured and shared. In this work, we explore how information about the body was collected as well as the circulation, impact, and persuasive force of the resulting images. We show how mindfulness of data cultural influences remain crucial for today’s designers, researchers, and consumers of visualizations. We conclude with a call for the field to reflect on how visualizations are not timeless and contextless mirrors on objective data, but as much a product of our time and place as the visualizations of the past.},
}

@article{Amer-Yahia2023,
  title = {From Large Language Models to Databases and Back: A Discussion on Research and Education},
  author = {Amer-Yahia, Sihem and Bonifati, Angela and Chen, Lei and Li, Guoliang and Shim, Kyuseok and Xu, Jianliang and Yang, Xiaochun},
  year = {2023},
  journal = {SIGMOD Rec.},
  volume = {52},
  pages = {49–56},
  doi = {10.1145/3631504.3631518},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3631504.3631518},
  abstract = {In recent years, large language models (LLMs) have garnered increasing attention from both academia and industry due to their potential to facilitate natural language processing (NLP) and generate highquality text. Despite their benefits, however, the use of LLMs is raising concerns about the reliability of knowledge extraction. The combination of DB research and data science has advanced the state of the art in solving real-world problems, such as merchandise recommendation and hazard prevention [30]. In this discussion, we explore the challenges and opportunities related to LLMs in DB and data science research and education.},
}

@article{Agarwal2024,
  title = {TrICy: Trigger-Guided Data-to-Text Generation With Intent Aware Attention-Copy},
  author = {Agarwal, Vibhav and Ghosh, Sourav and BSS, Harichandana and Arora, Himanshu and Raja, Barath Raj Kandur},
  year = {2024},
  journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
  volume = {32},
  pages = {1173–1184},
  doi = {10.1109/TASLP.2024.3353574},
  publisher = {IEEE Press},
  url = {https://doi.org/10.1109/TASLP.2024.3353574},
  abstract = {Data-to-text (D2T) generation is a crucial task in many natural language understanding (NLU) applications and forms the foundation of task-oriented dialog systems. In the context of conversational AI solutions that can work directly with local data on the user's device, architectures utilizing large pre-trained language models (PLMs) are impractical for on-device deployment due to a high memory footprint. To this end, we propose TrICy, a novel lightweight framework for an enhanced D2T task that generates text sequences based on the intent in context and may further be guided by user-provided triggers. We leverage an attention-copy mechanism to predict out-of-vocabulary (OOV) words accurately. Performance analyses on E2E NLG dataset [Novikova et al. 2017] (BLEU: 66.43%, ROUGE-L: 70.14%), WebNLG dataset [Gardent et al. 2017] (BLEU: &lt;italic&gt;Seen&lt;/italic&gt; 64.08%, &lt;italic&gt;Unseen&lt;/italic&gt; 52.35%), and our Custom dataset related to text messaging applications, showcase our architecture's effectiveness. Moreover, we show that by leveraging an optional trigger input, data-to-text generation quality increases significantly and achieves the new SOTA score of 69.29% BLEU for E2E NLG. Furthermore, our analyses show that TrICy achieves at least 24% and 3% improvement in BLEU and METEOR respectively over LLMs like GPT-3, ChatGPT, and Llama 2. We also demonstrate that in some scenarios, performance improvement due to triggers is observed even when they are absent in training.},
}

@article{Altammami2024,
  title = {What you see, What you get? Mapping Inconsistencies of Sustainability Judgements among Experts and Consumers},
  author = {Altammami, Alaa and Dimitrova, Vania and Pournaras, Evangelos},
  year = {2024},
  pages = {443–452},
  doi = {10.1145/3677525.3678695},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3677525.3678695},
  abstract = {Addressing sustainability issues requires collective action, with individuals playing a crucial role. Despite a willingness to shop responsibly, people lack the knowledge needed to make informed decisions, this information gap underpins the intention-behaviour gap. Online shopping, now dominant, can offer rich sustainability information. However, consumers, especially those new to the sustainability domain, face challenges in comprehending this information due to its complexity and volume, including confusion over the meaning of ecolabels. Product descriptions are a key decision-making resource, yet there is no significant research analysing their sustainability content or their potential for seamless in-situ/situated learning. We propose a framework using a Taxonomy for Product Sustainability (TPS) to automatically extract and analyse sustainability profiles from product descriptions. By comparing these profiles with expert judgements, we identify how alignments can be seen as an opportunity to enhance consumer awareness and how misalignments can introduce cognitive biases that impede ethical shopping. Our analysis of food product descriptions reveals distinct patterns of agreement and disagreement, highlighting cognitive biases that affect consumer decisions. These biases, driven by misalignment and information overload, contribute to the intention-behaviour gap in sustainable shopping. By identifying specific areas of confusion, we suggest targeted interventions, such as informative prompts, to facilitate seamless learning and improve consumer knowledge, eventually promoting more informed and ethical choices.},
}

@article{Taelman2024,
  title = {Towards Applications on the Decentralized Web using Hypermedia-driven Query Engines},
  author = {Taelman, Ruben},
  year = {2024},
  journal = {SIGWEB Newsl.},
  volume = {2024},
  doi = {10.1145/3690177.3690181},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3690177.3690181},
  abstract = {The Web is facing unprecedented challenges related to the control and ownership of data. Due to recent privacy and manipulation scandals caused by the increasing centralization of data on the Web into increasingly fewer large data silos, there is a growing demand for the re-decentralization of the Web. Enforced by user-empowering legislature such as GDPR and CCPA, decentralization initiatives such as Solid are being designed that aim to break personal data out of these silos and give back control to the users. While the ability to choose where and how data is stored provides significant value to users, it leads to major technical challenges due to the massive distribution of data across the Web. Since we cannot expect application developers to take up this burden of coping with all the complexities of handling this data distribution, there is a need for a software layer that abstracts away these complexities, and makes this decentralized data feel as being centralized. Concretely, we propose personal query engines to play the role of such an abstraction layer. In this article, we discuss what the requirements are for such a query-based abstraction layer, what the state of the art is in this area, and what future research is required. Even though many challenges remain to achieve a developer-friendly abstraction for interacting with decentralized data on the Web, the pursuit of it is highly valuable for both end-users that want to be in control of their data and its processing and businesses wanting to enable this at a low development cost.},
}

@article{Almeida2024,
  title = {Remixing and repurposing cultural heritage archives through a collaborative and AI-generated storytelling digital platform},
  author = {Almeida, Pedro and Teixeira, Ana and Velhinho, Ana and Raposo, Rui and Silva, Telmo and Pedro, Luis},
  year = {2024},
  pages = {100–104},
  doi = {10.1145/3672406.3672419},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3672406.3672419},
  abstract = {With the digitization of archives, fragments of cultural heritage are migrating to dedicated platforms and made available in curated experiences that rely on the curators’ work. Polariscope proposes a collaborative storytelling platform to share, visualize and co-create collective memories. It aims to gather citizens and institutions to promote rich digital experiences through interactive and narrative visualizations around meaningful cultural events. The project proposes a solution assisted by Artificial Intelligence (AI) able to generate automatic stories based on these archives. The development methodology and the architecture model of the integration of AI tools are presented in the paper. The paper also presents some promising preliminary results of user testing to achieve such a goal, based on the development of an interaction protocol with GPT 3.5 for the creation of stories.},
}

@article{Chai2023,
  title = {EAGER: Explainable Question Answering Using Knowledge Graphs},
  author = {Chai, Andrew and Vezvaei, Alireza and Golab, Lukasz and Kargar, Mehdi and Srivastava, Divesh and Szlichta, Jaroslaw and Zihayat, Morteza},
  year = {2023},
  doi = {10.1145/3594778.3594877},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3594778.3594877},
  abstract = {We present EAGER: a tool for answering questions expressed in natural language. Core to EAGER is a modular pipeline for generating a knowledge graph from raw text without human intervention. Notably, EAGER uses the knowledge graph to answer questions and to explain the reasoning behind the derivation of answers. Our demonstration will showcase both the automated knowledge graph generation pipeline and the explainable question answering functionality. Lastly, we outline open problems and directions for future work.},
}

@article{Kansal2024,
  title = {Implications of Privacy Regulations on Video Surveillance Systems},
  author = {Kansal, Kajal and Wong, Yongkang and Kankanhalli, Mohan S.},
  year = {2024},
  journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
  doi = {10.1145/3706108},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3706108},
  abstract = {Advanced video surveillance systems (VSS), which collect information of every individual who passes through a surveilled area, have become ubiquitous due to its utility for security. However, such proactive monitoring threatens the individual's privacy due to the public's lack of control over personal data. Additionally, individuals or organizations may unethically misuse VSS for other purposes (e.g. individual profiling and unwarranted monitoring). To safeguard individual privacy, various governments have introduced mandatory information privacy regulations (e.g. GDPR, PDPA, and CCPA) to provide extensive guidelines for the purpose of achieving identity confidentiality. Currently, there is a gap between the information privacy regulations and VSS. This paper aims to bridge this gap through four contributions. First, this paper conceptualizes VSS as comprising various data stages based on the idea of data life cycle and studies the implications of existing regulations on VSS. Second, we conducted a survey in ASEAN and European regions to understand the public perception of data risks at each data stage. Third, we review existing privacy-enhancing technologies and its relation to each data stage. Finally, we discuss open research problems in order to realize privacy-aware VSS.},
}

@article{Chhabria2025,
  title = {Invited: Toward an ML EDA Commons: Establishing Standards, Accessibility, and Reproducibility in ML-driven EDA Research},
  author = {Chhabria, Vidya A. and Hu, Jiang and Kahng, Andrew B. and Sapatnekar, Sachin S.},
  year = {2025},
  pages = {93–101},
  doi = {10.1145/3698364.3709131},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3698364.3709131},
  abstract = {Machine learning (ML) is transforming electronic design automation (EDA), offering innovative solutions for designing and optimizing integrated circuits (ICs). However, the field faces significant challenges in standardization, accessibility, and reproducibility, limiting the impact of ML-driven EDA (ML EDA) research. To address these barriers, this paper presents a vision for an ML EDA Commons, a collaborative open ecosystem designed to unify the community and drive progress through establishing standards, shared resources, and stakeholder-based governance. The ML EDA Commons focuses on three objectives: (1) Maturing existing EDA infrastructure to support ML EDA research; (2) Establishing standards for benchmarks, metrics, and data quality and formats for consistent evaluation via governance that includes key stakeholders; and (3) Improving accessibility and reproducibility by providing open datasets, tools, models, and workflows with cloud computing resources, to lower barriers to ML EDA research and promote robust research practices via artifact evaluations, canonical evaluators, and integration pipelines. Inspired by successes of ML and MLCommons, the ML EDA Commons aims to catalyze transparency and sustainability in ML EDA research.},
}

@article{Farzana2024,
  title = {Report on the 1st International Workshop on Open Web Search (WOWS 2024) at ECIR 2024},
  author = {Farzana, Sheikh Mastura and Fr\"{o}be, Maik and Granitzer, Michael and Hendriksen, Gijs and Hiemstra, Djoerd and Potthast, Martin and de Vries, Arjen P. and Zerhoudi, Saber},
  year = {2024},
  journal = {SIGIR Forum},
  volume = {58},
  pages = {1–13},
  doi = {10.1145/3687273.3687290},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3687273.3687290},
  abstract = {The first International Workshop on Open Web Search (WOWS) was held on Thursday, March 28th, at ECIR 2024 in Glasgow, UK. The full-day workshop had two calls for contributions: the first call aimed at scientific contributions to building, operating, and evaluating search engines cooperatively and the cooperative use of the web as a resource for researchers and innovators. The second call for implementations of retrieval components aimed to gain practical experience with joint, cooperative evaluation of search engines and their components. In total, 2 papers were accepted for the first call, and 11 software components were submitted for the second. The workshop ended with breakout sessions on how the Open-WebSearch.eu project can incorporate collaborative evaluations and a hub of search engines.Date: 28 March 2024.Website: https://opensearchfoundation.org/wows2024/.},
}

@article{Sebastian2024,
  title = {Emerging Technologies in Global South Classrooms: Teachers Imagining Future of Education},
  author = {Sebastian, Priyanka and Sharma, Sumita and Iivari, Netta and Kinnula, Marianne and Monga, Charu and Verma, Deepanshu and Abbas, Muhammad Shahroz},
  year = {2024},
  pages = {234–247},
  doi = {10.1145/3666094.3666109},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3666094.3666109},
  abstract = {Emerging technologies, including artificial intelligence, robots, and virtual reality, are reshaping our world, and driving educational transformations globally, revolutionizing traditional learning. Existing research has pointed out the need to address emerging technologies as part of computational empowerment of children. However, there is little research of computational empowerment of children from teachers’ perspective so far. Based on data from workshop with public school teachers in India, we advocate for an organized integration of smart technologies with an ethical emphasis in the curriculum. Furthermore, we delve into educators' perspectives, exploring their aspirations, needs, challenges, and concerns, offering insights into the future of education.},
}

@article{Becker2023,
  title = {Programming Is Hard - Or at Least It Used to Be: Educational Opportunities and Challenges of AI Code Generation},
  author = {Becker, Brett A. and Denny, Paul and Finnie-Ansley, James and Luxton-Reilly, Andrew and Prather, James and Santos, Eddie Antonio},
  year = {2023},
  pages = {500–506},
  doi = {10.1145/3545945.3569759},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3545945.3569759},
  abstract = {The introductory programming sequence has been the focus of much research in computing education. The recent advent of several viable and freely-available AI-driven code generation tools present several immediate opportunities and challenges in this domain. In this position paper we argue that the community needs to act quickly in deciding what possible opportunities can and should be leveraged and how, while also working on overcoming otherwise mitigating the possible challenges. Assuming that the effectiveness and proliferation of these tools will continue to progress rapidly, without quick, deliberate, and concerted efforts, educators will lose advantage in helping shape what opportunities come to be, and what challenges will endure. With this paper we aim to seed this discussion within the computing education community.},
}

@article{Balsebre2024,
  title = {City Foundation Models for Learning General Purpose Representations from OpenStreetMap},
  author = {Balsebre, Pasquale and Huang, Weiming and Cong, Gao and Li, Yi},
  year = {2024},
  pages = {87–97},
  doi = {10.1145/3627673.3679662},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3627673.3679662},
  abstract = {Pre-trained Foundation Models (PFMs) have ushered in a paradigm-shift in AI, due to their ability to learn general-purpose representations that can be readily employed in downstream tasks. While PFMs have been successfully adopted in various fields such as NLP and Computer Vision, their capacity in handling geospatial data remains limited. This can be attributed to the intrinsic heterogeneity of such data, which encompasses different types, including points, segments and regions, as well as multiple information modalities. The proliferation of Volunteered Geographic Information initiatives, like OpenStreetMap, unveils a promising opportunity to bridge this gap. In this paper, we present CityFM, a self-supervised framework to train a foundation model within a selected geographical area. CityFM relies solely on open data from OSM, and produces multimodal representations, incorporating spatial, visual, and textual information. We analyse the entity representations generated by our foundation models from a qualitative perspective, and conduct experiments on road, building, and region-level downstream tasks. In all the experiments, CityFM achieves performance superior to, or on par with, application-specific algorithms.},
}

@article{Wang2023_02,
  title = {Solo: Data Discovery Using Natural Language Questions Via A Self-Supervised Approach},
  author = {Wang, Qiming and Castro Fernandez, Raul},
  year = {2023},
  journal = {Proc. ACM Manag. Data},
  volume = {1},
  doi = {10.1145/3626756},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3626756},
  abstract = {Most deployed data discovery systems, such as Google Datasets, and open data portals only support keyword search. Keyword search is geared towards general audiences but limits the types of queries the systems can answer. We propose a new system that lets users write natural language questions directly. A major barrier to using this learned data discovery system is it needs expensive-to-collect training data, thus limiting its utility.In this paper, we introduce a self-supervised approach to assemble training datasets and train learned discovery systems without human intervention. It requires addressing several challenges, including the design of self-supervised strategies for data discovery, table representation strategies to feed to the models, and relevance models that work well with the synthetically generated questions. We combine all the above contributions into a system, Solo, that solves the problem end to end. The evaluation results demonstrate the new techniques outperform state-of-the-art approaches on well-known benchmarks. All in all, the technique is a stepping stone towards building learned discovery systems.},
}

@article{Haq2024,
  title = {History in Making: Political Campaigns in the Era of Artificial Intelligence-Generated Content},
  author = {Haq, Ehsan-Ul and Zhu, Yiming and Hui, Pan and Tyson, Gareth},
  year = {2024},
  pages = {1115–1118},
  doi = {10.1145/3589335.3652000},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3589335.3652000},
  abstract = {Web 2.0 provided impactful tools, based on user-generated content, for political campaigns and opinion engineering. However, in recent months, AI advances and the ease of access to AI-generated content (AIGC) have led to a paradigm shift in political participation by politicians and electorates alike. This paper aims to explore a historical analysis of this shift. We provide anecdotal evidence of new trends, potential impact, and challenges. We discuss the usage of AIGC in political campaigns, and how AIGC is used as a substitute for incarcerated politicians. Such a usage presents novel ways for leaders to reach the public and keep them politically active. However, AIGC also has risks when used for disinformation, such as DeepFake media and caller bots, to undermine and malign the opponents. On the other hand, the evidence shows that governments can nudge AIGC content by censoring Internet services. We also report challenges facing AIGC usage, such as model bias and hallucinations, along with a governance perspective on ethics and regulations.},
}

@article{Ali2025,
  title = {Privacy-preserved and Responsible Recommenders: From Conventional Defense to Federated Learning and Blockchain},
  author = {Ali, Waqar and Zhou, Xiangmin and Shao, Jie},
  year = {2025},
  journal = {ACM Comput. Surv.},
  volume = {57},
  doi = {10.1145/3708982},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3708982},
  abstract = {Recommender systems (RS) play an integral role in many online platforms. Exponential growth and potential commercial interests are raising significant concerns around privacy, security, fairness, and overall responsibility. The existing literature around responsible recommendation services is diverse and multidisciplinary. Most literature reviews cover a specific aspect or a single technology for responsible behavior, such as federated learning or blockchain. This study integrates relevant concepts across disciplines to provide a broader representation of the landscape. We review the latest advancements toward building privacy-preserved and responsible recommendation services for the e-commerce industry. The survey summarizes recent, high-impact works on diverse aspects and technologies that ensure responsible behavior in RS through an interconnected taxonomy. We contextualize potential privacy threats, practical significance, industrial expectations, and research remedies. From the technical viewpoint, we analyze conventional privacy defenses and provide an overview of emerging technologies including differential privacy, federated learning, and blockchain. The methods and concepts across technologies are linked based on their objectives, challenges, and future directions. In addition, we also develop an open source repository that summarizes a wide range of evaluation benchmarks, codebases, and toolkits to aid the further research. The survey offers a holistic perspective on this rapidly evolving landscape by synthesizing insights from both RS and responsible AI literature.},
}

@article{Mo2024,
  title = {From Information Seeking to Empowerment: Using Large Language Model Chatbot in Supporting Wheelchair Life in Low Resource Settings},
  author = {Mo, Wen and Singh, Aneesha and Holloway, Catherine},
  year = {2024},
  doi = {10.1145/3663548.3675609},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3663548.3675609},
  abstract = {To tackle the lack of wheelchair service information and training in low and middle-income countries (LMICs), we deployed Wheelpedia, a WhatsApp chatbot powered by a large language model (LLM) as a design probe for 2 months to concretely explore how it can support wheelchair users and professionals in Nigeria and Kenya. Through 18 semi-structured interviews and analysis of 471 messages, we focused on not only Wheelpedia's acceptability and usability but also how users orient themselves with the probe, integrate its information, and manage trust with it. The findings revealed participants' overwhelming enthusiasm towards the chatbot's potential in education, fostering empowerment, and reducing social stigma. We discuss challenges like users' difficulty in formulating questions, unfamiliarity with the concept of chatbots, and requests for image output. This paper contributes valuable insights into the design implications and research opportunities for deploying LLM chatbots in low-resourced settings with complex accessibility needs.},
}

@article{Feng2023,
  title = {Data and Resources Paper: A Multi-granularity Decade-Long Geo-Tagged Twitter Dataset for Spatial Computing},
  author = {Feng, Yunhe and Meng, Zexuan and Clemmer, Colton and Fan, Heng and Huang, Yan},
  year = {2023},
  doi = {10.1145/3589132.3625654},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3589132.3625654},
  abstract = {This paper presents a publicly accessible large-scale geo-tagged Twitter dataset, comprising 95.8 million tweets from 247 countries, spanning from Jan. 2012 to Dec. 2021. To systematically extract this dataset from over 57.18 TB of raw tweets, we employed parallel computing on a 40-node cluster with 480 CPU cores. Distinguishing it from most existing Twitter datasets, our dataset includes four-level granularity tweet locations, two-level granularity user profile locations, and tweet text languages, enabling personalized queries. To enhance the open accessibility of our dataset, we have designed an innovative interactive online query system (https://sigspatial.yunhefeng.me) and provided free-to-use JSON APIs (https://github.com/ResponsibleAILab/unt-geotweet-api) for customized queries to retrieve tweet IDs in tweet coordinate, tweet text-based location, and user location modes. Then users can use https://github.com/ResponsibleAILab/unt-tweet-rehydration to download complete tweet information. Furthermore, we have demonstrated the practical utility of our dataset through two applications: human movement modeling and geo-aware Large Language Model (LLM) tuning. Our geo-tagged Twitter dataset, along with the accompanying query system and APIs, contributes to the research community and opens up avenues for multidisciplinary investigations and the advancement of knowledge.},
}

@article{Murali2024,
  title = {AI-Assisted Code Authoring at Scale: Fine-Tuning, Deploying, and Mixed Methods Evaluation},
  author = {Murali, Vijayaraghavan and Maddila, Chandra and Ahmad, Imad and Bolin, Michael and Cheng, Daniel and Ghorbani, Negar and Fernandez, Renuka and Nagappan, Nachiappan and Rigby, Peter C.},
  year = {2024},
  journal = {Proc. ACM Softw. Eng.},
  volume = {1},
  doi = {10.1145/3643774},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3643774},
  abstract = {Generative LLMs have been shown to effectively power AI-based code authoring tools that can suggest entire statements or blocks of code during code authoring. In this paper we present CodeCompose, an AI-assisted code authoring tool developed and deployed at Meta internally. CodeCompose is based on the InCoder LLM that merges generative capabilities with bi-directionality. We have scaled up CodeCompose to serve tens of thousands of developers at Meta, across 9 programming languages and several coding surfaces. We present our experience in making design decisions about the model and system architecture for CodeCompose that addresses these challenges.         To release a LLM model at this scale, we needed to first ensure that it is sufficiently accurate. In a random sample of 20K source code files, depending on the language, we are able to reproduce hidden lines between 40% and 58% of the time, an improvement of 1.4\texttimes{} and 4.1\texttimes{} over a model trained only on public data.         We gradually rolled CodeCompose out to developers. At the time of this writing, 16K developers have used it with 8% of their code coming directly from CodeCompose.         To triangulate our numerical findings, we conduct a thematic analysis on the feedback from 70 developers. We find that 91.5% of the feedback is positive, with the most common themes being discovering APIs, dealing with boilerplate code, and accelerating coding. Meta continues to integrate this feedback into CodeCompose.},
}

@article{Deng2024_01,
  title = {Advances in Human Event Modeling: From Graph Neural Networks to Language Models},
  author = {Deng, Songgaojun and de Rijke, Maarten and Ning, Yue},
  year = {2024},
  pages = {6459–6469},
  doi = {10.1145/3637528.3671466},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3637528.3671466},
  abstract = {Human events such as hospital visits, protests, and epidemic outbreaks directly affect individuals, communities, and societies. These events are often influenced by factors such as economics, politics, and public policies of our society. The abundance of online data sources such as social networks, official news articles, and personal blogs chronicle societal events, facilitating the development of AI models for social science, public health care, and decision making. Human event modeling generally comprises both the forecasting stage, which estimates future events based on historical data, and interpretation, which seeks to identify influential factors of such events to understand their causative attributes. Recent achievements, fueled by deep learning and the availability of public data, have significantly advanced the field of human event modeling.This survey offers a systematic overview of deep learning technologies for forecasting and interpreting human events, with a primary focus on political events. We first introduce the existing challenges and background in this domain. We then present the problem formulation of event forecasting and interpretation. We investigate recent achievements in graph neural networks, owing to the prevalence of relational data and the efficacy of graph learning models. We also discuss the latest studies that utilize large language models for event reasoning. Lastly, we provide summaries of data resources, open challenges, and future research directions in the study of human event modeling.},
}

@article{Chen2024_05,
  title = {Enhancing Dataset Search with Compact Data Snippets},
  author = {Chen, Qiaosheng and Chen, Jiageng and Zhou, Xiao and Cheng, Gong},
  year = {2024},
  pages = {1093–1103},
  doi = {10.1145/3626772.3657837},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3626772.3657837},
  abstract = {In light of the growing availability and significance of open data, the problem of dataset search has attracted great attention in the field of information retrieval. Nevertheless, current metadata-based approaches have revealed shortcomings due to the low quality and availability of dataset metadata, while the magnitude and heterogeneity of actual data hindered the development of content-based solutions. To address these challenges, we propose to convert different formats of structured data into a unified form, from which we extract a compact data snippet that indicates the relevance of the whole data. Thanks to its compactness, we feed it into a dense reranker to improve search accuracy. We also convert it back to the original format to be presented for assisting users in relevance judgment. The effectiveness of our approach has been demonstrated by extensive experiments on two test collections for dataset search.},
}

@article{Wang2024_011,
  title = {A Roadmap for Software Testing in Open-Collaborative and AI-Powered Era},
  author = {Wang, Qing and Wang, Junjie and Li, Mingyang and Wang, Yawen and Liu, Zhe},
  year = {2024},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  doi = {10.1145/3709355},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3709355},
  abstract = {Internet technology has given rise to an open-collaborative software development paradigm, necessitating the open-collaborative schema to software testing. It enables diverse and globally distributed contributions, but also presents significant challenges to efficient testing processes, coordination among personnel, and management of testing artifacts. At the same time, advancements in artificial intelligence (AI) have enhanced testing capabilities and enabling automation, while also introducing new testing needs and unique challenges for AI-based systems. In this context, this paper explores software testing in the open-collaborative and AI-powered era, focusing on the interrelated dimensions of process, personnel, and technology. Among them, process involves managing testing workflows and artifacts to improve efficiency, personnel emphasizes the role of individuals in ensuring testing quality through collaboration and contributions, while technology refers to AI methods that enhance testing capabilities and address challenges in AI-based systems. Furthermore, we delve into the challenges and opportunities arising from emerging technologies such as large language models (LLMs) and the AI model-centric development paradigm.},
}

@article{Li2024_07,
  title = {TEESlice: Protecting Sensitive Neural Network Models in Trusted Execution Environments When Attackers have Pre-Trained Models},
  author = {Li, Ding and Zhang, Ziqi and Yao, Mengyu and Cai, Yifeng and Guo, Yao and Chen, Xiangqun},
  year = {2024},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  doi = {10.1145/3707453},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3707453},
  abstract = {Trusted Execution Environments (TEE) are used to safeguard on-device models. However, directly employing TEEs to secure the entire DNN model is challenging due to the limited computational speed. Utilizing GPU can accelerate DNN's computation speed but commercial widely-available GPUs usually lack security protection. To this end, scholars introduce TEE-shielded DNN partition (TSDP), a method that protects privacy-sensitive weights within TEEs and offloads insensitive weights to GPUs. Nevertheless, current methods do not consider the presence of a knowledgeable adversary who can access abundant publicly available pre-trained models and datasets. This paper investigates the security of existing methods against such a knowledgeable adversary and reveals their inability to fulfill their security promises. Consequently, we introduce a novel partition before training strategy, which effectively separates privacy-sensitive weights from other components of the model. Our evaluation demonstrates that our approach can offer full model protection with a computational cost reduced by a factor of 10. In addition to traditional CNN models, we also demonstrate the scalability to large language models. Our approach can compress the private functionalities of the large language model to lightweight slices and achieve the same level of protection as the shielding-whole-model baseline.},
}

@article{Gilman2024,
  title = {Addressing Data Challenges to Drive the Transformation of Smart Cities},
  author = {Gilman, Ekaterina and Bugiotti, Francesca and Khalid, Ahmed and Mehmood, Hassan and Kostakos, Panos and Tuovinen, Lauri and Ylipulli, Johanna and Su, Xiang and Ferreira, Denzil},
  year = {2024},
  journal = {ACM Trans. Intell. Syst. Technol.},
  volume = {15},
  doi = {10.1145/3663482},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3663482},
  abstract = {Cities serve as vital hubs of economic activity and knowledge generation and dissemination. As such, cities bear a significant responsibility to uphold environmental protection measures while promoting the welfare and living comfort of their residents. There are diverse views on the development of smart cities, from integrating Information and Communication Technologies into urban environments for better operational decisions to supporting sustainability, wealth, and comfort of people. However, for all these cases, data are the key ingredient and enabler for the vision and realization of smart cities. This article explores the challenges associated with smart city data. We start with gaining an understanding of the concept of a smart city, how to measure that the city is a smart one, and what architectures and platforms exist to develop one. Afterwards, we research the challenges associated with the data of the cities, including availability, heterogeneity, management, analysis, privacy, and security. Finally, we discuss ethical issues. This article aims to serve as a “one-stop shop” covering data-related issues of smart cities with references for diving deeper into particular topics of interest.},
}

@article{Blanco Lambruschini2024,
  title = {Transforming Unstructured Sensitive Information into Structured Knowledge},
  author = {Blanco Lambruschini, Braulio C. and Brorsson, Mats},
  year = {2024},
  pages = {831–838},
  doi = {10.1145/3677052.3698602},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3677052.3698602},
  abstract = {Information is crucial in today’s context, yet less than 20% of companies utilize their unstructured data due to its complexity. Information Extraction (IE) is vital for effective data use, but current IE models face four major issues. First, they often provide limited information, such as a simple entity-attribute relation. Second, they struggle with multiple languages. Models like GPT, Mistral, and Llama3 show promise but face a third issue: output reliability due to hallucinations. Fourth, there is a challenge in reducing sensitive data leakage after fine-tuning models. This study introduces an enhanced approach for fine-tuning GPT-based models, designed to extract and assess information involving multiple entities and attributes, performing both multientity extraction (MEE) and multirelation extraction (MRE), and presenting results in a JSON format. Our methodology evaluates the impact of using synthetic data for fine-tuning to ensure reliable outcomes. Applied to legal documents from the Luxembourg Business Registers (LBR), our findings show that replacing sensitive data with synthetic data significantly improves the fine-tuning of Llama3-based models, though not for Mistral-based models. Our top models outperform Mistral in various scenarios, requiring only 500 samples for fine-tuning and running efficiently on modest servers. This approach is suitable for multilingual Information Extraction in any domain.},
}

@article{Gomez-Vazquez2024,
  title = {Automatic Generation of Conversational Interfaces for Tabular Data Analysis},
  author = {Gomez-Vazquez, Marcos and Cabot, Jordi and Claris\'{o}, Robert},
  year = {2024},
  doi = {10.1145/3640794.3665577},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3640794.3665577},
  abstract = {Tabular data is the most common format to publish and exchange structured data online. A clear example is the growing number of open data portals published by public administrations. However, exploitation of these data sources is currently limited to technical people able to programmatically manipulate and digest such data. As an alternative, we propose the use of chatbots to offer a conversational interface to facilitate the exploration of tabular data sources, including support for data analytics questions that are responded via charts rendered by the chatbot. Moreover, our chatbots are automatically generated from the data source itself thanks to the instantiation of a configurable collection of conversation patterns matched to the chatbot intents and entities.},
}

@article{Jernite2022,
  title = {Data Governance in the Age of Large-Scale Data-Driven Language Technology},
  author = {Jernite, Yacine and Nguyen, Huu and Biderman, Stella and Rogers, Anna and Masoud, Maraim and Danchev, Valentin and Tan, Samson and Luccioni, Alexandra Sasha and Subramani, Nishant and Johnson, Isaac and Dupont, Gerard and Dodge, Jesse and Lo, Kyle and Talat, Zeerak and Radev, Dragomir and Gokaslan, Aaron and Nikpoor, Somaieh and Henderson, Peter and Bommasani, Rishi and Mitchell, Margaret},
  year = {2022},
  pages = {2206–2222},
  doi = {10.1145/3531146.3534637},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3531146.3534637},
  abstract = {The recent emergence and adoption of Machine Learning technology, and specifically of Large Language Models, has drawn attention to the need for systematic and transparent management of language data. This work proposes an approach to global language data governance that attempts to organize data management amongst stakeholders, values, and rights. Our proposal is informed by prior work on distributed governance that accounts for human values and grounded by an international research collaboration that brings together researchers and practitioners from 60 countries. The framework we present is a multi-party international governance structure focused on language data, and incorporating technical and organizational tools needed to support its work.},
}

@article{Wu2024_04,
  title = {Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision},
  author = {Wu, Shengguang and Chen, Zhenglun and Su, Qi},
  year = {2024},
  pages = {2719–2728},
  doi = {10.1145/3664647.3681533},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3664647.3681533},
  abstract = {Ancient artifacts are an important medium for cultural preservation and restoration. However, many physical copies of artifacts are either damaged or lost, leaving a blank space in archaeological and historical studies that calls for techniques to re-visualize these artifacts. Despite the significant advancements in open-domain text-to-image synthesis, existing approaches fail to capture the important domain knowledge presented in the textual descriptions of artifacts, resulting in errors in recreated images such as incorrect shapes and patterns. In this paper, we propose a novel knowledge-aware artifact image synthesis approach that brings lost historical objects accurately into their visual forms. We use a pretrained diffusion model as backbone and introduce three key techniques to enhance the text-to-image generation framework: 1) we construct prompts with explicit archaeological knowledge elicited from large language models (LLMs); 2) we incorporate additional textual guidance to correlated historical expertise in a contrastive manner; 3) we introduce further visual-semantic constraints on edge and perceptual features that enable our model to learn more intricate visual details of the artifacts. Compared to existing approaches, our proposed model produces higher-quality artifact images that align better with the implicit details and historical knowledge contained within written documents, thus achieving significant improvements both across automatic metrics and in human evaluation.},
}

@article{Wu2024_05,
  title = {FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model},
  author = {Wu, Feijie and Li, Zitao and Li, Yaliang and Ding, Bolin and Gao, Jing},
  year = {2024},
  pages = {3345–3355},
  doi = {10.1145/3637528.3671897},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3637528.3671897},
  abstract = {Large language models (LLMs) show amazing performance on many domain-specific tasks after fine-tuning with some appropriate data. However, many domain-specific data are privately distributed across multiple owners. Thus, this dilemma raises the interest in how to perform LLM fine-tuning in federated learning (FL). However, confronted with limited computation and communication capacities, FL clients struggle to fine-tune an LLM effectively. To this end, we introduce FedBiOT, a resource-efficient LLM fine-tuning approach to FL. Specifically, our method involves the server generating a compressed LLM and aligning its performance with the full model. Subsequently, the clients fine-tune a lightweight yet important part of the compressed model, referred to as an adapter. Notice that as the server has no access to the private data owned by the clients, the data used for alignment by the server has a different distribution from the one used for fine-tuning by clients. We formulate the problem into a bi-level optimization problem to minimize the negative effect of data discrepancy and derive the updating rules for the server and clients. We conduct extensive experiments on LLaMA-2, empirically showing that the adapter has exceptional performance when reintegrated into the global LLM. The results also indicate that the proposed FedBiOT significantly reduces resource consumption compared to existing benchmarks, all while achieving comparable performance levels.},
}

@article{Zhang2025_08,
  title = {Enhancing the Travel Experience for People with Visual Impairments through Multimodal Interaction: NaviGPT, A Real-Time AI-Driven Mobile Navigation System},
  author = {Zhang, He and Falletta, Nicholas J. and Xie, Jingyi and Yu, Rui and Lee, Sooyeon and Billah, Syed Masum and Carroll, John M.},
  year = {2025},
  pages = {29–35},
  doi = {10.1145/3688828.3699636},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3688828.3699636},
  abstract = {Assistive technologies for people with visual impairments (PVI) have made significant advancements, particularly with the integration of artificial intelligence (AI) and real-time sensor technologies. However, current solutions often require PVI to switch between multiple apps and tools for tasks like image recognition, navigation, and obstacle detection, which can hinder a seamless and efficient user experience. In this paper, we present NaviGPT, a high-fidelity prototype that integrates LiDAR-based obstacle detection, vibration feedback, and large language model (LLM) responses to provide a comprehensive and real-time navigation aid for PVI. Unlike existing applications such as Be My AI and Seeing AI, NaviGPT combines image recognition and contextual navigation guidance into a single system, offering continuous feedback on the user’s surroundings without the need for app-switching. Meanwhile, NaviGPT compensates for the response delays of LLM by using location and sensor data, aiming to provide practical and efficient navigation support for PVI in dynamic environments.},
}

@article{Milic-Frayling2023,
  title = {On the Cusp: Computing Thrills and Perils and Professional Awakening},
  author = {Milic-Frayling, Natasa},
  year = {2023},
  journal = {Proc. VLDB Endow.},
  volume = {16},
  pages = {4152–4159},
  doi = {10.14778/3611540.3611640},
  publisher = {VLDB Endowment},
  url = {https://doi.org/10.14778/3611540.3611640},
  abstract = {Over the past eight decades, computer science has advanced as a field, and the computing profession has matured by establishing professional codes of conduct, fostering best practices, and establishing industry standards to support the proliferation of technologies and services. Research and applications of digital computation continue to change all aspects of human endeavor through new waves of innovation. While it is clear that different research advances fuel innovation, the ways they come together to make an impact vary. In contrast to highly regulated sectors such as pharma, medicine and law, the process of transforming research into widely deployed technologies is not regulated. We reflect on collective practices, from discovery by scientists and engineers to market delivery by entrepreneurs, industry leaders, and practitioners. We consider ecosystem changes that are required to sustain the transformational effects of new technologies and enable new practices to take root. Every such transformation ruptures in the existing socio-technical fabric and requires a concerted effort to remedy this through effective policies and regulations. Computing experts are involved in all phases and must match the transformational power of their innovation with the highest standard of professional conduct. We highlight the principles of responsible innovation and discuss three waves of digital innovation. We use wide and uncontrolled generative AI deployments to illustrate risks from the implosion of digital media due to contamination of digital records, removal of human agency, and risk to an individual's personhood.},
}

@article{Wen2024,
  title = {AutoDroid: LLM-powered Task Automation in Android},
  author = {Wen, Hao and Li, Yuanchun and Liu, Guohong and Zhao, Shanhui and Yu, Tao and Li, Toby Jia-Jun and Jiang, Shiqi and Liu, Yunhao and Zhang, Yaqin and Liu, Yunxin},
  year = {2024},
  pages = {543–557},
  doi = {10.1145/3636534.3649379},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3636534.3649379},
  abstract = {Mobile task automation is an attractive technique that aims to enable voice-based hands-free user interaction with smartphones. However, existing approaches suffer from poor scalability due to the limited language understanding ability and the non-trivial manual efforts required from developers or endusers. The recent advance of large language models (LLMs) in language understanding and reasoning inspires us to rethink the problem from a model-centric perspective, where task preparation, comprehension, and execution are handled by a unified language model. In this work, we introduce AutoDroid, a mobile task automation system capable of handling arbitrary tasks on any Android application without manual efforts. The key insight is to combine the commonsense knowledge of LLMs and domain-specific knowledge of apps through automated dynamic analysis. The main components include a functionality-aware UI representation method that bridges the UI with the LLM, exploration-based memory injection techniques that augment the app-specific domain knowledge of LLM, and a multi-granularity query optimization module that reduces the cost of model inference. We integrate AutoDroid with off-the-shelf LLMs including online GPT-4/GPT-3.5 and on-device Vicuna, and evaluate its performance on a new benchmark for memory-augmented Android task automation with 158 common tasks. The results demonstrated that AutoDroid is able to precisely generate actions with an accuracy of 90.9%, and complete tasks with a success rate of 71.3%, outperforming the GPT-4-powered baselines by 36.4% and 39.7%.},
}

@article{Isagah2024,
  title = {Artificial Intelligence Readiness in Africa: Status Quo and Future Research},
  author = {Isagah, Tupokigwe and Ben Dhaou, Soumaya I.},
  year = {2024},
  pages = {430–437},
  doi = {10.1145/3680127.3680199},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3680127.3680199},
  abstract = {Artificial Intelligence (AI) technologies have the potential to accelerate economic growth and promote sustainable development in developing countries. Particularly in Africa, the technology is expected to solve societal challenges, leapfrog, and promote sustainable communities. The literature presents examples of AI use cases in Africa to exemplify the opportunities, benefits, challenges, and risks of AI. Also, available reports highlight similar narratives with recommendations to improve the status quo. While such recommendations are essential, assessing what actions have already been implemented toward AI adoption is equally important. Still, more research is needed to showcase a holistic view of efforts taken by African countries to prepare for planning and implementing AI responsibly. This paper bridges the gap by examining the readiness of AI adoption in African countries from the literature using the Technology-Organization-Environment (TOE) framework. Findings reveal the need for further initiatives to realize the positive outcomes of AI technologies. The paper concludes by proposing the research areas that require further exploration from the context of African countries.},
}

@article{Moayeri2024,
  title = {WorldBench: Quantifying Geographic Disparities in LLM Factual Recall},
  author = {Moayeri, Mazda and Tabassi, Elham and Feizi, Soheil},
  year = {2024},
  pages = {1211–1228},
  doi = {10.1145/3630106.3658967},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3630106.3658967},
  abstract = {As large language models (LLMs) continue to improve and gain popularity, some may use the models to recall facts, despite well documented limitations with LLM factuality. Towards ensuring that models work reliably for all, we seek to uncover if geographic disparities emerge when asking an LLM the same question about different countries. To this end, we present WorldBench, a dynamic and flexible benchmark composed of per-country data from the World Bank. In extensive experiments on state of the art open and closed source models, including GPT-4, Gemini, Llama-2, and Vicuna, to name a few, we find significant biases based on region and income level. For example, error rates are 1.5 times higher for countries from Sub-Saharan Africa compared to North American countries. We observe these disparities to be consistent over 20 LLMs and 11 individual World Bank indicators (i.e. specific statistics, such as population or CO2 emissions). WorldBench also enables automatic detection of citation hallucination, where models cite the World Bank itself while providing false statistics, and a manner to assess when an LLM’s stored facts begin to go out of date. We hope our benchmark will draw attention to geographic disparities in existing LLMs and facilitate the remedying of these biases.},
}

@article{Tahaei2025,
  title = {Surveys Considered Harmful? Reflecting on the Use of Surveys in AI Research, Development, and Governance},
  author = {Tahaei, Mohammad and Wilkinson, Daricia and Frik, Alisa and Muller, Michael and Abu-Salma, Ruba and Wilcox, Lauren},
  year = {2025},
  pages = {1416–1433},
  publisher = {AAAI Press},
  abstract = {Calls for engagement with the public in Artificial Intelligence (AI) research, development, and governance are increasing, leading to the use of surveys to capture people's values, perceptions, and experiences related to AI. In this paper, we critically examine the state of human participant surveys associated with these topics. Through both a reflexive analysis of a survey pilot spanning six countries and a systematic literature review of 44 papers featuring public surveys related to AI, we explore prominent perspectives and methodological nuances associated with surveys to date. We find that public surveys on AI topics are vulnerable to specific Western knowledge, values, and assumptions in their design, including in their positioning of ethical concepts and societal values, lack sufficient critical discourse surrounding deployment strategies, and demonstrate inconsistent forms of transparency in their reporting. Based on our findings, we distill provocations and heuristic questions for our community, to recognize the limitations of surveys for meeting the goals of engagement, and to cultivate shared principles to design, deploy, and interpret surveys cautiously and responsibly.},
}

@article{Xu2024_02,
  title = {What Makes It Mine? Exploring Psychological Ownership over Human-AI Co-Creations},
  author = {Xu, Yuxin and Cheng, Mengqiu and Kuzminykh, Anastasia},
  year = {2024},
  doi = {10.1145/3670947.3670974},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3670947.3670974},
  abstract = {As generative AI (GenAI) rapidly evolves, human-AI collaboration emerges as a prevalent new working style. However, within this collaborative pipeline, multiple stakeholders are involved besides the user and the system itself, raising controversy around ownership over co-creations. In this paper, we explored everyday users’ sense of ownership toward human-AI co-creation, aiming to provide insights for practitioners on future GenAI design to enhance user experience. We identify three primary factors associated with people’s perception of psychological ownership towards human-AI co-creation and systematically analyze individuals’ approaches to assessing these factors. The findings serve to inform strategies for facilitating an appropriate sense of ownership for productive and safe usage of GenAI tools.},
}

@article{Chen2025_02,
  title = {An Empirical Study on Challenges for LLM Application Developers},
  author = {Chen, Xiang and Gao, Chaoyang and Chen, Chunyang and Zhang, Guangbei and Liu, Yong},
  year = {2025},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  doi = {10.1145/3715007},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3715007},
  abstract = {In recent years, large language models (LLMs) have seen rapid advancements, significantly impacting various fields such as computer vision, natural language processing, and software engineering. These LLMs, exemplified by OpenAI’s ChatGPT, have revolutionized the way we approach language understanding and generation tasks. However, in contrast to traditional software development practices, LLM development introduces new challenges for AI developers in design, implementation, and deployment. These challenges span different areas (such as prompts, APIs, and plugins), requiring developers to navigate unique methodologies and considerations specific to LLM application development.Despite the profound influence of LLMs, to the best of our knowledge, these challenges have not been thoroughly investigated in previous empirical studies. To fill this gap, we present the first comprehensive study on understanding the challenges faced by LLM developers. Specifically, we crawl and analyze 29,057 relevant questions from a popular OpenAI developer forum. We first examine their popularity and difficulty. After manually analyzing 2,364 sampled questions, we construct a taxonomy of challenges faced by LLM developers. Based on this taxonomy, we summarize a set of findings and actionable implications for LLM-related stakeholders, including developers and providers (especially the OpenAI organization).},
}

@article{Zhao2024_04,
  title = {LLM App Store Analysis: A Vision and Roadmap},
  author = {Zhao, Yanjie and Hou, Xinyi and Wang, Shenao and Wang, Haoyu},
  year = {2024},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  doi = {10.1145/3708530},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3708530},
  abstract = {The rapid growth and popularity of large language model (LLM) app stores have created new opportunities and challenges for researchers, developers, users, and app store managers. As the LLM app ecosystem continues to evolve, it is crucial to understand the current landscape and identify potential areas for future research and development. This paper presents a forward-looking analysis of LLM app stores, focusing on key aspects such as data mining, security risk identification, development assistance, and market dynamics. Our comprehensive examination extends to the intricate relationships between various stakeholders and the technological advancements driving the ecosystem’s growth. We explore the ethical considerations and potential societal impacts of widespread LLM app adoption, highlighting the need for responsible innovation and governance frameworks. By examining these aspects, we aim to provide a vision for future research directions and highlight the importance of collaboration among stakeholders to address the challenges and opportunities within the LLM app ecosystem. The insights and recommendations provided in this paper serve as a foundation for driving innovation, ensuring responsible development, and creating a thriving, user-centric LLM app landscape.},
}

@article{Gao2023,
  title = {Application of large language model in intelligent Q&amp;A of digital government},
  author = {Gao, Shangsheng and Gao, Li and Li, Qi and Xu, Jianjun},
  year = {2023},
  pages = {24–27},
  doi = {10.1145/3605801.3605806},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3605801.3605806},
  abstract = {LLM (Large Language Model) is developing rapidly today, and it is very important to use LLM to help existing question answering systems improve. The government question answering system is of great value in improving government administrative efficiency. Existing intelligent questions and answers mostly use the combination of keyword matching and machine learning. But keyword matching is difficult to understand complex and multi-round dialogue questions, resulting in limited quality of reply content. Based on machine learning method, it has further improved the understanding of semantics, but because the understanding of words in the field of government affairs is too difficult and professional; and the semantic recognition of colloquial questions is difficult, the performance is not even as good as the question answering system based on keyword matching. This paper uses the large language model as a tool to help understand user questions, and integrates it into the existing government question answering system. On the premise of effectively utilizing the advantages of the large language model, it avoids its defects. And it is demonstrated by experiments that the above system has a great improvement compared with the previous method.},
}

@article{Fox2024,
  title = {A Generative Benchmark Creation Framework for Detecting Common Data Table Versions},
  author = {Fox, Daniel C. and Khatiwada, Aamod and Shraga, Roee},
  year = {2024},
  pages = {5365–5369},
  doi = {10.1145/3627673.3679157},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3627673.3679157},
  abstract = {Multiple versions of the same dataset can exist in a data repository (e.g., data warehouses, data lakes, etc.), mainly because of the interactive and collaborative nature of data science. Data creators generally update existing datasets and upload them as new datasets to data repositories without proper documentation. Identifying such versions helps in data management, data governance, and making better decisions using data. However, there is a dearth of benchmarks to develop and evaluate data versioning techniques, which requires a lot of human effort. Thus, this work introduces a novel framework to generate benchmarks for data versioning using Generative AI (specifically Large Language Models). The proposed framework offers properties that existing benchmarks do not have, including proper documentation, version lineage, and complex transformations generated by an LLM. We also share VerLLM-v1, the first version of the benchmark that features these properties, and compare it to existing benchmarks.},
}

@article{Barcellos2024,
  title = {Exploring Interpretability in Open Government Data with ChatGPT},
  author = {Barcellos, Raissa and Bernardini, Flavia and Zuiderwijk, Anneke and Viterbo, Jose},
  year = {2024},
  pages = {186–195},
  doi = {10.1145/3657054.3657079},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3657054.3657079},
  abstract = {The global initiative supporting open government data (OGD) has witnessed significant strides in the last decade. This study delves into the prospective integration of Artificial Intelligence (AI) with Hippolyta, a framework meticulously crafted to amplify the interpretability of government data. The aim is to scrutinize the viability of this integration, conducting a technical investigation in the realms of open government data and artificial intelligence. In contributing to the expansive field of OGD, this research focuses on elucidating the interpretability of data originating from governmental sources. Through an exploration of the technical feasibility surrounding the fusion of AI with Hippolyta, we aim to pave the path for advancements, fostering heightened interpretability and overarching enhancements in the understanding of government data.},
}

@article{Dom\'{\i}nguez Hern\'{a}ndez2024,
  title = {Mapping the individual, social and biospheric impacts of Foundation Models},
  author = {Dom\'{\i}nguez Hern\'{a}ndez, Andr\'{e}s and Krishna, Shyam and Perini, Antonella Maia and Katell, Michael and Bennett, SJ and Borda, Ann and Hashem, Youmna and Hadjiloizou, Semeli and Mahomed, Sabeehah and Jayadeva, Smera and Aitken, Mhairi and Leslie, David},
  year = {2024},
  pages = {776–796},
  doi = {10.1145/3630106.3658939},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3630106.3658939},
  abstract = {Responding to the rapid roll-out and large-scale commercialization of foundation models, large language models, and generative AI, an emerging body of work is shedding light on the myriad impacts these technologies are having across society. Such research is expansive, ranging from the production of discriminatory, fake and toxic outputs, and privacy and copyright violations, to the unjust extraction of labor and natural resources. The same has not been the case in some of the most prominent AI governance initiatives in the global north like the UK’s AI Safety Summit and the G7’s Hiroshima process, which have influenced much of the international dialogue around AI governance. Despite the wealth of cautionary tales and evidence of algorithmic harm, there has been an ongoing over-emphasis within the AI governance discourse on technical matters of safety and global catastrophic or existential risks. This narrowed focus has tended to draw attention away from very pressing social and ethical challenges posed by the current brute-force industrialization of AI applications. To address such a visibility gap between real-world consequences and speculative risks, this paper offers a critical framework to account for the social, political, and environmental dimensions of foundation models and generative AI. Drawing on a review of the literature on the harms and risks of foundations models, and insights from critical data studies, science and technology studies, and environmental justice scholarship, we identify 14 categories of risks and harms and map them according to their individual, social, and biospheric impacts. We argue that this novel typology offers an integrative perspective to address the most urgent negative impacts of foundation models and their downstream applications. We conclude with recommendations on how this typology could be used to inform technical and normative interventions to advance responsible AI.},
}

@article{Lin2024_01,
  title = {Applications of AI in Digital Governance Services for Local Taxes- a case of the Local Tax Bureau of Taichung City Government},
  author = {Lin, Kun-Hsien and Shen, Cheng-An and Cheng, Su-Chuan},
  year = {2024},
  pages = {6–18},
  doi = {10.1145/3657054.3657056},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3657054.3657056},
  abstract = {Since the onset of the COVID-19 pandemic, all Countries have been actively promoting adaptive governance in the digital government. Initiatives such as remote work, online application processes, and telemedicine have seen significant advancement. Pre-trained AI chatbots can be pre-equipped with specialized knowledge relevant to government agencies and can seamlessly convert non-structured verbal queries from the public into correctly formulated questions with standardized answers. This capability ensures that individuals no longer face difficulties in obtaining the desired responses due to variations in the order or inadequacy of question descriptions.This study has three main research objectives. First purpose is to enhance the precision of pre-trained AI chatbot responses through the utilization of normalized training corpora methods. Second purpose is to construct a functional robot capable of answering tax-related questions and providing services. The third purpose is to evaluate the original training corpora and system responses of the robot through practical inquiries in this study.},
}

@article{Degbelo2025,
  title = {Prolegomena to a Description Language for GenAI Tools in Cities},
  author = {Degbelo, Auriol},
  year = {2025},
  journal = {Digit. Gov.: Res. Pract.},
  volume = {6},
  doi = {10.1145/3652952},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3652952},
  abstract = {The potential of generative AI has been recently demonstrated through different applications. The open government and smart city initiatives can leverage this potential to produce innovations that improve government workflows and the lives of citizens. This commentary makes the case for a description language enabling the structured documentation of these upcoming innovations. The description language would facilitate the communication between governments, citizens, and innovators. The key elements of the description language are briefly sketched and its usefulness is shown by the generation of ideas for GenAI tools related to interactive maps in cities.},
}

@article{Mattis2024,
  title = {Faster Feedback with AI? A Test Prioritization Study},
  author = {Mattis, Toni and B\"{o}hme, Lukas and Krebs, Eva and Rinard, Martin C. and Hirschfeld, Robert},
  year = {2024},
  pages = {32–40},
  doi = {10.1145/3660829.3660837},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3660829.3660837},
  abstract = {Feedback during programming is desirable, but its usefulness depends on immediacy and relevance to the task. Unit and regression testing are practices to ensure programmers can obtain feedback on their changes; however, running a large test suite is rarely fast, and only a few results are relevant. Identifying tests relevant to a change can help programmers in two ways: upcoming issues can be detected earlier during programming, and relevant tests can serve as examples to help programmers understand the code they are editing. In this work, we describe an approach to evaluate how well large language models (LLMs) and embedding models can judge the relevance of a test to a change. We construct a dataset by applying faulty variations of real-world code changes and measuring whether the model could nominate the failing tests beforehand. We found that, while embedding models perform best on such a task, even simple information retrieval models are surprisingly competitive. In contrast, pre-trained LLMs are of limited use as they focus on confounding aspects like coding styles. We argue that the high computational cost of AI models is not always justified, and tool developers should also consider non-AI models for code-related retrieval and recommendation tasks. Lastly, we generalize from unit tests to live examples and outline how our approach can benefit live programming environments.},
}

@article{Ganapati2024,
  title = {Public Value Principles for Secure and Trusted AI},
  author = {Ganapati, Sukumar and Desouza, Kevin},
  year = {2024},
  pages = {251–257},
  doi = {10.1145/3657054.3657086},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3657054.3657086},
  abstract = {The objective of this paper is to establish the fundamental public value principles that should govern safe and trusted artificial intelligence (AI). Public value is a dynamic concept that encompasses several dimensions. AI itself has evolved quite rapidly in the last few years, especially with the swift escalation of Generative AI. Governments around the world are grappling with how to govern AI, just as technologists ring alarm bells about the future consequences of AI. Our paper extends the debate on AI governance that is focused on ethical values of beneficence to that of economic values of public good. Viewed as a public good, AI use is beyond the control of the creators. Towards this end, the paper examined AI policies in the United States and Europe. We postulate three principles from a public values perspective: (i) ensuring security and privacy of each individual (or entity); (ii) ensuring trust in AI systems is verifiable; and (iii) ensuring fair and balanced AI protocols, wherein the underlying components of data and algorithms are contestable and open to public debate.},
}

@article{Shen2024,
  title = {Directions of Technical Innovation for Regulatable AI Systems},
  author = {Shen, Xudong and Brown, Hannah and Tao, Jiashu and Strobel, Martin and Tong, Yao and Narayan, Akshay and Soh, Harold and Doshi-Velez, Finale},
  year = {2024},
  journal = {Commun. ACM},
  volume = {67},
  pages = {82–89},
  doi = {10.1145/3653670},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3653670},
  abstract = {Public sector AI procurement checklists can help guide efforts to create regulatable AI systems.},
}

@article{Maratsi2024_01,
  title = {On the Semantic Analysis of Open (Government) Data Portals’ Metadata Provision and Schema},
  author = {Maratsi, Maria Ioanna and Alexopoulos, Charalampos and Charalabidis, Yannis},
  year = {2024},
  pages = {147–157},
  doi = {10.1145/3680127.3680130},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3680127.3680130},
  abstract = {The ever-increasing amount of data available through Open Data portals follows the momentum and proliferation of Open Data initiatives and, apart from the numerous benefits offered, poses a reality which brings forth several challenges. Metadata quality and management are key aspects in this regard; in their absence affecting the interoperability, findability, and consequently, open data consumption. The present study aims to analyse several semantic aspects of 3 major international data portals and their metadata schemas mapped to the main DCAT schema classes in order to assess the current status regarding metadata provision and aiming towards prospective standardisation and improved semantic interoperability of the datasets available. The analysis revealed several persisting challenges, including the lack of standardised information (e.g., controlled vocabularies) as accepted values of the identified mandatory metadata fields in use, or, in the case of existing standardisation schemas, a misalignment and lack of consensus among the different portals. The study also pinpoints future research lines centred around the elicitation of guidelines and practices to standardise not only descriptive but also domain-specific metadata provision across the portals, facilitating the process towards the identification of minimum sets of metadata descriptions applicable to various contexts, and reaching one step closer to the envisioned interoperable ODPs paradigm.},
}

@article{Oppenlaender2023,
  title = {Perceptions and Realities of Text-to-Image Generation},
  author = {Oppenlaender, Jonas and Silvennoinen, Johanna and Paananen, Ville and Visuri, Aku},
  year = {2023},
  pages = {279–288},
  doi = {10.1145/3616961.3616978},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3616961.3616978},
  abstract = {Generative artificial intelligence (AI) is a widely popular technology that will have a profound impact on society and individuals. Less than a decade ago, it was thought that creative work would be among the last to be automated&nbsp;– yet today, we see AI encroaching on many creative domains. In this paper, we present the findings of a survey study on people’s perceptions of text-to-image generation. We touch on participants’ technical understanding of the emerging technology, their fears and concerns, and thoughts about risks and dangers of text-to-image generation to the individual and society. We find that while participants were aware of the risks and dangers associated with the technology, only few participants considered the technology to be a personal risk. The risks for others were more easy to recognize for participants. Artists were particularly seen at risk. Interestingly, participants who had tried the technology rated its future importance lower than those who had not tried it. This result shows that many people are still oblivious of the potential personal risks of generative artificial intelligence and the impending societal changes associated with this technology.},
}

@article{van Meerten2025,
  title = {The Impact of Digital Twin Technology on the Policy Lifecycle in Government Agencies and the Role of Value-Sensitive Design in Guiding its Ethical Development},
  author = {van Meerten, John and Smit, Koen},
  year = {2025},
  pages = {37–44},
  doi = {10.1145/3715885.3715891},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3715885.3715891},
  abstract = {The rise of Digital Twin Technology (DTT) in spatial planning practices could transform how governmental agencies develop and manage policies. This paper conceptually explores the influence of DTT on the policy lifecycle, examining how it enhances decision-making, governance efficiency, and service delivery, but doing so in a Value-Sensitive manner using Value-Sensitive Design (VSD) theory. VSD can be employed to ensure the ethical development and implementation of DTT, which is important as spatial planning affects many human values of individuals over a prolonged period. Drawing the state-of-the-art on the central topics of this work, this research provides a comprehensive understanding of DTT's implications for governance and the potential of VSD to shape its ethical design and implementation.},
}

@article{O'Leary Jr2024,
  title = {USING LARGE LANGUAGE MODELS FOR ARMCHAIR AUDITORS},
  author = {O'Leary Jr, Daniel E.},
  year = {2024},
  journal = {Digit. Gov.: Res. Pract.},
  doi = {10.1145/3676280},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3676280},
  abstract = {Armchair auditors are citizens who use open data to investigate and monitor government activities, typically using analytics and other approaches. Armchair auditors provide a valuable role in holding governments and organizations accountable. This paper investigates the potential use of large language models (LLM) to support armchair auditor analyzes of different governmental entities. Unfortunately, the literature, prior to the development of LLM suggested several challenges for armchair auditors. However, the analysis in this paper suggests that LLM can provide substantial data and analytic process support for armchair auditors mitigating issues such as, providing guidelines for analyses, guiding users to appropriate communities, suggesting potential data availability opportunities, doing analysis and other issues. As part of an approach to unifying armchair auditor searches, this paper also suggests a prompt library designed to support, standardize and promote best practice analyzes among armchair auditors. In addition to these issues, this paper also analyzes emerging ethical issues associated with armchair auditors and their use of open data and LLMs. Finally, this paper extends the activity theory model to account for LLMs.},
}

@article{Duncan2024,
  title = {Unmasking Bias in Chat GPT Responses},
  author = {Duncan, Clay and Mcculloh, Ian},
  year = {2024},
  pages = {687–691},
  doi = {10.1145/3625007.3627484},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3625007.3627484},
  abstract = {Generative artificial intelligence (AI) has gained a great deal of recent attention with the release of Chat GPT 4. It has been praised for its ability to generate human-like responses but has perhaps faced even more criticism over potential concerns for biased responses, misinformation, and generation of harmful or inappropriate content. Chat GPT utilizes large sources of data to curate responses to all kinds of questions. The generative AI models are designed to be objective and avoid any sort of bias in their output. However, in the age of misinformation, social media, user-generated content and the 24-hour news cycle, biased information has never been more plentiful. This paper investigates the possibility of biased responses produced by Chat GPT 4 utilizing public data from biased media sources through Support Vector Machines. We find Chat GPT tends to have bias in its responses.},
}

@article{Haverinen2024,
  title = {Automating Cybersecurity Compliance in DevSecOps with Open Information Model for Security as Code},
  author = {Haverinen, Henry and Janhunen, Tomi and P\"{a}iv\"{a}rinta, Tero and Lempinen, Sami and Kaartinen, Suvi and Meril\"{a}, Sami},
  year = {2024},
  pages = {93–102},
  doi = {10.1145/3685651.3686700},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3685651.3686700},
  abstract = {Software development teams meet increasing requirements to implement cybersecurity management in compliance with standards and regulations. However, adopting a compliant cybersecurity management system and DevSecOps practices as part of a software development process has turned out to be tedious and expensive in practice. Open-source communities and open ecosystems, which lack tools and realistic practices for compliant cybersecurity management, face these difficulties as well. This paper suggests a set of requirements and a solution that are based on long-term experience in adopting standard compliant DevSecOps processes in industry. The proposed solution, called Cyberismo, facilitates the adoption of compliance and cybersecurity management, improves collaboration on cybersecurity in company internal projects, cross-company projects, and open-source projects, and automates the compliance and cybersecurity management in software development by way of an open information model representation format, and an open-source tool to manage the information model. As the information model uses a simple plain text format that can be managed by automated DevSecOps tool chains, it can be understood as an instance of the Everything as Code and Security as Code paradigms. The proposed solution is designed as modular, tailorable to the organisation and its existing tools, and flexible enough to model both process- and technology-related information. It automates both the validation of how compliance requirements have been met and the gathering and archiving of evidence of compliance. The information model is mapped to a logic program conforming to the Answer Set Programming (ASP) paradigm for knowledge representation. The mapping enables flexible query evaluation and reasoning, including the calculation of performance measures and automated policy checks. However, developers, product owners and other end-users of the solution do not necessarily need to know how to write logic programs, as logic programs can be encapsulated in content modules made available for the users. By putting the ease of adoption of compliant DevSecOps processes by the practitioners in the spotlight, this paper concludes that it is both necessary and possible to meet all the proposed requirements.},
}

@article{Ter\'{a}n2024,
  title = {Introduction to the Special Issue on Smart Government Development and Applications},
  author = {Ter\'{a}n, Luis and Vaca, Carmen and Riofrio, Daniel and St\"{u}rmer, Matthias},
  year = {2024},
  journal = {Digit. Gov.: Res. Pract.},
  volume = {5},
  doi = {10.1145/3691353},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3691353},
  abstract = {An information and knowledge society creates value by gathering, processing, evaluating, and sharing digital products and services. Knowledge society relies on emerging technologies with intelligence and automation for information and communication processes between governmental institutions, private companies, and citizens to facilitate and support better planning and decision-making. Relying on advanced information and communication technologies demands new ways of smart government with data-driven, citizen-centric, and performance-focused governance. The main objectives of the Special Issue are as follows: first, to share the theoretical advances and practical cases of smart government applied in the academic and public sectors, as well as industry; second, to discuss the progress and challenges in innovative smart government service design, implementation, and delivery pipelines. The special issue features contributions from representatives of governments, international organizations, and researchers around the globe on technical and non-technical aspects of smart government, smart health, smart and cognitive cities, smart democracy, smart society, and digital ethics.},
}

@article{Chen2024_06,
  title = {Exploiting Duality in Open Information Extraction with Predicate Prompt},
  author = {Chen, Zhen and Liu, Jingping and Yang, Deqing and Xiao, Yanghua and Xu, Huimin and Wang, Zongyu and Xie, Rui and Xian, Yunsen},
  year = {2024},
  pages = {125–133},
  doi = {10.1145/3616855.3635799},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3616855.3635799},
  abstract = {Open information extraction (OpenIE) aims to extract the schema-free triplets in the form of (subject, predicate, object) from a given sentence. Compared with general information extraction (IE), OpenIE poses more challenges for the IE models, especially when multiple complicated triplets exist in a sentence. To extract these complicated triplets more effectively, in this paper we propose a novel generative OpenIE model, namely DualOIE, which achieves a dual task at the same time as extracting some triplets from the sentence, i.e., converting the triplets into the sentence. Such dual task encourages the model to correctly recognize the structure of the given sentence and thus is helpful to extract all potential triplets from the sentence. Specifically, DualOIE extracts the triplets in two steps: 1) first extracting a sequence of all potential predicates, 2) then using the predicate sequence as a prompt to induce the generation of triplets. Our experiments on two benchmarks and our dataset constructed from Meituan demonstrate that DualOIE achieves the best performance among the state-of-the-art baselines. Furthermore, the online A/B test on Meituan platform shows that 0.93% improvement of QV-CTR and 0.56% improvement of UV-CTR have been obtained when the triplets extracted by DualOIE were leveraged in Meituan's search system.},
}

@article{Luckett2023,
  title = {Regulating Generative AI: A Pathway to Ethical and Responsible Implementation},
  author = {Luckett, Jonathan},
  year = {2023},
  journal = {J. Comput. Sci. Coll.},
  volume = {39},
  pages = {47–65},
  publisher = {Consortium for Computing Sciences in Colleges},
  abstract = {Artificial intelligence (AI) is becoming more and more prevalent in our daily lives, and its potential applications are practically limitless. However, as with any technology, there are concerns about how AI could be misused or abused. One of the most serious concerns is the potential for discrimination, particularly against women or minorities, when AI systems are used for tasks like job hiring. Additionally, there are concerns about privacy and security, as AI could be used to monitor people's movements or launch cyberattacks. To address these concerns, regulations must be developed to ensure that AI is developed and used ethically and responsibly. These regulations should address issues like safety, privacy, security, and discrimination. Finally, it is important to educate the public about AI and how to use it safely and responsibly. In this paper, I will examine the AI regulations and challenges that exist today, particularly in the United States. Two regulations I will focus on are the AI in Government Act of 2020 and the National Artificial Intelligence Initiative Act of 2020. Additionally, I will examine two Executive Orders that have addressed the issue of AI in the federal government. This paper examines AI generative tools, such as Bing, Bard, and Chat-GPT. Finally, the paper concludes with some policy considerations and recommendations for federal agencies.},
}

@article{Kanza2024,
  title = {A Geospatial Perspective on Data Ownership, the Right to be Forgotten, Copyrights, and Plagiarism in Generative AI},
  author = {Kanza, Yaron and Krishnamurthy, Balachander and Srivastava, Divesh},
  year = {2024},
  pages = {477–480},
  doi = {10.1145/3678717.3691269},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3678717.3691269},
  abstract = {Ethical use of data in generative AI is a growing concern. Large generative AI models are trained on pervasive data sets from a variety of sources, where the data records are fused into the models and become inseparable from them. This raises questions regarding data ownership and the use of personal data, artwork, and copyrighted content in generative AI models. Should people and businesses be allowed to request the removal of their data from a model, even if the data were collected in public places? Should the use of data in generative AI vary across different places based on local copyright laws? How should local laws and regulations regarding data misuse and harmful content be enforced? Can people and organizations verify that their data records have been removed from models or are being used properly? In this paper we discuss the geospatial aspects of data ownership in large generative AI models. We present a vision of generative AI applications that are aware of data ownership and location provenance, based on spatio-temporal features of the data and the usage. These aspects of location-aware AI governance could mitigate some of the risks associated with generative AI and support ethical use of it, in both local and global applications.},
}

@article{Luna2025,
  title = {Navigating Governance Paradigms: A Cross-Regional Comparative Study of Generative AI Governance Processes &amp; Principles},
  author = {Luna, Jose and Tan, Ivan and Xie, Xiaofei and Jiang, Lingxiao},
  year = {2025},
  pages = {917–931},
  publisher = {AAAI Press},
  abstract = {As Generative Artificial Intelligence (GenAI) technologies evolve at an unprecedented rate, global governance approaches struggle to keep pace with the technology, highlighting a critical issue in the governance adaptation of significant challenges. Depicting the nuances of nascent and diverse governance approaches based on risks, rules, outcomes, principles, or a mix, across different regions around the globe, is fundamental to discern discrepancies and convergences, and to shed light on specific limitations that need to be addressed, thereby facilitating the safe and trustworthy adoption of GenAI. In response to the need and the evolving nature of GenAI, this paper seeks to provide a collective view of different governance approaches around the world. Our research introduces a Harmonized GenAI Framework, "H-GenAIGF", based on the current governance approaches of six regions: (European Union (EU), United States (US), China (CN), Canada (CA), United Kingdom (UK), and Singapore (SG)). We have identified four constituents, fifteen processes, twenty-five sub-processes, and nine principles that aid the governance of GenAI, thus providing a comprehensive perspective on the current state of GenAI governance. In addition, we present a comparative analysis to facilitate identification of common ground and distinctions based on coverage of the processes by each region. The results show that risk-based approaches allow for better coverage of the processes, followed by mixed approaches. Other approaches lag behind, covering less than 50% of the processes. Most prominently, the analysis demonstrates that amongst the regions, only one process aligns across all approaches, highlighting the lack of consistent and executable provisions. Moreover, our case study on ChatGPT reveals process coverage deficiency, showing that harmonization of approaches is necessary to find alignment for GenAI governance.},
}

@article{Barde2024,
  title = {Applications of Generative AI in Fintech},
  author = {Barde, Kalpesh and Kulkarni, Parth Atul},
  year = {2024},
  doi = {10.1145/3639856.3639893},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3639856.3639893},
  abstract = {While Generative Artificial Intelligence (GenAI) has made significant strides in various sectors, there’s a discernible gap in literature regarding its specific applications and implications within the Financial Technology (FinTech) landscape. This paper endeavors to address this void, evaluating the potential of FinTech to harness GenAI’s capabilities and the existing discussions on GenAI’s impact in the domain. Through a comprehensive analysis of its integration by leading financial institutions such as Bloomberg, Goldman Sachs, Wells Fargo, and Capital One, the study offers insights into GenAI’s transformative potential in FinTech. We discuss the main problems these companies are solving, from identification and mitigation of fraudulent activities and ensuring compliance with regulatory frameworks to improving customer service and operations and making better data-driven decisions. By addressing both the opportunities and challenges, this paper aims to provide a holistic perspective on the role of GenAI in the evolving FinTech sector.},
}

@article{Wu2024_06,
  title = {Reacting to Generative AI: Insights from Student and Faculty Discussions on Reddit},
  author = {Wu, Chuhao and Wang, Xinyu and Carroll, John and Rajtmajer, Sarah},
  year = {2024},
  pages = {103–113},
  doi = {10.1145/3614419.3644014},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3614419.3644014},
  abstract = {Generative Artificial intelligence (GenAI) such as ChatGPT has elicited strong reactions from almost all stakeholders across the education system. Education-oriented and academic social media communities provide an important venue for these stakeholders to share experiences and exchange ideas about GenAI, which is constructive for developing human-centered policies. This study examines early user reactions to GenAI, consisting of 725 Reddit threads between 06/2022 and 05/2023. Through natural language processing (NLP) and content analysis, we observe an increasingly negative sentiment in the discussion and identify six main categories of student and faculty experiences of GenAI in education. These experiences reflect concerns about academic integrity and AI’s negative impact on the values of traditional education. Our analysis also highlights the tension and burden imposed by new technologies. Our findings suggest that dialogue between stakeholders in the education community is critical and can mitigate sources of tension between students and faculty.},
}

@article{Wei2024,
  title = {Exploring the Use of Abusive Generative AI Models on Civitai},
  author = {Wei, Yiluo and Zhu, Yiming and Hui, Pan and Tyson, Gareth},
  year = {2024},
  pages = {6949–6958},
  doi = {10.1145/3664647.3681052},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3664647.3681052},
  abstract = {The rise of generative AI is transforming the landscape of digital imagery, and exerting a significant influence on online creative communities. This has led to the emergence of AI-Generated Content (AIGC) social platforms, such as Civitai. These distinctive social platforms allow users to build and share their own generative AI models, thereby enhancing the potential for more diverse artistic expression. They also provide artists with the means to showcase their creations (generated from the models), engage in discussions, and obtain feedback, thus nurturing a sense of community. Yet, this openness also raises concerns about the abuse of such platforms, e.g., using models to disseminate deceptive deepfakes or infringe upon copyrights. To explore this, we conduct the first comprehensive empirical study of an AIGC social platform, focusing on its use for generating abusive content. As an exemplar, we construct a comprehensive dataset covering Civitai, the largest available AIGC social platform. Based on this dataset of 87K models and 2M images, we explore the characteristics of content and discuss strategies for moderation to better govern these platforms.},
}

@article{Logacheva2024,
  title = {Evaluating Contextually Personalized Programming Exercises Created with Generative AI},
  author = {Logacheva, Evanfiya and Hellas, Arto and Prather, James and Sarsa, Sami and Leinonen, Juho},
  year = {2024},
  pages = {95–113},
  doi = {10.1145/3632620.3671103},
  publisher = {Association for Computing Machinery},
  url = {https://doi.org/10.1145/3632620.3671103},
  abstract = {Programming skills are typically developed through completing various hands-on exercises. Such programming problems can be contextualized to students’ interests and cultural backgrounds. Prior research in educational psychology has demonstrated that context personalization of exercises stimulates learners’ situational interests and positively affects their engagement. However, creating a varied and comprehensive set of programming exercises for students to practice on is a time-consuming and laborious task for computer science educators. Previous studies have shown that large language models can generate conceptually and contextually relevant programming exercises. Thus, they offer a possibility to automatically produce personalized programming problems to fit students’ interests and needs. This article reports on a user study conducted in an elective introductory programming course that included contextually personalized programming exercises created with GPT-4. The quality of the exercises was evaluated by both the students and the authors. Additionally, this work investigated student attitudes towards the created exercises and their engagement with the system. The results demonstrate that the quality of exercises generated with GPT-4 was generally high. What is more, the course participants found them engaging and useful. This suggests that AI-generated programming problems can be a worthwhile addition to introductory programming courses, as they provide students with a practically unlimited pool of practice material tailored to their personal interests and educational needs.},
}

